project,issue_number,issue_section,text,classification,indicator,related_commits
camel,149,description,Brian McCallister had this great idea.. add a thread() method to the DSL instead of using the seda component to do async processing. We should be able to: or ThreadPoolExecutor pool = ...,design_debt,non-optimal_design,0ea40fae717241f5322d95b60233f982ae810040
camel,201,comment_0,"I finally got around to doing this one up. There are no XQuery specific tests (mainly because of not knowing where to put them ;)), but it is generic enough to work for any expression language. Let me know if you have any questions!",test_debt,lack_of_tests,"e8173e734f7aa8c40b8caca80fcd4b9ad451eb68,ab60c93e1c3ffe738f8f0bf7023a3462a779f53a"
camel,201,comment_6,"IIRC one of my issues for putting a spring transform test in camel-saxon was that the public camel xsd did not contain the transform element *yet* and so it failed to resolve. The camel-spring module has some magic in there to use a locally built xsd, so the transform element could be tested there fine. Since putting an XQuery test in camel-spring is a bad idea (circular dependency!), I opted for a test case using the ""simple"" expression language. Make sense?",test_debt,low_coverage,"e8173e734f7aa8c40b8caca80fcd4b9ad451eb68,ab60c93e1c3ffe738f8f0bf7023a3462a779f53a"
camel,251,description,"displays the file instead of directory (a boolean) automagically attempt to reconnect if from their destinations. This should probably be shared with Consumers, and could maybe done more cleanly using camel's try/catch features, but there you go. Smarter directory/file building/handling. This lets you handle URI's like similarly, avoid putting files in '/' on servers that expose the full filesystem, etc. More verbose logging. Stuff like ""what file went where,"" reconnection attempts, etc.",code_debt,low_quality_code,35c8dd74eb0f96c9d242234936fa4dc4d9252104
camel,383,comment_1,A patch with the following changes: - copyright headers for missing files - removed an unused import - new feature to transfer exchange objects using TCP protocol see how to use it The new feature only works for the TCP protocol and for the default codec (= object). So do not use textline=true as option.,code_debt,low_quality_code,cd5d8b725502b48c16b7f3d4458161dd4702162b
camel,383,comment_2,I think this ticket should be changed from bug to new feature. Also anyone got a better name for the option (transferExchange)? I considered the option bodyOnly also but then I needed to make sure the defaults would be bodyOnly=true even when the parameter is not specified.,code_debt,low_quality_code,cd5d8b725502b48c16b7f3d4458161dd4702162b
camel,383,comment_4,"Hi Claus, I just committed your patch. Here is one issue for the since we do not mashal or unmashal the exchange's exchangeId, so I comment out the process assertion for the exchangeId to pass the test . BTW, the document for the transferExchange properties is highly demanded, and I like to use the transferExchange as the parameter name since it is more clear for the user :) Regards, Willem.",test_debt,lack_of_tests,cd5d8b725502b48c16b7f3d4458161dd4702162b
camel,612,description,"When you define a route with a choice() and no matching when() clause is found, the Exchange just ends successfully without doing anything. In my mind, it should fail by default in this case (or we should at least have an easy way to get this behavior).",design_debt,non-optimal_design,db1b885c8c2a3b52b575ecb39e655e4573bf5833
camel,612,comment_0,Gert I do think that the choice() should *always* have an otherwise() so there always is a match. If the otherwise() is missing on the choice() then Camel should thrown an exception. Maybe checked during the route creation stuff.,design_debt,non-optimal_design,db1b885c8c2a3b52b575ecb39e655e4573bf5833
camel,612,comment_2,-1 [edit: retracting my +1] In other words if there is no explicit otherwise() to go the errorHandler (no redelivery of course). Thinking more about this I think the behavior of choice should be the way it already is. If no clause matches and there is no otherwise it should silently succeed with a noop. Users are familiar with this from switch() statements and is i think a reasonable expectation. I think it's perfectly reasonable to add the otherwise (as one would add a default: ) to make it explicit that the exchange should fail if there is no match: What I would do though is adding a log (below INFO) in the if otherwise == null that would say that exchange is not processed.,code_debt,low_quality_code,db1b885c8c2a3b52b575ecb39e655e4573bf5833
camel,721,summary,Log component should use ExchangeFormatter for formatting log output,code_debt,low_quality_code,0d04b89b7c121aee71ebf672779da6d37a637d7c
camel,721,description,"Currently log component only logs the payload. We have a nice ExchangeFormatter that can format an exchange with all kind of options. The options could be enabled on the log component so you can customize your logging. Also there should be a *multiline* option to the exchange formatter so it can log all the stuff on multi lines if for instance there are many options, they get very long.",code_debt,low_quality_code,0d04b89b7c121aee71ebf672779da6d37a637d7c
camel,789,description,When downloading artifacts we have a many repos. Several of them are specific for a single/few components. We should move these repo settings to these targeted components and let the uber pom.xml have only the major public maven repos. Maybe some of the repos is out-of-date and not used anymore.,architecture_debt,violation_of_modularity,8c56ead7e728550c0f086e51070c7a5973cc28aa
camel,872,comment_0,"This is the third time I attempt this and the change set becomes pretty large, making it quite hard to deal with failures. I will make the fixes in a few stages and this will impact other developers with potential need for merge. It would be great if you could avoid commits for the next 24-48 hours or so (if it's not too much to ask).",code_debt,low_quality_code,"deef0d6d75e1a87a34623deb91a89d307f58818d,89fcd7500607e878f6e164a51bf130b98581ef15,879b3029e1178adb531db7633699af1201bdc86d,6e44825b18f58a6746fcc20772c9480a2b23a324,ebc56ec526bac8e5608dbd210f0c9f868b764467,da8ee4d6238458c08f5e8fb8f198d9a69fefc941,aaf819ef375814f37303f963163c66ae6f9b3172,5aa8465d47d7fabd4a2ef1e7d37714ac31f23850,e6e43fe6d7c7eb7703dbaccc12e1866d4176da66,8ba630242883185118fabd97063351aa5b6a27a6,b0af6b856c9f48441481f9391fa31673bf40ef1f"
camel,903,description,"The camel-ftp component uses stored as last poll time. The remote file timestamp is used for comparing against last poll time if its new and thus a candidate for download. As timestamps over FTP is not reliable we should not use this feature by default, but turn it off. It should only be there for test or experimental usage. Most FTP servers only sent file timestamp as HH:mm (no seconds). And as a bonus we avoid timezone issues as well. Instead end-users should use a different strategy for ""marking files done"" such as: - delete consumed files - rename consumed files See nabble:",design_debt,non-optimal_design,72b3b8243ee201705e295d638fbb077ba9f97e4f
camel,910,comment_0,#9 its not easily understood how the client can send using the template. Maybe divide the server and client into each section. The client just need much less code than the server. #10 webservice example. It is not clear that its the *true* parameter that turns the multicast into parallel mode. This is not documented to well in the code.,code_debt,low_quality_code,"872947062d9c16bfafb8160eb08858447909dc07,b478b1493f6da617e5ebe2c0a13711a160df6cde,61a98d09e51b31d0e38e5e3da46c6197c07b6066"
camel,990,comment_3,BTW - its not really a big deal to use annotations from any particular framework in your code as annotations are a soft dependency. e.g. adding Camel annotations to your code doesn't add any direct dependency on camel. You can then use your code totally fine without any camel jars on the classpath. So there's not a huge need to 'hide' dependencies on Camel annotations - as you only need them to compile your source code. But using indirect annotations can be useful; e.g. you can have a foo.jar which defines a @Foo annotation which builds itself with camel - then folks can just add foo.jar to their classpath to get camel goodness without adding camel jars to the build. So you could introduce a kinda macro annotation that includes various framework annotations on it (say spring and camel and guice annotations :),design_debt,non-optimal_design,"881332fe1a6eed6dabbcff7118f788a366821a81,258cd2d693ef41a08796e75638118787c7143223,22a1767fb1b5ca4d09b24b5365e15bc10b600216"
camel,990,comment_4,"The original need is the following: Let's say I build a Routing System based on Camel. Of course my System will depend on Camel. But I don't want the end user of my System (who will be responsible for adding routes, converters...) to depend on any camel jar. Your solution is better (and also more time consuming than my simple metaannotation patch :)), will think about it.",design_debt,non-optimal_design,"881332fe1a6eed6dabbcff7118f788a366821a81,258cd2d693ef41a08796e75638118787c7143223,22a1767fb1b5ca4d09b24b5365e15bc10b600216"
camel,1107,description,"When using the options to configure httpClient using URI option, they should be removed from the uri that is left over to the HTTPProducer. should remove the httpClient.xxx so it's",code_debt,dead_code,4bd210597d24567c52e2e397531c9037341becf3
camel,1112,description,"when reading files from file,ftp/sftp component, it is often neccesarry to get the files in a predifined order (e.g. created date or filename). to solve this, an extra uri parameter could be used, to set the order of files beeing processed. ordering can be done after reading the file-list. i know this can also be solved by applying a resequencer pattern in the route, but i think for large files it would be hard to apply this. also a processing in an created date order would be more natural.",design_debt,non-optimal_design,"e4dd67557bf9766a7bf7e42c128c5488a5c83627,8c7f06e80c665492a6fcad3e3d85abd51aa74cf3,4ef8b8962304f16061825f76a52c5cb6dcc6d29e,ff5c824476fbf30fd1394439e93a870f4b6e5411,4e611beeb16041bd39aa04ea295958576fbb0faa,e7158c7e9fcfff63426401c798685df266d16f00,0891b2303a88ec8fab2be7af0a0f13b49e803f22"
camel,1173,comment_1,Put in an improved scatter-gather example in revision 740056. The problem with the first example was that it was not dynamic enough. Also added wiki docs here,code_debt,low_quality_code,"1ee86d6e60cb0e91b881919f7d3a267eba16282c,f6cbe6529a1084383b72c06ab215a28dee3ebed7,7afe11b51a14e9b8723dddefb73b8038b220feb7"
camel,1196,summary,MockEndpoint - sleep for empty test doesnt work,test_debt,lack_of_tests,6bebc319bfb7375d78b50974814f1bb73f85756a
camel,1228,summary,Clean up the OSGI bundle profiles,code_debt,low_quality_code,235092f3a890d4bf573c6c9c71b69a2f67805b12
camel,1256,summary,Clean up camel-cxf,code_debt,dead_code,"9be406692d2b2a92d547dafbb93452c02a06bf4e,1da750c3d50bfc5dbed8bdcaf491eeeb843de465,89b323f8a57bfaa2f47849d4d987c72445ae5b5b"
camel,1256,description,"The camel-cxf component is dued for some code cleanup and refactoring. We can clean some of of the interfaces and redundant code, etc.",code_debt,dead_code,"9be406692d2b2a92d547dafbb93452c02a06bf4e,1da750c3d50bfc5dbed8bdcaf491eeeb843de465,89b323f8a57bfaa2f47849d4d987c72445ae5b5b"
camel,1256,comment_0,"A fix has been submitted. Here is some highlights. * The fix is for 2.0 only since it involves some APIs changes that is an overkill to keep it backward compatible. Will update wiki. * It depends on CXF 2.2-SNAPSHOT now. Will upgrade CXF once the next release of CXF is available. * CxfBinding, Bus [CAMEL-1239], can be looked up from registry by the ""#"" notation. * Decouple CXF Message from Camel Message. That is, users are no longer required to cast Camel's message body to CXF Message in order to access SOAP headers and body in PAYLOAD. In PAYLOAD mode, Camel message body now returns a new type CxfPayload which contains SOAP headers and body. With CxfPayload being the body in PAYLOAD mode, It makes using Camel converter to convert to other types pretty easy and transparent. * Cleaning up the old CxfBinding and ""invoking context"". Both tried to do Camel/CXF message binding. CxfBinding is now an interface that an custom impl can be set on each endpoint. impl can also be set on each endpoint. * [CAMEL-1254] support serviceClass=#bean * some major refactoring to make code cleaner.",code_debt,low_quality_code,"9be406692d2b2a92d547dafbb93452c02a06bf4e,1da750c3d50bfc5dbed8bdcaf491eeeb843de465,89b323f8a57bfaa2f47849d4d987c72445ae5b5b"
camel,1270,description,The countdown latch in MainSupport is not completed when Main is stopping. Then we have a hanging thread. Can bee seen using ctrl + \,code_debt,multi-thread_correctness,2ebe070905933331c175d51836b0334fe3a28013
camel,1320,comment_1,"It is very similar to zip data format - it even uses the same deflate algorithm. The point is, that zip data format uses deflate algorithm directly, while gzip would use gzip file format, that is (I believe more popular). The problem with zip data format is that is is not a zip really - zip is a format that compresses set of files, while our zip data format doesn't - it just compresses data using compression algorithm of zip. On the other hand gzip is a format that compresses data (not files), uses the same algorithm, but has its own headers, that has to be interpreted. I even believe that our zip data format should be renamed to 'deflate' data format as it is the name that describes what it really is.",design_debt,non-optimal_design,"527370f8c142d373c4543d0eb26cb2f0912ed9f0,af0e8d19e1e4d4f40d6f323bf3d315c30138d335"
camel,1447,description,"See article, listening 2 We should consider renaming *handle* to *catchBlock* so its using the same name as try .. catch. And we could add a finallyBlock so end user got them all.",code_debt,low_quality_code,"bc5251bbf9f1af4c9d42524ee78d6b547b9a9264,5a301062369e28c6288c1ab3ffeb045d9c665c84,c51a64a641fe559c675849ada49a0ed31155f3a8,86e1a1c0ad9c97a3ab734739b6178eb4ea14c887"
camel,1507,description,"To send a email you have to follow a pretty specific course. This adds a property (which is poorly named in this patch) to the MailConfiguration that names the header that contains the plaintext version of the email, and adds a property where you can embed images inline. If an attachment has a filename starting with ""cid:"" then this will add the ""Content-ID"" header to that multipart body - which will allow the email client to put the image in the appropriate place when it is viewed. (i.e. the html email has something like <image src=""cid:0001"" /",code_debt,low_quality_code,"0eec0b41e2005de2adf422785c3a5b86e5a8f4a1,6cb542d49d20c5fe62e5aed80787feef682821fa,997ea7a82318563ba18c4fb948f7f6ee0057fe41,8abeb70f5d1b23a8f814e96b4c659025c3b2ffeb,da37ef162241e8dd6ad9b08bb08b3e30f0cbc316"
camel,1507,comment_0,might need some cleanup - but I have tested it and it works for me,code_debt,low_quality_code,"0eec0b41e2005de2adf422785c3a5b86e5a8f4a1,6cb542d49d20c5fe62e5aed80787feef682821fa,997ea7a82318563ba18c4fb948f7f6ee0057fe41,8abeb70f5d1b23a8f814e96b4c659025c3b2ffeb,da37ef162241e8dd6ad9b08bb08b3e30f0cbc316"
camel,1507,comment_4,I renamed the constant so its according to the Camel 2.0 syntax: rev 761182 on trunk,code_debt,low_quality_code,"0eec0b41e2005de2adf422785c3a5b86e5a8f4a1,6cb542d49d20c5fe62e5aed80787feef682821fa,997ea7a82318563ba18c4fb948f7f6ee0057fe41,8abeb70f5d1b23a8f814e96b4c659025c3b2ffeb,da37ef162241e8dd6ad9b08bb08b3e30f0cbc316"
camel,1641,comment_0,trunk: 778354. When/if CAMEL-1644 is implemented then this bug is already fixed. However remember to add the unit test in java. There is a TODO.,test_debt,lack_of_tests,"2ff122ae7255e67483691a4986741fb233d91a2f,33b21fe975a10ff0fa98a57037e53fd94c0b10fb"
camel,1842,description,"camel-cxf and camel-mail are the popular components in camel, so we need add some OSGi integration test for them.",test_debt,lack_of_tests,7b7ab09e22500c6140fafbf59e46c5d3843571c7
camel,1863,comment_1,trunk: 798902. TODO: HL7 segment that fails validation and add it to the validate unit test,test_debt,low_coverage,"b6baef0eabfd72119994b78106237e68de1ed249,d6bc88de1d057403a1dd343e21f3e12ac236175e"
camel,1881,comment_1,"@Stan, maybe you want to add a unit test? Many thanks for the patch!",test_debt,lack_of_tests,821baa57e521f7a0011f99b3bb09a13f5747298e
camel,1944,description,Currently only the camel-irc producer installs an event listener on the IRC connection to log incoming messages from the IRC server. This patch revamps the logging so that the component installs an event adapter to do logging before the connection is even established so it's easier to debug various issues that could be occurring with the connection to the IRC server.,code_debt,low_quality_code,7a3f1561f25770f35ead4c24d7896ab877398a01
camel,2094,description,We now have option {{autoStartup}} on routes. Lets also have this on the camel context itself so it replaces the option that is a bit confusing. For example the code above will not start Camel on startup. You can then later start camel manually by And then all runs. This also means that we will remove the option so if you use this option you must migrate to the autoStartup instead.,design_debt,non-optimal_design,ea55e23638226420322851307a06cce597a65c44
camel,2163,comment_3,"Stan, yeah I can see the pain now. The original code is a producer which doesnt easily allow to spawn exchanges",design_debt,non-optimal_design,02d95d4650196228ef63acffc033e99c924b7dd0
camel,2535,summary,Get ride of the cxfsoap component,code_debt,dead_code,"0d654ef76d63f65a81f465f2c5d02cc32e6a0b48,8a7a55b75361c2f2fbbdafa8609fd229884e613f"
camel,2535,description,"As we don't use the CxfSoap component any more, it's time to clean it up.",architecture_debt,using_obsolete_technology,"0d654ef76d63f65a81f465f2c5d02cc32e6a0b48,8a7a55b75361c2f2fbbdafa8609fd229884e613f"
camel,2559,comment_3,"I think I got it covered now, will do more testing. But I have reworked camel-http and camel-jetty a little big to - use endpoint configured options over component configured - not mess with the component configured - using a single shared as that is whats meant - properly registering http/https with correct port number on SchemeRegistry on",test_debt,low_coverage,"755e4647c3bdc08cf7f1d6d411cf69c2839b953f,9041239036fd53a40ff6708e982882a77182e5b6"
camel,2650,description,"This only applies for prototype scoped POJOs which uses @Produce or @Consume to inject a Camel Producer/Consumer. As the prototype scoped POJO is short lived and there could potentially be created many of those POJOs. Then @Produce / @Consume will inject new instances of Producer / Consumer as well. And since there is no standard way of knowing when the POJO is no longer in need, which is where we can to stop the Producer/Consumer. For singleton scoped this is not a problem as CamelContext will keep track on the created Producer/Consumer in its internal _servicesToClose_. Which is then stopped when CamelContext stops. For prototype we need a different strategy such as - proxy it to use pooled producers/consumers which CamelContext manage the lifecycle - use a shared ProducerTemplate / ConsumerTemplate instead which CamelContext manages the lifecycle - other - maybe some thread local tricks",design_debt,non-optimal_design,"1ab7b8904e240361c4b8aa72427e9a606ecf9aa4,bbdfc68642cc63aa8f379c318fe82d8f5aeef5cb"
camel,2670,comment_1,"@ Charles I just checked out the example, I think you can put all the camel routes configuration files into a single module and use Profile to start the services one by one.",architecture_debt,violation_of_modularity,"ad16da89937bcaf254b5751e0ad3f37a0d5d3ad8,8cb47fa2734ae1f51cfb56158d13b98fa870a8d2"
camel,2682,description,This allows end users to continue routing the original message. Instead of now the just get the last splitted message which is confusing.,design_debt,non-optimal_design,"4b00f97f5d1ae88f3224037675bfa91033e6c453,e855544f18e2a23d13086712f9517612ac323d0d"
camel,2756,comment_0,Willem I think the xxxConverter name is misleading as you would think its a Camel TypeConverter. I wonder if there is a better naming for this? or the likes.,code_debt,low_quality_code,"a9ba53e80da66b422f05fda5dbb527b17deaab39,dd1cf5253db0e921fb086182e3cd9b18f494b6ad,4d73162f027af5549a780960cee48f518f48db1c"
camel,2879,description,"The file strategies was not designed to be as flexible as we may want them today, when introducing more and more options to dictate what should happens with the files when success or failure. As we got - noop - delete - move - moveFailed etc. you may combine then in ways in which we current doesn't support. You should be able to say that if OK then just delete the file, but if ERROR then move it to this error folder instead.",design_debt,non-optimal_design,5799e6d0b2bfcdb3fcafc87ce6a6849804fdefd1
camel,2892,summary,remove ugly warnings running tests since upgrading to jetty 7,code_debt,dead_code,6d8acc781a940470cf21cd196bdf995111f873d1
camel,2979,comment_3,"Hi I've tested this and it works. When logging in with incorrect user/pass it only polls once and skips, which is nice. However could it be possible for the component to throw an exception as I would like to catch this exception and make sure that no further polls are done to the ftp (ie shut down the route/application).",design_debt,non-optimal_design,"b36c9ef8799afed4cda45bc09312ab5b86e3a593,a70187fdac9058f5483d98bdc16b62538ab9d194"
camel,3048,summary,Archetype for creating component should have better sample component,design_debt,non-optimal_design,df7ee396ac7c4f9beb917652a0c4e653a97e589b
camel,3048,description,The current sample is based on Direct component which is a bad sample as its a very special component. We should do a HelloWorldComponent - the producer will just print the message body to system out - the consumer is scheduled and triggers every 5th second with a dummy exchange,design_debt,non-optimal_design,df7ee396ac7c4f9beb917652a0c4e653a97e589b
camel,3050,description,"Spring 3 changed internally how dependency resolution works. Its now worse as we have to play tricks in the Camel namespace handler to tell Camel the various pieces you can ref, should depend on Camel. Otherwise the dependency resolution in Spring is not working properly. This used to work like a charm in Spring 2.5.6.",design_debt,non-optimal_design,85340945239689f51f25d29bbb79727d728b2076
camel,3077,summary,Cache Component needs to check for null values during GET operations,design_debt,non-optimal_design,1fab0408c5493502fcba171b11b66954d846e092
camel,3077,description,"EhCache often has a bit of lag time when invalidating expired cache elements, first setting the Element value to null and then removing the key. If you are hitting a cache rapidly one often will run across a key that is present in the cache that still has a null element entry. The logic for successfully key retrieval just needs to be slightly tweaked to check for null values.",design_debt,non-optimal_design,1fab0408c5493502fcba171b11b66954d846e092
camel,3100,summary,${file:length} should return 0 instead of null if the file length is 0,code_debt,low_quality_code,"559993248cdaa590466e708652b1a5d4bcdf17b0,8ee585df74e4b1bfacca3df90cc99e47b91e2c64,75b0290c78e33ae2ca3b0b38f01f5c0df17ba092"
camel,3112,description,To make it easier to read/write files using that given encoding. Currently you have to set that as a property on the Exchange using setProperty in the DSL. But when using @Produce or @Consume you only got the endpoint uri. So having this in the component makes it easier.,design_debt,non-optimal_design,"be1a55e6e82f3e615fe1c6337a72ecb9f52c1682,e11a4a3749e23953e00ccfcdfdd05c36ac610130,a33da51055cf141780e714a74e7bad79ce9a5382"
camel,3125,summary,"When the fetching of a feed causes, polling continues endlessly, flooding the logs and creating unwanted network load.",design_debt,non-optimal_design,1fad136fe822819607c0de79df61b3c9ace4f8a5
camel,3125,description,"The problem is with the default implementation of When an error occurs rollback() is called, and this method just logs the error and returns false, which means the polling will not be retried for that execution of the run() method. This means it will be retried after the delay. I created an implementation that suspends the Consumer, which seems like acceptable default behavior. It also allows for extension and adding some hooks for custom stuff. In order for that to be useful it should be made easier to set your own implementation.",design_debt,non-optimal_design,1fad136fe822819607c0de79df61b3c9ace4f8a5
camel,3139,summary,Use as the default uuid generator as its faster than the JDK UUID generator,design_debt,non-optimal_design,ae86bdf88cb4d25a622a8f25253aa80e1e9805da
camel,3139,description,The Java UUID is too slow. Switching back to AMQ based will improve performance. We should just have to switch to use the JDK UUID for the camel-gae component as it cannot use the AMQ based.,design_debt,non-optimal_design,ae86bdf88cb4d25a622a8f25253aa80e1e9805da
camel,3287,summary,remove http feature from Camel features as its duplicated in Karaf,code_debt,duplicated_code,a775051d8c639b24d8cc1f6365593e1197ce4924
camel,3315,comment_2,Tracy I think the SMX bundle is *wrong* as Jasypt 1.7 got rid of commons-lang and commons-codec (the latter has bugs). So we need an updated SMX bundle which *do not* important/use any of those commons stuff.,architecture_debt,using_obsolete_technology,9367683f82685d1569d53d3986d85bf94d3b29bc
camel,3315,comment_5,I would like to upgrade to Jasypt 1.7 as we can lose the JDK1.5 stuff and other dependencies which is no longer needed.,architecture_debt,using_obsolete_technology,9367683f82685d1569d53d3986d85bf94d3b29bc
camel,3349,summary,Race condition found in CxfRsEndpoint while getting the endpoint binding under load and performing sync and async invocation,code_debt,multi-thread_correctness,52776db21906cc1d1e23aff3917cf9c65a16c54e
camel,3349,comment_0,"Made the getBinding() method synchronized to overcome this issue. The penalty for this is very minimal since, the need for creating a binding is only on the first set of invocations. The binding is then held until the endpoint is in operation.",code_debt,multi-thread_correctness,52776db21906cc1d1e23aff3917cf9c65a16c54e
camel,3349,comment_2,Nice catch. I wonder if the initialization of the binding can be done in doStart. This is much better as it would avoid the synchronized block on the getter. Which I assume is invoked lazy at runtime on processing a new Exchange. Generally initialization should be done in doStart because starting it is single threaded and we don't care _so much_ about performance at startup.,code_debt,multi-thread_correctness,52776db21906cc1d1e23aff3917cf9c65a16c54e
camel,3351,comment_1,All irc events are just logged (I missed it in the logs). In this case the collision is logged as well as the server initiated disconnect. However the component code is unaware of the state. In general there are a lot of useful things that could be done with the various IRC events. Specifically though the code needs to be aware of disconnects.,design_debt,non-optimal_design,8f192e57dcc08277193d07500f4414cf8c17e3f9
camel,3351,comment_2,I got carried away and made several changes and fixed a minor bug. Here are my notes: IrcComponent: - Removed IrcConfiguration member variable. Didn't make sense. Removed constructor with IrcConfiguration as the param. - ircLogger moved to method. IrcProducer: - Changed doStart to call instead of doing it in the start method. Ditto for IrcConsumer removing dupe code. - Check to see if we're still connected before sending a message in process. If disconnected throw a - Removed unused imports - Add a listener so we can get error messages - Change listener type from to IRCEventAdapter and added getter/setter for easier testing - doStop didn't remove the listener fixed. Added. This would've caused an NPE if a user was stopping individual routes. IrcConsumer: - Changed doStart to call instead of doing it in the start method. - Removed unused imports - Added check in onKick to see if we got kicked and rejoin if so - Change listener type from to IRCEventAdapter and added getter/setter for easier testing IrcEndpoint: - Extracted method getExchange. Same 2 lines of code in 9 methods. Slightly cleaner this way. - Added handleIrcError to handle any IRC errors that the producer or consumer hit - Added handleNickInUse to handle nick in use errors. On endpoint startup this would cause a failed connection. For the Consumer this would just mean we'd never consume anything. For the Producer sends would throw an NPE (now there's a check for a valid connection and is thrown instead) - Added method joinChannels - Added method joinChannel IrcConfiguration - Add autoRejoin setting - Changed key storage to a Dictionary. Added several new tests using mockito,code_debt,duplicated_code,8f192e57dcc08277193d07500f4414cf8c17e3f9
camel,3524,description,"By default Jetty uses 30 sec timeout for continuations. We should allow end users to configure a value of choice, in case their system may take longer time to process an exchange.",design_debt,non-optimal_design,522c5fa937a63e45104bc700c0fe878ede2f0501
camel,3576,description,When using camel-jms the consumers dont provide a default task executor. That means spring just creates a thread manually and dont reuse the thread. We should provided a task executor from camel using the This allows us to use human readable thread names which can provide details about the consumer. The thread pool is also managed and even a 3rd party provider can manage thread creation. This only works when using Spring 3 as the task executor API is now aligned with the executor service api from the JDK. For Spring 2.x users we should not do this. We will then have to detect the spring version in use.,design_debt,non-optimal_design,"a19f79f23ee423c71477f47a3e20829927e72e13,abc2315ff7c69d15a65c835d46b12e79ff9f8a8f"
camel,3576,comment_1,"trunk: 1067682. Don't use thread pool for single threaded reply manager as Spring DMLC is a bit pants as it will keep using new tasks every second when idle, and that just confuses people, as task count will grow very large.",design_debt,non-optimal_design,"a19f79f23ee423c71477f47a3e20829927e72e13,abc2315ff7c69d15a65c835d46b12e79ff9f8a8f"
camel,3709,description,"When using together with from(Endpoint), the below Exception occurs during the routes building process. Looking at reveals, that the FromDefintion just created has no URI. That causes the comparison to all the interceptFroms' URIs to fail. As far as I can tell, the way to fix this would be to add in the constructor endpoint)}}. Below the stack trace, there is a unit test that demonstrates the issue. Until it if fixed, it can be easily circumvented by adding the commented-out line, and then change to",test_debt,low_coverage,"4c37e77322ae827f0e87de052c376eeb549f5ce6,36c26459b728d21ca3b3ef4c6727b82d0f6881be,c290dfc21e97b04173186a1ea362a83303a390bb"
camel,3764,description,to replace the use of the hacked and make it consistent across all components.,design_debt,non-optimal_design,6582b8a3268af4db9306fee7a78bbed7328d945c
camel,3769,comment_0,I suggest to change this section of the code to always use getInstance instead of getDefaultInstance.,code_debt,low_quality_code,f37b27a8e4eccd58ec782a5e8e24abf5cb9c9ed9
camel,3888,comment_1,"Dan as said before just because a method is deprecated do *not* mean we should not unit test it. And hence why you would see unit test that uses the deprecated methods. The handled(true|false) on try .. catch, error handler, was a mistake and there are *no* alternative on those. Instead you should use for example onException (exception clause). Or in case of a doCatch you can rethrow the exception if you want that.",code_debt,low_quality_code,12ebe280ed618b92a9c32999f8be745b3858aff4
camel,4007,description,"If attachment file names contain unicode characters, they do not appear correctly encoded in the message.",code_debt,low_quality_code,bf8e04425bf9e4eeccefb1a2cd200aa2f76a53db
camel,4132,description,The camel-atom component is using an ancient incubator version of abdera which will make it hard to work with camel-cxf. Updating to 1.1.2 which is what CXF uses would help.,architecture_debt,using_obsolete_technology,85645b06a14f07c3ce43990df9964dd01b6878cd
camel,4139,description,"Currently, spring parses into a CxfEndpointBean whereas blueprint parses into a CxfEndpoint directly. This mismatch then requires extra casting in the CxfComponent. Also, it means there are features supported by Spring that are not supported yet by blueprint (like configuring interceptors). I'm attaching a patch that removes the spring specific CxfEndpointBean stuff and allows both spring and blueprint to parse directly into the CxfEndpoint (well, subclasses of) so the two approaches can be maintained together. It also reduces the usage of ""Strings"" for properties in CxfEndpoint and uses QNames where appropriate (added a Converter for that) and Class<?",code_debt,complex_code,"c15e5239065be1ca3ed2d145b007d7eb053f5329,f40d6d9cdfd24acf51be6b1dea7e5435590d2ffb"
camel,4202,summary,camel-jms - Using request/reply over persistent queues should be faster,code_debt,slow_algorithm,"24d805b9c41be7bac1c3e40d47ceac8b46ee2f94,b785ab7666c28ad402efea4f7db73b5f2bcaaf32,ead75bffc392cf682af0b2e179d08177c8748c53"
camel,4202,description,"See nabble When using persistent replyTo queues for request/reply over JMS, then the ReplyManager should be faster to pickup replies. The default spring-jms timeout is 1 sec and it impacts the performance. Likewise the receiveTimeout should not be set on the reply managers as that does not apply here.",code_debt,slow_algorithm,"24d805b9c41be7bac1c3e40d47ceac8b46ee2f94,b785ab7666c28ad402efea4f7db73b5f2bcaaf32,ead75bffc392cf682af0b2e179d08177c8748c53"
camel,4202,comment_0,"Okay I think there is a tradeoff if we lower the spring receiveTimeout to lower than 1 sec. For example if we use 1 millis, then it will overload and send pull packets to the AMQ broker to receive messages. The reason why temporary queues is faster is because they dont have any JMSMessageListener as the persistent does. So with temporary queues it can pull every 1sec and pickup messages as fast as possible, as the received message is always for the reply manager.",code_debt,slow_algorithm,"24d805b9c41be7bac1c3e40d47ceac8b46ee2f94,b785ab7666c28ad402efea4f7db73b5f2bcaaf32,ead75bffc392cf682af0b2e179d08177c8748c53"
camel,4202,comment_1,A workaround is for the end user to use to eg pull messages 4 times per sec and thus be 4x faster.,design_debt,non-optimal_design,"24d805b9c41be7bac1c3e40d47ceac8b46ee2f94,b785ab7666c28ad402efea4f7db73b5f2bcaaf32,ead75bffc392cf682af0b2e179d08177c8748c53"
camel,4202,comment_2,"If there is an use case for using persistent queues that are *exclusive* for the Camel consumer, then we can avoid using JMSMessageSelector and thus be as fast as temporary queues.",code_debt,slow_algorithm,"24d805b9c41be7bac1c3e40d47ceac8b46ee2f94,b785ab7666c28ad402efea4f7db73b5f2bcaaf32,ead75bffc392cf682af0b2e179d08177c8748c53"
camel,4230,summary,BeanProcessor - Improved exception message if failed to invoke method,code_debt,low_quality_code,864f8112e9fbe1839d8e35b760d7c9dd47dedd9f
camel,4230,description,"See nabble If for some reason the method cannot be invoked, you may get a caused exception We should catch this and provide a wrapped exception with a more descriptive error message, about the bean and method attempted to invoke.",code_debt,low_quality_code,864f8112e9fbe1839d8e35b760d7c9dd47dedd9f
camel,4273,summary,Favor static member classes over nonstatic ones.,code_debt,low_quality_code,"6487011b23ff0cad6a78cdf2fbd87861426671d1,5119455ae595407626f3d075be4d3332ba8246e0"
camel,4357,description,I am currently looking into the dependencies betwen packages in camel-core. The packages org.apache.camel and form the camel api. So I am trying to make them not depend on other packages from camel-core. One problem there is the starter class Main. It needs access to impl packages as it needs to start camel. So it should not live in org.apache.camel. I propose to move it to To not break anything right now I will create a deprecated class Main in org.apache.camel that extends the moved Main. We can remove the deprecated version in camel 3.0,architecture_debt,violation_of_modularity,"64d1cf21c7e7194708de510e65f218d4d5d4dd98,6b1df1436772651407de9e1e636dde19999d4bee,b4e1b2b65175dfaeb780902c279c9df23d0a30c4,f659853b62c1cfff15777f25f789861c74ce95a0"
camel,4357,comment_2,"There is a CS error as the license header is missing in one file. When adding a new package in camel-core, a package.html file should be included, which briefly summarizes the package. See some of the other for examples to copy. And in the camel-core/pom.xml file there is some javadoc grouping of packages. I guess the new main package is to be added as well.",code_debt,low_quality_code,"64d1cf21c7e7194708de510e65f218d4d5d4dd98,6b1df1436772651407de9e1e636dde19999d4bee,b4e1b2b65175dfaeb780902c279c9df23d0a30c4,f659853b62c1cfff15777f25f789861c74ce95a0"
camel,4398,comment_0,"Sorry the issue is the tools JAR is added as dep on the SNAPSHOTs, but not on the releases etc. So its a bit weird. Track down to be possible in parent/pom.xml with some antrun tasks that does some osgi versioning regex replacement. And since this dependency is added to camel-core it gets inheirted by all the other artifcats. But only for SNAPSHOTs, and not releases. So I guess the release profile makes something different. The tools JAR is thus not needed at all as a dependency to use Camel, so what we want is to get rid of tools JAR in the camel-core/pom.xml file.",design_debt,non-optimal_design,"1124b903535f2e322808a9be0812405c0a120798,3e4326d12519bbbf818c1539b5f29c1fb61f9aa2,930b6c9055f5f47b2bb4479c7ac775854bc0a028"
camel,4417,description,"Several classes in impl are used or extended by components. We should avoid this. The base classes should be moved to support. Examples are DefaultComponent, DefaultEndpoint, DefaultProducer. Another case is the The typeconverter is well placed in impl but the class also has a public static convert method that is used from many components. So this functionality should be moved to processor so it is available to components.",design_debt,non-optimal_design,"03440b1874fd8e3aa7921638dc2d95645932dd49,e374344645d7dc2d9f3655e700d731cfd0dda50d,e320619cfca9758635279296001a3d43c10cacef"
camel,4417,comment_2,"Patch moving all support classes from impl to support. I moved the following classes and placed compatibility classes in their impl location: DefaultComponent, DefaultConsumer, DefaultEndpoint, DefaultExchange, DefaultMessage, DefaultProducer, DefaultUnitOfWork, ExpressionSupport, ProcessorEndpoint, ProducerCache, I moved the following classes without compat stubs as they were not needed outside camel-core: DefaultRouteNode, ExpressionAdapter, MDCUnitOfWork, MessageSupport, SimpleUuidGenerator I moved from processor to util as it is needed from support. The only problematic class I moved was DefaultConsumer as it needed So the above including the inner class had to move to util. The problem here was that the Bridge had to extend DelegateProcessor which of course is in processor. As util should not depend on processor I had to introduce an interface DelegateProcessor in camel that could be used to abstract from DelegateProcessor and This is a good thing anyway and I will open a jira to do this first. I also had to move PipelineHelper and to util as they were used from support classes. This is a fairly large patch. So I am not sure if it is good for 2.9. On the other hand if we wait with this till 3.0 we are either really incompatible or we can not remove the deprecated classes",design_debt,non-optimal_design,"03440b1874fd8e3aa7921638dc2d95645932dd49,e374344645d7dc2d9f3655e700d731cfd0dda50d,e320619cfca9758635279296001a3d43c10cacef"
camel,4417,comment_3,"@Christian, all the changes you make in 2.9 should be backwards compatible. So if you make any changes, please make sure leave existing classes in place (even as extensions of refactored classes) and change as few tests as possible, ideally none. That ensures two things: one that we didn't break anything and existing code still works, second that users have a migration path that could take at any time. We can remove the old classes later in 3.0. Changes that break backward compatibility I'd leave for later.",code_debt,low_quality_code,"03440b1874fd8e3aa7921638dc2d95645932dd49,e374344645d7dc2d9f3655e700d731cfd0dda50d,e320619cfca9758635279296001a3d43c10cacef"
camel,4417,comment_5,This change is probably too destructive even for 3.0,design_debt,non-optimal_design,"03440b1874fd8e3aa7921638dc2d95645932dd49,e374344645d7dc2d9f3655e700d731cfd0dda50d,e320619cfca9758635279296001a3d43c10cacef"
camel,4430,description,"As the proxy lifecycle cleanup work are done in CXF 2.4.3, we are facing some test failed in camel-cxf. After digging the code, I found the proxy instance which is created by the CxfProxyFactoryBean will be GC and the CXF client which is used in CxfProducer will be affect. The endpoint which is set on the conduit will gone, and CxfProducer will complain it with a NPE exception. We can use the to create client instead of CxfProxyFactoryBean to avoid the GC and NPE exception. I checked the difference between using CxfProxyFactoryBean and they are same in most case. We just need to take care of handler setting part which is used in JAXWS frontend.",design_debt,non-optimal_design,a88a7f8756dafb4884b32e97ca56f5c67493982f
camel,4543,description,"Camel Blueprint support is limited/hardcoded to Aries. This makes it impossible to use in JBoss 7 or with another blueprint implementation like Gemini. The following classes use various things from Aries. * * * * * Now obviously the last three are related to the custom namespace handler for Aries. It would be good if these were moved into their own module, something like or the like. That people can choose to include if they would like to use the custom blueprint namespace handler in Aries. Otherwise the camel-blueprint module should be implementation agnostic and work on all blueprint containers. Not just Aries.",architecture_debt,violation_of_modularity,f8b61b8b1ec9d465242093fbb8a76a6dd2349ea0
camel,4543,comment_1,"Patch that creates a new module called and moves all aries specific stuff into that. camel-blueprint no longer depends on any Aries classes. Tested camel-blueprint on JBoss 7.0.2 and works wonderfully. Haven't been able to test the new jar yet, as I haven't setup ServiceMix or the like..",test_debt,lack_of_tests,f8b61b8b1ec9d465242093fbb8a76a6dd2349ea0
camel,4602,description,"See nabble By invoking the filter for directories, it allows end users to skip entire directories, which would make the polling faster. And no need to walk down unwanted directories. The logic need to apply for both file/ftp consumers, as well having unit tests to ensure it works.",test_debt,lack_of_tests,2b1a99186d2eacdd97543cdeb365ac5740447be3
camel,4676,description,"split into camel-script-groovy camel-script-jruby so that we can control granularity much better, and version upgrades will be more manageable. In most cases, if the user does scripting, he'll probably standardise on only one particular language for all his routes.",design_debt,non-optimal_design,fe67eb9a44618969743212458e4ec2b176b7d812
camel,4736,description,"There's been a mail in the mailing list about camel-xstream not being blueprint friendly. Even though in the current trunk, it does work its a nice chance to add an integration test for it.",test_debt,lack_of_tests,f4b563250b290bad4bfc26a2ab037d96966437ce
camel,4741,description,"This is a trivial issue, yet it is sometimes a bit annoying. People that are familiar with jms/activemq components expect that the default behavior of the a queue producer is to add the message to the queue, without having to specify a special operation. Hazelcast Queue Producer instead will through an exception if no operation is defined. It would be good if it was consistent with the rest of the queue producers.",design_debt,non-optimal_design,f52ed6a960ddc3297d47d4bcb3d6a44cc98861d3
camel,4793,description,I would like to improve the aws-sdb component in the following places: - Using the [Amazon operation instead of CamelAwsSdbXXX -- Add OSGI integration tests I would like to resolve this issue for Camel 2.9.0 because renaming of the operation names are not backwards compatible. I will resolve this issue today.,design_debt,non-optimal_design,f1ac4119a4eafd564db0bb5e5b153df50671ac4d
camel,4793,comment_1,"Hi Christian, if you need any help with this let me know. BTW is there any agreed naming convention for components URI options and message headers? Looking at the existing components seems like URI options can have component specific names whereas the headers use format? But in this case, if you want to override an option from the URI using the message header, you have to specify a different name, which may be lead to confusion. Any thoughts?",design_debt,non-optimal_design,f1ac4119a4eafd564db0bb5e5b153df50671ac4d
camel,4928,description,It would be great to have timer component support asynchronous API. Such a feature can be useful when timer component generates events which must be processed by multiple threads. Current implementation of the timer component makes a blocking call so the usage of thread pools hardly possible to process multiple timer event simultaneously.,design_debt,non-optimal_design,561a38ce9f2b194728099fe9756a07ed03c8a91e
camel,4958,summary,camel-jms - JmsConsumer make it less verbose logging in case of an exception or TX rollback,code_debt,low_quality_code,"768a7cfc07e771e6234b1a352fd7fb3d9f1a5faf,2ad7d773f84bad894596fe0c9df4aad09b775f17"
camel,4958,description,"When using a JMS route, and an exception occurs, then you get a WARN logging from Spring JMS that there is no error handler configured. And you get the stacktrace etc. However Camel itself will also by default log the stacktrace, so we have it logged twice. We should make this less verbose. And allow end users to more easily customize the logging level and/or whether stracktraces should be included.",code_debt,low_quality_code,"768a7cfc07e771e6234b1a352fd7fb3d9f1a5faf,2ad7d773f84bad894596fe0c9df4aad09b775f17"
camel,4958,comment_0,"Is now less verbose, and Spring no longer complains about no error handler configured. Added options to tweak logging levels and verbosity.",code_debt,low_quality_code,"768a7cfc07e771e6234b1a352fd7fb3d9f1a5faf,2ad7d773f84bad894596fe0c9df4aad09b775f17"
camel,4959,comment_0,That is correct. Andrey can you provide a patch with unit test that test this fix?,test_debt,low_coverage,"5612882639444e83903b929bd15b26af06bac98e,34dcaa102b8546be92d7a4b8a1a735363d762ab1,a5cd4ff766371163bcc359d90db91658229f956e,80df8e81fa349ffb63e2e0ebb4c76f4efa989b09"
camel,4959,comment_2,"Yeah lets return 0 for the primitive types. Patches is welcome, with unit tests.",test_debt,low_coverage,"5612882639444e83903b929bd15b26af06bac98e,34dcaa102b8546be92d7a4b8a1a735363d762ab1,a5cd4ff766371163bcc359d90db91658229f956e,80df8e81fa349ffb63e2e0ebb4c76f4efa989b09"
camel,4959,comment_4,"IMHO the changes introduced by of this ticket causes regression failure. To avoid null values being misinterpreted as cache misses we do now convert e.g. Float.NaN = which was not the case before. Why not just simply *not* mark the conversion as miss *if* the conversion result is (Float.NaN / Double.NaN == And I simply don't get why the misses cache is a ConcurrentMap: To my understanding would be just fine! Why do we need a Map to memorize the cache misses? Where we then do things like On it which is not really intuitive while reading the code, is this maybe because ideally we want to do lookup by the misses cache in O(1) instead of O(N)? See also:",design_debt,non-optimal_design,"5612882639444e83903b929bd15b26af06bac98e,34dcaa102b8546be92d7a4b8a1a735363d762ab1,a5cd4ff766371163bcc359d90db91658229f956e,80df8e81fa349ffb63e2e0ebb4c76f4efa989b09"
camel,4959,comment_5,"Yeah I have corrected the coverters to deal with NaN. And yes we need to lookup fast in type converter registry as its used a lot in Camel, and it should be as fast as possible, as it can become a bottleneck.",code_debt,slow_algorithm,"5612882639444e83903b929bd15b26af06bac98e,34dcaa102b8546be92d7a4b8a1a735363d762ab1,a5cd4ff766371163bcc359d90db91658229f956e,80df8e81fa349ffb63e2e0ebb4c76f4efa989b09"
camel,4993,comment_1,"Applied the patch into trunk with thanks to Joshua, I also fixed some Checkstyle errors.",code_debt,low_quality_code,"5a9553974d120503f0fa296a3a9acce07ef842e2,3c1704e6fa42aa9b5fdd6b5224e972fe4805b6d4"
camel,4993,comment_3,I found an issue with the previous patch. Fixes and more tests attached in second patch.,test_debt,lack_of_tests,"5a9553974d120503f0fa296a3a9acce07ef842e2,3c1704e6fa42aa9b5fdd6b5224e972fe4805b6d4"
camel,5012,summary,Starting and stopping Camel should be less verbose,code_debt,low_quality_code,255ad374c0ca3c76233114926243e72743933447
camel,5012,description,"When starting and shutting down Camel, it reports a bit stuff at INFO level. We should make it less verbose. For example the type converter logs 3-4 lines, we should just log 1 line instead.",code_debt,low_quality_code,255ad374c0ca3c76233114926243e72743933447
camel,5012,comment_0,Now Camel is less verbose by default.,code_debt,low_quality_code,255ad374c0ca3c76233114926243e72743933447
camel,5045,summary,Memory leak when adding/removing a lot of routes with JMX enabled,design_debt,non-optimal_design,ac1c7d4c014c050111ff31cbb8980aa9b9d4adca
camel,5045,description,"If you add and remove a lot of routes to CamelContext and have JMX enabled, then the will accumulate a map of provisional JMX performance counters for the route mbeans. The map should be cleared after usage, as the map is no longer needed. Memory will accumulate as the map has reference to old objects which cannot be GC.",design_debt,non-optimal_design,ac1c7d4c014c050111ff31cbb8980aa9b9d4adca
camel,5045,comment_0,"Also CAMEL-4500 introduced a leak as well, in terms of ManagedTracer being kept around in a separate Map. We need to remove not needed tracer from that map as well.",design_debt,non-optimal_design,ac1c7d4c014c050111ff31cbb8980aa9b9d4adca
camel,5045,comment_1,I created CAMEL-5046 to track the leak from CAMEL-4500 with ManagedTracer as it only affects 2.9 onwards.,design_debt,non-optimal_design,ac1c7d4c014c050111ff31cbb8980aa9b9d4adca
camel,5184,summary,Faster testing with seda endpoints by shutting down seda endpoints faster,test_debt,expensive_tests,d9ed005c8fc7e80a3429219ce1e055c0671310d9
camel,5184,description,"When shutting down seda endpoints they take a lille while to shutdown properly. However during testing we dont need to do that, so we could shortcut this and shutdown faster. For example the camel-test kit could tweak that. As well in camel-core. I suspect we can cut down minutes of testing times.",test_debt,expensive_tests,d9ed005c8fc7e80a3429219ce1e055c0671310d9
camel,5527,comment_1,I removed the surefire plugin as its no longer needed,code_debt,dead_code,"127855086e55bff00cf87fb286ffdc4875866cd8,579b8c685eb8502e3bcf39aba5e8207819758f95"
camel,5586,comment_2,I polished the camel-elasticsearch codebase a bit:,code_debt,low_quality_code,"f19e792bd3839040ffbdc622e75684da62211ea5,458c290a2c29af8eb097eb8b0c3594bb896bf438,8ed02716df5e9a977b89cd2b091affc046bf6816,1497ebd53ca7131e43e00a3b198d8a07045d876f"
camel,5586,comment_3,O.K. it's fixed now. The tests also run now much faster than before (through overriding the On my box now running all the tests take about 3 seconds compared to around 30 seconds before!,test_debt,expensive_tests,"f19e792bd3839040ffbdc622e75684da62211ea5,458c290a2c29af8eb097eb8b0c3594bb896bf438,8ed02716df5e9a977b89cd2b091affc046bf6816,1497ebd53ca7131e43e00a3b198d8a07045d876f"
camel,5983,description,"We've got bunch of (negative) tests on the current codebase expecting a thrown {{XYZException}} however they don't realize if the expected exception is *not* thrown, the typical pattern for this is: Which correctly should be:",test_debt,expensive_tests,"3a475cb10581cde501481aa6f5824171db912b79,dac18f7d158c6892e21d9a7cb78ce2b7c925e88c,8411b98d89904a340bab4d9a815ab2a73102142a,d4ae4233c78a087c560945331b16f7e3df57f9c3,fb6a345a3c1551f0204cbd6332abdb3d75745964,366c2aa31f5e61067ff5b6bb1a3591a5476bc396,7104c9be82b55db688def774d42bae55b065073d,cf8314c57da258e4fa3bfb426d678544828eacf1,179011948cd4e1a69e3b02319b803a1213782b0e,48d759846e0438358a43a8f127877a3b9208fc94,6b1c7953dbbbf8bac75d4a5884b30dec46851b5f"
camel,6043,summary,Improve the BeanInfo performance,code_debt,slow_algorithm,"14669b432024f7321ad0dd9a1df68b4aa38bf156,bdf5686bac11f742fa0339accdc039566533aeeb"
camel,6043,comment_3,"This further improved performance for bean and simple language when using OGNL like expressions, that is evaluated at runtime. Now the cache ensures we do not introspect the same class types over and over again.",code_debt,slow_algorithm,"14669b432024f7321ad0dd9a1df68b4aa38bf156,bdf5686bac11f742fa0339accdc039566533aeeb"
camel,6188,description,"The CXF consumer copies the content-type http header to the camel exchange. This header may indicate the character set used in the request (for instance and if so this should be made available in the normal place for Camel (i.e. a property in the exchange called This may (of course) be done in each route by a separate processor, but it simplifies life if this is done by default. seems the logical place) Sample processor that performs this job.",design_debt,non-optimal_design,a70fc51a8daba51866fcbaf8aa1a3d08f7326d1b
camel,6296,description,"parameters are not supported for HttpClient3 on Camel-Http component at least for version of camel 2.9.4. Here is a patch that provides support for parameters support. There is no test case provided, but a patch.",test_debt,lack_of_tests,"821e1e810f1a78563005d33fa3fcd8263b5b5c69,fd82b447f2862aab10bf10a39ca1537d5d09ad7b"
camel,6320,comment_1,I think your test file data.ics is missing. Could you please add it? Adding a bit of logging would help too. Thanks for your contribution.,code_debt,low_quality_code,3bbbfcdd4ac5460db62e6ee35c064fdbbff5d864
camel,6403,description,"When using a {{camel-jetty}} endpoint, the {{UnitOfWork}} is not being managed by the servlet handling the request but by the Camel route that's being invoked. This means that some resources have already been removed/cleaned up when the servlet is writing the response, e.g. files for cached streams have already removed before the servlet gets a chance to read from them. It would be nice to have an option available to configure the servlet itself to handle the unit of work and mark it {{done}} after the HTTP response has been written. That way, the unit of work can be matched up with the actual HTTP request.",design_debt,non-optimal_design,"497ea3717e596af7a1a753e6458a5c5cd9cc6f6e,24e01f17149ea3690d1fe4d2fabe4c78a5d88225"
camel,6563,description,"When using a route to listen to UDP multicast address , no messages seem to get consumed. No exceptions are observed. Multicast address is defined as addresses in the range of 224.0.0.0 through 239.255.255.255 Input was simple string (e.g. ""Test String"") Example Route: <route <from Found an old topic in the user discussion forum that seems related. Did not find any unit tests in the Camel source code exercising this behavior.",test_debt,lack_of_tests,8c90678d56db8ab75a56b75d7350042b5a04dafc
camel,6635,description,"Due the recent SPI which allows to plugin a different scheduler we can improved this and use a non scheduled thread pool, which avoids the suspend/resume and run for at least one poll ""hack"" we have today in the codebase. Instead we can use a regular thread pool as the scheduler, and then submit the task on demand when receive() is called on the PollingConsumer API.",design_debt,non-optimal_design,"36f48fb0942630cb1179b2eb0bb474affb5b3742,c7ebd0c13b95c108e91584d2a3625a780bf3844c,68b357d8b792676c44b7b4c88ec72f73c6501e65"
camel,6826,summary,Use Mock Objects Instead of Live HazelcastInstances to Speed Up Testing,test_debt,expensive_tests,"67aa3b3a9ebcd4044664ae64845f18821fc1d9a0,f83c64b1c1a965a5736634b19edf8fc73f33a63a,41b4c88250cab4e509d5f473da0b6ea844e64285,dae6366cc50b04ece05897d9a3e0648c6ee86ea7"
camel,6826,description,"The unit tests for the camel-hazelcast component use real HazelcastInstance objects, which is very slow. We should use mock objects instead to speed up testing. Testing the integration can be done with a few, select tests, but the majority of the logic can be tested with mocks.",test_debt,expensive_tests,"67aa3b3a9ebcd4044664ae64845f18821fc1d9a0,f83c64b1c1a965a5736634b19edf8fc73f33a63a,41b4c88250cab4e509d5f473da0b6ea844e64285,dae6366cc50b04ece05897d9a3e0648c6ee86ea7"
camel,6826,comment_0,"I have been able to successfully speed up the producer tests. However, the consumer tests are a bit tricky. They use a callback API to do their work. I don't know if it's worth mocking all of that out or not, but I'll play with it. I'm committing what I have, though.",test_debt,expensive_tests,"67aa3b3a9ebcd4044664ae64845f18821fc1d9a0,f83c64b1c1a965a5736634b19edf8fc73f33a63a,41b4c88250cab4e509d5f473da0b6ea844e64285,dae6366cc50b04ece05897d9a3e0648c6ee86ea7"
camel,6826,comment_1,"I have implemented mocks for the consumer tests. The spring-based tests still need to be mocked out as well as the seda tests. It is much faster now, though.",test_debt,expensive_tests,"67aa3b3a9ebcd4044664ae64845f18821fc1d9a0,f83c64b1c1a965a5736634b19edf8fc73f33a63a,41b4c88250cab4e509d5f473da0b6ea844e64285,dae6366cc50b04ece05897d9a3e0648c6ee86ea7"
camel,7139,comment_1,I reproduced the problem with itest please see the attach. To Fix the problem a possible solution is remove TCCL to MVEL until is not resolved Please see the attach for the patch;I hope it's ok for you. If it's ok for you please clean it a bit Any feedback is welcome.,code_debt,low_quality_code,d6646e648cfc7e5c5ce332f5fe540f346f88ef2c
camel,7139,comment_2,Yeah you are welcome to cleanup the patch.,code_debt,low_quality_code,d6646e648cfc7e5c5ce332f5fe540f346f88ef2c
camel,7201,description,"In the current PGPDataFormat implementation, you provide the public and secret keyring as file or byte array and the keyrings are parsed into object representation during each call. This is fine if you want dynamically exchange the keyrings. However, this has a performance impact. In the provided patch I added the possibility that the PGP keys can be cached so that the performance can be improved. I did this by adding the two interfaces PGPPublicKeyAccess and PGPSecretKeyAccess. So user can now implement his own key access or use the provided default implementations and",design_debt,non-optimal_design,"62c8b7ad2215668e8d45f874ab06a5d04aee19b5,fe052fc920b7c84002a0f3564f4c0d301ecda35b,5f9e4f23e5b6c623a66fa81f0d42af7c85ae183c"
camel,7300,comment_0,Pushed to master. Not this patch makes use of a deprecated API call. In time we should move the entire camel-hl7 component to the new HAPI API: CAMEL-7301,code_debt,low_quality_code,bd60a2b3a7377a37543c4263b3eee5882526f479
camel,7319,summary,Dead not working JUnittest,code_debt,dead_code,aef0426c9be010b757c41587a3506e0a053e2aea
camel,7319,description,"The test in is ""dead"" since checkin git-svn-id: if you enable xalan on testing classpath.",code_debt,dead_code,aef0426c9be010b757c41587a3506e0a053e2aea
camel,7587,description,"The MessageHistory feature currently keeps passwords in plain text in case they are part of the URI. does some sanitizing, but only for the from node - other nodes/processors are currently not sanitized. In order to prevent handling sensitive information in the message history in general, I would suggest to sanitize the URI already when storing a MessageHistory item.",design_debt,non-optimal_design,6b0623ebd7fbd0b3be48139fec8d1db3c783c97c
camel,7644,description,"Since the camel DSL is invoked prior to being invoked there is no camel context set on the delegate java RouteBuilder which causes it to create a new context when the first dsl method is invoked. With the implementation of CAMEL-7327 introduced in 2.13.1 which stores created camel contexts in a set in this causes instances of DefaultCamelContext to be leaked, they are never removed from the static set. This is especially aparrent during unit testing. The following test shows that an additional context is registered for the scala route builder as opposed to java. Verification of the leak can be requires profiler and capturing of heap after termination of the test case (in ParentRunner.java).",code_debt,low_quality_code,"c469ad776a86086c5edbed2cac998dd0596aa7d8,01f08a6267232089adba90a380050ba5dc9c43d2,c0db627b8fc360848259875647397242407fddad,87c9a9da4ec409c82d710ebd806a08f24579e8c2"
camel,7715,summary,SjmsConsumer and SjmsProducer do not remove thread pool when stop,code_debt,multi-thread_correctness,67ee84ed03ca8cd63df8d783bfc256c646e67067
camel,7715,description,"SjmsConsumer and SjmsProducer always register a new ThreadPool on Camel context every time a new instance is created for an endpoint. If consumer or producer is stopped or removed or even component is removed, thread pool still exists.",code_debt,low_quality_code,67ee84ed03ca8cd63df8d783bfc256c646e67067
camel,7813,summary,Make camel-jms more robust for replyTo,design_debt,non-optimal_design,812fa060bfeac5b320624b5d6d4833ac441d42c9
camel,7813,description,"When a JMS queue is used as a camel consumer for a route it may well be one of possibly many intermediate stops in a chain of processing. If the previous processing step itself used Camel to route the message, then both the JMS replyTo and the camel-header JMSReplyTo will both be populated with the same value. This will cause an infinite loop. Of course, this is in some sense a developer error, but it is a pain to constantly add code to clear the camel JMSReplyTo header if it equals the destination. This should probably be internal to the camel-jms component itself.",design_debt,non-optimal_design,812fa060bfeac5b320624b5d6d4833ac441d42c9
camel,7954,description,Currently camel-olingo2 uses SSLContext directly to create connections; it should use,design_debt,non-optimal_design,d06f3af13aa3ec895c99db01a0a44be40d6113f9
camel,7956,description,Currently camel-olingo2 uses SSLContext directly to create connections; it should use,design_debt,non-optimal_design,608c99c5b88f86bcb2a42dd81454605cc075e485
camel,8068,comment_2,Thanks for the patch. We do like when there is an unit test with code changes so we can verify this.,test_debt,lack_of_tests,e5bab46e1cc683a53c7fc9659979ddf0374eff39
camel,8068,comment_3,"Currently I'm not able to build the test, they complain about missing documentation... Also compilation fails on my pc becuase MailSorter uses Java 7 style and the compile level seems java 1.6 ... So I'm currently try to sort out how to build camel at all from the git checkout :-(",test_debt,lack_of_tests,e5bab46e1cc683a53c7fc9659979ddf0374eff39
camel,8091,description,"The does not consider the context property that is supposed to limit the size of the logged payload. It is possible to set a maxChars on the but that has a different semantics (limits the length of the formatted exchange, not of the message payload) and is complicated to set in some cases (e.g. in the case of the default error handler) The attached extension also honors the context property when formatting the exchange.",design_debt,non-optimal_design,43d02628287c0623672714a016e324d6da36d71c
camel,8174,comment_3,"Willem, we have a better solution out of the box with the default inflight repoistory. It has a browse API now, that returns details about how long time the exchange has been inflight. And its also available from JMX so end users can use that to track long processing exchanges. And for Karaf we have a Camel command also. So I would like to remove the as its no longer needed, and also its code is not complete. Any thoughts?",design_debt,non-optimal_design,"4f5902e3a36afae8b5e67c22bb03b1be0ba412e2,6cb504252967d4e178bb27b84569a79844195c86,642a91eaef6402385a05bf34de7bf4abc561a4f4,cdfee48f7545ba21c46d8ce2c6714dad147da6c4,2db1d8d1ee342313da2601740b1c526aa337b11d"
camel,8174,comment_4,"Hi Claus, It's good to see that we have better API to track the long processing exchange, I will remove the from the repo shortly.",code_debt,dead_code,"4f5902e3a36afae8b5e67c22bb03b1be0ba412e2,6cb504252967d4e178bb27b84569a79844195c86,642a91eaef6402385a05bf34de7bf4abc561a4f4,cdfee48f7545ba21c46d8ce2c6714dad147da6c4,2db1d8d1ee342313da2601740b1c526aa337b11d"
camel,8312,comment_0,"One thing I overlooked: the javadoc of the method says: ... For example you can set it to InputSource to use SAX streams. By default Camel uses Document as the type. ... Using InputSource as documentType is a particularily bad idea. If the XPath implementation supports it (Saxon does, the JDK implementation doesn't), SAXSource can be a more memory efficient choice for the docuementType. We should probably also change the Javadoc here...",design_debt,non-optimal_design,1df559649a96a1ca0368373387e542f46e4820da
camel,8312,comment_2,"Hi Claus, are you sure that you want to delay this till 2.15.0? I think this issue is serious. Best regards Stephan",defect_debt,uncorrected_known_defects,1df559649a96a1ca0368373387e542f46e4820da
camel,8321,comment_3,"Why are Box headers being used/fed in other These headers are only supposed to be fed to the Box component, then should be cleaned up n the next step. I wasn't aware of the Camel 'no dots in header' convention. I'll have to take a look at the API framework to see if that's affected. At the very minimum code generation maven plugin is affected since it uses that convention for all generated components, and also would make the new components backward incompatible.",design_debt,non-optimal_design,"ddbc66b8d5fefd7d003dfa4d113464a06e05d5f8,4afe1a7a7cc2840ebe9de2dae07328326607f70e"
camel,8734,comment_0,"Hi, Please consider that URLs should be case sensitive according to W3. [quote]URLs in general are case-sensitive (with the exception of machine names). There may be URLs, or parts of URLs, where case doesn't matter, but identifying these may not be easy. Users should always consider that URLs are The header CamelHttpPath should provide me what the user actually entered.",code_debt,low_quality_code,"55cb57fba1c0758a99f5e5ef5225f14ca9b133b7,1abbae99f2821faa6cf37e45a5471e094795212f,64971720dc41aef56f61004b8f6fbda29ed197ab"
camel,8844,description,"Currently there are issues when directory in (s)ftp(s) endpoint is absolute (or contains arbitrary number of leading slashes). Sometimes the path is normalized, sometimes don't and camel-ftp code can't find correct prefix in _absolute_ path when retrieving remote file. [Documentation I suggest normalizing the behavior and treating absolute FTP endpoint URI directories as relative. When URI contains absolute path, WARNing will be printed:",design_debt,non-optimal_design,ca6d74205815269b7b3caf32ca57cb73c1a7299a
camel,8879,comment_1,Would you be able to attach an unit test also?,test_debt,lack_of_tests,ae2012ee7faf12fadcbb3760e4e56990563fdf1a
camel,9338,description,"Should be updated to latest release as we are far behind on this one. It may no longer work in OSGi and that is fine, just remove the feature",architecture_debt,using_obsolete_technology,"ccfa2dedaf54a46052aca5fb3ee32db80ef28314,74b9e987d4116ee27315a2fbe4fac05fe675a0be,7d715c895d6b4ff82342b55af8f125085c47f3f2,64008dec503f4466f03318406ae39f5d08d5323d"
camel,9403,summary,camel-examples should not be in BOM,architecture_debt,violation_of_modularity,4442b75b81965730cce4009a6b19d889a46f8dbf
camel,9616,description,"For the moment, a custom {{MetricRegistry}} bean can be provided but must be named. It would be easier to relax that constraint in case only one {{MetricRegistry}} bean exist and do the lookup by type only.",design_debt,non-optimal_design,"62b080e5986efc028cd30bacd5534fa96ad8b8ad,95b60ce5e444f33d83c6ae8a36c8c4dd19f1f019"
camel,9690,description,"Hello Im using camel 2.16.2 and Im finding the bean parameter binding doesnt seem to work very well on overloaded methods. See below for an example Here are the routes And here are the tests I dont understand why test2Param_string and test2Param_classB throw ambiguous call exceptions. Heres a sample stack trace. From looking at the code in BeanInfo, I *think* it just tries to match the type on the body and if it sees multiple possible methods then it throws the exception. I believe it should go further and try to match the type on the other parameters as well? To get around this issue temporarily, Ive had to write an adapter class that wraps around ClassA but its not an ideal solution.",design_debt,non-optimal_design,17dc42bcf5e845c4bd6ff0dc3217e148ebdcf76e
camel,9752,description,"When any file:// or ftp:// consumer has a quartz2 schedule it can start throwing exceptions because too many worker-threads are busy at the same time Both workers can find the same files during filling the maxMessagesPerPoll, or during processing of those files. This happens when the route can not process all files before the next trigger happens. example (every minute): Attached you can find a stacktrace that would happen very often if worker1 processes and moves files that worker2 would also like to start processing. This does not happen when using scheduler=spring or when using delay=1m. The only way I have found to make sure a file or ftp component does not use multiple threads while consuming very large batches is to annotate the class with I am not familiar enough with the Camelcode to say what side effects it has and if this would prevent any quartz job in camel to now be single threaded, even if the user does not want it to be. But to me it looks like an oversight when moving from quartz to quartz2. A file or ftp consumer should be single threaded while retrieving.",code_debt,multi-thread_correctness,3680450e447e2e8ed4a550916e92bc94ff0f6245
camel,9812,description,"After shutting down a camel context, there are still threads running kafka consumers. In the logs after the shutdown I can see: So in theory the context is stopped, but I can see threads running with the polling of the sockets of kafka consumers (see attached immage). This deployed in an application server (wilfly in my case), causes a lot of issues, because apps get deployed and undeployed without stopping the JVM, but threads from previous deployments are left there. Please also bear in mind that kafka (9.0.1) throws warning messages due to the fact that un expected config items are thrown to the kafka consumer properties. Thanks!",code_debt,multi-thread_correctness,d90a3f9d89f7ee4e12efad65a099eef3ef2e532e
camel,9812,comment_0,"Thanks for reporting. Sound like the consumer need to call some stop/shutdown on kafka somewhere if its not already doing that, or missing something.",design_debt,non-optimal_design,d90a3f9d89f7ee4e12efad65a099eef3ef2e532e
camel,10048,summary,Memory leak in RoutingSlip,code_debt,low_quality_code,"e3890695b8cb92dff1d14b38f2876ee925d9acff,ddb852cdf7da29827fcab0b25a2b2ed6ee443cf9"
camel,10048,description,"RoutingSlip has a cache of error handlers implemented as a ConcurrentHashMap. This map stores error handlers as values, and uses some synthetic objects as keys. For some kind of destinations provided in routing slip, map lookup operation does not work. Hence, new error handlers are always added to the map and existing error handlers never reused. Finally, the program runs out of memory. The synthetic keys are actually instances of class Such key is based on two objects: RouteContext and destination Processor. Neither RouteContext nor Processor do not require their implementations to provide equals() and hashCode() methods. Strictly speaking, caching implementation in RoutingSlip is incorrect, because it uses hash map in the discouraged way. However, for some cases it works. The problem occurs when routing slip contains a 'sync' destination, in other words - destination is a Processor that does not implement AsyncProcessor interface. RoutingSlip determines destination producer via and the latter uses method. This method creates new instance of Processor for every processor that is not an instance of AsyncProcessor. This is where the problem hides: new object has different hash code (defined by Object.hashCode()) and new object isn't equal to the object used as a key in the hash map (well, Object.equals()). Finally, new key for the hash map is calculated, lookup operation cannot find this key in the hash map, new key-value pair is put into the hash map.",design_debt,non-optimal_design,"e3890695b8cb92dff1d14b38f2876ee925d9acff,ddb852cdf7da29827fcab0b25a2b2ed6ee443cf9"
camel,10048,comment_1,"How to fix? I think, the best way is to remove this dangerous caching at all. There might be a temptation to implement equals() and hashCode() methods in the helper class in a way to delegate both these calls to the processor wrapped by this class. However, the root cause of the problem is the incorrect usage of a hash map. Key must implement equals() and hashCode(). We cannot require all implementations of Processor and RouteContext to implement these methods - it would be an unmotivated bloating of their contracts with irrelevant functionality. Error handlers in RoutingSlip are short-living objects, they shouldn't get into Old Gen, so GC will clean them without significant performance overhead.",design_debt,non-optimal_design,"e3890695b8cb92dff1d14b38f2876ee925d9acff,ddb852cdf7da29827fcab0b25a2b2ed6ee443cf9"
camel,10476,description,"Problem: When running with a Camel Blueprint project a configAdminFile is not used to populate propertyplacehoders in when exectued with So a user can't run camel locally in a similar way to running in Karaf with file based property placeholder values. Workaround: I think, but haven't tested yet, that you can work around this locally using the methods described here: and/or how this solution appears to use exec:java locally and loads the properties via To reproduce the problem: Create a new project using (You need to change the log4j config to make it run.) To reduce the time, I created a project that runs here: Instead of using a default in the blueprint XML for the I setup the POM to include the following: In Camel 2.15.2 or earlier, this file would be loaded when mvn camel:run was invoked and the properties would be available via the PID at run time. After the changes made in CAMEL-9313, it appears that the method is only called in when the createTestBundle pathway is taken in java.lang.String, boolean, java.lang.String, java.lang.String, java.lang.String, So it appears test using get this functionality (as shown by the tests) but things executed from camel:run do not. Here you can see in Camel 2.14 that call to is made after the bundelContext is created. In the master branch version, that call is no longer made from main after the context is returned. I made a change locally to add a similar call to in Camel 2.18: Here is the output of the log statement from the example before this change: Here is the output of the log statement from the example after this change: As you can see before the change, the ${greeting} property is not poplulated via propertyplacehoder. After the change it is replaced. Given all the discussion of timing related issues in CAMEL-9313, I'm hesitant to say this is a good enough solution or that it aligns with the intention of the changes made in that fix. Given that configAdminFileName and configAdminPid are passed into perhaps the call to should happen inside createBundleContext or one of it sub-methods. Overall, I ""think"" a user should be able to use the configAdminPid and configAdminFileName settings to load properties via camel:run rather than work aound it, but I could be persumptious there.",design_debt,non-optimal_design,35a8fb65ce2f4feffe0b663d4ed48fdf00e98f44
camel,10476,comment_1,Let me know if I can provide any more information or if something isn't clear. I also was able to successfully test the workaround and I can run locally(in an IDE) using java:exec with the same properties file that the unit tests load by overriding and reading that config file. The only thing I am not confident in with the workaround is the difference between running in a DefaultCamelContext vs a in the IDE.,design_debt,non-optimal_design,35a8fb65ce2f4feffe0b663d4ed48fdf00e98f44
camel,10507,description,"Addressing the issue raised in the review, Jackson TypeReferences should be declared constant.",code_debt,low_quality_code,0c5786f4dc162fc6203145de2ae54da47fe780b0
camel,10517,summary,Remove unnecessary SuppressWarnings,code_debt,low_quality_code,39a9f52232fcaa1d82b290622d24aa45523094d0
camel,10563,comment_0,Need to add better handling for hz instance cleanup,code_debt,low_quality_code,"4be2c5127c4c26667ceabaddaa5484163e19d618,97c021e47cf777b676fc57e35beec9cc322532c4"
camel,10678,description,We should use each individual fields instead of a string field eg use fields - from - to - name - model As individual fields. And use name as the index field if its intended to be unique. This allows users to better query/browse the registry from JMX as well to find which transformers there is. Also remember to adjust the code in the jolokia command.,design_debt,non-optimal_design,2bca939186660f1b0abdb98d7be17a6ec8e066e4
camel,10950,description,"One of the drawbacks with the docker-java library used in the camel-docker component is that it depends on Jersey for the JAX-RS client. This can be problematic if you already have another JAX-RS implementation on the classpath. Therefore, it'd be nice if you could specify the that you want to work with. By default it'd be but you could choose to use the if you wanted to avoid Jersey. Similarly, users could implement their own and have camel-docker load this when it comes to build the Docker client.",design_debt,non-optimal_design,efdf259da9c2927251c7cf99d93e75a5d00c44ff
camel,11171,description,"component has an issue with the usage of {{RAW()}} function in child endpoint configuration. will mishandle the the content of {{RAW()}} , when at some point the escaped ampersand symbol is unescaped, and a wrong set of parameters is used. The attached PR fixed the issues and adds a unit test to verify the behavior before and after the fix.",test_debt,low_coverage,d6088eb5175e4c66c7c8a467694a8d60ff8d3eb5
camel,11196,description,"A Camel connector can be configured on two levels - component - endpoint Just like a regular Camel component. But we should allow users to configure a connector in one place, and not worry about if its component or endpoint level. And then let camel-connector when it prepares the connector figure out all of this for you. This may require supporting loading configuration from external resource files such as .properties files etc. Or something else. But from end users we can make this easier.",design_debt,non-optimal_design,3dd29006c3aa1c9612dc25461b64a4845e31138d
camel,11196,comment_0,"We should also make it easy to reuse an existing connector and have separate configuration, eg so the same user can use a twitter-connector and then configure it for user A and user B in the same application without configuration clashes. Today the component level would reuse the same component / configuration. So we may want to find a way to clone a component or whatever we can think of. A little bit of problem is the generated spring boot auto configuration which is hard-coded to one prefix.",design_debt,non-optimal_design,3dd29006c3aa1c9612dc25461b64a4845e31138d
camel,11282,description,We should extend the plain DefaultComponent (the is deprecated) ensure there is a plain default no-arg constructor so it makes using and configuring components easier. See eg SO If it was just a plain no-arg constructor then the bean style would have worked.,design_debt,non-optimal_design,"420f06e6fc0c70d26b19e183e96d696c4114ffc6,c0f063c763783b67668b366a1ac501df03eb9de3"
camel,11524,comment_0,"No there is not, your workaround is to not use $ in the file name, which also is a bad habit to do so. The source code needs to be patched where you need to quote the file name in the GenericFileEndpoint method via You are welcome to work on a github PR to fix this",code_debt,low_quality_code,1fa638ec156135ef9d5c6bb8684b81459c6c7c82
camel,11524,comment_1,"Hi Claus Thanks for the response. The source of the '$' is Ola Hallengren's SQL maintenance script and that in turn is escaping a '\' in a database engine name. It seems like a better idea to fix the Camel code and make it more robust. I will look into providing a patch for this. Regards, Saycat",code_debt,low_quality_code,1fa638ec156135ef9d5c6bb8684b81459c6c7c82
camel,11655,comment_0,"IMHO this's a breaking change for the upcoming {{2.20.0}} version as the query parameter has been simply removed and replaced with a new {{encryption}} query parameter, which _would_ break user's code making use of this option. AFAIK a minor release should not include any breaking changes, right? Shouldn't we better somehow mark as deprecated and encourage users to make use of the new {{encryption}} parameter instead? Other than that some feedback on your made code changes: What would be wrong to do: Instead of the following if / else if: As because: Also maybe mark the enum itself as deprecated so we don't forget to remove it in Camel 3.",design_debt,non-optimal_design,"efee1f707a41f3091955e11beab26d0afbf916cd,6e5dce3acaa3e1f6b669362198ae39f2cbd587e9"
camel,11655,comment_4,"LOL it's not critical for me but for the community :-) I am not convineced by your answers and kindly ask  &  to share their thoughts about this ticket as well as it's corresponding code changes. To summurize: - The made changes are breaking for the upcoming 2.20.0 _minor_ release if a Camel based application already makes use of the query parameter today in production. - IMHO Camel should be transparent and not restrict an underlying library API just because it seems to be buggy or non-working. Bugs _should_ be resolved by the underlying library itself and not _artificially_ through Camel by hiding/restricting a given API. See my comments above regarding this point. - In general, shouldn't we mark a Camel API as deprecated and encourage users to make use of the right/new API. E.g. as it's done when some {{StringHelper}} new utiliy methods were extracted out of {{ObjectHelper}} and the corresponding {{ObjectHelper}} methods were marked as deprecated, see CAMEL-10389. This would make it much much easier both for Camel code base itself as well as user's code to avoid using the legacy API. I guess that's exactly what the {{@Deprecated}} annotation good for. I don't seem to understand what it would be wrong with such an approach, that's, marking query parameter as deprecaterd and encourage useres to make use of the new {{encryption}} query parameter and then support both of them. Supporting both would not break a pre-existing application on the production and then we could find and remove the deprecated API by Camel 3 much easier. WDYT?",design_debt,non-optimal_design,"efee1f707a41f3091955e11beab26d0afbf916cd,6e5dce3acaa3e1f6b669362198ae39f2cbd587e9"
camel,11868,description,The current java transport client is due EOL in near future and it will be a good idea we switch the the new high level rest client instead. The new high level rest client is only released from version 5.6.0 and towards so an general upgrade of the dependencies is required for the ELK component. This also add the support for basic authentication without the need of x-pack. This is usefull when running ELK behind a reverse proxy.,architecture_debt,using_obsolete_technology,"928f185f9c5fcb1eb48d8f626b7f569210bca5f7,037aee4da369950c6ce206926203a3c8ba0aaa4d,3f2ef7371189c03e9ffdb2b79e919ca782cf831c,9b60081c049cbc89413e6497023f514fb6e0303a,f929b940da164b4c4bd11638c8fe693c57f59c3a"
camel,11868,comment_1,"It might be a good idea, since not all features is supported in the high level rest client yet I already got it to work with the rest client, but then I would suggest we create a super class for and a and In the we decide which one we should instantiate based on the type client on the class path. This of course mean the client dependency is not included in the the component. Regarding the it's not possible to use the Hight level REST client since it only support ELK from 5.6.0 and forward. One think to be aware of with the REST API that it is slightly different, which mean various request and response object is not called the same with the REST client. So either we have to document this in the manual or we have to come up with a common request / response object for both clients",design_debt,non-optimal_design,"928f185f9c5fcb1eb48d8f626b7f569210bca5f7,037aee4da369950c6ce206926203a3c8ba0aaa4d,3f2ef7371189c03e9ffdb2b79e919ca782cf831c,9b60081c049cbc89413e6497023f514fb6e0303a,f929b940da164b4c4bd11638c8fe693c57f59c3a"
camel,11868,comment_2,"Can we not create a new component that use this new rest client only. And then the two older components can stay for older users, and eventually we can start marking them as deprecated because the java client is EOL from Elastic and the rest client is the future/way to go.",design_debt,non-optimal_design,"928f185f9c5fcb1eb48d8f626b7f569210bca5f7,037aee4da369950c6ce206926203a3c8ba0aaa4d,3f2ef7371189c03e9ffdb2b79e919ca782cf831c,9b60081c049cbc89413e6497023f514fb6e0303a,f929b940da164b4c4bd11638c8fe693c57f59c3a"
camel,12104,summary,Unintuitive default cxf timeout behavior,design_debt,non-optimal_design,"4a9a7c636732b4a92e54c9d6b181840723df56d4,5805a124ee5e76b61bf7bdb12ff0273669e2e730,c789e1f6b09b57d1b0882268ebbe8fecfa9c05b2,a26ac21754fb303fee14490702e1383b6ee388e1,7d938d2db472e5f568d18cc77d015a113e4c4dec,007fded40fcf5108d75b5ffc32789d7c3dc5ca66,0e53232d67252b15a2575c5c3bd08146f54edc76"
camel,12104,description,"There is very strange behavior in Camel cxf and cxfrs timeouts which could lead to sensitive data being released. Below is a code sample which illustrates the unexpected behavior. I think any developer would expect the test API to return ""Valid Response"" or some kind exception, but in fact it returns ""SENSITIVE DATA"" due to the default continuationTimeout of 30 seconds. This issue seems to have been introduced by",design_debt,non-optimal_design,"4a9a7c636732b4a92e54c9d6b181840723df56d4,5805a124ee5e76b61bf7bdb12ff0273669e2e730,c789e1f6b09b57d1b0882268ebbe8fecfa9c05b2,a26ac21754fb303fee14490702e1383b6ee388e1,7d938d2db472e5f568d18cc77d015a113e4c4dec,007fded40fcf5108d75b5ffc32789d7c3dc5ca66,0e53232d67252b15a2575c5c3bd08146f54edc76"
camel,12104,comment_0,Would you be able to build an unit test of this sample code so we can take that and add to the tests of camel-cxf and work on a fix.,test_debt,lack_of_tests,"4a9a7c636732b4a92e54c9d6b181840723df56d4,5805a124ee5e76b61bf7bdb12ff0273669e2e730,c789e1f6b09b57d1b0882268ebbe8fecfa9c05b2,a26ac21754fb303fee14490702e1383b6ee388e1,7d938d2db472e5f568d18cc77d015a113e4c4dec,007fded40fcf5108d75b5ffc32789d7c3dc5ca66,0e53232d67252b15a2575c5c3bd08146f54edc76"
camel,12104,comment_5,I think we can extend cxf Continuation interface a bit to add an isTimeout method so that we know the timeout happen and can handle this situation accordingly outside CXF,design_debt,non-optimal_design,"4a9a7c636732b4a92e54c9d6b181840723df56d4,5805a124ee5e76b61bf7bdb12ff0273669e2e730,c789e1f6b09b57d1b0882268ebbe8fecfa9c05b2,a26ac21754fb303fee14490702e1383b6ee388e1,7d938d2db472e5f568d18cc77d015a113e4c4dec,007fded40fcf5108d75b5ffc32789d7c3dc5ca66,0e53232d67252b15a2575c5c3bd08146f54edc76"
camel,12624,description,"Currently we are running Camel AMQP component against Active MQ 5 (Amazon MQ) we want to move to an Artemis solution but this hasn't worked seamlessly. In CAMEL-9204 I believe a hardcoded topic prefix of ""topic://"" was introduced I think for a workaround of an Active MQ 5 bug. In Artemis this means Camel connects to a topic named instead of and therefore receives no events. The only possible workaround is to manually create the connection factory which means then the topic prefix is not set. My believe is that the hardcoded topic prefix is a bug and should be removed or an option to override at the AMQP component level should be introduced. Bug location: AMQPComponent.java line 59 Workaround:",design_debt,non-optimal_design,d673acd3035365ef85aae9b6836ee489a8e6a18e
camel,12646,summary,camel-spring-boot - Auto configuration of complex types should be more tooling friendly,design_debt,non-optimal_design,"ca56ad36990672b92057bfdae51232f453f4211d,11fe34439c6b16c652c7d5bb16e75bd998079280,4dfc9f30d9002c06ae496cca997a5265e2cab262,2b4e3cf5d625f88c0d01a5cc1730954aaec9f7a4,bd51298fe84ebcdd020ac93a0c8bbf1e581eded6,4dfc9f30d9002c06ae496cca997a5265e2cab262,2b4e3cf5d625f88c0d01a5cc1730954aaec9f7a4,bd51298fe84ebcdd020ac93a0c8bbf1e581eded6"
camel,12646,description,"If you have complex types like and wants to allow to configure this via spring boot autoconfiguration in - then the generated spring boot classes with all the options will use getter/setter of types That seems correct, but the spring-boot tooling itself (that generates additional json file) will skip those as it only support primitives and string types. So we may need to fool, and generate the getter/setter as String type as you use it for configuring it as a bean reference by id anyway, eg = #myDataSource We can add in the javadoc that the type is",design_debt,non-optimal_design,"ca56ad36990672b92057bfdae51232f453f4211d,11fe34439c6b16c652c7d5bb16e75bd998079280,4dfc9f30d9002c06ae496cca997a5265e2cab262,2b4e3cf5d625f88c0d01a5cc1730954aaec9f7a4,bd51298fe84ebcdd020ac93a0c8bbf1e581eded6,4dfc9f30d9002c06ae496cca997a5265e2cab262,2b4e3cf5d625f88c0d01a5cc1730954aaec9f7a4,bd51298fe84ebcdd020ac93a0c8bbf1e581eded6"
camel,13214,comment_0,"It should not be moved, but be there both places. Then endpoint override component level, this is how its done in other components.",design_debt,non-optimal_design,"e3172f5c8d148ef5a2a5b638d3ec4210a180275d,1e61784650d8272633ae3270bb6e052fb239c13e,aad01f16d43bc5fe3271043c7dfbf3912e860bb2"
camel,13533,comment_0,Xml related tests are now ignored for composite batch api,test_debt,low_coverage,f46a9f185482fa6016fae3ed716d29ccff5d7b87
camel,13681,description,Any of the options you can configure via such as: camel.main.name And so on should be configurable via ENV variables which will override any existing configuration. This is good practice in containers and also how SB can do etc.,design_debt,non-optimal_design,"68a93674a82e01b1d0c0312ff5998c05de94b8ae,e37c4a4dcc04ee14e5bf0008dfc9973da92e4af8,13a2457ae293e954bb36daac3cce08f3027360d2,1076cc0e55b13a350e5bc7b3a800641a098ce5cc,2eb00b5168889b59c81392531412e9b7406b2f4e"
hadoop,2776,comment_1,"Also note that the new EC2 scripts add SOCKS support: hadoop-ec2 proxy <cluster name By installing FoxyProxy or setting you socks firewall settings, you can browse your whole cluster. that said, Nate's suggestion might be less clunky... see:",design_debt,non-optimal_design,6234f3d91c6f35afd0cee2d6bc210b117f88d5a0
hadoop,2776,comment_7,Any more updates on this? Running Hadoop inside Amazon EC2 is super annoying because most of the links are broken (internal vs external addresses). This bug has been open for FOUR YEARS.,design_debt,non-optimal_design,6234f3d91c6f35afd0cee2d6bc210b117f88d5a0
hadoop,2776,comment_8,"I'm going to close this as won't fix. I don't think this is anything that we actually can fix here other than providing a complicated hostname mapping system for web interfaces. Part of the frustration I'm sure stems from a misunderstanding of what is actually happening: The slaves file is only used by the shell code to run ssh connections. It has absolutely zero impact on the core of Hadoop. Hadoop makes the perfectly valid assumption that the hostname the system tells us is a valid, network-connectable hostname. It is, from the inside of EC2. We have no way to know that you are attempting to connect from a completely different address that is being forwarded from some external entity. Proxying connections into a private network space is a perfectly valid solution.",defect_debt,uncorrected_known_defects,6234f3d91c6f35afd0cee2d6bc210b117f88d5a0
hadoop,3957,summary,Fix javac warnings in DistCp and the corresponding tests,code_debt,low_quality_code,9270635f3358092a7ee28a740e4aa5c552476b82
hadoop,3957,description,There are a few javac warning in DistCp and TestCopyFiles.,code_debt,low_quality_code,9270635f3358092a7ee28a740e4aa5c552476b82
hadoop,3957,comment_0,fixed unchecked warnings. Removed some not-so-useful methods and the use of deprecated API.,code_debt,dead_code,9270635f3358092a7ee28a740e4aa5c552476b82
hadoop,6145,comment_3,"Attaching patch for trunk. Problem was the trash method was throwing FileNotFound, which was not being handled. There are three different places where the non-existence of the file being deleted could conceivably be handled. Fixed by just checking for the file's existence at the beginning and exiting if not found. No unit test because the affected unit test is TestHDFSCLI, which is in HDFS project. Manually tested and it works. The reason this wasn't detected previously is that it only manifests itself when the trash feature is enabled, which apparently it isn't on the minidfscluster that powers TestCLI. We should probably looking at running Test*CLI with both trash on and trash off.",test_debt,lack_of_tests,466f93c8b6ccd88572025df79665f496f8367060
hadoop,6145,comment_7,srcFs.exists(src) and both call We could call once in FsShell.delete(..) and save a rpc.,code_debt,low_quality_code,466f93c8b6ccd88572025df79665f496f8367060
hadoop,6151,comment_5,"* The unit test should use JUnit4 test annotations instead of JUnit3 TestCase * looks useful for debugging, but should probably be left out * The static \*Bytes fields should be final * The @return docs for ""needsQuoting"" could be more explicit",code_debt,low_quality_code,366b1b1dd6f1ade1996c7c0eec1aca185c68d6cb
hadoop,6182,summary,Adding Apache License Headers and reduce releaseaudit warnings to zero,code_debt,low_quality_code,"2e34ced206c388ae5ee78875d3d4d158c6ca0b29,7ffe5b7f87069c5fb1e44d240416d766c9db6a75,07b43463b8cb3aee80510c2cc3f70cd631f9a69b"
hadoop,6182,description,As of now rats tool shows 111 RA warnings [rat:report] Summary [rat:report] - [rat:report] Notes: 18 [rat:report] Binaries: 118 [rat:report] Archives: 33 [rat:report] Standards: 942 [rat:report] [rat:report] Apache Licensed: 820 [rat:report] Generated Documents: 11 [rat:report] [rat:report] JavaDocs are generated and so license header is optional [rat:report] Generated files do not required license headers [rat:report] [rat:report] 111 Unknown Licenses [rat:report] [rat:report],code_debt,low_quality_code,"2e34ced206c388ae5ee78875d3d4d158c6ca0b29,7ffe5b7f87069c5fb1e44d240416d766c9db6a75,07b43463b8cb3aee80510c2cc3f70cd631f9a69b"
hadoop,6182,comment_0,"with this patch the releaseaudit warnings is down to 1. [rat:report] 1 Unknown Licenses [rat:report] [rat:report] Unapproved licenses: [rat:report] this is an empty file. I'm not sure why we have this empty java files. If we can remove this empty java file, we can get rid of this releaaseaudit warning. Thanks,",code_debt,dead_code,"2e34ced206c388ae5ee78875d3d4d158c6ca0b29,7ffe5b7f87069c5fb1e44d240416d766c9db6a75,07b43463b8cb3aee80510c2cc3f70cd631f9a69b"
hadoop,6182,comment_7,"Looks like the original patch inserted text blocks into the XML files ahead of the block: <?xml version=""1.0""? <?xml-stylesheet type=""text/xsl"" I corrected this for checkstyle in HADOOP-6185, but because the error is pervasive, it fails a wide variety of unit tests.",code_debt,low_quality_code,"2e34ced206c388ae5ee78875d3d4d158c6ca0b29,7ffe5b7f87069c5fb1e44d240416d766c9db6a75,07b43463b8cb3aee80510c2cc3f70cd631f9a69b"
hadoop,6182,comment_8,Let's remove the file separately once the build is stable.,code_debt,dead_code,"2e34ced206c388ae5ee78875d3d4d158c6ca0b29,7ffe5b7f87069c5fb1e44d240416d766c9db6a75,07b43463b8cb3aee80510c2cc3f70cd631f9a69b"
hadoop,6188,comment_2,I'd suggest to declare method's parameters final: it will improve readability and make testing easier.,code_debt,low_quality_code,38a84a6c98b05ff8d6084bd443fe8bb91ce3c322
hadoop,6188,comment_5,+1 on the new patch. I also manually tested that with the new jar TestHDFSTrash no longer fails. I'm not overly concerned with the non-final parameters as the function is just a few lines and the variables aren't being abused. Of note is the necessity of checking for null from listStatus. This is to avoid the fact that the listStatus implementation differs between and I've opened HDFS-538 to address this.,code_debt,low_quality_code,38a84a6c98b05ff8d6084bd443fe8bb91ce3c322
hadoop,6220,comment_0,This turns an interrupt into an includes the original exception as the cause. No test. This is a tricky one to set up a test for as you need to block the startup and then interrupt the starting thread.,test_debt,lack_of_tests,c5179b16ecc2c26f693eed692a6c556b6ac2e845
hadoop,6220,comment_5,"Would adding to the throws list of HttpServer::start, and rethrowing, be clearer? should be reserved for, well, interrupted I/O. Have you seen wrapped in logs or other evidence that would support this change? It is difficult to evaluate this without corresponding changes to the callers. What one might do with this is speculative in its current form.",code_debt,low_quality_code,c5179b16ecc2c26f693eed692a6c556b6ac2e845
hadoop,6220,comment_11,No tests as it's so hard to recreate this situation in a test; you need one thread sleeping and another interrupting.,test_debt,lack_of_tests,c5179b16ecc2c26f693eed692a6c556b6ac2e845
hadoop,6220,comment_14,+1 voting by self no tests as there is no easy way to generate the race condition.,test_debt,lack_of_tests,c5179b16ecc2c26f693eed692a6c556b6ac2e845
hadoop,6229,comment_2,Took Nikolas' suggestion. Created new class Turns out mapred already has such class. After this patch is committed I will remove mapred's one and make it use this one (from common). Also I will need to change HDFS to use this new exception instead of FileNotFound.,code_debt,low_quality_code,9b0b37e410afaf8fe0f7c6e0938a2095bef9e6d2
hadoop,6229,comment_4,I believe the original code isn't defensive enough and there's a possibility for NPE to be thrown if {{Path f}} happens to be {{null}}. I'd add an extra check for this as the very first line of the method. Looks good otherwise.,code_debt,low_quality_code,9b0b37e410afaf8fe0f7c6e0938a2095bef9e6d2
hadoop,6229,comment_8,"+1 This will be tested by the test being introduced in HDFS-303, right? Minor nit: we tend not to add serialVersionUID fields. (Any warnings in IDEs should be turned off.)",code_debt,low_quality_code,9b0b37e410afaf8fe0f7c6e0938a2095bef9e6d2
hadoop,6279,comment_0,"Does not include unit tests - this is an existing open issue HADOOP-3634. This patch is a one-liner, though. Manual verification steps: 1) Launch daemon with configured in 2) Go to /metrics on web UI 3) Verify totalMemoryM is present",test_debt,lack_of_tests,b903b25072a6e77ff66a3d669c71a0b0463a3f96
hadoop,6283,summary,The exception meessage in is not clear,code_debt,low_quality_code,710d5177685529c95b49eac02b7cc25a084d6b5f
hadoop,6313,comment_0,This patch 1. defines Syncable interface 2. makes FSDataOutputStream to implement Syncable interface 3. makes of RawLocalFileSystem to implement the Syncable interface and also makes it a 4. implement a unit test to test 2 and 3.,test_debt,lack_of_tests,b5c31f4ee65addc5e43b5601b5579ebd17f03475
hadoop,6313,comment_2,This patch removes Syncable implementation in RawLocalFileSystem and makes the default implementation of hflush & hsync to be flush in FSDataOutputStream.,code_debt,duplicated_code,b5c31f4ee65addc5e43b5601b5579ebd17f03475
hadoop,6374,summary,JUnit tests should never depend on anything in conf,architecture_debt,violation_of_modularity,c7a3c785f837505f3b79e22141fabd055aedd66c
hadoop,6374,description,The recent change to mapred-queues.xml that causes many mapreduce tests to break unless you delete out of your build tree is bad. We need to make sure that nothing in conf is used in the unit tests. One potential solution is to copy the templates into build/test/conf and use that instead.,architecture_debt,violation_of_modularity,c7a3c785f837505f3b79e22141fabd055aedd66c
hadoop,6413,comment_3,Updated to JUnit 4 (annotation) style,code_debt,low_quality_code,"a0e1451108a60344d965c3a5f50c634c66e4f157,1a2ab63f87bceaa08341c607c7237adcd19cd188"
hadoop,6435,description,"The public RPC.waitForProxy() method waits for Long.MAX_VALUE before giving up, ignores all interrupt requests. This is excessive. The version of the method that is package scoped should be made public. Interrupt swallowing is covered in HADOOP-6221 and can be done as a separate patch",code_debt,low_quality_code,d115e2cc3a15c7fb26cd9af509a6a5b203031914
hadoop,6492,comment_8,"I just committed this, but without the AvroFsInput class. Trivial as it is, it should probably be submitted as a separate issue, with some unit tests. Thanks, Aaron!",test_debt,lack_of_tests,4cd159e83eb54daef2ef054fef74d9b0c86ff059
hadoop,6536,comment_2,+1 Ravi's proposal. I would also add that fullyDelete should delete dangling links (it currently does but we should add a test). The current behavior is due to java making it difficult to identify symlinks.,test_debt,lack_of_tests,2c0598a9646bb8a43375d2dee83c0c60cbfc52df
hadoop,6536,comment_11,Changes look good. Can you add a test for deleting dangling links also?,test_debt,lack_of_tests,2c0598a9646bb8a43375d2dee83c0c60cbfc52df
hadoop,6584,comment_8,Small patch to fix javadoc warnings introduced into Y20 branch. Not to be committed.,code_debt,low_quality_code,4b34109a727ab585574bea5fed61e25d4a25c077
hadoop,6589,summary,Better error messages for RPC clients when authentication fails,design_debt,non-optimal_design,c93a9128ff14605fe9c08c0f5bb3fa374d852eaf
hadoop,6589,description,"Currently when authentication fails, RPC server simply closes the connection. Sending certain error messages back to the client may help user debug the problem. Of course, those error messages that are sent back shouldn't compromise system security.",design_debt,non-optimal_design,c93a9128ff14605fe9c08c0f5bb3fa374d852eaf
hadoop,6632,comment_3,A minor fix for the MR side to reuse filesystem handles,code_debt,low_quality_code,fa3a3bf5d8f3167f9725a3f91a0f4ae0481f24ea
hadoop,6644,summary,util.Shell method name - should use common naming convention,code_debt,low_quality_code,e346c2f4e08a38a80c7f505d8a8f3554b408e997
hadoop,6644,description,util.Shell method name - should use common naming convention,code_debt,low_quality_code,e346c2f4e08a38a80c7f505d8a8f3554b408e997
hadoop,6658,summary,Exclude Public elements in generated Javadoc,code_debt,low_quality_code,980f99bfb7b1c404b6e8346771803e113d16f71d
hadoop,6658,description,"Packages, classes and methods that are marked with the or annotation should not appear in the public Javadoc. Developer Javadoc generated using the ""javadoc-dev"" ant target should continue to generate Javadoc for all program elements.",code_debt,low_quality_code,980f99bfb7b1c404b6e8346771803e113d16f71d
hadoop,6709,description,"To make the FileSystem API in the 0.21 release (which should be treated as a minor release) compatible with 0.20 the deprecated methods removed in HADOOP-4779 need to be re-instated. They should still be marked as deprecated, however.",code_debt,low_quality_code,"b56827c59182d099b1719e93843f67eaf4948d49,494ef1e48c4f526d24929fe7ddf9270e5639b20e"
hadoop,6709,comment_2,+1 Looks good to me. Nit: please put @Deprecated for getBlockSize and getLength on their own line to be consistent with the other methods.,code_debt,low_quality_code,"b56827c59182d099b1719e93843f67eaf4948d49,494ef1e48c4f526d24929fe7ddf9270e5639b20e"
hadoop,6709,comment_4,More than a Nit. We have to leave the deprecation in. The goal of this Jira is to merely reinstate deleted methods. Old deprecation should remain.,code_debt,low_quality_code,"b56827c59182d099b1719e93843f67eaf4948d49,494ef1e48c4f526d24929fe7ddf9270e5639b20e"
hadoop,6727,summary,Remove from public FileContext APIs,code_debt,low_quality_code,322f9ac7b152713ae012cbeaa7ec6dc01860db15
hadoop,6727,description,"HADOOP-6537 added to the throws clause and java docs of FileContext public APIs. FileContext fully resolves symbolic links, exception is only used internally by FSLinkResolver. I'll attach a patch which updates the APIs and javadoc.",code_debt,low_quality_code,322f9ac7b152713ae012cbeaa7ec6dc01860db15
hadoop,6727,comment_1,"Just updating throws clauses and java docs, hence the lack of tests.",code_debt,low_quality_code,322f9ac7b152713ae012cbeaa7ec6dc01860db15
hadoop,6730,description,"Thanks to Eli, He noticed that there is no test for FileContext#Copy operation. On further investigation with the help of Sanjay we found that there is bug in *FileStatus dstFs = should be in try...catch block.",test_debt,low_coverage,fe49f6e47391ff3eb6372dc5acc592bbc2e8f683
hadoop,6730,comment_3,"Hey Ravi, per the previous comment I left the getFileStatus outside the try block so the is now swallowed silently and is thrown up to copy (whose API wants This deserves a comment. The patch just has util classes - perhaps you forgot to add the test class in your diff?",test_debt,lack_of_tests,fe49f6e47391ff3eb6372dc5acc592bbc2e8f683
hadoop,6730,comment_6,"Review of the patch: * Lines 47-52: These notes aren't necessary since the test writers should know about @Before and @After. Also, the provided methods (setUp() and tearDown()) conflict with the provided names. * Lines 55 & 56: These values should be all capitalized and final. Alternatively, it may be good to provide these values via functions so that implementing classes may override as needed. * Line 64: catch should be on the same line as closing brace, per our coding style * Line 70: fc should not be declared static. It is the responsibility of each implementing class to provide an instance of it. Also, fc should probably be protected * Line 91: Please provide assert message for assertion failure * Since this is a test that the copy method worked, we should prove it by comparing the contents of the copied file with the original. * Lines 135-138: This code: is a no-op and should be removed. Nits: * Rather than Assert.assertTrue() you can do a static import of * Line 57: Remove extra line * Line 47: ""Since this a junit 4"" ->""Since this is a junit 4 test""",code_debt,low_quality_code,fe49f6e47391ff3eb6372dc5acc592bbc2e8f683
hadoop,6730,comment_8,Updated patch with changes. 1. Removed unnecessary tearDown method. 2. Changed file read write to bytes from String.,code_debt,dead_code,fe49f6e47391ff3eb6372dc5acc592bbc2e8f683
hadoop,6794,comment_2,"The following need to be added to the patch. - configuration property is not present in common-project but is present and needed in mapreduce. - Nine settings for 'Job Summary Appender' are present and needed in mapreduce project but are not there in common project. - Two settings for mapreduce task logs are present and needed in mapreduce project but are not there in common project. -- -- - One more thing: copying this file over from mapreduce will not work because six settings for 'Security appender' are in common but not present in the mapreduce project's log4j.properties file. May be we should also split both the above files per project. But that may be long term, will open a separate ticket if you agree. Other issues - Minor: bin/rcc still refers to hadoop-core at one place. - when i run `ant binary` in hdfs bin directory isn't getting copied into the packaged binary, neither are the hdfs script files copied. - when i run `ant binary` in mapreduce, files aren't getting copied into the packaged bin directory Verified the rest of the changes in all the three patches, they look good. Still couldn't get the cluster up with different installation directories, will be trying in the meanwhile..",design_debt,non-optimal_design,32469f1bdff2bff774674f6a266ec7af215b8cde
hadoop,6826,comment_8,"hi amar, this API never made it to any ""release"" of Hadoop. Better to remove it now than to ship a release with a bad API.",code_debt,low_quality_code,35a4624771f84a60deb62a3807a3ac0701b0d7d0
hadoop,6839,description,"Develop a new method for getting the user list. Method signature is public ArrayList<StringAdd new attribute in system-test.xml file for getting userlist path. For submitting the jobs as different user a proxy user id is needed. So,get the available users from a userlist and then pass the user as proxy instead of hardcoding user id in a test.",code_debt,low_quality_code,a73dfef140f293cd191f56e412f0a7328faea437
hadoop,6839,comment_4,Looks good overall. Please fix the formatting (white space are inconsistent) and make exception and error messages more meaningful. Submit the patch for verification whenever is ready.,code_debt,low_quality_code,a73dfef140f293cd191f56e412f0a7328faea437
hadoop,6869,comment_3,"- Looks good with one nit. I'd suggest to have methods with fewer arguments simply call ones with more args. E.g. in this case instead of duplicating their exact implementation. You have done similar in already. - Also, in you already have a ref to the filesystem so you can just call its {{createFile}} methods instead of implementing your own logic just call to have new file created for you. - also you are writing some content into this file which seems to be an inadvertent side-effect. Doesn't look like a good idea.",code_debt,low_quality_code,d010dd60e1eef32d7ebf875a77b9a9e90a338a1f
hadoop,6869,comment_4,In continuation of the comment above: since there's {{getFS()}} method which exposes filesystem API from a particular daemon then creating additional 4 wrappers around the standard API doesn't make much sense to me. These wrappers are too obvious and then will simply crowd Herriot API without adding much value.,design_debt,non-optimal_design,d010dd60e1eef32d7ebf875a77b9a9e90a338a1f
hadoop,6869,comment_5,"Hi Cos, I do agree with you. However my requirement is creating the user define files and folders in task attempt id folder with different kind of permissions while job is running. Later while cleaning up, i just wanted make sure whether all the user defined files and folders are cleaned up or not for that job. So that I have defined the above methods for fulfilling my requirement. As my understand, i don't think so, the FileSystem is providing the no such straight forward and createNewFolder) like you mentioned. In the four methods, the first method creates the files with full permission.Suppose if user wants to give his own permissions while creating file in that case he can use the second method.Same way for folder creation also. I will remove this part in the code.",code_debt,dead_code,d010dd60e1eef32d7ebf875a77b9a9e90a338a1f
hadoop,6869,comment_6,This reasoning sounds like an argument to move this functionality up the stack to the MR cluster concrete implementation. But let's suppose they are useful enough and let them be in the Common. I do understand that. What I have said is that you need a generic enough method which does most of needed functionality (i.e. create a file with specified permissions) and a wrapper around it which creates a file with all permissions. The implementation of the latter is essentially a call to the former with *all* permissions being passed. This is already done in this very patch for the implementation of methods. Look at your own code.,code_debt,low_quality_code,d010dd60e1eef32d7ebf875a77b9a9e90a338a1f
hadoop,6879,comment_8,The patch isn't final yet because there's not way to run framework validation tests from Ant,test_debt,lack_of_tests,95649aca30b28497e0838b7d60688227d6ddd86f
hadoop,6879,comment_10,Base implementing method should use {{portNumber}} variable instead of a constant.,code_debt,low_quality_code,95649aca30b28497e0838b7d60688227d6ddd86f
hadoop,6879,comment_13,Minor nits: 1. The method needs to be implemented or removed: 2. There shouldn't be any @author tags in the javadoc. Other than that patch looks fine.,code_debt,dead_code,95649aca30b28497e0838b7d60688227d6ddd86f
hadoop,6925,comment_0,Patch fixes the read() implementation to correctly mask with 0xff before upcasting to int. Also augments the unit test to check the single-byte read() function - the new test fails before this patch.,test_debt,low_coverage,19e46e358e31ef730d7486c517a61aff5f3d0149
hadoop,7098,comment_0,"+1 The lack of a definition looks like an oversight (these variables were introduced in HADOOP-2551). is already honoured by bin/mapred, so no changes are needed there. Bernd, have you tested this manually? There's currently no easy way to write an automated test for this change.",test_debt,lack_of_tests,1cd6bac5ff13de3adcf13d780cb15dcec8bc8c98
hadoop,7098,comment_1,"I did test this manually indeed. The patch is in active use in my env. There are things like which might help setting up a test, but just for this small improvement it seem like overkill.",test_debt,lack_of_tests,1cd6bac5ff13de3adcf13d780cb15dcec8bc8c98
hadoop,7118,description,"In HADOOP-7082 I stupidly introduced a regression whereby will throw an NPE if it is called before any .get() call is made, since the properties member is not initialized. This is causing a failure in on my box, but doesn't appear to trigger any failures in the non-contrib tests since .get() is usually called first. This JIRA is to fix the bug and add a unit test for writeXml in common (apparently it never had a unit test)",test_debt,lack_of_tests,160b6fd4966f5189f988eaf0f094867fb2155c04
hadoop,7118,comment_3,"+1 LGTM. I would also convert the test to JUnitv4 but one can't have everything, I guess ;)",test_debt,expensive_tests,160b6fd4966f5189f988eaf0f094867fb2155c04
hadoop,7154,comment_10,"I was very confused by this discussion and dug into it a bit more; here's what I learned. The takeaway is, ARENA_MAX=4 is a win for Java apps. # Java doesn't use {{malloc()}} for object allocations; instead it uses its own directly {{mmap()}}ed arenas. # however, a few things such as direct {{ByteBuffer}}s do end up calling malloc on arbitrary threads. There's not much thread locality in the use of such buffers. As a result, the glibc arena allocator is using a lot of VSS to optimize a codepath that's not very hot. So decreasing the number of arenas is a win, overall, even though it will increase contention (the malloc arena locks are pretty cold so this doesn't matter much) and potentially increase cache churn. But fewer arenas should decrease total cache footprint by increasing reuse.",design_debt,non-optimal_design,ad459690e0032bf0387b2e40ff7f21e932048590
hadoop,7154,comment_13,"- I know this bug is pretty old, but do you mind doing me the favor of explaining this statement: Perhaps I am revealing too much of my naivety, but what issues the vmem size presents nor the reasons are necessarily obvious to me. The reason I ask is not directly related to this JIRA nor even Hadoop. I am just trying to learn more about the glibc change and its potential impacts. I've noticed high virtual memory size in another Java-based application (a Zabbix agent process if you care) and I'm struggling slightly to decide if I should worry about it. presents what appears to me to be a rational explanation as to why the virtual memory size shouldn't matter too much. I could push on Zabbix to implement a change to set MALLOC_ARENA_MAX and I feel relatively confident the change wouldn't hurt anything but I'm not sure it would actually help anything either. The Zabbix agent appears to be performing fine and the only reason I noticed the high vmem size was because someone pointed me to this JIRA and I did an audit looking for processes with virtual memory sizes that looked suspicious. I guess the biggest problem I have with the affect the glibc change has on reported vmem size is that it seems to make vmem size meaningless where previously you could get some idea about what a process was doing from its vmem size but your comment suggests maybe there are other things I should be concerned about as well. If you could share those with me I would greatly appreciate it and perhaps others will benefit as well. Thanks!",defect_debt,uncorrected_known_defects,ad459690e0032bf0387b2e40ff7f21e932048590
hadoop,7358,summary,Improve log levels when exceptions caught in RPC handler,design_debt,non-optimal_design,"e3c696551952941ae0e38afed0323b80e02168c0,c36326a462bd0ed99c3c8029e5f32ec4228ddc7d"
hadoop,7358,description,"When a server implementation throws an exception handling an RPC, the Handler thread catches it and logs it before responding with the exception over the IPC channel. This is currently done at INFO level. I'd like to propose that, if the exception is an unchecked exception, it should be logged at WARN level instead. This can help alert operators when they might be hitting some kind of bug.",design_debt,non-optimal_design,"e3c696551952941ae0e38afed0323b80e02168c0,c36326a462bd0ed99c3c8029e5f32ec4228ddc7d"
hadoop,7375,comment_0,AFS#getFileStatus is now called instead of which means fixRelativePart is no longer used to make the path absolute in FileContext relative to the working dir before passing the path to AFS right? Nit: lines 568 and 2231 need indenting. Otherwise looks great.,code_debt,low_quality_code,50688744ba4c8cd8962272066e6ce66dde6880aa
hadoop,7375,comment_4,"Ah, never mind, I thought it was previously calling FC#getFileStatus. +1 feel free to address the nits directly in the commit since it's just indentation.",code_debt,low_quality_code,50688744ba4c8cd8962272066e6ce66dde6880aa
hadoop,7684,comment_7,rpm package doesnt seem to include the historyserver and secondarynamenode init scripts. looks like and init.d script should be added to the list of init.d scripts in the spec file.,code_debt,low_quality_code,3edf3579c8b01e613eaa90976dfa80bab545a685
hadoop,7786,description,"HADOOP-4952 added a couple HDFS-specific configuration values to common (the block size and the replication factor) that conflict with the HDFS values (eg have the wrong defaults, wrong key name), are not used by common or hdfs and should be removed. After removing these I noticed the rest of FsConfig is only used once outside a test, and isn't tagged as a public API, I think we can remove it entirely.",code_debt,low_quality_code,"877021eb50dd0168b15a713602c0295bb79615eb,1852c2c1daa4339a6b2a45165c99a0498d019115"
hadoop,7786,comment_13,"I think ""dfs.block.size"" introduced in HADOOP-4952 was just a mistake. I think the intention was to have it ""dfs.blocksize"", same as in HDFS. I see a bunch of other file system keys in FsConfig, that duplicate their HDFS counterparts, but the naming is consistent across them. So I assume that extra dot between block and size was a typo. Having said that I think it is not a good idea to duplicate key definitions. The reason is exactly the typos or inconsistent renaming of those properties or the default values. E.g. in common = 32 MB}} while in HDFS = 64 MB}} This is really messy. The only method from {{FsConfig}} that is used in the code is I propose to remove everything else from {{FsConfig}} in order to avoid confusion. The rational behind this is that {{FsConfig}} should only contain keys that are specified in core-site.xml. The keys that belolng to hdf-site.xml shoud be described in {{DFSConfigKeys}}. It also looks that Tom's documentation change HDFS-671 describes keys consistently with this assumption.",code_debt,duplicated_code,"877021eb50dd0168b15a713602c0295bb79615eb,1852c2c1daa4339a6b2a45165c99a0498d019115"
hadoop,7786,comment_14,"I agree with Konst, we should remove the overlap between FsConfig and DFSCOnfig.",code_debt,duplicated_code,"877021eb50dd0168b15a713602c0295bb79615eb,1852c2c1daa4339a6b2a45165c99a0498d019115"
hadoop,7786,comment_17,"Eli, the patch looks great. One thing: Instead of It should be +1 other than that. Findbugs come from protobuf packages untouched here. Don't understand why jenkins reported them as new.",code_debt,low_quality_code,"877021eb50dd0168b15a713602c0295bb79615eb,1852c2c1daa4339a6b2a45165c99a0498d019115"
hadoop,7924,comment_1,"- maybe we should rename FailoverController to or even just ManualFailover? IMO has always had a connotation of being some kind of daemon. - in the preFailoverCheck, I'd expect us to support the case where ""svc1"" is down -- ie the admin has noticed the active crashed, and therefore wants to initiate failover to svc2. We should treat this the same as a failure in calling transitionToStandby - i.e fence and continue. - {{svc1}} and {{svc2}} aren't very descriptive parameter names - maybe {{fromSvc}} and {{toSvc}} or {{oldActive}} and {{newActive}}? The javadoc in {{failover(...)}} is also incorrect here for {{svc2Name}} - in no need to duplicate the standard exception javadoc from its parent class",code_debt,low_quality_code,fb9cdcfa60cb641faded2d3843c81e969ac33147
hadoop,7971,comment_6,Updated with better error msg and exit status .,code_debt,low_quality_code,5ffff3aea946b762e9ee9f427109bcdb92e12328
hadoop,7974,comment_0,Patch that uses a get-parent call instead of hacking with strings.,code_debt,low_quality_code,aee1bb81c69f11ace3d25a89c66441250b61e79d
hadoop,8084,comment_2,No I haven't done benchmarks. But it seemed to make sense to avoid a copy if it could be avoided.,code_debt,slow_algorithm,ae7e43139dda16c501f6d7606f04d631e0a23c3e
hadoop,8124,summary,Remove the deprecated Syncable.sync() method,code_debt,dead_code,b2f67b47044a5cbb0c3aaac83299afba541aa771
hadoop,8124,description,The Syncable.sync() was deprecated in 0.21. We should remove it.,code_debt,dead_code,b2f67b47044a5cbb0c3aaac83299afba541aa771
hadoop,8124,comment_0,"- removes Syncable.sync(); - removes the deprecated - removes unnecessary ""throws IOException"" declarations.",code_debt,dead_code,b2f67b47044a5cbb0c3aaac83299afba541aa771
hadoop,8168,comment_5,I think you meant {{Math.min}}. Although I'd suggest maybe something like this to avoid spurious whitespace:,code_debt,low_quality_code,32d3ed55d0a463dbe19010edcc42cae01cb315c0
hadoop,8168,comment_7,"Hi Daryn, Your fix looks a bit cleaner than mine, thanks! -Eugene",code_debt,low_quality_code,32d3ed55d0a463dbe19010edcc42cae01cb315c0
hadoop,8210,comment_8,"I think it's worth changing the return type of this function to LinkedHashSet, so it's clear that the ordering here is on purpose. Perhaps also add a comment here saying something like: - Nits: please un-abbreviate ""first"" for better readability. Also, ""e.g."" instead of ""Eg."" -- or just say ""For example"" - I think it's more idiomatic to just put the postincrement inside the []s - - there's a small spurious whitespace change in NetUtils.java - looks like the pom change is still in this patch (redundant with HADOOP-8211)",code_debt,low_quality_code,950273bde4ccfc3721667898bbef39660fa0ad25
hadoop,8245,summary,Fix flakiness in,test_debt,flaky_test,b74d7427855eb7e20be70155c11acac0e333bd6a
hadoop,8245,comment_0,"For problem #1, the solution is the same as is already done in some other test cases. We just need to add a workaround to clear the ZK MBeans before running the tearDown method. It's a hack, but in the absense of a fix for ZOOKEEPER-1438, it's about all we can do. I spent some time investigating problem #2. The bug is as follows: - these test cases create a new and call on it before running the main body of the tests. Although they don't call {{joinElection()}}, the creation of the elector does create a {{zkClient}} object with an associated Watcher. - in the test case, we shut down and restart ZK. This causes the above Watcher instance to fire its Disconnected and then Connected events. There was a bug in the handling of the Connected event that would cause it to re-monitor the lock znode regardless of whether it was previously in the election. - So, when ZK comes back up, there was not two but *three* electors racing for the lock. However, two of the electors actually corresponded to the same dummy service. In some cases this race would be resolved in such a way that the test timed out. I don't think this is a problem in practice, since the ""formatZK"" call runs in its own JVM in the current code. However, it's worth fixing to get the tests to not be flaky, and to have a more reasonable behavior. There are several fixes to be done: - Add extra asserts for to catch cases where we might accidentally re-join the election when we weren't supposed to be in it. - Fix the handling of the ""Connected"" event to only re-join if the elector wants to be in the election - Cause exceptions thrown by watcher callbacks to be propagated back as fatal errors Will post a patch momentarily.",architecture_debt,using_obsolete_technology,b74d7427855eb7e20be70155c11acac0e333bd6a
hadoop,8245,comment_1,Attached patch fixes the flaky behavior for me. I looped for 30+ minutes and didn't see failures after applying this.,test_debt,flaky_test,b74d7427855eb7e20be70155c11acac0e333bd6a
hadoop,8283,description,Tests in projects other than common need to be able to change whether the token service uses an ip or a host.,code_debt,low_quality_code,fccbc53357d7387724f4468c9260b1942811b686
hadoop,8288,summary,Remove references of mapred.child.ulimit etc. since they are not being used any more,code_debt,dead_code,f6cadd8610d9ca371d8278f486f17144a76c7bbf
hadoop,8288,description,"Courtesy Philip Su, we found that were not being used at all. The configuration exists but is never used. Its also mentioned in mapred-default.xml and . Also the method in Shell.java is now useless and can be removed.",architecture_debt,using_obsolete_technology,f6cadd8610d9ca371d8278f486f17144a76c7bbf
hadoop,8316,description,"HADOOP-7633 made hdfs, mr and security audit logging on by default (INFO level) in log4j.properties used for the packages, this then got copied over to the non-packaging log4j.properties in HADOOP-8216 (which made them consistent). Seems like we should keep with the v1.x setting which is disabled (WARNING level) by default. There's a performance overhead to audit logging, and HADOOP-7633 provided not rationale (just ""We should add the audit logs as part of default confs"") as to why they were enabled for the packages.",design_debt,non-optimal_design,e2af2f1b87c300dae0b3d816f5a64b0dcd006c35
hadoop,8316,comment_0,"Patch attached. - update to NullAppender in log4j.properties, it is set explicitly by default in the bin and env scripts, so this is mostly a nop - and now default to the NullAppender in log4j.properties. Update hdfs.audit.logger in hadoop-env.sh to match. This is being made configurable in HADOOP-8224. mapred.audit.logger is not set in the bin or env scripts and is dead code, filed HADOOP-8392 for that (and to hookup RM/NM). Testing, verified the hdfs audit log is no longer automatically created and logged to when run from a tarball install.",code_debt,dead_code,e2af2f1b87c300dae0b3d816f5a64b0dcd006c35
hadoop,8341,description,Now that the precommit build can test hadoop-tools we need to fix or filter the many findbugs warnings that are popping up in there.,code_debt,low_quality_code,a9808de0d9a73a99c10a3e4290ec20778fed4f24
hadoop,8358,description,"Looks easy to fix, and we should avoid using old config params that we ourselves deprecated.",code_debt,low_quality_code,d2b57e00bc277442c2eb97630e86352ef44a1df2
hadoop,8358,comment_0,"This patch is in HADOOP cause its slightly wide/crossproject. Lemme know if I should split it though. Changes summary: * The NodeManager log (from where I noticed this first) showed this cause there was a hdfs-default.xml config set for this deprecated prop, and its web filter loaded that up. Cleaned up hdfs-default.xml. * The old property lookup existed in JspHelper in HDFS. Changed that to use the new property. * Cleaned up the usage of constants for this property, via classes instead of its own constant refs. * Added new prop and default to core-default.xml",code_debt,low_quality_code,d2b57e00bc277442c2eb97630e86352ef44a1df2
hadoop,8358,comment_4,Any further comments on the patch? Its quite trivial a change and helps remove unnecessary WARN noise.,code_debt,dead_code,d2b57e00bc277442c2eb97630e86352ef44a1df2
hadoop,8358,comment_10,"Failing test is unrelated to this change. Findbugs succeeded this time, so the previous issue was something else on trunk at the time or was a flaky result.",test_debt,flaky_test,d2b57e00bc277442c2eb97630e86352ef44a1df2
hadoop,8395,summary,Text shell command unnecessarily demands that a SequenceFile's key class be WritableComparable,design_debt,non-optimal_design,810ae618fd2b308bc65a3264a90233658010380e
hadoop,8395,description,"Text from Display set of Shell commands (hadoop fs -text), has a strict subclass check for a loaded key class to be a subclass of WritableComparable. The sequence file writer itself has no such checks (one can create sequence files with just plain writable keys, comparable is needed for sequence file's sorter alone, which not all of them use always), and hence its not reasonable for Text command to carry it either. We should relax the check and simply just check for ""Writable"", not",design_debt,non-optimal_design,810ae618fd2b308bc65a3264a90233658010380e
hadoop,8405,summary,ZKFC tests leak ZK instances,code_debt,low_quality_code,7b1d347b43cc0c49bdf6e72d377fad5789013e71
hadoop,8405,description,"The ZKFC code wasn't previously terminating the ZK connection in all cases where it should (eg after a failed startup or after formatting ZK). This didn't cause a problem for CLI usage, since the process exited afterwards, but caused the test results to get clouded with a lot of ""Reconecting to ZK"" messages, which make the logs hard to read.",code_debt,low_quality_code,7b1d347b43cc0c49bdf6e72d377fad5789013e71
hadoop,8431,comment_6,"It should just print the usage. Eg in the following I'd remove the ERROR log, the backtrace and the ""Invalid arguments"" log.",code_debt,low_quality_code,404ab8ec66434f347ec929c73f9e7aceee28514c
hadoop,8548,description,Precommit builds show an incorrect link for javac warnings. Note that 'trunk' appears twice. Do we need $(basename $BASEDIR) in the following? Other places don't have it.,code_debt,low_quality_code,72a5f92e69f68a8c9757dbb2e7af55a84491b6b3
hadoop,8597,comment_1,This looks like a useful addition. Can you please add a unit test for it?,test_debt,lack_of_tests,de2efbe3baeccbf59ab7fb993de922ef6eedd099
hadoop,8597,comment_6,Jenkins says that should be static.,code_debt,low_quality_code,de2efbe3baeccbf59ab7fb993de922ef6eedd099
hadoop,8633,comment_4,The changes look OK. I'm not sure I really like TargetFileSystem. I just don't see a lot of benefit from making TargetFileSystem. I think it would be cleaner to just have your own list of Paths to delete and then delete them in the finally block. Having it be a FileSystem just seems confusing to me. Because TargetFileSystem is never added to the FileSystem cache I assume that if a System.exit is called somewhere while processing the command the TargetFileSystem will not be closed and the files will not actually be deleted on exit. Is this important?,code_debt,low_quality_code,12d0e025919cfb375ecb1739fe99e92421beb56c
hadoop,8819,summary,Should use && instead of & in a few places in,code_debt,low_quality_code,6afabd7e573ea436186f150753833936f52d2d00
hadoop,8819,description,Should use && instead of & in a few places in,code_debt,low_quality_code,6afabd7e573ea436186f150753833936f52d2d00
hadoop,8819,comment_6,"Note that the change in FTPFileSystem actually is a behavior change, and perhaps an incompatible one. All of the rest of these changes seem harmless, but that one seems a little suspect.",design_debt,non-optimal_design,6afabd7e573ea436186f150753833936f52d2d00
hadoop,8819,comment_8,"In this code if {{created}} is true it enters the if condition and then executes If {{created}} is false that part of the code is not entered. Aaron, can you add details on why this would change the behavior?",code_debt,low_quality_code,6afabd7e573ea436186f150753833936f52d2d00
hadoop,8843,comment_4,"Looks good to me, except for one small nit: it would be worth a javadoc comment on OLD_CHECKPOINT saying something like: /** Format of checkpoint directories used prior to Hadoop 0.23. */ and then a small comment at the point where it's used, like // Check for old-style checkpoint directories left over after an upgrade from Hadoop 1.x Otherwise I think people may be confused what ""old"" refers to, a few years down the line.",code_debt,low_quality_code,6ddbb22567bf6bb07cd3d84c7c5a34deaa22e691
hadoop,8866,description,"does O(N) calls LinkedList#get() in a loop, rather than using an iterator. This makes query O(N^2), rather than O(N).",code_debt,slow_algorithm,1ced82cc812b830cf755d2d300351ea92a0dc9a2
hadoop,8866,comment_1,"While you're at it, why not get rid of size() and numeric iteration too? eg:",code_debt,low_quality_code,1ced82cc812b830cf755d2d300351ea92a0dc9a2
hadoop,8912,description,Source code in hadoop-common repo has a bunch of files that have CRLF endings. With more development happening on windows there is a higher chance of more CRLF files getting into the source tree. I would like to avoid that by creating .gitattributes file which prevents sources from having CRLF entries in text files. I am adding a couple of links here to give more primer on what exactly is the issue and how we are trying to fix it. # # This issue for adding .gitattributes file to the tree.,design_debt,non-optimal_design,"1dd6522903038ddeb20deb5a447a9d79c62cd2c9,62c70a228fe2bdc90d43d477130a72916698f298"
hadoop,8912,comment_7,"Ah, got it. Thanks for the explanation. It will help for those developers who clone the git repo and `git add ...' their files before generating a patch. In that case, it will help somewhat to commit this, but it won't solve the problem 100% of the time. Raja, do you happen to know if there's a semantically equivalent thing we could do for svn, to ensure that committers don't check in bad line endings?",design_debt,non-optimal_design,"1dd6522903038ddeb20deb5a447a9d79c62cd2c9,62c70a228fe2bdc90d43d477130a72916698f298"
hadoop,8929,description,"The new SampleQuantiles class is useful in the context of benchmarks, but currently there is no way to print it out outside the context of a metrics sink. It would be nice to have a convenient way to stringify it for logging, etc. Also: - made it Comparable and changed the HashMap to TreeMap so that the printout is in ascending percentile order. Given that this map is always very small, and snapshot() is only called once a minute or so, the runtime/memory differences between treemap and hashmap should be negligible. - changed the behavior to return null instead of throw, because all the catching, etc, got pretty ugly. In implementing toString, I figured I'd clean up the other behavior along the way.",code_debt,low_quality_code,e0db1e9e1071f19426becc8adbc7055945a9d704
hadoop,8929,comment_2,"Sorry, should have explained the patch in more detail: - made it Comparable and changed the HashMap to TreeMap so that the printout is in ascending percentile order. Given that this map is always very small, and snapshot() is only called once a minute or so, the runtime/memory differences between treemap and hashmap should be negligible. - changed the behavior to return null instead of throw, because all the catching, etc, got pretty ugly. In implementing toString, I figured I'd clean up the other behavior along the way.",code_debt,low_quality_code,e0db1e9e1071f19426becc8adbc7055945a9d704
hadoop,8985,description,"Currently .proto files use java_package to specify java packages in proto files, but namespace are not specified for other languages such as cpp, this causes name collision in cpp. we can add namespace declarations to avoid this. In Java, the package specifier is used as the Java package, unless you explicitly provide a option java_package in your .proto file. So the original java package will not be affected. About namespace name, how about in cpp) for all common sub-project proto files, and in cpp) for all hdfs sub-project proto files?",code_debt,low_quality_code,22377ebae54ed9bd71861921ba5ba04aadd8397e
hadoop,9254,comment_4,"+1, lgtm. One minor nit: has an extra license comment in the middle of the imports which I'll cleanup on checkin.",code_debt,low_quality_code,d9610d5299cb8e7f2321504dd2858ccff51df123
hadoop,9259,summary,should be less brittle in teardown,design_debt,non-optimal_design,a10055cf6de058d10dec54705a6de746ecca111f
hadoop,9259,description,the teardown code in assumes that {{fs!=null}} and that it's OK to throw an exception if the delete operation fails. Better to check the {{fs}} value and catch and convert an exception in the {{fs.delete()}} operation to a {{LOG.error()}} instead. This will stop failures in teardown becoming a distraction from the root causes of the problem (that your FileSystem is broken),design_debt,non-optimal_design,a10055cf6de058d10dec54705a6de746ecca111f
hadoop,9259,comment_0,straightforward hardening of teardown logic,design_debt,non-optimal_design,a10055cf6de058d10dec54705a6de746ecca111f
hadoop,9267,comment_0,"Patch for {{hadoop}} and {{hdfs}}. Ran TestHDFSCLI and TestCLI successfully. No unit tests since this is kind of trivial, but I could wrangle it if desired.",test_debt,lack_of_tests,8271fae9211cbd60d506872c4a1486ded9dc8ebe
hadoop,9278,summary,HarFileSystem may leak file handle,code_debt,low_quality_code,0b565a967d7abb1bdab9827f1209118bf17e4471
hadoop,9278,description,fails on Windows due to invalid HAR URI and file handle leak. We need to change the tests to use valid HAR URIs and fix the file handle leak.,code_debt,low_quality_code,0b565a967d7abb1bdab9827f1209118bf17e4471
hadoop,9278,comment_0,"The invalid HAR URI is caused here in On Windows, this creates a path that includes the drive specifier. Later, this gets used to construct a HAR URI for testing, which isn't valid, because it doesn't adhere to the the protocol-host format used by HAR URIs. The file handle leak happens in This method opens the _masterindex file, but not all code paths guarantee that the file will be closed. For example, parsing an invalid version throws an exception before the _masterindex file gets closed. There is a test that covers this case, so it leaks a file handle when that test runs. On Windows, this ultimately causes tests to fail during their post-test cleanup, because Windows file locking behavior causes the delete to fail.",code_debt,low_quality_code,0b565a967d7abb1bdab9827f1209118bf17e4471
hadoop,9305,comment_0,"Here's a simple patch which addresses the issue. The only behavior changes are to conditionally load the AIX64LoginModule and the UsernamePrincipal classes if we're on a 64-bit AIX box, instead of the AIXLoginModule and AIXPrincipal classes. This patch also refactors the getOsPrincipalClass a little bit to reduce some code repetition. No tests are included since to test this properly would require an AIX box. I tested this manually by running with both 32-bit and 64-bit AIX clients and confirming that it works as expected, both with and without Kerberos enabled. Without the patch only 32-bit clients will work. I also ensured there are no regressions by testing the Hadoop client with both IBM Java and Sun Java on Linux both with and without Kerberos enabled. Everything worked as expected.",code_debt,duplicated_code,a3ddc8ef968fc175d41f5232e026e9031553d992
hadoop,9305,comment_1,"+1 pending jenkins. ATM, what about opening a JIRA to clean this spaghetti of conditionals replacing it with a MAP and a simple struct having the needed settings?",code_debt,low_quality_code,a3ddc8ef968fc175d41f5232e026e9031553d992
hadoop,9336,description,"Querying is synch'ed and inefficient for short-lived RPC requests. Since the connection already contains the UGI, there should be a means to query it directly and avoid a call to",design_debt,non-optimal_design,c5368561f9b0326cbd185ace9cbf7b37be7b3f2c
hadoop,9336,comment_1,"The semantics will be returning the UGI of the connection, so it will always report the UGI of the original user making the connection, not of any subsequent {{UGI.doAs}} calls. However, this jira will not universally affect anything that doesn't explicitly use it. I intend for only the {{FSNamesystem}} audit calls to currently use it to reduce the significant performance bottlenecks we are encountering. I'm contemplating filing another jira for {{UGI.doAs}} to cache a stack of UGIs. That would greatly accelerate in general.",code_debt,slow_algorithm,c5368561f9b0326cbd185ace9cbf7b37be7b3f2c
hadoop,9369,comment_4,"Hi Karthik, the patch seems fine to me, and I agree that the patch is so simple and writing a test sufficiently difficult that it seems unnecessary to write a test for this. One question - can you comment on the ramifications of this issue? How does it manifest itself? And what triggers it?",test_debt,lack_of_tests,"21a1c8acbafc8364a204f23369a3adbefdff1f3e,06f27d8731e6f2b5c8bbaf6259a3f02a1b43d571"
hadoop,9669,summary,Reduce the number of byte array creations and copies in XDR data manipulation,code_debt,low_quality_code,c16442c45958f6c6a106d2a3949d84b1404a256c
hadoop,9669,comment_0,Here is a more version that utilizes Java's ByteBuffer. It should be more efficient. The APIs are compatible with the previous version. The APIs might not be ideal towards an efficient implementation of XDR serialization / deserialization. I'm tracking my proposals to the APIs in a separate JIRA.,design_debt,non-optimal_design,c16442c45958f6c6a106d2a3949d84b1404a256c
hadoop,9669,comment_3,"Thanks, Haohui. Some comments: 1. please try to keep the original javadoc for the same named methods 2. can you make ""State state"" as final? 3. please fix the javadoc /** check if the rest of data has more than <len""len"" is not visible in generated javadoc 4. readFixedOpaque still has a copy not sure if it's possible to generat a read-only bytebuffer from another bytebuffer 5. it would be nice to remove the extra copy for writeFixedOpaque For 4 and 5, I am ok if you think it's out of scope of this JIRA.",code_debt,low_quality_code,c16442c45958f6c6a106d2a3949d84b1404a256c
hadoop,9669,comment_6,"+1. New patch looks good. Nit: I noticed that most NFS related RPC call response size is less than 256 bytes, so we can have as 256 instead of 512.",code_debt,low_quality_code,c16442c45958f6c6a106d2a3949d84b1404a256c
hadoop,9748,description,"EnsureInitialized is always sync'ed on the class, when it should only sync if it actually has to initialize.",design_debt,non-optimal_design,aba335279a55573dd7d6f37d032794a2b190fe25
hadoop,9748,comment_0,Minor change. This helps reduce unnecessary class lock contention between such common methods as and etc.,design_debt,non-optimal_design,aba335279a55573dd7d6f37d032794a2b190fe25
hadoop,9748,comment_4,"ensureInitialized() forced many frequently called methods to unconditionally acquire the class lock. This patch certainly reduces the lock contention, but the highly contended getCurrentUser() can still block many threads. I understand this is being addressed in HADOOP-9749. There may be something else we can do to improve this for namenode. According to the ""worst"" jstack of a busy namenode I took, there were 29 BLOCKED handler threads. All of them were blocked on the UGI class lock. Here is the breakdown: - 2 ensureInitialized() - from non static synchronized methods. This Jira will unblock these. - 27 getCurrentUser() Among the 27 threads that were blocked at getCurrentUser(), - 18 - from in most namenode RPC methods - 8 - getBlockLocations() - 1 I think FSPermissionChecker can be modified to be created with a passed in UGI. FSNamesystem can the one already stored in RPC server by calling getRemoteUser(). This will eliminate a bulk of getCurrentUser() calls from namenode RPC handlers. A similar change can be made to mkdirs. Block token generation is not as straightforward. Even without it we can eliminate majority of the calls. We could potentially do the same for other RPC servers. I will file a HDFS jira for this. HADOOP-9749 is still needed since getCurrentUser() is used everywhere beyond namenode RPC server. +1 for this patch.",design_debt,non-optimal_design,aba335279a55573dd7d6f37d032794a2b190fe25
hadoop,9763,comment_0,adds LightWeightCache. Still need to test it.,test_debt,lack_of_tests,3024030b24d47218b210018b5bfa8d5c88b4192c
hadoop,9763,comment_4,"Nicholas, this should be moved to hadoop-common, right?",architecture_debt,violation_of_modularity,3024030b24d47218b210018b5bfa8d5c88b4192c
hadoop,9763,comment_5,GSet and LightWeightGSet are currently in hdfs. I agree that we should move them to common. Let's do it in a separated issue?,architecture_debt,violation_of_modularity,3024030b24d47218b210018b5bfa8d5c88b4192c
hadoop,9763,comment_6,"The patch looks good to me. Some thoughts after discussing with : 1. Maybe we do not need to refresh a cache entry's access time and position in the priority queue when the entry is accessed, since the expected timeout on the client side is based on the time the first request is sent. 2. It would be better if we can add an upper limit for the size of the GSet. This can guarantee NN continues to function even when a large amount of retry requests come within a short period of time.",design_debt,non-optimal_design,3024030b24d47218b210018b5bfa8d5c88b4192c
hadoop,9763,comment_10,adds more tests.,test_debt,lack_of_tests,3024030b24d47218b210018b5bfa8d5c88b4192c
hadoop,9763,comment_13,- adds more cases to the short test; - disables the longer test; - randomizes initial value of currentTestTime and the increments; - adds slightly more comments in,code_debt,low_quality_code,3024030b24d47218b210018b5bfa8d5c88b4192c
hadoop,9791,description,We've seen historically that paths longer than 260 chars can cause things not to work on Windows if not properly handled. Filing a tracking Jira to add a native io test case with long paths for new FileUtil access check methods added with HADOOP-9413.,test_debt,low_coverage,3720956a6afc2014fbe2b942300d0723486f7d79
hadoop,9929,comment_3,I don't believe the globbing code I cleaned up for v23+ has these issues. You want want to use it for reference.,code_debt,low_quality_code,1c4f4a38ca958dbb8be911b7b7d6bf57722b7514
hadoop,10067,comment_2,looks better. The version declaration needs to go into for unified versions across the entire project. HADOOP-9594 is an example of this,architecture_debt,violation_of_modularity,bd5b23f4ce1f0780d28b592688f78cd9a37a4ead
hadoop,10067,comment_3,Moved version declaration into,architecture_debt,violation_of_modularity,bd5b23f4ce1f0780d28b592688f78cd9a37a4ead
hadoop,10106,summary,Incorrect thread name in RPC log messages,code_debt,multi-thread_correctness,763f073f41e3eaa9ecd11c6ec0b76234739272aa
hadoop,10106,description,"INFO IPC Server listener on 8020: readAndProcess from client 10.115.201.46 threw exception Unknown out of band call #-2147483647 This is thrown by a reader thread, so the message should be like INFO Socket Reader #1 for port 8020: readAndProcess from client 10.115.201.46 threw exception Unknown out of band call #-2147483647 Another example is which can also be called by handler thread. When that happend, the thread name should be the handler thread, not the responder thread.",code_debt,multi-thread_correctness,763f073f41e3eaa9ecd11c6ec0b76234739272aa
hadoop,10106,comment_3,You seemed to have unnecessarily moved the doRead() function's location in the server.java file. Can you please leave it in its original place in the file and please resubmit the patch.,architecture_debt,violation_of_modularity,763f073f41e3eaa9ecd11c6ec0b76234739272aa
hadoop,10169,summary,remove the unnecessary synchronized in JvmMetrics class,code_debt,dead_code,71b4903ea41a17c9305b25d24be786aed5b6e82f
hadoop,10169,description,"When i looked into a HBase JvmMetric impl, just found this synchronized seems not essential.",code_debt,dead_code,71b4903ea41a17c9305b25d24be786aed5b6e82f
hadoop,10169,comment_10,Thanks for updating the patch. One last nit is that this code: can be simplified to the following so there aren't so many return statements to track:,code_debt,low_quality_code,71b4903ea41a17c9305b25d24be786aed5b6e82f
hadoop,10214,summary,Fix multithreaded correctness warnings in,code_debt,low_quality_code,519d5d3014c2a4c77d4b3b575bc34807e7c0ec50
hadoop,10214,description,"When i worked at HADOOP-9420, found the unrelated findbugs warning:",code_debt,low_quality_code,519d5d3014c2a4c77d4b3b575bc34807e7c0ec50
hadoop,10214,comment_0,"A simple fix. I think it was introduced by: r1556103 | vinodkv | 2014-01-07 09:56:11 +0800 (Tue, 07 Jan 2014) | 2 lines YARN-1029. Added embedded leader election in the ResourceManager. Contributed by Karthik Kambatla. The above change made public, so the multi-threaded warning be observed.",code_debt,multi-thread_correctness,519d5d3014c2a4c77d4b3b575bc34807e7c0ec50
hadoop,10291,comment_0,The root cause is that s property used by some test cases has a side effect. The tests set The broken test case assumed that it will be set when other tests are invoked. The fix is to explicitly set the property instead of depending on the execution order.,design_debt,non-optimal_design,874d3d4f5a5f3ba2ca9860738e31d1718187cccf
hadoop,10295,comment_0,"Initial patch for review. The patch adds a option to distcp. If the option is set, it simply checks the checksum type for each source file and uses it for creating the corresponding temp file in the target FS. Still need to add unit tests and do some system tests.",test_debt,lack_of_tests,067d52b98c1d17a73b142bb53acc8aaa9c041f38
hadoop,10295,comment_1,"Funny, I have been preparing a patch for this very same issue for a week. Some comments regarding your patch: * instead of a new commandline option, it may be better to extend FileAttribute enum * and are probably HDFS specific (although being available in hadoop-common). I opened HADOOP-10297 for having * Instead of doing two instanceof check, it is possible to use the super class * is not equivalent of setting overwrite argument to true. From it is * Having a test to check if the option actually works would be a nice to have (according to me) Since I also have a patch, I'll attach it to this ticket to, and let have a hadoop maintainer help us sorting them out :)",design_debt,non-optimal_design,067d52b98c1d17a73b142bb53acc8aaa9c041f38
hadoop,10295,comment_4,"Thanks for the comment ! That's right. I also found this problem in my patch. I personally like your idea in HADOOP-10297. That can simplify the logic there. However, FileChecksum is a public API marked as stable, to add a new abstract method there may cause incompatibility (e.g., other ppl may have implemented their own FileChecksum). A workaround here can be adding getChecksumOpt() to FileChecksum and let it return null. Totally agree. Actually I've added a new unit test in my 001 patch, and the new unit test is very similar to yours :) I thought about this problem. To me checksum type may be a little bit different from other file attributes, since other file attributes are all metadata stored in NN. Thus in my first patch I just add a new option. But now I think to put the checksum type in the FileAttribute enum should be more clear. Currently I have a 001 patch which fixes the CreateFlag bug and adds a unit test. My original plan is to post it after I finish system test in my local cluster. But since you've worked on this issue for some time and already have a decent patch, I'd like to review your patch and commit it when it is ready.",code_debt,complex_code,067d52b98c1d17a73b142bb53acc8aaa9c041f38
hadoop,10295,comment_6,"Besides the concern on FileChecksum, some other comments on the current patch: # We may want to change ""checksum"" to ""checksumtype"" in the changes of PRESERVE_STATUS and FileAttribute. # We actually do not need to pass a FileChecksum to In if we need to preserve the checksum type, we get the checksum type of the source file and we reuse this checksum in compareCheckSums(). In that case we only need to call once (note that getFileChecksum is very costly). # We should use in the following change (see boolean, int, short, long, Progressable)) # The new added unit test does not cover there scenario where source files have different REAL checksum types (CRC32 and CRC32C), in which case copy with preserving checksum type should succeed and the original checksum types should be preserved in the target FS. We should add unit tests for this. # There are some unnecessary whilespace and blank line changes.",code_debt,low_quality_code,067d52b98c1d17a73b142bb53acc8aaa9c041f38
hadoop,10295,comment_8,"Thanks for working on this, Jing. One thing to note is that the block size needs to be identical in addition to the checksum parameters in order for the checksums to match. So it might make more sense to introduce an option to preserve the two together.",design_debt,non-optimal_design,067d52b98c1d17a73b142bb53acc8aaa9c041f38
hadoop,10295,comment_14,"Thanks for the review, Nicholas and Sangjin! , that is originally implicitly contained in the FileSystem#create call (see boolean, int, short, long, Progressable)). I just pulled it out to make the code not too long.",code_debt,low_quality_code,067d52b98c1d17a73b142bb53acc8aaa9c041f38
hadoop,10343,description,in we print logs at info level when we drop responses. This causes lot of noise on the console.,code_debt,low_quality_code,2417ca71d5115f16bd13a737087dab5edd04fb99
hadoop,10353,description,"The class uses a plain {{HashMap}} for caching. When the number of inserted values exceeds the the map's load threshold, it triggers a rehash. During this time, a different thread that performs a get operation on a previously inserted key can obtain a null value instead of the actual value associated with that key. The result is a NPE potentially being thrown when calling protocol)}} concurrently.",code_debt,multi-thread_correctness,"e5ccaa5d0341bc62d625a94a02b409efaca65051,0d03b29f912aa6c9ead58238005633e480ee85f3"
hadoop,10374,description,"There are valid use cases for accessing the InterfaceAudience annotations programatically. In HBase, we are writing a unit test to check whether every class in the client packages are annotated with one of the annotations. Plus, we are also thinking about a golden file to containing all public method sigs, so that we can ensure public facing API-compatibility from a unit test. Related: HBASE-8546, HBASE-10462, HBASE-8275",test_debt,lack_of_tests,d7aa12e0e6849235523d404b3c97b3b567f58001
hadoop,10427,comment_2,"patch adds documentation, synchronized keywords and synchronization via a read write lock. it is obvious and not that easy to write a testcase",test_debt,lack_of_tests,98a98ea0c57d01b875b820f53d43dbf885d07711
hadoop,10432,description,"The method is private and takes a configuration to fetch a hardcoded property. Having a public method to resolve a verifier based on the provided value will enable getting a verifier based on the verifier constant (DEFAULT, STRICT, STRICT_IE6, ALLOW_ALL).",code_debt,low_quality_code,9a2ec694fe6f1cf72b60d4f406b010bfa55ff04b
hadoop,10485,summary,Remove dead classes in hadoop-streaming,code_debt,dead_code,0862ee6520da2140e5f1b201042ae33adebc1943
hadoop,10485,description,Hadoop-streaming no longer requires many classes in o.a.h.record. This jira removes the dead code.,code_debt,dead_code,0862ee6520da2140e5f1b201042ae33adebc1943
hadoop,10485,comment_1,The v0 patch introduces no functionality changes. It only removes the unused classes.,code_debt,dead_code,0862ee6520da2140e5f1b201042ae33adebc1943
hadoop,10485,comment_2,"+1. Thank you, Haohui, for the code cleaning up.",code_debt,low_quality_code,0862ee6520da2140e5f1b201042ae33adebc1943
hadoop,10496,summary,Metrics system FileSink can leak file descriptor.,code_debt,low_quality_code,9ac54b5480f7ac03d76f5a894eab87829123d2db
hadoop,10496,description,"{{FileSink}} opens a file. If the {{MetricsSystem}} is shutdown, then the sink is discarded and the file is never closed, causing a file descriptor leak.",code_debt,low_quality_code,9ac54b5480f7ac03d76f5a894eab87829123d2db
hadoop,10498,comment_7,"Thanks Daryn for the confirmation. A nice to have. I do not have strong argument for this. The header name could be configurable instead of hard coding to ""X-Forwarded-For"". The default value being ""X-Forwarded-For"".",code_debt,low_quality_code,bf7b2125739ab0ecc896f93af66e2f7a639ec94b
hadoop,10499,summary,Remove unused parameter from,code_debt,dead_code,dd7d032457f93bf600a1322a34873b1142303da2
hadoop,10499,description,The Configuration parameter is not used in the authorize() function. It can be removed and callers can be updated. Attaching the simple patch which removes the unused _conf_ parameter and updates the callers. The ProxyUsers is defined as a private audience and so there shouldn't be any external callers.,code_debt,dead_code,dd7d032457f93bf600a1322a34873b1142303da2
hadoop,10499,comment_3,"Although it makes sense to remove an unused parameter, I wonder if the parameter should be honored? It would allow different rpc services to have different proxy superuser configurations as opposed to the current static configuration... It probably made sense when daemons generally had only one rpc service. I don't have a strong opinion because I suppose it could be by another jira.",code_debt,low_quality_code,dd7d032457f93bf600a1322a34873b1142303da2
hadoop,10499,comment_6,"I believe , the static nature of the ProxyUser capability should be addressed separately, if there is a requirement for different services to have different ProxyUser capabilities. HADOOP-10448 would make it easier since it will move the state out of ProxyUsers. I am trying to clean up the interface exposed by _ProxyUsers_ in order to work on HADOOP-10448 and that's why this jira.",code_debt,low_quality_code,dd7d032457f93bf600a1322a34873b1142303da2
hadoop,10499,comment_19,"Hi, Sorry for the inconvenience this caused. Is it possible to patch HBase to stop calling the 3-arg form of that was removed? The third argument was unused anyway, so this won't change any functionality in HBase. Additionally, this class is currently annotated {{Private}}, so I expected we didn't need to check for downstream impacts before I committed. That means one of two additional changes needs to happen: # Change HBase to stop calling altogether. I'm not really sure what alternative you would have though, so maybe this isn't feasible. # Change the annotation to {{LimitedPrivate}} for HBase, so devs know to check for downstream impacts on interface-breaking changes in the future. What are your thoughts on this?",code_debt,low_quality_code,dd7d032457f93bf600a1322a34873b1142303da2
hadoop,10508,comment_2,The patch looks good. Can you please provide a unit test for this functionality?,test_debt,lack_of_tests,02d28907beab7110abf768fd4006b076a6bf2bd2
hadoop,10508,comment_6,"Actually, , can you move the test from to +1 once it is addressed.",architecture_debt,violation_of_modularity,02d28907beab7110abf768fd4006b076a6bf2bd2
hadoop,10526,summary,Chance for Stream leakage in CompressorStream,design_debt,non-optimal_design,291af51b654e9de533e084042051ef6650b05fc2
hadoop,10673,comment_0,The patch updates the existing rpc metrics in the case of exception. HDFS unit tests will be updated to cover this.,test_debt,lack_of_tests,790ee456439729073d75ccf91e1f63b3d360b1c7
hadoop,10673,comment_7,", thank you for the suggestion. I think this improvement is useful, too. One minor comment: both {{rpcMetrics}} and are not final field, so how about adding null check before accessing them?",code_debt,low_quality_code,790ee456439729073d75ccf91e1f63b3d360b1c7
hadoop,10673,comment_10,"Thanks for the updating, Ming. It looks better approach to me. One additional point: if we can make the variables final, we can remove null check in {{Server#stop()}}.",code_debt,low_quality_code,790ee456439729073d75ccf91e1f63b3d360b1c7
hadoop,10681,description,"The current implementation of SnappyCompressor spends more time within the java loop of copying from the user buffer into the direct buffer allocated to the compressor impl, than the time it takes to compress the buffers. The bottleneck was found to be java monitor code inside SnappyCompressor. The methods are neatly inlined by the JIT into the parent caller which unfortunately does not flatten out the synchronized blocks. The loop does a write of small byte[] buffers (each IFile key+value). I counted approximately 6 monitor enter/exit blocks per k-v pair written.",code_debt,slow_algorithm,8f9ab998e273259c1e7a3ed53ba37d767e02b6bb
hadoop,10681,comment_1,"Added a SnappyCode impl into hive, which should come before in the classpath for hive+tez. Tested out TPC-H Query5, which has a spill-merge on the JOIN. Query times went from 539.8 seconds to 464.89 seconds, mostly from speedup to a single reducer stage.",code_debt,slow_algorithm,8f9ab998e273259c1e7a3ed53ba37d767e02b6bb
hadoop,10681,comment_3,"The patch removes unsafe synchronized blocks from the individual methods in Snappy and Zlib codecs. This synchronization is slow and when used in the most common pattern for CompressionCodec is still thread-unsafe for sharing streams with loops running across multiple threads and would ideally require explicit code { while { 0, buffer.length); } } to get correct stateful behaviour. As code exists today it is not thread-safe and does slow lock-prefix x86_64 instructions. The JNI library below in SnappyCodec.c actually does its own locking mutexes for the actual critical sections within.",design_debt,non-optimal_design,8f9ab998e273259c1e7a3ed53ba37d767e02b6bb
hadoop,10681,comment_7,Address findbugs warnings,code_debt,low_quality_code,8f9ab998e273259c1e7a3ed53ba37d767e02b6bb
hadoop,10681,comment_17,Can we do the same for {{Lz4Compressor}} and I noticed a significant performance overhead using {{Lz4Compressor}} for compared to due to the same problem.,code_debt,slow_algorithm,8f9ab998e273259c1e7a3ed53ba37d767e02b6bb
hadoop,10729,description,"We have ProtocolInfo specified in protocol interface with version info, but we don't have unit test to verify if/how it works. We should have tests to track this annotation work as expectation.",test_debt,lack_of_tests,c4084d9bc3b5c20405d9da6623b330d5720b64a1
hadoop,10748,description,Currently HttpServer2 loads the JspServlet by default. It should be removed as JSP support is no longer required.,code_debt,dead_code,0ac698864dbaba31806fbf16cb06b55bea792353
hadoop,10770,comment_0,This is belongs to a string of patches built on top of each other: * HADOOP-10817 proxyuser logic supporting custom prefix properties * HADOOP-10799 HTTP delegation token built in client/server authentication classes * HADOOP-10800 HttpFS using HADOOP-10799 and removing custom code * HADOOP-10835 HTTP proxyuser logic built in client/server authentication classes * HADOOP-10836 HttpFS using HADOOP-10835 and removing custom code * HADOOP-10770 KMS delegation support using HADOOP-10799 * HADOOP-10698 KMS proxyuser support using HADOOP-10835,code_debt,dead_code,e86c9ef6517313aaa0e4575261a462f2f55d43dc
hadoop,10770,comment_5,"Patch looks pretty good to me, Tucu. Only one little nit - seems like you've got some excessive indentation in +1 otherwise.",code_debt,low_quality_code,e86c9ef6517313aaa0e4575261a462f2f55d43dc
hadoop,10930,summary,HarFsInputStream should implement PositionedReadable with thead-safe.,code_debt,multi-thread_correctness,"aeecfa24f4fb6af289920cbf8830c394e66bd78e,dcedb72af468128458e597f08d22f5c34b744ae5"
hadoop,10979,description,It would make adding common options to hadoop_usage output easier if some entries were auto-populated. This is similar to what happens in FsShell and other parts of the Java code.,design_debt,non-optimal_design,ee36f4f9b87b194c965a2c5ace0244ab11e1d2d6
hadoop,11013,summary,"CLASSPATH handling should be consolidated, debuggable",code_debt,low_quality_code,d8774cc577198fdc3bc36c26526c95ea9a989800
hadoop,11013,description,"As part of HADOOP-9902, java execution across many different shell bits were consolidated down to (effectively) two routines. Prior to calling those two routines, the CLASSPATH is exported. This export should really be getting handled in the exec function and not in the individual shell bits. Additionally, it would be good if there was: so that bash -x would show the content of the classpath or even a '--debug classpath' option that would echo the classpath to the screen prior to java exec to help with debugging.",code_debt,low_quality_code,d8774cc577198fdc3bc36c26526c95ea9a989800
hadoop,11013,comment_1,a) adds --debug option b) reworks CLASSPATH export to be consistent/safer,code_debt,low_quality_code,d8774cc577198fdc3bc36c26526c95ea9a989800
hadoop,11013,comment_6,"-01: * Replaced all of the if's with a function * Added more messages that fill in some of the blanks (e.g., when does get added?) * Because I'm never happy, even with my own code",code_debt,low_quality_code,d8774cc577198fdc3bc36c26526c95ea9a989800
hadoop,11014,summary,Potential resource leak in due to unclosed stream,code_debt,low_quality_code,b351086ff66ca279c0550e078e3a9d110f3f36a5
hadoop,11063,summary,"KMS cannot deploy on Windows, because class names are too long.",code_debt,low_quality_code,b44b2ee4adb78723c221a7da8fd35ed011d0905c
hadoop,11063,description,"Windows has a maximum path length of 260 characters. KMS includes several long class file names. During packaging and creation of the distro, these paths get even longer because of prepending the standard war directory structure and our share/hadoop/etc. structure. The end result is that the final paths are longer than 260 characters, making it impossible to deploy a distro on Windows.",code_debt,low_quality_code,b44b2ee4adb78723c221a7da8fd35ed011d0905c
hadoop,11103,summary,Clean up RemoteException,code_debt,low_quality_code,d4a2830b63f0819979b592f4ea6ea3abd5885b71
hadoop,11117,description,"If something is failing with kerberos login, should fail with useful information. But not all exceptions from the inner code are caught and converted to LoginException. Those exceptions that aren't wrapped have their text and stack trace lost somewhere in the javax code, leaving on the text ""login failed"" and a stack trace of no value whatsoever.",code_debt,low_quality_code,a469833639c7a5ef525a108a1ac70213881e627d
hadoop,11117,comment_1,stack/message of little or no value,code_debt,low_quality_code,a469833639c7a5ef525a108a1ac70213881e627d
hadoop,11117,comment_5,"I really wish we could catch these and not throw a stack trace to figure out why/what/where it broke. From an end user perspective, these stacks are completely unfriendly.",code_debt,low_quality_code,a469833639c7a5ef525a108a1ac70213881e627d
hadoop,11117,comment_7,I agree ... but this is something minimal which can go in with little/no work though it looks like a couple of tests are not picking up the changed text. What we could do long term is replicate some of the stuff we did for the networking errors: point to wiki pages. But how is anyone going to make sense of a ? I don't want to write the wiki page for that.,test_debt,low_coverage,a469833639c7a5ef525a108a1ac70213881e627d
hadoop,11117,comment_8,"Some of the test failures are spurious, the only regression appears to be These tests are failing because the test code is doing an {{assertEquals}} on the expected string; the propagation of the underlying exception message is breaking this comparison. Fix is what could have been done in the first place: use as the probe",code_debt,low_quality_code,a469833639c7a5ef525a108a1ac70213881e627d
hadoop,11117,comment_9,patch -002 which patches the test to make it less brittle to exception strings,code_debt,low_quality_code,a469833639c7a5ef525a108a1ac70213881e627d
hadoop,11201,comment_4,"Ah, now it makes sense. In that case, I think renaming to {{justPaths}} would help clarify the code.",code_debt,low_quality_code,79301e80d7510f055c01a06970bb409607a4197c
hadoop,11309,comment_4,", thanks for the patch. It looks good to me. One minor nit: it might be good to enclose the last condition (the one that checks for the nested class) in parentheses for clarity:",code_debt,low_quality_code,b4ca7276902ad362f746ea997f7e977a7a6abd0e
hadoop,11309,comment_5,"Thanks, , for review! Clarity is a subjective matter. I find redundant parentheses distracting especially when indentation has already been used to emphasize the grouping of operators as in this case. I leave it to committers if they have a strong preference one way or another.",code_debt,low_quality_code,b4ca7276902ad362f746ea997f7e977a7a6abd0e
hadoop,11355,comment_2,"+1 LGTM, small nit for the future, you can use when checking exception text. Will commit shortly, thanks Arun.",code_debt,low_quality_code,9cdaec6a6f6cb1680ad6e44d7b0c8d70cdcbe3fa
hadoop,11379,summary,Fix new findbugs warnings in hadoop-auth*,code_debt,low_quality_code,6df457a3d7661a890e84fc89567f29d0fe23c970
hadoop,11379,description,"When locally run findbugs 3.0, there are new warnings generated. This Jira aims to address the new warnings in hadoop-auth and",code_debt,low_quality_code,6df457a3d7661a890e84fc89567f29d0fe23c970
hadoop,11379,comment_0,In this patch I fixed warnings raised by findbugs 3 against hadoop-auth and They're all encoding related warnings.,code_debt,low_quality_code,6df457a3d7661a890e84fc89567f29d0fe23c970
hadoop,11409,comment_4,"Thanks for the patch, Gera! When we throw the exception due to lacking a scheme, it would be very helpful to users to mention the property where the bad URI originated to show them what needs to be changed. The unit test should have an Assert.fail call or something similar after the getFileContext call, otherwise the lack of throwing an exception for a bad fs URI will still pass the test. ""Excpected"" s/b ""Expected""",test_debt,low_coverage,b9d49761f72078a0a83137ba8197d08b71f385e0
hadoop,11421,description,"We should have a drop-in replacement for File#listDir that doesn't hide IOExceptions, and which returns a ChunkedArrayList rather than a single large array.",code_debt,low_quality_code,9937eef7f7f04a7dd3d504ae7ec5852d488a1f6a
hadoop,11421,comment_0,"Thanks for working on this Colin. It'll be nice to swap this in where we can, JDK7 does a much better job at exposing filesystem APIs. I wonder if we should really return a ChunkedArrayList here. It only implements a subset of the AbstractList interface, and this is a pretty general-purpose method. For huge dirs, we should probably just be using the DirectoryStream iterator directly. I do see the use of these helper functions for quick-and-dirty listings though. I'd be okay providing variants of these functions that return a ChunkedArrayList, but it seems like the default should just be a normal ArrayList. Couple other things: * Need {{<p/* I read the docs at and it'd be nice to do like the example and unwrap the into an IOException.",code_debt,low_quality_code,9937eef7f7f04a7dd3d504ae7ec5852d488a1f6a
hadoop,11421,comment_2,"I think maybe later will become more general-purpose. But you're right; for now, we better use {{ArrayList}}. ok Yeah, that's important... io errors should result in io exceptions. Looks like is a probably in order to conform to the {{Iterator}} interface. I removed the variant that returns a list of File, since I found that the JDK6 file listing interfaces actually returned an array of String, so returning a list of String is compatible-ish.",code_debt,low_quality_code,9937eef7f7f04a7dd3d504ae7ec5852d488a1f6a
hadoop,11523,description,"In current WASB (Windows Azure Storage - Blob) implementation, when rename operation succeeds, WASB will update the parent folder's ""last modified time"" property. By default we do not acquire lease on this folder when updating its property and simply pass ""null"" to it. In HBase scenario, when doing distributed log splitting, there might be a case that multiple processes from different region servers will access the same folder, and randomly we will see this exception in regionserver's log, which makes log splitting fail. So we should acquire the lease when updating the folder property rather than pass ""null"" to it.",design_debt,non-optimal_design,f2c91098c400da6db0f5e8e49e9bf0e6444af531
hadoop,11523,comment_3,"Hi Duo. Thank you for the patch. The current patch would acquire the lease for all {{rename}} operations, covering both block blobs and page blobs. During initial development of atomic rename, we were careful to limit the scope to only page blobs (typically used by HBase logs). Existing applications using block blobs might not be expecting the leases, so I think we need to make sure the lease acquisition only happens after checking In the error handling, if an exception is thrown from the try block, and then another exception is also thrown from freeing the lease, then this would drop the original exception and throw the exception from freeing the lease. I suspect in general the main exception from the try block is going to be more interesting for root cause analysis, so I recommend throwing that one and just logging the one from freeing the lease, like so: In addition to the unit tests, I ran the test suite against a live Azure storage account and confirmed that everything passed. The release audit warning from Jenkins is unrelated to this patch.",design_debt,non-optimal_design,f2c91098c400da6db0f5e8e49e9bf0e6444af531
hadoop,11523,comment_9,"Thanks, . I have just 2 more minor nitpicks. I suggest changing this: to this: This way, we'll log the full stack trace on lease free errors and have more information for troubleshooting. This is just a minor style point, but for the following, the keyword should be inline with the closing brace. Instead of this: We do this: Instead of this: We do this:",design_debt,non-optimal_design,f2c91098c400da6db0f5e8e49e9bf0e6444af531
hadoop,11544,summary,Remove unused configuration keys for tracing,code_debt,low_quality_code,42548f4dc2b2e8ace891b970c2c6712c02bb43ea
hadoop,11544,description,are no longer used.,code_debt,low_quality_code,42548f4dc2b2e8ace891b970c2c6712c02bb43ea
hadoop,11544,comment_2,Thanks for the comment . I removed the entry of from core-default.xml because the default (NeverSampler) is defined in I would like to left the description in Tracing.apt.vm because users still can use for setting trace sampler though the config key should be specified as + rather than in java code.,code_debt,low_quality_code,42548f4dc2b2e8ace891b970c2c6712c02bb43ea
hadoop,11607,summary,Reduce log spew in S3AFileSystem,code_debt,low_quality_code,aa1c437b6a806de612f030a68984c606c623f1d9
hadoop,11607,description,"{{S3AFileSystem}} generates INFO level logs in {{open}} and {{rename}}, which are not necessary.",code_debt,low_quality_code,aa1c437b6a806de612f030a68984c606c623f1d9
hadoop,11607,comment_1,"+1 Note that as s3a uses SLF4J for its log API, it can switch to {{log.info(""item {}"", value)}} for terser/more efficient logging logic (we still recommend wrapping debug statements though, just to skip string creation. If you switch to that mode, I'd be even happier. But I'm OK with what you've done so far",code_debt,low_quality_code,aa1c437b6a806de612f030a68984c606c623f1d9
hadoop,11658,comment_2,Thanks  for the report and the patch! Two comments: * Can we use in * Some inserted lines are longer than 80 characters. Would you render these?,code_debt,low_quality_code,ca1c00bf814a8b8290a81d06b1f4918c36c7d9e0
hadoop,11677,comment_3,Added missing import. Could not find any test class for HTTPServer2. I will try to add tests for this class,test_debt,lack_of_tests,611aa77f750986190a94ee88d1148a05f66513d0
hadoop,11730,description,"s3n attempts to read again when it encounters IOException during read. But the current logic does not reopen the connection, thus, it ends up with no-op, and committing the wrong(truncated) output. Here's a stack trace as an example. It seems this is a regression, which was introduced by the following optimizations. Also, test cases should be reviewed so that it covers this scenario.",test_debt,low_coverage,19262d99ebbbd143a7ac9740d3a8e7c842b37591
hadoop,11730,comment_4,"patch applied; tested against s3 EU. Given the nature of these problems, it may be good to start thinking about whether we can do things with better simulate failures; the test here is a good start, though we may want more complex policies...mockito might be the tool to reach for.",test_debt,low_coverage,19262d99ebbbd143a7ac9740d3a8e7c842b37591
hadoop,11740,comment_0,"This initial patch simply removes {{ErasureEncoder}} and {{ErasureDecoder}}. I think the following further simplifications are possible: # We can get rid of {{ErasureCoder}} since it has a single subclass now # Similarly, maybe we can get rid of since provides enough abstraction anyway # If {{ECBlockGroup}} can provide erased indices, we can further combine encoding and decoding classes",code_debt,low_quality_code,"e54a74b566f89a424a2f4735a35553ece3bd35d9,aac73c21c3e3f7552a3b32cd7238108b83a8fbe3,f166e67a23bc49e932b23876ede6f8ab9c9f76d6,bc2833b1c91e107d090619d755c584f6eae82327,0c1da5a0300f015a7e39f2b40a73fb06c65a78c8,ec480134262fe72a8687a25f9a06dd1e8a39b164,e50bcea83d5f6b02ab03b06a3fbf1ed6b8ff4871,9f19eb9fcf47c703c7b9ed7be573266d18baa051,b29f3bde4d2fd2f2c4abd6d7b5f97a81bb50efb2,ba9371492036983a9899398907ab41fe548f29b3,dae27f6dd14ac3ed0b9821a3c5239569b13f6adf,9f2f583f401189c3f4a2687795a9e3e0b288322b,9af1f4779b646fb2f09b5e36447c8b8abe920a7c,1e1e93040748231dc913190aec1e031c379d8271,c0945a8971097d56a37e6d0a4085df3f0b9d0589,39a0a85fb77872911089aac3f7792ab48d9eca68,8f89d7489df0d5c8236a1929c93e3f5ab7149031,f05c21285ef23b6a973d69f045b1cb46c5abc039,292e367d0772f47e434de4a31af4edf1b07241dc,68caf8728e35546aed30158e3006f37d41303397,11585883a9eb30ba080b9aa49dba42cb0a797d75,26773d9d6c10479982a3cdbea3a0933f4476add3,a38a37c63417a3b19dcdf98251af196c9d7b8c31,c3bc083405d84a368bf7281bc109bac2a7c62d0c,e3dbfeb13827b6d78cedcb5e3b1ede3a7b6aecba,57a84c0d149b693c913416975cafe6de4e23c321,90d332d6be4ded4fb666670ecb009b36d7ced7db,df297245a74be1a1baa1a34740dcd856eef22ea4,544f75d6512fefd0e36f24a35e6b7472ca7bf301,46dac3595fa2e2c14290154c3c12ea799ee5043d,d0d75a833907f6cf723a42a007ca04e0004a8e52,ea2e60fbcc79c65ec571224bd3f57c262a5d9114,f166e67a23bc49e932b23876ede6f8ab9c9f76d6,bc2833b1c91e107d090619d755c584f6eae82327,0c1da5a0300f015a7e39f2b40a73fb06c65a78c8,ec480134262fe72a8687a25f9a06dd1e8a39b164,e50bcea83d5f6b02ab03b06a3fbf1ed6b8ff4871,9f19eb9fcf47c703c7b9ed7be573266d18baa051,b29f3bde4d2fd2f2c4abd6d7b5f97a81bb50efb2,ba9371492036983a9899398907ab41fe548f29b3,dae27f6dd14ac3ed0b9821a3c5239569b13f6adf,9f2f583f401189c3f4a2687795a9e3e0b288322b,9af1f4779b646fb2f09b5e36447c8b8abe920a7c,1e1e93040748231dc913190aec1e031c379d8271,c0945a8971097d56a37e6d0a4085df3f0b9d0589,39a0a85fb77872911089aac3f7792ab48d9eca68,8f89d7489df0d5c8236a1929c93e3f5ab7149031,f05c21285ef23b6a973d69f045b1cb46c5abc039,292e367d0772f47e434de4a31af4edf1b07241dc,68caf8728e35546aed30158e3006f37d41303397,11585883a9eb30ba080b9aa49dba42cb0a797d75,26773d9d6c10479982a3cdbea3a0933f4476add3,a38a37c63417a3b19dcdf98251af196c9d7b8c31,c3bc083405d84a368bf7281bc109bac2a7c62d0c,e3dbfeb13827b6d78cedcb5e3b1ede3a7b6aecba,57a84c0d149b693c913416975cafe6de4e23c321,90d332d6be4ded4fb666670ecb009b36d7ced7db,df297245a74be1a1baa1a34740dcd856eef22ea4,544f75d6512fefd0e36f24a35e6b7472ca7bf301,46dac3595fa2e2c14290154c3c12ea799ee5043d,d0d75a833907f6cf723a42a007ca04e0004a8e52,ea2e60fbcc79c65ec571224bd3f57c262a5d9114"
hadoop,11740,comment_1,"Thanks  for the good thoughts. Yes it's often a good question to think about either interface or abstract class in Java. My feeling is that if it's in a framework, subject to be pluggable and implemented by customers, an interface would be good to have. So I guess we could keep {{ErasureCoder}} interface, and convert interface to a class. I'm not sure, as erased indices have to be be computed according to input blocks and output blocks just for decoders, and encoders don't have the related logics. Currently in RS coder the decoding is rather simple but I believe it will be much complicated for codes like LRC and Hitchhiker, so separating encoding class and decoding class is desired.",code_debt,low_quality_code,"e54a74b566f89a424a2f4735a35553ece3bd35d9,aac73c21c3e3f7552a3b32cd7238108b83a8fbe3,f166e67a23bc49e932b23876ede6f8ab9c9f76d6,bc2833b1c91e107d090619d755c584f6eae82327,0c1da5a0300f015a7e39f2b40a73fb06c65a78c8,ec480134262fe72a8687a25f9a06dd1e8a39b164,e50bcea83d5f6b02ab03b06a3fbf1ed6b8ff4871,9f19eb9fcf47c703c7b9ed7be573266d18baa051,b29f3bde4d2fd2f2c4abd6d7b5f97a81bb50efb2,ba9371492036983a9899398907ab41fe548f29b3,dae27f6dd14ac3ed0b9821a3c5239569b13f6adf,9f2f583f401189c3f4a2687795a9e3e0b288322b,9af1f4779b646fb2f09b5e36447c8b8abe920a7c,1e1e93040748231dc913190aec1e031c379d8271,c0945a8971097d56a37e6d0a4085df3f0b9d0589,39a0a85fb77872911089aac3f7792ab48d9eca68,8f89d7489df0d5c8236a1929c93e3f5ab7149031,f05c21285ef23b6a973d69f045b1cb46c5abc039,292e367d0772f47e434de4a31af4edf1b07241dc,68caf8728e35546aed30158e3006f37d41303397,11585883a9eb30ba080b9aa49dba42cb0a797d75,26773d9d6c10479982a3cdbea3a0933f4476add3,a38a37c63417a3b19dcdf98251af196c9d7b8c31,c3bc083405d84a368bf7281bc109bac2a7c62d0c,e3dbfeb13827b6d78cedcb5e3b1ede3a7b6aecba,57a84c0d149b693c913416975cafe6de4e23c321,90d332d6be4ded4fb666670ecb009b36d7ced7db,df297245a74be1a1baa1a34740dcd856eef22ea4,544f75d6512fefd0e36f24a35e6b7472ca7bf301,46dac3595fa2e2c14290154c3c12ea799ee5043d,d0d75a833907f6cf723a42a007ca04e0004a8e52,ea2e60fbcc79c65ec571224bd3f57c262a5d9114,f166e67a23bc49e932b23876ede6f8ab9c9f76d6,bc2833b1c91e107d090619d755c584f6eae82327,0c1da5a0300f015a7e39f2b40a73fb06c65a78c8,ec480134262fe72a8687a25f9a06dd1e8a39b164,e50bcea83d5f6b02ab03b06a3fbf1ed6b8ff4871,9f19eb9fcf47c703c7b9ed7be573266d18baa051,b29f3bde4d2fd2f2c4abd6d7b5f97a81bb50efb2,ba9371492036983a9899398907ab41fe548f29b3,dae27f6dd14ac3ed0b9821a3c5239569b13f6adf,9f2f583f401189c3f4a2687795a9e3e0b288322b,9af1f4779b646fb2f09b5e36447c8b8abe920a7c,1e1e93040748231dc913190aec1e031c379d8271,c0945a8971097d56a37e6d0a4085df3f0b9d0589,39a0a85fb77872911089aac3f7792ab48d9eca68,8f89d7489df0d5c8236a1929c93e3f5ab7149031,f05c21285ef23b6a973d69f045b1cb46c5abc039,292e367d0772f47e434de4a31af4edf1b07241dc,68caf8728e35546aed30158e3006f37d41303397,11585883a9eb30ba080b9aa49dba42cb0a797d75,26773d9d6c10479982a3cdbea3a0933f4476add3,a38a37c63417a3b19dcdf98251af196c9d7b8c31,c3bc083405d84a368bf7281bc109bac2a7c62d0c,e3dbfeb13827b6d78cedcb5e3b1ede3a7b6aecba,57a84c0d149b693c913416975cafe6de4e23c321,90d332d6be4ded4fb666670ecb009b36d7ced7db,df297245a74be1a1baa1a34740dcd856eef22ea4,544f75d6512fefd0e36f24a35e6b7472ca7bf301,46dac3595fa2e2c14290154c3c12ea799ee5043d,d0d75a833907f6cf723a42a007ca04e0004a8e52,ea2e60fbcc79c65ec571224bd3f57c262a5d9114"
hadoop,11740,comment_7,"Thanks for the update. I looked the new patch, just two minor comments: 1. In the test codes, may be better to use {{ErasureCoder}} instead of or since the interface type is good enough, which is why we're here. With this refining, from caller's point of view, nothing different from between encoder and decoder, so it should use the common interface. 2. Those unnecessary Javadoc are there to conform Javadoc conventions and format. In future someone may fill them. I suggest we don't remove them, you can find so many in the project.",code_debt,low_quality_code,"e54a74b566f89a424a2f4735a35553ece3bd35d9,aac73c21c3e3f7552a3b32cd7238108b83a8fbe3,f166e67a23bc49e932b23876ede6f8ab9c9f76d6,bc2833b1c91e107d090619d755c584f6eae82327,0c1da5a0300f015a7e39f2b40a73fb06c65a78c8,ec480134262fe72a8687a25f9a06dd1e8a39b164,e50bcea83d5f6b02ab03b06a3fbf1ed6b8ff4871,9f19eb9fcf47c703c7b9ed7be573266d18baa051,b29f3bde4d2fd2f2c4abd6d7b5f97a81bb50efb2,ba9371492036983a9899398907ab41fe548f29b3,dae27f6dd14ac3ed0b9821a3c5239569b13f6adf,9f2f583f401189c3f4a2687795a9e3e0b288322b,9af1f4779b646fb2f09b5e36447c8b8abe920a7c,1e1e93040748231dc913190aec1e031c379d8271,c0945a8971097d56a37e6d0a4085df3f0b9d0589,39a0a85fb77872911089aac3f7792ab48d9eca68,8f89d7489df0d5c8236a1929c93e3f5ab7149031,f05c21285ef23b6a973d69f045b1cb46c5abc039,292e367d0772f47e434de4a31af4edf1b07241dc,68caf8728e35546aed30158e3006f37d41303397,11585883a9eb30ba080b9aa49dba42cb0a797d75,26773d9d6c10479982a3cdbea3a0933f4476add3,a38a37c63417a3b19dcdf98251af196c9d7b8c31,c3bc083405d84a368bf7281bc109bac2a7c62d0c,e3dbfeb13827b6d78cedcb5e3b1ede3a7b6aecba,57a84c0d149b693c913416975cafe6de4e23c321,90d332d6be4ded4fb666670ecb009b36d7ced7db,df297245a74be1a1baa1a34740dcd856eef22ea4,544f75d6512fefd0e36f24a35e6b7472ca7bf301,46dac3595fa2e2c14290154c3c12ea799ee5043d,d0d75a833907f6cf723a42a007ca04e0004a8e52,ea2e60fbcc79c65ec571224bd3f57c262a5d9114,f166e67a23bc49e932b23876ede6f8ab9c9f76d6,bc2833b1c91e107d090619d755c584f6eae82327,0c1da5a0300f015a7e39f2b40a73fb06c65a78c8,ec480134262fe72a8687a25f9a06dd1e8a39b164,e50bcea83d5f6b02ab03b06a3fbf1ed6b8ff4871,9f19eb9fcf47c703c7b9ed7be573266d18baa051,b29f3bde4d2fd2f2c4abd6d7b5f97a81bb50efb2,ba9371492036983a9899398907ab41fe548f29b3,dae27f6dd14ac3ed0b9821a3c5239569b13f6adf,9f2f583f401189c3f4a2687795a9e3e0b288322b,9af1f4779b646fb2f09b5e36447c8b8abe920a7c,1e1e93040748231dc913190aec1e031c379d8271,c0945a8971097d56a37e6d0a4085df3f0b9d0589,39a0a85fb77872911089aac3f7792ab48d9eca68,8f89d7489df0d5c8236a1929c93e3f5ab7149031,f05c21285ef23b6a973d69f045b1cb46c5abc039,292e367d0772f47e434de4a31af4edf1b07241dc,68caf8728e35546aed30158e3006f37d41303397,11585883a9eb30ba080b9aa49dba42cb0a797d75,26773d9d6c10479982a3cdbea3a0933f4476add3,a38a37c63417a3b19dcdf98251af196c9d7b8c31,c3bc083405d84a368bf7281bc109bac2a7c62d0c,e3dbfeb13827b6d78cedcb5e3b1ede3a7b6aecba,57a84c0d149b693c913416975cafe6de4e23c321,90d332d6be4ded4fb666670ecb009b36d7ced7db,df297245a74be1a1baa1a34740dcd856eef22ea4,544f75d6512fefd0e36f24a35e6b7472ca7bf301,46dac3595fa2e2c14290154c3c12ea799ee5043d,d0d75a833907f6cf723a42a007ca04e0004a8e52,ea2e60fbcc79c65ec571224bd3f57c262a5d9114"
hadoop,11837,comment_4,LGTM. Minor nits: Change to +1 once addressed.,code_debt,low_quality_code,c6b5203cfdfccfa22ad5379b7fee75fed850d95e
hadoop,11966,description,"HADOOP-11464 reinstated support for running the bash scripts through Cygwin. The logic involves setting a {{cygwin}} flag variable to indicate if the script is executing through Cygwin. The flag is set in all of the interactive scripts: {{hadoop}}, {{hdfs}}, {{yarn}} and {{mapred}}. The flag is not set through hadoop-daemon.sh though. This can cause an erroneous overwrite of {{HADOOP_HOME}} and inside hadoop-config.sh.",code_debt,low_quality_code,93b770f7e778835a9dd76854b435c5250835d1a8
hadoop,12135,summary,cleanup releasedocmaker,code_debt,dead_code,e8b62d11d460e9706e48df92a0b0a72f4a02d3f5
hadoop,12135,comment_1,"-00: * removed hadoop hard-codes * added a flag to turn on/off asf license * supports multiple projects, but only as a single file * support for ranges of versions * more of my inability to write python",code_debt,low_quality_code,e8b62d11d460e9706e48df92a0b0a72f4a02d3f5
hadoop,12135,comment_5,+1 we can make things more pythonic in follow-ons.,code_debt,low_quality_code,e8b62d11d460e9706e48df92a0b0a72f4a02d3f5
hadoop,12268,comment_2,"Nice catch, ! The fix looks good to me. Two comments: 1. Would you remove unused imports in and 2. Would you please fix the following javadoc comment? ""concat"" should be ""append"".",code_debt,low_quality_code,a628a6f126473a99f9370ba642994ce2909a2d9e
hadoop,12368,description,"These are test base classes that need to be subclassed to run, can mark as abstract.",code_debt,low_quality_code,7ad3556ed38560585579172aa68356f37b2288c8
hadoop,12460,comment_2,"Corrected the checkstyle errors and white spaces & Attached the patch, Please review",code_debt,low_quality_code,7f0e1eb43d3fb173e3c7932739ef20095e28ed7b
hadoop,12484,comment_1,"Hello . If {{acquireLease}} succeeds, but then the {{delete}} fails, then this will leak the lease. Can you please use a {{finally}} block to guarantee that the lease gets released in all code paths? You can look at other points in the class that acquire and free a lease for an existing example. Could you please review the checkstyle and whitespace warnings from the pre-commit run and fix them? If it's not feasible to write a unit test to simulate the race condition, then can you please describe any manual testing that you've done to verify the change? Thank you!",design_debt,non-optimal_design,cb282d5b89fdece4719cc4ad37a6e27f13371534
hadoop,12484,comment_4,", there is one more problem here: At this point, it's possible that {{lease}} is still {{null}} if the earlier {{acquireLease}} call threw an exception. I recommend checking this for null before calling {{free}}. Otherwise, it will cause a",code_debt,low_quality_code,cb282d5b89fdece4719cc4ad37a6e27f13371534
hadoop,12484,comment_11,"This part is triggering a Checkstyle warning: We can clean that up by changing to: I realize there are existing instances of empty statements like this in the class, but let's avoid introducing new instances. The mvninstall failure appears to be a side effect of something in the bats testing of the bash scripts. It's unrelated to this patch. Since you need to upload one more patch revision to address the Checkstyle warning, let's do one more test run and see if it happens again. If it does, then I'll follow up. The license check warning is caused by a test copying a file to a location that it shouldn't, which is then covered by the license check. I'll follow up separately on that.",code_debt,low_quality_code,cb282d5b89fdece4719cc4ad37a6e27f13371534
hadoop,12484,comment_13,"This looks good overall, aside from the remaining whitespace and Checkstyle problems. I'm attaching patch v06, which is the same thing with the whitespace and Checkstyle warnings fixed.",code_debt,low_quality_code,cb282d5b89fdece4719cc4ad37a6e27f13371534
hadoop,12484,comment_14,Thanks a lot Chris. Apologise for the whitespace and checkstyle issues; I was about to put in a new patch myself but since you have already done it I will let Jenkins run again. What are the next steps here after this? Is any further action required from my end?,code_debt,low_quality_code,cb282d5b89fdece4719cc4ad37a6e27f13371534
hadoop,12520,description,"The hadoop-azure tests support execution against the live Azure Storage service if the developer specifies the key to an Azure Storage account. The configuration works by overwriting the file. This can be an error-prone process. The azure-test.xml file is checked into revision control to show an example. There is a risk that the tester could overwrite azure-test.xml containing the keys and then accidentally commit the keys to revision control. This would leak the keys to the world for potential use by an attacker. This issue proposes to use XInclude to isolate the keys into a separate file, ignored by git, which will never be committed to revision control. This is very similar to the setup already used by hadoop-aws for integration testing.",design_debt,non-optimal_design,73822de7c38e189f7654444ff48d15cbe0df7404
hadoop,12701,description,Test source files are not checked by checkstyle because Maven checkstyle plugin parameter is *false* by default. Propose to enable checkstyle on test source files in order to improve the quality of unit tests.,code_debt,low_quality_code,e097c9a124dc0b9ae9076994d19663f29d771ef0
hadoop,12701,comment_0,"After the fix, mvn for entire Hadoop tree took 01:27 min on my Macbook Pro. In comparison, it took 38.677 s before.",code_debt,slow_algorithm,e097c9a124dc0b9ae9076994d19663f29d771ef0
hadoop,12733,description,The following variables appear to no longer be used.,code_debt,dead_code,01d31fe9389ccdc153d7f4bf6574bf8e509867c1
hadoop,12733,comment_2,"RE: ASF license warnings This looks like a bunch of generated files, not checked in files.",code_debt,low_quality_code,01d31fe9389ccdc153d7f4bf6574bf8e509867c1
hadoop,12811,description,The HBase's HMaster port number conflicts with Hadoop kms port number. Both uses 16000. There might be use cases user need kms and HBase present on the same cluster. The HBase is able to encrypt its HFiles but user might need KMS to encrypt other HDFS directories. Users would have to manually override the default port of either application on their cluster. It would be nice to have different default ports so kms and HBase could naturally coexist.,design_debt,non-optimal_design,a74580a4d3039ff95e7744f1d7a386b2bc7a7484
hadoop,12829,summary,swallows interrupt exceptions,code_debt,low_quality_code,d9c409a4286e36387fb39e7d622e850c13315465
hadoop,12829,description,"The implemented in HADOOP-12107 swallows interrupt exceptions. Over in Solr/Sentry land, we run thread leak checkers on our test code, which passed before this change and fails after it. Here's a sample report: And here's an indication that the interrupt is being ignored: This is inconsistent with how other long-running threads in hadoop, i.e. PeerCache respond to being interrupted. The argument for doing this in HADOOP-12107 is given as I'm unclear on what ""spurious wakeup"" means and it is not mentioned in So, I believe this thread should respect interruption.",code_debt,low_quality_code,d9c409a4286e36387fb39e7d622e850c13315465
hadoop,12829,comment_0,"Attached a patch. Doesn't include any tests -- not sure exactly what to test. I internally tested by changing STATS_DATA_CLEANER to package-private and writing the following test: which passes with the change and hangs without it. I'm unclear on if hadoop even wants something like this, since I'm not up to speed on how hadoop handles JVM reuse for unit tests.",test_debt,lack_of_tests,d9c409a4286e36387fb39e7d622e850c13315465
hadoop,12829,comment_1,"Thank you, . I can't think of any reason why this thread should swallow without a trace. It is not performing any operations that should inherently generate INE, as far as I can see, and if someone else sends an INE we ought to... interrupt the thread. Can you include the stack trace so that this exception has a better chance of getting noticed? Also, capitalize the error? +1 pending those changes",code_debt,low_quality_code,d9c409a4286e36387fb39e7d622e850c13315465
hadoop,12829,comment_11,"Hello guys, I hit the same thread leak issue in FLINK-15239. In our use case, a Flink cluster is started w/o Hadoop dependencies, and accepts jobs submitted from various clients. If a job needs to access Hadoop, client will include Hadoop jars as its dependencies. On Flink side, we create a class loader to load all the dependencies of each job. As a result, we start a thread each time we load the Hadoop classes. While the change here made the thread respond to interrupt exception, it seems there's no one to interrupt the thread. So I guess the thread only dies when the JVM exits. If that's the case, is it possible to provide a way to explicitly interrupt the thread?",design_debt,non-optimal_design,d9c409a4286e36387fb39e7d622e850c13315465
hadoop,12864,comment_1,"Actually, it looks like bin/rcc should have been removed with HADOOP-10474.",code_debt,dead_code,77031a9c37e7e72f8825b9e22aa35b238e924576
hadoop,12888,description,"HDFS _client_ requires dangerous permission, in particular _execute_ on _all files_ despite only trying to connect to an HDFS cluster. A full list (for both Hadoop 1 and 2) is available here along with the place in code where they occur. While it is understandable for some permissions to be used, requiring {{FilePermission <<ALL FILES To make matters worse, the code is executed to initialize a field so in case the permissions is not granted, the VM fails with which is unrecoverable. Ironically enough, on Windows this problem does not appear since the code simply bypasses it and initializes the field with a fall back value ({{false}}). A quick fix would be to simply take into account that the JVM {{SecurityManager}} might be active and the permission not granted or that the external process fails and use a fall back value. A proper and long-term fix would be to minimize the use of permissions for hdfs client since it is simply not required. A client should be as light as possible and not have the server requirements leaked onto.",code_debt,low_quality_code,5a725f0ab8ef9e2a8b08f088ba4e87531ae4530d
hadoop,12888,comment_0,"Hadoop is notoriously bad for security manager support (See HADOOP-5731) ... though being able to go securely client-side would be good. As you note: server-side requirements shouldn't impact client side. # what happens if you try to use webhdfs rather than hdfs:// ? # show us the stack trace? Shell is used client-side to detect OS, there is a languishing patch to isolate OS checks...someone needs to refresh that patch and we can get it into trunk. It's also critical on windows. Which field is causing the problem?",code_debt,low_quality_code,5a725f0ab8ef9e2a8b08f088ba4e87531ae4530d
hadoop,12888,comment_5,"attach the patch to this JIRA, use a name of the form hit the the ""submit patch"" button and it'll be trigger an automatic review. Yetus will complain about the lack of tests, but we'll have to go with that. Did it work for you in any manual tests Moving the issue from HDFS to Hadoop as its in the common module",test_debt,lack_of_tests,5a725f0ab8ef9e2a8b08f088ba4e87531ae4530d
hadoop,12888,comment_8,...afraid you'll have to look at the checkstyle issues. Don't worry about the test and whitespace complaints,code_debt,low_quality_code,5a725f0ab8ef9e2a8b08f088ba4e87531ae4530d
hadoop,12888,comment_9,Fix checkstyle errors.,code_debt,low_quality_code,5a725f0ab8ef9e2a8b08f088ba4e87531ae4530d
hadoop,12888,comment_11,Yet another update to fix checkstyle errors.,code_debt,low_quality_code,5a725f0ab8ef9e2a8b08f088ba4e87531ae4530d
hadoop,12888,comment_15,"I went for debug: 1. to be consistent (similar to isSetsid 2. just like on windows on the other code paths, having these features disabled, on the client, has no impact. On info they would simply add noise and confusion in my opinion.",code_debt,low_quality_code,5a725f0ab8ef9e2a8b08f088ba4e87531ae4530d
hadoop,12923,summary,Move the test code in ipc.Client to test,architecture_debt,violation_of_modularity,1898810cda83e6d273a2963b56ed499c0fb91118
hadoop,12923,description,Some code is used only by tests. Let's relocate them.,architecture_debt,violation_of_modularity,1898810cda83e6d273a2963b56ed499c0fb91118
hadoop,12952,description,"The examples for building distributions include how to create one without any documentation. But it includes the javadoc stage in the build, which is very slow. Adding skips that phase, and helps round out the parameters to a build.",code_debt,slow_algorithm,a107cee14bb5446057da81d1c95d7fffd759e497
hadoop,13030,summary,Handle special characters in passwords in KMS startup script,code_debt,low_quality_code,6f26b665874f923d50087f68357ac822fa9fe709
hadoop,13030,description,{{kms.sh}} currently cannot handle special characters.,code_debt,low_quality_code,6f26b665874f923d50087f68357ac822fa9fe709
hadoop,13030,comment_2,Patch 2 fixes the shellcheck warnings.,code_debt,low_quality_code,6f26b665874f923d50087f68357ac822fa9fe709
hadoop,13030,comment_4,"Patch 3 adds my first part of comment to the function, for better readability.",code_debt,low_quality_code,6f26b665874f923d50087f68357ac822fa9fe709
hadoop,13030,comment_11,Pretty much this exact same code exists in httpfs.sh ....,code_debt,duplicated_code,6f26b665874f923d50087f68357ac822fa9fe709
hadoop,13039,comment_4,"Hi , I think we can remove this string ""as potentially malicious"". The patch looks good otherwise. Thanks!",code_debt,dead_code,ea5475d1c125ff96b0650d0182f694380412c0da
hadoop,13051,description,"On {{branch-2}}, the below is the (incorrect) behaviour today, where paths with special characters get dropped during globStatus calls: Whereas trunk has the right behaviour, subtly fixed via the pattern library change of HADOOP-12436: (I've placed a ^M explicitly to indicate presence of the intentional hidden character) We should still add a simple test-case to cover this situation for future regressions.",test_debt,lack_of_tests,d8faf47f32c7ace6ceeb55bbb584c2dbab38902f
hadoop,13138,comment_2,", thank you for the patch. This looks good to me. I think it will be ready to go after addressing the Checkstyle nitpicks.",code_debt,low_quality_code,bad85f3e3a53b5fb692c77df6c11dcf968ad7e62
hadoop,13138,comment_3,Attaching the patch with fixed checkstyle nits,code_debt,low_quality_code,bad85f3e3a53b5fb692c77df6c11dcf968ad7e62
hadoop,13158,description,The {{cannedACL}} field of {{S3AFileSystem}} can be {{null}}. The {{toString}} implementation has an unguarded call to so there is a risk of,code_debt,low_quality_code,08ea07f1b8edbc38c99015c81a62ca127a247bf7
hadoop,13233,summary,help of stat is confusing,code_debt,low_quality_code,cc45da79fda7dfba2795ac397d62f40a858dcdd9
hadoop,13233,comment_0,Moved to Hadoop Common project because this code is in hadoop-common module.,architecture_debt,violation_of_modularity,cc45da79fda7dfba2795ac397d62f40a858dcdd9
hadoop,13233,comment_4,"TestKDiag succeeded on my machine, I suspect it's a random failure. checkstyle could be fixed, but it would either look weird (indented differently than the other lines in Stat.DESCRIPTION) or I would have to reindent a few more lines.",code_debt,low_quality_code,cc45da79fda7dfba2795ac397d62f40a858dcdd9
hadoop,13233,comment_8,+1 No need to fix checkstyle warning. Failed tests not reproducible.,code_debt,low_quality_code,cc45da79fda7dfba2795ac397d62f40a858dcdd9
hadoop,13353,comment_1,The fix looks good to me. Thanks for the contribution. Would you also like to add a regression test?,test_debt,lack_of_tests,49ba09a9221ad1e25e89800f6c455bbaad41483b
hadoop,13353,comment_2,"I see that when the IOException is thrown, the log message does not preserve its stacktrace. Could you also update the fix to include the stacktrace? Basically, change it to:",code_debt,low_quality_code,49ba09a9221ad1e25e89800f6c455bbaad41483b
hadoop,13353,comment_3,"Thanks for the initial patch. I created a v02 patch based on the initial one, adding a regression test, removed redundant code and improved error logging.",code_debt,complex_code,49ba09a9221ad1e25e89800f6c455bbaad41483b
hadoop,13353,comment_5,Sorry for late reply because I was on vacation. Thanks  for the v2 patch. But some code about datanode blockpool in the v2 patch seems irrelevant ?,code_debt,low_quality_code,49ba09a9221ad1e25e89800f6c455bbaad41483b
hadoop,13386,description,Avro 1.8.x makes generated classes serializable which makes them much easier to use with Spark. It would be great to upgrade Avro to 1.8.x,architecture_debt,using_obsolete_technology,"61e809b245c5a2dc0e4db6638093932b0b34ca5a,2b3f3af0debadb5350d6150c0de1ad7e3691c07c,20b71c4841235481b33d53895f787db2312f0f56,7a2e075e83c6fd25bff6577247624e580b8d07d4"
hadoop,13386,comment_0,"successor to HADOOP-12527; given the discourse there I'm going to change the title of that one and close this as a duplicate. see also: for some coverage of the problem. We aren't scared of Avro, but do need to take care of its transitive dependencies. Help there showing what they are and testing all down the stack is very much appreciated",test_debt,low_coverage,"61e809b245c5a2dc0e4db6638093932b0b34ca5a,2b3f3af0debadb5350d6150c0de1ad7e3691c07c,20b71c4841235481b33d53895f787db2312f0f56,7a2e075e83c6fd25bff6577247624e580b8d07d4"
hadoop,13386,comment_10,"Real issue is that it forces people downstream to recompile their code too, if they have this version of avro on their CP. If they exclude it, then you can't use the compiled classes in Hadoop -but there aren't that many. Maybe: tag as incompatible and explain what to do: recompile or exclude",design_debt,non-optimal_design,"61e809b245c5a2dc0e4db6638093932b0b34ca5a,2b3f3af0debadb5350d6150c0de1ad7e3691c07c,20b71c4841235481b33d53895f787db2312f0f56,7a2e075e83c6fd25bff6577247624e580b8d07d4"
hadoop,13529,description,1. argument and variant naming 2. abstract utility class 3. add some comments 4. adjust some configuration 5. fix TODO 6. remove unnecessary commets 7. some bug fix 8. add some unit test,code_debt,low_quality_code,d33e928fbeb1764a724c8f3c051bb0d8be82bbff
hadoop,13529,comment_5,some incremental updates: 1. add docs 2. remove UserInfo 3. remove comments for overrie function 4. codestyle issue,code_debt,low_quality_code,d33e928fbeb1764a724c8f3c051bb0d8be82bbff
hadoop,13638,comment_9,Checkstyle warnings can't be removed unless we refactor test methods.,code_debt,low_quality_code,fa397e74fe988bcbb05c816de73eb738794ace4b
hadoop,13638,comment_11,That was copied from an existing test case.,code_debt,duplicated_code,fa397e74fe988bcbb05c816de73eb738794ace4b
hadoop,13768,comment_4,"Looking at the following codes: 1. Please give {{l}} a more readable name. 2. Can you give some comments to explain some bit about the procedure? I (probably others) wouldn't know why it's like that without querying the SDK's manual. I know it now, there're 2 modes in the operation, one mode to return successfully deleted objects, and the other returning the deleting-failed objects. You're using the latter, and use a loop to try some times to delete and delete the failed-to-delete objects.",code_debt,low_quality_code,5b151290ae2916dc04d6a4338085fcefafa21982
hadoop,13768,comment_6,"assuming that the file limit is always1000, why not just list the path in 1000 blocks and issue delete requests in that size. There are ultimate limits to the size of responses in path listings (max size of an HTTP request), and inevitably heap problems well before then.",design_debt,non-optimal_design,5b151290ae2916dc04d6a4338085fcefafa21982
hadoop,13770,description,"creates a bash shell command to verify if the system supports bash. However, its error message is misleading, and the logic should be updated. If the shell command throws an IOException, it does not imply the bash did not run successfully. If the shell command process was interrupted, its internal logic throws an which is a subclass of IOException. An example of it appeared in a recent jenkins job The test logic in starts a thread, wait it for 1 second, and interrupt the thread, expecting the thread to terminate. However, the method swallowed the interrupt, and therefore failed. The original design is not desirable, as it swallowed a potential interrupt, causing to fail. Unfortunately, Java does not allow this static method to throw exception. We should removed the static member variable, so that the method can throw the interrupt exception. The node manager should call the static method, instead of using the static member variable. This fix has an associated benefit: the tests could run faster, because it will no longer need to spawn a bash process when it uses a Shell static method variable (which happens quite often for checking what operating system Hadoop is running on)",design_debt,non-optimal_design,c017171da00a6cd71a2901c84a0298ce14a49e23
hadoop,13770,comment_2,"Rev02: the original design is not desirable, as it swallowed a potential interrupt, causing to fail. Unfortunately, Java does not allow this static method to throw exception. The rev02 removed the static member variable, so that the method can throw the interrupt exception.",design_debt,non-optimal_design,c017171da00a6cd71a2901c84a0298ce14a49e23
hadoop,13770,comment_6,"+1, kicked Jenkins again to get a fresh run. I also marked this as a Blocker for 2.8. We will not be allowed to remove the public member of a Public class once the public member ships in a release or we risk breaking backwards compatibility. Fortunately this public member hasn't been released yet, so we have a chance to fix it cleanly.",design_debt,non-optimal_design,c017171da00a6cd71a2901c84a0298ce14a49e23
hadoop,14092,comment_5,Thanks for the review and commit! Wrong file name is even worse than other misspelled words because folks might get puzzled when they follow instructions and fail to get the result.,code_debt,low_quality_code,3a2e30fa9fe692fe44666c78fbaa04e8469f9d17
hadoop,14351,comment_4,Thanks  for reviewing the patch. Attached another patch containing the checkstyle fixes excluding the fix for the {{FileLength}} related warning. The below findbugs warning is unrelated to changes done in the patch which was introduced in [Commit of HADOOP-10809,code_debt,low_quality_code,8b5f2c372e70999f3ee0a0bd685a494e06bc3652
hadoop,14359,summary,Remove unnecessary shading of commons-httpclient,code_debt,dead_code,b7d769d02002c2101cc157500732f1f8d8dbd5da
hadoop,14359,description,commons-httpclient dependency was removed in HADOOP-10105 but there are some settings to shade commons-httpclient. Probably they can be safely removed.,code_debt,dead_code,b7d769d02002c2101cc157500732f1f8d8dbd5da
hadoop,14359,comment_3,Thanks for the pointer! I really wish we could get rid of commons-httpclient in Hadoop 3. ran mvn dependency:tree and I see commons-httpclient 3 is exposed in two jars: [INFO] [INFO] +- [INFO] | +- [INFO] | | \- [INFO] | +- [INFO] | +- [INFO] [INFO] +- [INFO] | +- [INFO] | | \- [INFO] | +- [INFO] | +-,code_debt,dead_code,b7d769d02002c2101cc157500732f1f8d8dbd5da
hadoop,14479,comment_0,"Thanks for the report . I tried this locally too and ran into the same problem. We're supposed to be running the ISA-L tests as part of precommit, so I'm not sure what happened here. Seems like there's a test gap for ISA-L vs. the Java coder (time to revisit HDFS-11066?)  /  could you assist with debugging this?",test_debt,low_coverage,ea1da39b1908de7739eaecded64074355f0fd5d0
hadoop,14479,comment_3,"Hi , I agree, it would be good to fill the test gap. Let's revisit HDFS-11066. Thanks!",test_debt,low_coverage,ea1da39b1908de7739eaecded64074355f0fd5d0
hadoop,14479,comment_4,"Hmm, I thought the test gap was majorly caused by the precommit missing ISA-L building. Quite some time ago it was done in HADOOP-12626 but removed in HADOOP-13342. I thought we should bring it back and re-enable the ISA-L building & tests in precommit. Andrew, do you think so? If sounds good, we can fire a new issue to do it. Thanks!",test_debt,low_coverage,ea1da39b1908de7739eaecded64074355f0fd5d0
hadoop,14479,comment_8,"First of all, these three failed unit tests are not related with ISA-L version change. There are actually hidden issues. The implementation of native XOR encoder/decoder has array out of index issue which cause the JVM crash. One failed unit test is to test underlying encoder reuse by execution testCoding twice, while the underlying encoder is released explicitly by By review the context, I think is not the right place to release the encoder. 3. One failed unit test is because it doesn't consider the native RS coder situation. I have tried the patch locally. , thanks for reporting this. You can have a try with the new patch. Besides, I will close HADOOP-14593 later since I have merged fixes for 3 unit cases into one patch.",test_debt,expensive_tests,ea1da39b1908de7739eaecded64074355f0fd5d0
hadoop,14479,comment_14,"did we ever file that JIRA to get ISA-L re-enabled? ISA-L is required for production usage of EC, so having testing is really important.",test_debt,low_coverage,ea1da39b1908de7739eaecded64074355f0fd5d0
hadoop,14634,comment_1,"+1, committed. Thanks ray. We all hate spurious JARs sneaking in. the ones we deliberately add are troublesome enough",code_debt,low_quality_code,09653ea098a17fddcf111b0da289085915c351d1
hadoop,14634,comment_4,Thanks for the commit Steve! It's going to be good to have all these sorts of things cleaned up in time for the final Hadoop 3 release.,code_debt,low_quality_code,09653ea098a17fddcf111b0da289085915c351d1
hadoop,14692,description,We should upgrade Apache RAT to something modern.,architecture_debt,using_obsolete_technology,5f4808ce73a373e646ce324b0037dca54e8adc1e
hadoop,14942,summary,DistCp#cleanup() should check whether jobFS is null,code_debt,low_quality_code,f36cbc847560d53e7955ced9ce7ce2773c805793
hadoop,14942,description,"Over in HBASE-18975, we observed the following: came from second line below: in which case jobFS was null. A check against null should be added.",code_debt,low_quality_code,f36cbc847560d53e7955ced9ce7ce2773c805793
hadoop,14942,comment_1,LGTM +1 cleanup code is always good to make robust,code_debt,low_quality_code,f36cbc847560d53e7955ced9ce7ce2773c805793
hadoop,15476,comment_1,"Thanks for catching this . Can we just remove this log message? There is another message below: For wildcard address, localAddr will be null. So perhaps we can fix this other log message to print wildcard if localAddr is null.",code_debt,low_quality_code,f2e29acbfa0b7e1fcecbdcf3e791c96114b456a5
hadoop,15486,description,"Whenever a datanode is restarted, the registration call after the restart received by NameNode lands in via requires write lock on This registration thread is getting starved by flood of calls, which are triggered by clients those who were writing to the restarted datanode. The registration call which is waiting for write lock on is holding write lock on causing all the other RPC calls which require the lock on wait. We can make lock fair so that the registration thread will not starve.",code_debt,multi-thread_correctness,51ce02bb54d6047a8191624a86d427b0c9445cb1
hadoop,15486,comment_0,There's no need for a config. Just make the lock always be fair. Updates to the topology should be rare compared to block placements so the throughput degradation of the fair lock will be minimal.,code_debt,multi-thread_correctness,51ce02bb54d6047a8191624a86d427b0c9445cb1
hadoop,15645,comment_0,"Patch 001 * better setup of implementations, especially bucket config, where per-bucket settings are cleared * and both check the metastore type of the test FS, skip if not correct -and include details on store in the message. * testDiff test erases test FS in testDiff setup through rawFS and metastore FS, in case it gets contaminated This addresses the double setup of dynamodb being picked up on a local test run, the ddb metastore getting tainted with stuff which shouldn't be there, and the s3 bucket getting stuff into it which the getFileSystem() fs doesn't see/delete.",design_debt,non-optimal_design,a13929ddcb3b90044350ae1c23a1150e8b4b975b
hadoop,15742,description,Currently we don't log the info of ipc backoff. It will look good to print this as well so that makes users know if we enable this.,code_debt,low_quality_code,ee051ef9fec1fddb612aa1feae9fd3df7091354f
hadoop,15742,comment_7,"Other than checkstyle issue, +1 for others.",code_debt,low_quality_code,ee051ef9fec1fddb612aa1feae9fd3df7091354f
hadoop,15859,description,"As a follow up to HADOOP-15820, I was doing more testing on ZSTD compression and still encountered segfaults in the JVM in HBase after that fix. I took a deeper look and realized there is still another bug, which looks like it's that we are actually [calling on the ""remaining"" variable on the class itself (instead of an instance of that class) because the Java stub for the native C init() function [is marked leading to memory corruption and a crash during GC later. Initially I thought we would fix this by changing the Java init() method to be non-static, but it looks like the ""remaining"" setInt() call is actually unnecessary anyway, because in reset() we [set ""remaining"" to 0 right after calling the JNI init() So init() doesn't have to be changed to an instance method, we can leave it as static, but remove the JNI init() call's ""remaining"" setInt() call altogether. Furthermore we should probably clean up the class/instance distinction in the C file because that's what led to this confusion. There are some other methods where the distinction is incorrect or ambiguous, we should fix them to prevent this from happening again. I talked to  who further pointed out the ZStandardCompressor also has similar problems and needs to be fixed too.",code_debt,low_quality_code,9abda83947a5babfe5a650b3409ad952f6782105
hadoop,15859,comment_0,Attached a patch that removes the JNI setting of the remaining field per Ben's analysis above and cleans up the naming re: objects vs. classes in the JNI function arguments.,code_debt,low_quality_code,9abda83947a5babfe5a650b3409ad952f6782105
hadoop,16013,description,"sets up a {{Timer}} to schedule a decay of the weights it tracks: However this Timer is not set up as a daemon thread. I have seen this cause my JVM to refuse to exit when running, for example, with FairCallQueue enabled.",design_debt,non-optimal_design,01cb958af44b2376bcf579cc65d90566530f733d
hadoop,16044,summary,ABFS: Better exception handling of DNS errors followup,code_debt,low_quality_code,30863c5ae3a7ad69b6b4853bad2e8f22c7c67639
hadoop,16044,comment_0,"Following up for -HADOOP-15662, c-ompared to L173: - Updated the format to: *Unknown host name: %s. Retrying to resolve the host name...* - Replace *""ex.getMessage()*"" with although they both return host name string, but the *getHost()* seems to be more readable code. - Removed the else if logic as it breaks the current exception trace. Test: testUnknownHost() would take about 14 minutes due to the retry, I verified the warning format in the console: I haven't come up with a good way to retrieve the log msg for test, any suggestions? Testes against US west account passed: All tests passed my US west account: XNS account oauth 35, 0, Errors: 0, Skipped: 0 324, 0, Errors: 0, Skipped: 22 168, 0, Errors: 0, Skipped: 21 XNS account sharedKey: 35, 0, Errors: 0, Skipped: 0 324, 0, Errors: 0, Skipped: 20 168, 0, Errors: 0, Skipped: 15 non-xns account sharedKe: 35, 0, Errors: 0, Skipped: 0 324, 0, Errors: 0, Skipped: 206 168, 0, Errors: 0, Skipped: 15",code_debt,low_quality_code,30863c5ae3a7ad69b6b4853bad2e8f22c7c67639
hadoop,16044,comment_3,"This is a silly question, but why is being retried? Because JVMs do have a history of caching negative DNS lookup results. Looking at the latest javadocs, is set to 10s, so it will eventually come up, but you do have to trust DNS to be updating its records Elsewhere i the code we're just treating this as unrecoverable. Which may not be correct action in a world of dynamicness...but if we do start changing this policy, we should think about doing it consistently everywhere (including HA failover events)",design_debt,non-optimal_design,30863c5ae3a7ad69b6b4853bad2e8f22c7c67639
hadoop,16044,comment_4,"ABFS has been retrying on since it previewed because our understanding is that this exception is thrown for transient name resolution failures. Our retry policy last longer than the typical DNS TTL (or negative cache) of 5 minutes, so the driver could recover and enable a long running task to complete successfully. WASB also retries for these. I expect ADL retries too, although have not confirmed. Mostly we do this for status quo, I mean, it is less likely to cause a regression if we keep the current behavior. With that said, if you have evidence this is a bad design, we should change it. I see that we do the opposite for S3, but I don't know what led to that decision nor do I have a good sense for the behavior in the wild, so I don't know what's best. Certainly retrying is not going to increase the recovery time on the node in question.",design_debt,non-optimal_design,30863c5ae3a7ad69b6b4853bad2e8f22c7c67639
hadoop,16044,comment_5,"bq With that said, if you have evidence this is a bad design, we should change it. no, I don't think it is a bad design. I'm curious. In a classic physical deployment, DNS failures are a bad sign. And, because the JVM cached -ve DNS results *forever* , spinning never fixed things. If the JVMs have stopped doing this, then in a dynamic world, this makes sense. I wondering something broader, which is: is the assumption that not worth retrying"" no longer valid? And if so, what to do about all those existing uses?",design_debt,non-optimal_design,30863c5ae3a7ad69b6b4853bad2e8f22c7c67639
hadoop,16044,comment_6,"Good questions, and the answers are complicated by JVM implementations which may not honor DNS TTL for negative caching. Testing with fault injection could shed light here, but not sure how easy/difficult this is? Could we commit this change while we dig into this more?",test_debt,lack_of_tests,30863c5ae3a7ad69b6b4853bad2e8f22c7c67639
hadoop,16207,comment_1,"could be more fundamental as in ""I'm not sure the committers are correctly telling S3Guard about parent directories"". After each PUT is manifest, we call finishedWrite() , but that seems to do more about purging spurious deleted files, rather than adding dir entries into S3Guard. Provided mkdirs() is called Proposed: build a list of all directories which need to exist as part of a job commit, and only create those entries The other strategy is for to mkdir on the parent. That's a bit more expensive though. Better to not worry about whether it exists and do all this stuff during job commit only",design_debt,non-optimal_design,f44abc3e11676579bdea94fce045d081ae38e6c3
hadoop,16207,comment_8,"Also, to run the tests in parallel - the jobs need to start using a different directory name. Currently, all of them use testMRJob (The method name in the common class that all tests inherit from). The issue with the local dir conflict is a MR configuration afaik (Likely the MR tmp dir config property). YARN clusters should already be able to run in parallel (different ports, random dir names, etc) I'd also be careful trying to run too many of these in parallel, given the amount of memory they consume. Maybe a different parallelism flag for any tests running on a cluster? How about simplifying the code and letting the tests reside in the same class, which makes the code easier to read and allows sharing a cluster more easily. Haven't seen the WIP patch - but sharing a cluster across different tests, which may or may not trigger at the same time seems like it may cause problems. The tests also use a 1 s sleep for the InconsistentFS to get into a consistent state. That can lead to flakiness in the tests. A higher sleep, unless InconsistentFS can be set up with an actual waitForConsistency method which is not time based.",test_debt,flaky_test,f44abc3e11676579bdea94fce045d081ae38e6c3
hadoop,16226,comment_1,thanks for the patch . Maybe use a simple double-slash comment to avoid this checkstyle issue: +1 pending ^,code_debt,low_quality_code,aaaf856f4b7b53d424eb1eab010311de0d5fbe1e
hadoop,16226,comment_2,Thanks  for the review. Added a period to fix the checkstyle warning.,code_debt,low_quality_code,aaaf856f4b7b53d424eb1eab010311de0d5fbe1e
hadoop,16265,summary,is not consistent between default value and manual settings.,code_debt,low_quality_code,1ddb48872f6a4985f4d0baadbb183899226cff68
hadoop,16265,description,"When call getTimeDuration like this: {color:#333333}If ""nn.interval"" is set manually or configured in xml file, 10000 will be retrurned.{color} If not, 10 will be returned while 10000 is expected. The logic is not consistent.",code_debt,low_quality_code,1ddb48872f6a4985f4d0baadbb183899226cff68
hadoop,16265,comment_0,"It is a little wired to return only the raw literal number when default time units and literal number is provided. It may mislead us to unexpected behave if we just take into account its name and parameters. Though changing long 10 to string '10s' will return expected result, I think it should return the same result when long 10 and default time unit SECOND is given.",code_debt,low_quality_code,1ddb48872f6a4985f4d0baadbb183899226cff68
hadoop,16265,comment_1,Maybe I should move it to hadoop-common project.,architecture_debt,violation_of_modularity,1ddb48872f6a4985f4d0baadbb183899226cff68
hadoop,16265,comment_2,"Hey , this is a good catch. It looks like  and I missed this when doing HDFS-14346. Would it not be simpler to just do: It seems a little weird to me to convert a number to a string just so that we can re-parse it into a number.",design_debt,non-optimal_design,1ddb48872f6a4985f4d0baadbb183899226cff68
hadoop,16265,comment_4,Thanks  for catching this! Should we use {{defaultUnit}} instead of in the latest patch v2?,code_debt,low_quality_code,1ddb48872f6a4985f4d0baadbb183899226cff68
hadoop,16409,description,"currently requires a qualified URI (e.g. s3a://bucket/path) which is how I see this being used most immediately, but it also make sense for someone to just be able to configure /path, if all of their buckets follow that pattern, or if they're providing configuration already in a bucket-specific context (e.g. job-level configs, etc.) Just need to qualify whatever is passed in to allowAuthoritative to make that work. Also, in HADOOP-16396 Gabor pointed out a few whitepace nits that I neglected to fix before merging.",code_debt,low_quality_code,"de6b7bc67ace7744adb0320ee7de79cf28259d2d,f99e4f3f2a91b1ef04ffcb5c1913a6cec2dddd55"
hadoop,16435,comment_8,I don't think this is about a single session - these metrics are only used in the [IPC If you create an server and then shut it down - would you expect its metrics to be retained? (IMHO: if it silently retaines any information related to the actual instance that's a resource leak),code_debt,low_quality_code,cbfa3f3e988b8a3f142abadfe4f18201b6438ac9
hadoop,16461,description,"The lock now has a ShutdownHook creation, which ends up doing which ends up doing a ""new Configuration()"" within the locked section. This indirectly hurts the cache hit scenarios as well, since if the lock on this is held, then the other section cannot be entered either. slowing down the RawLocalFileSystem when there are other threads creating HDFS FileSystem objects at the same time.",design_debt,non-optimal_design,aebac6d2d2e612e400a7d73be67dafb47e239211
hadoop,16504,description,"Because default value is too small, TCP's ListenDrop indicator along with the rpc request large. The upper limit of the system's semi-join queue is 65636 and maximum number of fully connected queues is 1024. I think this default value should be adjusted.",design_debt,non-optimal_design,5882cf94ea5094626eb86ff7ac7f8cd32aacb139
hadoop,16504,comment_4,"The problem now is that since listen queue is full, TCP can drop many packet and resend packet after a while. So tcp connection to be slow or even timed out. Changing this default value can reduce client connection timeout when the request is large for nn or dn. Update the v1 patch and upload it. Thank you.",code_debt,slow_algorithm,5882cf94ea5094626eb86ff7ac7f8cd32aacb139
hadoop,16504,comment_6,Looks reasonable to me. We even have customers increasing this number to 2k or even 16k. The only potential downside is it could result in more memory usage. Not sure how much it could be but I don't think that's a concern for typical deployments. I'd like to wait for a day or two for any one else to have a chance to assess & comment before I commit this patch.,design_debt,non-optimal_design,5882cf94ea5094626eb86ff7ac7f8cd32aacb139
hadoop,16523,summary,Minor spell mistake in comment (PR#388),code_debt,low_quality_code,"269b5433672770f43f0061423945d5b02f719187,15f35083896441fea34674acb678120cc2584beb"
hbase,23,description,"Currently, looking at a cluster under load, you'll often trip over the disorientating listing of more than one region with a null end key (Was seen by Billy yesterday and psaab today). UI should list out all of its attributes. Also sort region listings by server address so easier finding servers.",code_debt,low_quality_code,"822ba8bc3ba3f472047b916c9809aa657ac450b0,99fc06705d8d60690a315e4e0287ca1c67d3ad28,7bbe456b0f3a960d05dff66535cc088df5be6502"
hbase,23,comment_16,"FYI, leave out the CHANGES.txt changes. Your patch will fail if someone has made a commit ahead of yours. I tried the patch. Looks really good. I love that we're now showing server names and thanks for renaming catalog tables section. Would suggest that no reason catalog tables shouldn't be clickable as user tables are. Usually will be one region only but even so, the new Table page shows where that region is located and gives a never-before available quick-path to the hosting region server. The Is Split column in the table will probably never be true especially when we are doing this: Would suggest you remove it (Sorry - -my fault for suggesting it in the first place) Would you mind fixing the requests calcuation? Its kinda weird at the moment. Its requests per (default 3 seconds). It should be showing requests per second. Thats what people expect. On the code, FYI, the hadoop convention is two-spaces for tabs. Regards your TODO, that you've duplicated code until we add MetaTable, thats fine. Was wondering if the .META. table was made of multiple regions and your scanner had to cross from one to the other, do you handle that case? Otherwise, the patch is great.",code_debt,duplicated_code,"822ba8bc3ba3f472047b916c9809aa657ac450b0,99fc06705d8d60690a315e4e0287ca1c67d3ad28,7bbe456b0f3a960d05dff66535cc088df5be6502"
hbase,23,comment_17,"It think so. The first loop scans over each region of .META. Regards the calculations, I searched in the whole source and it doesn't seem to be used, only referred to in the text so I think I will just remove it. Correct me if I'm wrong. So that means that they are clickable? I'm confused.",code_debt,dead_code,"822ba8bc3ba3f472047b916c9809aa657ac450b0,99fc06705d8d60690a315e4e0287ca1c67d3ad28,7bbe456b0f3a960d05dff66535cc088df5be6502"
hbase,23,comment_18,"Review please. Things that changed : + Request load is now expressed in req/sec. + ROOT and META tables are clickable. The fact that these tables are not handled like user tables internally impacts the way I could show information. Suggestions appreciated. + Small indentation change. - Removed ""is split"" information on regions.",code_debt,low_quality_code,"822ba8bc3ba3f472047b916c9809aa657ac450b0,99fc06705d8d60690a315e4e0287ca1c67d3ad28,7bbe456b0f3a960d05dff66535cc088df5be6502"
hbase,506,description,"Every so often, we find ourselves trying to debug a problem that happens in HTable where we exhaust all our retries trying to contact the region server hosting the region we want to operate on. Oftentimes the last exception that comes out is something like which should just never be the case. As a way to improve our debugging capabilities, when we decide to throw an exception out of ServerCallable, let's show not just the last exception but all the exceptions that caused all the retries in the first place. This will help us understand the sequence of events that led to us running out of retries.",design_debt,non-optimal_design,37cf8a8cfbef72cc34dd135bafdfe77b21f6098d
hbase,798,comment_1,"After discussing with Jim on IRC, there won't be very many additional functions exposed through HTable. It will actually just be one new version of each of: batchUpdate, deleteAll x 2, and deleteFamily. So it would be those 4, plus the lockRow/unlockRow. You think it would be okay to include just those in HTable w/o subclassing it?",design_debt,non-optimal_design,"e1ab934e5d80c32b63db7569f76b12375fe2a6f7,20165cc25c853220d17a70da6cc57c1a6199ee2d"
hbase,798,comment_2,"I""m fine w/ that. You might want to subclass anyways just because it groups this new functionality nicely.",design_debt,non-optimal_design,"e1ab934e5d80c32b63db7569f76b12375fe2a6f7,20165cc25c853220d17a70da6cc57c1a6199ee2d"
hbase,798,comment_3,"Here's my first go. This adds lockRow and unlockRow methods to the client/HTable. The return type of lockRow is a new client object RowLock (contains long lockid and byte[] row). unlockRow takes a RowLock as argument. Also adds new versions of commit, deleteAll, and deleteFamily that also can take RowLock's. Within HRS I created new versions of the same functions which take the actual lock ids: long in HRS (randomized id as with scannerids, used for leases, etc), Integer in HR (existing type used for row locks). Existing functions, which do not explicitly take locks, now call the same HR functions but with nulls as row locks. Internally HR checks if it was passed a valid row lock or not. If it's a null, it obtains a row lock and releases it at the end of the call. If it's valid, it makes use of the lock, and does nothing at the end. Otherwise an exception is thrown. Code has not been completely cleaned up, but this demonstrates how the client row locks are being implemented.",code_debt,low_quality_code,"e1ab934e5d80c32b63db7569f76b12375fe2a6f7,20165cc25c853220d17a70da6cc57c1a6199ee2d"
hbase,798,comment_11,"Jim, I may have misspoken in my explanation. There are only single HRS methods that take all arguments. Well, there are two of deleteAll but there's different HR implementations. public void deleteAll(final byte [] regionName, final byte [] row, final byte [] column, final long timestamp, final long lockId) public void deleteAll(final byte [] regionName, final byte [] row, final long timestamp, final long lockId) public void deleteFamily(byte [] regionName, byte [] row, byte [] family, long timestamp, final long lockId) public void batchUpdate(final byte [] regionName, BatchUpdate b, final long lockId)",code_debt,duplicated_code,"e1ab934e5d80c32b63db7569f76b12375fe2a6f7,20165cc25c853220d17a70da6cc57c1a6199ee2d"
hbase,798,comment_16,"Reviewed patch. -1 because: - should renew the lease if it is passed an outstanding lock id - should use as it is much more efficient than - In HRegion, factor out the multiple occurrances of: into a private or protected method, such as: - isRowLocked should be protected or private",code_debt,slow_algorithm,"e1ab934e5d80c32b63db7569f76b12375fe2a6f7,20165cc25c853220d17a70da6cc57c1a6199ee2d"
hbase,847,comment_2,"Patch does not apply. Patches must be in svn diff format to be accepted. Please add a test case to demonstrate that getting multiple versions works (should also include multiple versions with timestamp specified) Please do not include a patch for HBASE-52 and HBASE-33 in this patch. Even though they are similar, changes to scanners are more difficult. We try to limit the scope of a single patch in general. Insure that the sub issues of this Jira, HBASE-857, HBASE-31 and HBASE-44 are addressed. Thanks.",test_debt,lack_of_tests,914e6de8de63c1aa5d9739bc7d884856ebac146d
hbase,980,description,"Profiling, I learned that the HBASE-975 makes things worse rather than better. For every Reader opened -- one is opened per store file when we open a region as well as a Reader per file when we compact and then another Reader whenever a Scanner is opened -- the change adds about 4 seeks and at least in the case of compacting and scanning, to no benefit. Even where it is of benefit, when going against HalfMapFiles or when many Store files and we're testing to see if row is in file, it looks like the number of seeks saved are miniscule -- definetly not something that would show up in timings. This issue is about undoing the get of first and last key on open of a store file, the heart of HBASE-975 (975 included a bunch of cleanup refactoring. That'll stay). Profiling seeks, I did notice that we do an extra seek during a get, a reset that takes us to the start of the file. Then internally to getClosest, the core of our get, we're also doing a seek to closest index. Let me try undoing the extra seek and see if it breaks things.",design_debt,non-optimal_design,062e954a03e04749b608270fd2ba28099ce4c17c
hbase,1054,description,"I had some tables refuse to find their indexes even though they were defined and had been updated. Scanning .META. I see that some regions from the table don't have the indexes... Oddly, during the .META. scan, I would see that I had 3 entries per table. (I have very little data in each table, def not splitting yet) But when I visited the UI it showed just one region per table... This patch also addes some toStrings that helped in diagnostics, and a new null check that I found I needed in IndexedTableScanner",design_debt,non-optimal_design,7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8
hbase,1089,comment_5,Committed. Thanks for the nice patch Samuel. We might consider adding this to hbase 0.19.1. Could be used to indicate cruft in the hbase.rootdir; e.g. there is loads of cruft in pset hbase.rootdir since its been migrated across multiple versions.,design_debt,non-optimal_design,2a3f6a5c97c5646906e308032db26806ea0e4e13
hbase,1271,comment_3,"* LocalHBaseCluster - is not so general thing... as I understand it is used for testing.. never for real cluster... so my change will simplify this testing by providing automatic another port binding. * HRS InfoServer is really general thing... but port of InfoServer is not so critical... we can document it like a future... ""Automatic InfoServer port binding"". And if you want explicit port setting we can add console param for HRS start without this changes we cant star't several HRS on one node with info servers * thx... will fix my spelling",code_debt,complex_code,b6ccf3081ae0b7c093ec763db5ee686b755a4a44
hbase,1298,description,"is a key in my ""userdata"" table which happens to be the start key for a region named lists a link to which is incorrect because the "" + "" character is not properly URI Encoded. This results in a misleading user error message: ""This region is no longer available. It may be due to a split, a merge or the name changed. "" manually escaping the "" + "" character as ""%2B"" produces the correct output. A quick skim of master.jsp suggests it has a similar problem: it doesn't URI Encode table names when constructing links to table.jsp",code_debt,low_quality_code,e2eacecb290eac0e7160d07231b9b05ae51ae96b
hbase,1309,comment_2,"In my opinion HBase should not dictate to users what they can or cannot store into the system. I did not say anything about empty keys, which of course makes no sense. The particular circumstance in this place is a crawling application that stores content indexed by SHA-1 hash but also includes a family 'url:' which stores, for each unique object according to hash, the list of URLs where it was encountered. The URLs are stored as column qualifiers to produce a list of unique values with no need for scanning for duplicate elimination etc. There is no need to store data also because all the data is already in the key.",design_debt,non-optimal_design,90e8591cb4ed45a17a2624c412380c8bc4aa5fef
hbase,1359,description,"If you see I removed and ip or something for security reasons Once I truncate the table, hbase freaks out for about 10 seconds and all the thrift servers die. Thrift server log: 2009-04-02 12:09:08,971 INFO Retrying connect to server: /:60020. Already tried 0 time(s). You see this a bunch of times and then it times out The hbase shell 13:01:08 INFO Quorum servers: Truncating t2; it may take a while Disabling table... 09/04/30 13:01:19 INFO client.HBaseAdmin: Disabled t2 0 row(s) in 10.3417 seconds Dropping table... 09/04/30 13:01:19 INFO client.HBaseAdmin: Deleted t2 0 row(s) in 0.1592 seconds Creating table... 0 row(s) in 14.7567 seconds undefined local variable or method `lsit' for #<Object:0x3bbe9a50 from (hbase):3 undefined local variable or method `lsit' for #<Object:0x3bbe9a50 from (hbase):4 null from `proc essRow' from `metaScan' from `metaScan' from `list Tables' from `listTables' from `invoke0' from `invoke' from `invoke' from `invoke' from ndling' from `invoke' from `call' from `cacheAndCal l' from `call' from `interpret' from `interpret' ... 113 levels... from `call' from `call ' from `call' from `cacheAndCal l' from `call' from `__file_ _' from `__file__ ' from `load' from `runScript' from `runNormally' from `runFromMain' from `run' from `run' from `main' from `list' from",code_debt,slow_algorithm,87956cea5bd9866c9110a63dd0bbfb5e3e780262
hbase,1396,summary,Remove unused sequencefile and mapfile config. from hbase-default.xml,code_debt,dead_code,b8854372bc2400d7bde7cae4c2e97f9e2adee47d
hbase,1558,comment_1,"here's a prototype fix, but we need tests.",test_debt,lack_of_tests,"88de4001bb3519710e0ba5d5537c429f2a00c47f,3599514ff658611cd63bf632e397f6db673c2914"
hbase,1655,summary,Usability improvements to HTablePool,design_debt,non-optimal_design,6f242ea63f200fe07dd23fc35984e7a9fd7d8215
hbase,1655,description,"A discussion on the HBase user mailing list led to some suggested improvements for the class. I will be submitting a patch that contains the following changes to HTablePool: * Remove constructors that were not used. * Change access to remaining contstructor from public to private to enforce use of the static factory method getPool. * Change internal map from TreeMap to HashMap because I couldn't see any reason it needed to be sorted. * Remove HBaseConfiguration and tableName member variables since they aren't really properties of the pool itself. They are associated with the HTable that should get instantiated when one is requested from the pool, but not already there.",code_debt,dead_code,6f242ea63f200fe07dd23fc35984e7a9fd7d8215
hbase,1655,comment_1,"Patch looks pretty good. Small nitpicky issues with some needless reorderings of stuff and also some tabbing oddities. Can be fixed on commit. I remember why we're using TreeMap now instead of HashMap. HashMap with byte[] as a key does not work with default comparator, and you cannot pass one in. There is no real downside to using TreeMap unless you have hundreds or thousands of tables, and even then logarithmic on the client-side on the order of 100-1000 is negligible. More efficient in memory but a little dirtier on the GC... however this is almost exclusively client-side (and there's ever only one) so really makes no difference IMO. So, back to TreeMap. The API seems to have grown quite a bit. Do we need all these permutations of getTable() ? Could we drop the String taking ones besides the simplest one? (This is why we don't support String in most of the HBase API now, leads to very long and ugly APIs) The default getTable(byte [] tableName) also requires instantiating a new each time internally, even if we are reusing an existing HTable... Whether there is a significant overhead or not to that, we should avoid it when unnecessary. Typical usage is probably to not supply your own HBC, so if someone wants that level of control, give them the ability to build the Pool themselves rather than expose the friendlier, higher-level methods with all the different ways to instantiate the pool. Blah, this is better said in a new patch....",code_debt,low_quality_code,6f242ea63f200fe07dd23fc35984e7a9fd7d8215
hbase,1655,comment_2,"This doesn't include any stargate stuff, only issues there are the tab issues, reordering, and whether to expose the pools or not. I'm not sure we want to remove the ability to work with HTablePool directly or not. This patch re-publics a bunch of stuff, cuts down on the API quite a bit, and reduces re-instantiation of HBC when getting pools that already exist and will always reuse the original when getting tables. We should definitely keep that. Otherwise the changes are open to discussion, I'm not particularly sold on my own patch, just providing it so we have a point of reference for discussion. The biggest thing to figure out is whether we even expose HTablePool non-statically to the user. If not, we go down the path of a much longer easy-to-use API... /me sleeping on it",code_debt,low_quality_code,6f242ea63f200fe07dd23fc35984e7a9fd7d8215
hbase,1655,comment_4,"A few questions/comments on the comments: - Why does the key to a HashMap need a comparator? - I much prefer the type of String vs byte[] for the tableName throughout all the method signatures and as a key to the internal map, but I tried to keep it as-is because it seemed to fit more with other HBase code. Can we just change it to String throughout and go back to the HashMap? - I am not a fan of the current API either. I'd much prefer that the HTablePool be instantiated by a client and we get rid of the static methods and static Map altogether. This fits in much more nicely for people like me who are using IoC containers like Spring. It also allows the ability to have multiple HTablePool instances, maybe each with their own configuration (which is currently just the max size). - Sorry about the tab/spaces issue. I didn't clean it up carefully enough. - Sorry about the reordering of imports. I use Eclipse which does this automatically and I was too lazy to try to restore the order back to the way it was. I'll try harder next time. So what is the next step? Should I make a new patch with an HTablePool that is meant to be instantiated and w/o the static methods?",code_debt,low_quality_code,6f242ea63f200fe07dd23fc35984e7a9fd7d8215
hbase,1655,comment_6,"- Even though TreeMap uses the comparator rather than the equals method to compare keys, using a byte[] as the key seems to break the contract of a java.util.Map. The last paragraph of the [Map says that impls are free to avoid calling equals() but the containsKey(Object key) method should ""return true if and only if this map contains a mapping for a key k such that (key==null ? k==null : key.equals(k))"". As you mentioned, you'd need a byte[] wrapper in order to be compliant. So a byte[] key will work in a TreeMap (and not in a HashMap), but we'd break the Map contract. Seems better to just use String and HashMap which works well and satisfies the Map contract. - Allowing both the static methods and the public construction would bring me back to the complaint I had originally with this class. :) It becomes confusing for a user trying to quickly understand how he/she is supposed to interact with this class. I guess this can all be explained away with documentation, but, it doesn't feel ""good"". - Also, yes, there would be a problem with re-instantiating the static map multiple times, but this could be prevented by implementing a Singleton pattern so that the static access operates on an internal singleton instance with a member Map rather than a static Map. - I agree that there is no reason to construct HBC's for each invocation. An internal HBC instance can be constructed and reused. - Is there a checkstyle.xml file for HBase? That would make it easy to check my code for formatting problems before submitting patches.",design_debt,non-optimal_design,6f242ea63f200fe07dd23fc35984e7a9fd7d8215
hbase,1655,comment_7,".bq So a byte[] key will work in a TreeMap (and not in a HashMap), but we'd break the Map contract. Seems better to just use String and HashMap which works well and satisfies the Map contract. Looking at the TreeMap equals implementation, yes, it breaks the above contract for Map. We're impure (insert lots of sack cloth and ashes, self-flagellation, etc., here -- smile). We are zealous about using byte [] everywhere. As Jon allows above, client-side, we can relax some. IMO, we should not be allowing public construction and static methods. +1 on Singleton to prevent multiple instantiations. There is no checkstyle (maybe there is one up in hadoop). In general 80 characters per line and two spaces for tab. Anything else we'll take. Don't worry about it too much.",design_debt,non-optimal_design,6f242ea63f200fe07dd23fc35984e7a9fd7d8215
hbase,1655,comment_11,"I've submitted another patch, that reflects a different approach taken to improve HTablePool. In the new approach, there will be no static methods at all. Instead, the normal usage of the class will be to instantiate an HTablePool directly and use the getTable and putTable methods to get an HTable and return an HTable to the pool. The pool class will automatically instantiate a new HTable in the case that no HTable for the given table name is in the pool OR if the internal pool for that table name is full (has more elements than its configured max size). The default max size is Integer.MAX_VALUE. While we lose the convenience of calling static methods, we gain a design with less code that is easy to understand, easy to use in an IoC container, and easy to unit test. This patch also includes a small modification to HTable that allows an instantiation to HTable with a null HBaseConfiguration object. This makes it possible to unit test classes such as HTablePool that internally use an HTable without requiring an environment in which the HTable can get and use a connection. Finally, this patch includes modifications to the stargate package reflecting the new HTablePool API. Let me know if this is suitable for inclusion in the project. Thanks to Jon Gray and Stack for their help and input.",design_debt,non-optimal_design,6f242ea63f200fe07dd23fc35984e7a9fd7d8215
hbase,1723,comment_0,"Use get.setTimeStamp instead of get.setTimeRange This patch rearranges getRowWithColumnsTs a bit, because the original version was unneccessary repetitive.",code_debt,duplicated_code,"027f01aa9948a615b06857858a3256beac1b80fc,2f98fe462560d628df98d0acdf36e594ebd96d1d"
hbase,1723,comment_7,substitute setTimeRange with setTimeStamp in several methods for consistent behavior. Also adjust TestThriftServer.,design_debt,non-optimal_design,"027f01aa9948a615b06857858a3256beac1b80fc,2f98fe462560d628df98d0acdf36e594ebd96d1d"
hbase,2068,comment_2,Another inconsistency is that MasterMetrics uses a MetricsIntValue and the uses a MetricsRate class I suggest after fixing MetricsRate to replace the MasterMetrics one with it as well. I am also wondering if it would have been better to call it MetricsIntRate to indicate the size of the internal counter.,code_debt,low_quality_code,"dfd0303cd7e845d8bf4a261d734b5fa5315cbaef,b378ea615f05e7ed2f4441049dac1b676ed3f5cb"
hbase,2068,comment_13,"+1 Works great! There are few now obsolete imports, like in the changed Statistics classes. Could remove on commit. Otherwise please commit.",code_debt,low_quality_code,"dfd0303cd7e845d8bf4a261d734b5fa5315cbaef,b378ea615f05e7ed2f4441049dac1b676ed3f5cb"
hbase,2068,comment_14,Committed. Thanks for the patches lads. I removed the unused import. Added a couple of licenses too. Excellent.,code_debt,low_quality_code,"dfd0303cd7e845d8bf4a261d734b5fa5315cbaef,b378ea615f05e7ed2f4441049dac1b676ed3f5cb"
hbase,2085,description,Some references in toString() converted from StringBuffer to StringBuilder as concurrency is probably not needed in those contexts as the references do not get out of scope.,design_debt,non-optimal_design,94464432f8e2b342dcca3da0c4f585a70ca5a1ab
hbase,2241,description,"This is a quick workaround until we do a better balancer. Taking a region off line when cluster is under load is bad news. Latency goes up as we wait on regions to come up in new locations. The load balancer should only cut in if the cluster is way out of alignment. I'd argue that 10% deviance from the avg. is not good enough reason moving regions around when cluster is under load. Balancer already has a knack for cutting in at most inopportune moments: during cluster startup, when new node is added to a small cluster, or moving a region just after its been opened on a node. We'll need to do a better balancer but meantime lets just allow that region loading can be sloppier, say 20% or 30% off the average before balancer cuts in.",code_debt,slow_algorithm,"ecb8ecae3fbc7f893245996a5b841f4e0d791b24,3c4ae2d59808ff3e931228a558b7f8af2998a507"
hbase,2295,comment_6,"@Dhruba If you want to use a SortedSet and pass a comparator instead, so you can retain byte [] elements, there is a bytes comparator here: Otherwise, patch looks good. Regards TRUNK, its been mavenized so to build it, you need maven -- see -- and then to build and run tests do something like mvn install",design_debt,non-optimal_design,"782f78f29de05f611aef7833977161745832d330,bfd6812b409c56269349db51b1bb5af7134a59c3,c9d8e720bb5fee1f62687683b905d3a480304650,1030b603161b831838e7cb04de226c680adb60ad,8102fb326997b11bd1498e22fd9f692c9df260ad"
hbase,2295,comment_10,I agree isEmpty is better than size... I'll just put fix in under this issue. Thanks Todd. I changed it to do this instead on trunk and branch:,code_debt,low_quality_code,"782f78f29de05f611aef7833977161745832d330,bfd6812b409c56269349db51b1bb5af7134a59c3,c9d8e720bb5fee1f62687683b905d3a480304650,1030b603161b831838e7cb04de226c680adb60ad,8102fb326997b11bd1498e22fd9f692c9df260ad"
hbase,2341,comment_8,Cosmin suggests that we instrument the code with a coverage tool while running a long-running cluster test. We can then identify edge cases / dark corners better (and write tests to exercise them),test_debt,lack_of_tests,bb5984fe823929b24e62fe9a820b43cd54774bfe
hbase,2555,summary,Get rid of,design_debt,non-optimal_design,25580f47a7a875b0474084e20853a4acaa2a17a6
hbase,2555,description,"Now that we have HFile, it seems that the constant is no longer useful. I've grepped around the codebase and couldn't find it used elsewhere. Perhaps kept around in case folks decide to store their HBase data into a MapFile?",code_debt,dead_code,25580f47a7a875b0474084e20853a4acaa2a17a6
hbase,2555,comment_0,Patch removes constant and two unused HCD methods which use constant. HBase does not support anything but HFiles right now.,code_debt,dead_code,25580f47a7a875b0474084e20853a4acaa2a17a6
hbase,2694,comment_6,"Final patch for commit. Includes changes from Todd's comments, added a few licenses, and also fixed mixed newlines on final patch file.",code_debt,low_quality_code,"b1ef73ff1fbb62f9993edd64e622af550d8b089d,3fc439728920c397d1fdd3519b093e91d97d09e9,ebb698fd094d8b09e498c88c107a1a2ddd20eb25"
hbase,2925,comment_6,"@stack, thank you for picking up this issue. Feel free to reuse the code provided in in any way you want. (Let me know if this is not enough I could submit another file with Apache license added.) Back to the discussion, I agree that removing the {{hasCode()}} and {{equals()}} methods does resolve this issue. However, I wonder if it defeats the purpose of having a connection cache from the first place. You also alluded to the problem of the configuration settings changing after the connection is open, which, if we rely solely on Object equality, could lead to subtle error or unexpected behavior. Hence my suggestion to pin down the configuration once the TableServers instance is created. The disadvantage there is that small/unrelated changes in the configuration will invalidate the cache entry as well. If that is deemed a real problem given the common use cases, I was thinking maybe we define ""equality"" of the configuration for the purposes of caching as the equality of all properties that start with {{hbase.}} or {{zk.}} or any other prefix whose properties could be used as possible connection parameters. Not sure if the comparison becomes too slow in that case, but what is more important is whether correctness is compromised if we make the assumption that only properties starting with those prefixes affect the connection.",design_debt,non-optimal_design,f0f9240de4e0ba6f2267d1d371eccf88deabe5cc
hbase,2925,comment_10,You may have left out a couple of print statements in TestHCM. Not sure if that was intentional. Otherwise it's good to go.,code_debt,low_quality_code,f0f9240de4e0ba6f2267d1d371eccf88deabe5cc
hbase,2969,description,"Considering that the method _getTable(String)_ in is invoked by multiple threads, it may happen that while _'queue == null'_ is true, it is possible to have a queue mapped to that name into the tables map when queue)'_ is executed. However, I don't think it will cause any harm because the overwritten queue will eventually be garbage-collected. :-)",design_debt,non-optimal_design,e144e8f32dfd9f36036e0402c43eb93956ba82ca
hbase,3057,summary,Race condition when closing regions that causes flakiness in TestRestartCluster,test_debt,flaky_test,8863ae5a0e249e255485d4474aaa4b1791e4db3f
hbase,3057,description,"In we spin up cluster, create three tables, shut it down, start it back up, and ensure we still have three regions. A subtle race condition during the first shutdown makes it so the flush of META doesn't finish so when we start back up there are no user regions. I'm not sure if there are reasons the ordering is as such, but the is the section of code in CloseRegionHandler around line 118: We remove from the online map of regions before actually closing. But what the main run() loop in the RS is waiting on to determine when it can shut down is that the online region map is empty. Any reason not to swap these two and do the close before removing from online regions?",code_debt,multi-thread_correctness,8863ae5a0e249e255485d4474aaa4b1791e4db3f
hbase,3257,comment_1,"Patch was put to review board, but it's not forwarded to jira. Here is the review request: Summary: Coprocessors: Extend server side integration API to include HLog operations Coprocessor based extensions should be able to: - Observe, rewrite, or skip WALEdits as they are being written to the WAL - Write arbitrary content into WALEdits - Act on contents of WALEdits in the regionserver context during reconstruction Code changes: - a new coprocessor interface WALCPObserver is added which provides preWALWrite() and postWALWrite() upcalls to HLog, before and after at doWrite(). - added 2 new upcalls for RegionObserver for WAL replay, preWALRestore() and postWALRestore(). - a sample implementation -- -- was create which can add, remove, modify WALEdit before writing it to WAL. - test cases which use to test WAL write and replay. - added coprocessor loading at TestHLog to make sure it doesn't affect HLog. I need feedback for: - The new cp interface name -- WALCPObserver -- is not perfect. The ideal name is WALObserver but it's been used already. Other options include HLogObserver(H is not preferred), - No support for monitor master log splitting. I don't have a use case to support the requirement right now.",code_debt,low_quality_code,ba3426321abc327246dd869243eb54de98ba05c4
hbase,3368,comment_1,"Agreed that we should move all messaging up into zk and remove heartbeating carrying messages but I was thinking that for 0.90, delete of region clears it from HMaster in-memory too. The worst that could happen is balancer ran meantime. If so, it'll fail close of a region not opened anywhere but the in-memory PENDING_CLOSE would be cleared by the delete-of-region cleanup. What you think?",architecture_debt,violation_of_modularity,"1cb79368d36cba4076e37bd5cead7b0b6536ec72,caf235d34ddf78839a529f02e4abc3f8aae6b2c5"
hbase,3368,comment_3,"As we discussed, I think comment is a little misleading with ""just in case"". I agree that this should not be necessary but that's because the current design is suboptimal. Otherwise this seems fine. When were we doing the regionOffline previously?",design_debt,non-optimal_design,"1cb79368d36cba4076e37bd5cead7b0b6536ec72,caf235d34ddf78839a529f02e4abc3f8aae6b2c5"
hbase,3510,description,"The IPC readers come out of a thread pool but have no name, which is annoying.",design_debt,non-optimal_design,81022285b9aa787d531bd3f6f2bc9dd30faefd40
hbase,3510,comment_1,Same comment as last time - it's difficult to get to the RS name from here. So I just added the port - that makes it consistent with the other IPC threads.,design_debt,non-optimal_design,81022285b9aa787d531bd3f6f2bc9dd30faefd40
hbase,3597,comment_4,Looks fine. Aren't all those expensive?,code_debt,slow_algorithm,6aa6a4ad8fa525f47121816a5276a2a940472c97
hbase,3597,comment_5,"That would be called on average once or twice per second on a normal cluster, I'm pretty sure is a few order of magnitudes more expensive than what those metrics are doing.",code_debt,slow_algorithm,6aa6a4ad8fa525f47121816a5276a2a940472c97
hbase,3625,description,"Currently the surefire plugin configuration defines the following exclusion: AFAICT the '{{/$**}}' does not resolve to anything meaningful. Adding support to exclude one or more tests via Maven property, i.e.",design_debt,non-optimal_design,4953a4fd11448d095dce0be3faae740fde78afff
hbase,3673,description,"In the case of medium-to-large sized HTable pools, the amount of time the client spends blocking on the underlying map and queue data structures turns out to be quite significant. Using an efficient wait-free implementation of maps and queues might serve to reduce the contention on the pool. In particular, I was wondering if we should replace the synchronized map with a concurrent hash map, and linked list with a concurrent linked queue.",design_debt,non-optimal_design,a344cd98b78a8b3e0e533789911178c62e4d6016
hbase,3673,comment_1,"The patch did seem to make a difference in our particular use case, in terms of the average time it took to get a htable from the pool. For the sake of a more formal evaluation, I put together a benchmark (see attached test case), which did show noticeable difference. Specifically, when you've a htable pool of size 150, and a worker pool of 100 threads, where each thread does a get followed by a put a million times, the total time spent was 7775614 ms with the patch, as opposed to 12654820 ms without. That's turns out to be a 40% improvement. That said, using different htable pools for different use cases (e.g., different htables) might be the way to go, as that will tend to reduce the level of concurrency on any given htable pool.",code_debt,slow_algorithm,a344cd98b78a8b3e0e533789911178c62e4d6016
hbase,3807,description,Currently the metrics are a mix of MB and bytes. Its confusing.,design_debt,non-optimal_design,a0478e5aed6ffceb1dd76884065b90d08f95acf6
hbase,3807,comment_5,Removing the unused code for determining time elapsed in RegionServerMetrics : int seconds = - if (seconds == 0) { seconds = 1; } Will upload the patch ASAP,code_debt,dead_code,a0478e5aed6ffceb1dd76884065b90d08f95acf6
hbase,4016,description,"We wanted to use an int (32-bit) atomic counter and we initialize it with a certain value when the row is created. Later, we increment the counter using This call results in one of two outcomes. 1. The call succeeds, but the column value now is a long (64-bit) and is corrupt (by additional data that was in the buffer read). 2. Throws offset (65547) + length (8) exceed the capacity of the array: 65551 Source) Based on our incorrect usage of counters (initializing it with a 32 bit value and later using it as a counter), I would expect that we fail consistently with mode 2 rather than silently corrupting data with mode 1. However, the exception is thrown only rarely and I am not sure what determines the case to be executed. I am wondering if this has something to do with flush. Here is a HRegion unit test that can reproduce this problem. We modified our code to initialize the counter with a 64 bit value. But, I was also wondering if something has to change in to handle inconsistent counter sizes gracefully without corrupting existing data. Please let me know if you need additional information.",design_debt,non-optimal_design,95dc02182e6edd5d39d32b938c30b370ee87b9bc
hbase,4016,comment_0,"Your code is storing strings in the cells, but expects a big-endian encoded long, not a string.",design_debt,non-optimal_design,95dc02182e6edd5d39d32b938c30b370ee87b9bc
hbase,4088,comment_1,Small fix to logging message -- check for null before getting list size.,code_debt,low_quality_code,148627fe9c029405b393a0bbcf5c16cec5cd48a9
hbase,4247,comment_1,"On 1., agree. I agree on 2. too. On your 'Secondly', yes. That seems dirty. Agree on calling stop from abort (Doesn't it do this in a few places? IIRC).",code_debt,low_quality_code,"f70dd57e1ebee1a4a3251a7c0378789c394843fa,3b0b606aa8a89bfc08fc52b51e8110558434f246"
hbase,4247,comment_5,Plz excuse me about the formatting. I didn't know the Markup tags the codes should be placed under,code_debt,low_quality_code,"f70dd57e1ebee1a4a3251a7c0378789c394843fa,3b0b606aa8a89bfc08fc52b51e8110558434f246"
hbase,4250,summary,"developer.xml - restructuring, plus adding common patch feedback",code_debt,low_quality_code,b0782cbfc04d0d2cf0c5f54a6719e7586999682e
hbase,4250,description,"* Did some restructuring of sections * Added section for common patch feedback (based on my own experiences in a recent patch). ** I ran these past Todd & JD at this week's hackathon. * mentioning ReviewBoard in patch submission process ** this didn't appear anywhere before. I still need to add more on tips on using it, but this is a start.",code_debt,low_quality_code,b0782cbfc04d0d2cf0c5f54a6719e7586999682e
hbase,4303,summary,has bad quoting,code_debt,low_quality_code,547e9a60a1af45217693662e6b246644f3bca224
hbase,4303,description,"Currently it's outputting: REGION = Notice the missing quotes around tableName, etc.",code_debt,low_quality_code,547e9a60a1af45217693662e6b246644f3bca224
hbase,4341,description,"This's the reason of why did get failure . In this test, one case was timeout and cause the whole test process got killed. [logs] Here's the related logs(From [Analysis] One region was opened during the RS's stopping. This is method of HRS#onlineRegions is a ConcurrentHashMap. So walk down this map may not get all the data if some entries are been added during the traverse. Once one region was missed, it can't be closed anymore. And this regionserver will not be stopped normally. Then the following logs occurred:",design_debt,non-optimal_design,52ccf8db3514f992d5bdffb62eeb61834839fda1
hbase,4437,comment_3,Move to 0.20.205.0 hadoop. Also includes edits to our jsp and jamon templates commenting out DOCTYPE to get around bug where css is served as text/html. See tail of HBASE-2110 for discussion.,design_debt,non-optimal_design,"d3927860a892feedabf2e34e709b77e6df118798,666236a8b83b9584ba83d72d3bf546675348ad6c"
hbase,4459,description,"There are about 90 classes/codes in HbaseObjectWritable currently and Byte.MAX_VALUE is 127. In addition, anyone wanting to add custom classes but not break compatibility might want to leave a gap before using codes and that's difficult in such limited space. Eventually we should get rid of this pattern that makes compatibility difficult (better client/server protocol handshake) but we should probably at least bump this to a short for 0.94.",design_debt,non-optimal_design,8a0699e585722b7e69ade8c71a3e32a396993fda
hbase,4459,comment_7,- Why is Queue added within the scope of this JIRA? Seems unrelated. - Can you remove the unnecessary import re-org at the top? - Can we have a unit test which shows the backwards compatibility of this? Thanks for working on this Ram.,code_debt,low_quality_code,8a0699e585722b7e69ade8c71a3e32a396993fda
hbase,4459,comment_8,What is this about? Is it necessary to this patch? And this: Looks good Ram. Any chance of a test to prove it works the way it used to?,test_debt,lack_of_tests,8a0699e585722b7e69ade8c71a3e32a396993fda
hbase,4740,description,"Running more frequently seems to show that it has become flaky. It is difficult to tell if this is because of a unrecoverable failure or a recoverable failure. To make this visiable from test and for users, we need to make a change to bulkload call's interface on HRegionServer. The change should make successful rpcs return true, recoverable failures return false, and unrecoverable failure throw an IOException. This is an RPC change, so it would be really good to get this api right before the final 0.92 goes out.",test_debt,flaky_test,78ebe94d05b29c84e01895460cdb10d2d3095857
hbase,4740,comment_6,"@Stack Yeah, 0 is actually the original behavior in the pre-HBASE-4552 version it I think would just eat exceptions and bail out without completing. It is more complicated because of bulk atomicity. Will update boolean if it works -- there is some template checking in another place so assumed it needed boxed type. The difference is that the version uses a different instance. I'll refactor to exclude that portion and require it in the test. I tried the previous version with a small data set on psuedo-dist cluster and live cluster. For this particular patch I tried this one by looping the relevant unit tests 100 times and seeing that they passed all the time. I haven't tested this exact version on real cluster.",code_debt,low_quality_code,78ebe94d05b29c84e01895460cdb10d2d3095857
hbase,4778,comment_3,"@Ted: In this particular case, I think we ended up finding that HDFS wasn't reading truncated HFiles properly. Overall though, the idea is to prioritize data integrity & consistency over availability. We shouldn't be silently opening regions with missing data. We should instead understand why the data is missing. If someone wants to add a flag and allow this to happen, then that's fine.",code_debt,low_quality_code,6ebf31873539e0e1b5d6badff5cc0d4dee73a146
hbase,4859,description,"See description at HBASE-3553. We had a patch ready for this in HBASE-3620 but never applied it publicly. Testing showed massive speedup in HBCK, especially when RegionServers were down or had long response times.",code_debt,slow_algorithm,2261fc23c97e596be917253bbaa9afdf9a0c8c6a
hbase,5039,summary,"[book] book.xml - more cleanup of Arch chapter on regions, adding a FAQ entry",code_debt,low_quality_code,5543c548c40bc4fc500ebb86df70d89cf55a5534
hbase,5039,description,book.xml * Arch chapter/Regions. clearing up a little more in region assignment * FAQ. Adding an architecture section. * MapReduce chapter. Fixed nit that Ian Varley brought to my attention on RDBMS summary.,code_debt,low_quality_code,5543c548c40bc4fc500ebb86df70d89cf55a5534
hbase,5052,comment_0,"We could work on better cleaning the region name, but I think it is far better to depend on something like the hashed name",design_debt,non-optimal_design,25d465426aaf064d4f857d0fe304425da7007de3
hbase,5282,summary,Possible file handle leak with truncated HLog file.,code_debt,low_quality_code,36788d1cc258e56f48df68e706db36db53eb484b
hbase,5282,description,"When debugging hbck, found that the code responsible for this exception can leak open file handles.",code_debt,low_quality_code,36788d1cc258e56f48df68e706db36db53eb484b
hbase,5282,comment_0,"When debugging, open region file was attempting to open either a truncated or 0 size hlogfile (which is throws IOException at out from getReader), and leaking a handle on every open attempt. Patch applies on 0.92 and trunk.",code_debt,low_quality_code,36788d1cc258e56f48df68e706db36db53eb484b
hbase,5329,comment_8,Thanks for activating this JIRA. Indentation is off for the above block. I prefer the old way of keeping long lockId so that the above parsing can be omitted.,code_debt,low_quality_code,bbe297121d127ef14863d8763d20fcedf198904d
hbase,5466,description,"Having upgraded to CDH3U3 version of hbase we found we had a zookeeper connection leak, tracking it down we found that closing the connection will only close the zookeeper connection if all calls to get the connection have been closed, there is incCount and decCount in the HConnection class, When a table is opened it makes a call to the metascanner class which opens a connection to the meta table, this table never gets closed. This caused the count in the HConnection class to never return to zero meaning that the zookeeper connection will not close when we close all the tables or calling true);",code_debt,low_quality_code,49904bcf1d62fb711b0a65c857b6ee209ee88784
hbase,5466,comment_1,Thanks for the finding. We use two spaces for indentation. Can you regenerate patch ? Refer to HBASE-3678.,code_debt,low_quality_code,49904bcf1d62fb711b0a65c857b6ee209ee88784
hbase,5466,comment_5,+1 on patch (except for the spacing that is not like the rest of the file),code_debt,low_quality_code,49904bcf1d62fb711b0a65c857b6ee209ee88784
hbase,5466,comment_6,"TestZooKeeper passed locally with patch v2. There should be a space between } and finally, finally and {, if and (, ) and { Overall, +1 on patch v2. Please fix formatting in v3.",code_debt,low_quality_code,49904bcf1d62fb711b0a65c857b6ee209ee88784
hbase,5523,description,"A Delete at time T marks a Put at time T as deleted. In parent I invented special logic that insert a virtual millisecond into the tr if the encountered KV is a delete marker. This was so that there is a way to specify a timerange that would allow to see the put but not the delete: Discussed this today with a coworker and he convinced me that this is very confusing and also not needed. When we have a Delete and Put at the same time T, there *is* not timerange that can include the Put but not the Delete. So I will change the code to this (and fix the tests): It's easier to understand, and does not lead to strange scenarios when the TS is used as a controlled counter. Needs to be done before 0.94 goes out.",design_debt,non-optimal_design,723e8559e399a130bc7a5a589575f72e95de8e8f
hbase,5523,comment_3,"I remember the initial motivation for the +1 shift now. If somebody accidentally places a Delete at T it would not be possible to get at any Puts of T with normal Scan API. The +1 allow setting an interval that includes the Puts but not the Delete. I.e. setting the range to [0,T+1) would include the Puts and Deletes. (note that the lower bound inclusive and the upper bound is exclusive hence the [x,y) notation). With the +1 shift [0,T+1) would not contain the Delete, but [0,T+2) would. This is very confusing, since [0,T+1) doesn't actually mean that when it comes to deletes. To recover the above mentioned Puts one could use a raw scan, instead. I'm going to commit the attached patch; it makes these scenarios much clearer.",design_debt,non-optimal_design,723e8559e399a130bc7a5a589575f72e95de8e8f
hbase,5591,comment_0,"sc requested code review of ""HBASE-5591 [jira] is identical to Bytes.getBytes()"". Reviewers: tedyu, dhruba, JIRA is identical to Bytes.getBytes() Remove the redundant method. Task ID: # Blame Rev: TEST PLAN Revert Plan: Tags: REVISION DETAIL AFFECTED FILES MANAGE HERALD DIFFERENTIAL RULES WHY DID I GET THIS EMAIL? Tip: use the X-Herald-Rules header to filter Herald messages in your client.",code_debt,duplicated_code,a379a41e3d5b719fbd57b3f0a993d9ff68a5a94b
hbase,5591,comment_5,"sc has commented on the revision ""HBASE-5591 [jira] is identical to Bytes.getBytes()"". INLINE COMMENTS It was added by me actually. Because I checked bb) and found that it's different from this one. But this one is the same as bb). These names are really confusing. REVISION DETAIL BRANCH getbytes",code_debt,low_quality_code,a379a41e3d5b719fbd57b3f0a993d9ff68a5a94b
hbase,5635,comment_0,"Yes, I think, continuing without SplitLogWroker may not be a good behaviour. Because that particular regionServer may have more capacity to take up the new regions. With the current behaviour it may not compete for taking any new splilog work. I feel we can retry for some times and then we can shutdown regionServer? or other option is to retry forever on any ZK exception. And can exit only on interrupted exception. Also i am seeing this issue may be bit dangerous bacause, if ZK is not available for some time, all RegionServer may face this problem and no one will take up the splitlog work. will return null only if node does not exist. If it is not able to find any children then it will return empty list. So, will be always set. On Other keeperExceptions like ZK unavalability and all, we have to handle.",design_debt,non-optimal_design,e2cd6750204db2c617a790658a6f14d3aa79566c
hbase,5635,comment_2,Please change the wording in the following log: In this case the worker thread is not exiting.,code_debt,low_quality_code,e2cd6750204db2c617a790658a6f14d3aa79566c
hbase,5635,comment_7,Then we should log a message saying what we wait for every X minutes so that user doesn't have to use jstack.,design_debt,non-optimal_design,e2cd6750204db2c617a790658a6f14d3aa79566c
hbase,5635,comment_11,As per the patch the below variable is of no use now,code_debt,dead_code,e2cd6750204db2c617a790658a6f14d3aa79566c
hbase,6050,summary,"HLogSplitter renaming recovered.edits and CJ removing the parent directory race, making the HBCK think cluster is inconsistent.",design_debt,non-optimal_design,e17357d77a4f181af1dd9c419c345459efddb655
hbase,6050,description,The scenario is like this There if the regiondir doesnot exist we tend to create and then add the recovered.edits. Because of this HBCK thinks it to be an orphan region because we have the regiondir but with no regioninfo. Ideally cluster is fine but we it is misleading.,design_debt,non-optimal_design,e17357d77a4f181af1dd9c419c345459efddb655
hbase,6050,comment_8,Patch looks good. Minor: Please insert spaces around regionDir:,code_debt,low_quality_code,e17357d77a4f181af1dd9c419c345459efddb655
hbase,6201,comment_6,"Sorry for chiming in late, but here's how I see it after quite a bit of internal discussions with some of the HBase and HDFS devs. First of all, lets not got caught up in terminology of what is a unit test, what is a functional test, etc. If we assume this stance than all the tests, essentially, fall under the 3 main categories: # tests that muck about with the internals of the particular single project (HDFS, HBase, etc) using things like private APIs (or sometimes even things like reflections, etc to really get into the guts of the system) # tests that concern themselves with a single project (HDFS, HBase, etc) but use only public APIs AND don't use # tests that concern themselves with multiple projects at the same time (imagine a test that submits an Oozie workflow that has some Pig and Hive actions actively manipulating data in HBase) but only using public APIs It is pretty clear that #3 definitely belongs to Bigtop while #1 definitely belongs to individual projects. For quite some time I was thinking that #2 belongs to Bigtop testbase as well, but I've changed my mind. I now believe that such tests should reside in individual projects and: # be clearly marked as such not to be confused with class #1 (test suites, test lists, naming convention work, etc) # be written/refactored in such a way that doesn't tie them to a particular deployment strategy. IOW they should assume the subsystem to be deployed. # be hooked up to the project build's system in such a way that takes care of deploying the least amount of a system to make them run (e.g. MiniDFS, MiniMR, etc.) Thus if HBase can follow these rules and have a subset of tests that can be executed in different envs. both HBase core devs and bigger Bigtop dev community win, since we can leverage each other's work. Makes sense? If it does I can help with re-factoring.",code_debt,low_quality_code,74b66e46e8ee36bc96f7fed8b2bfcfb21c178cf3
hbase,6201,comment_10,"Yes. At the current state, most of our unit tests, which are candidates to be upgraded to be system tests does start a mini-cluster of n-nodes, load some data, kill a few nodes, verify, etc. We are them to do the same things on the actual cluster. A particular test case, for example, starts 4 region servers, put some data, kills 1 RS, checks whether the regions are balanced, kills one more, checks agains, etc. Some basic functionality we can use from itest are: - Starting / stopping / sending a signal to daemons (start a region server on host1, kill master on host2, etc). For both HBase and Hadoop processes. - Basic cluster/node discovery (give me the nodes running hmaster) - Run this command on host3 (SSH)",design_debt,non-optimal_design,74b66e46e8ee36bc96f7fed8b2bfcfb21c178cf3
hbase,6244,description,"Now, the RowResultGenerator and the will fit the column family if the request doesn't contain any column info. The will cost 10+ milliseconds in our hbase cluster each request. We can remove these code because the server will auto add the columns.",design_debt,non-optimal_design,3ec92cc8b1627fe017ac5138759899aa41953245
hbase,6282,comment_8,"Lets apply this and close this issue. W/ this applied, if you enable TRACE, you see this kinda of stuff: It is ridiculous detail but could be life saver debugging. Could later work on pb toStringing so it doesn't dump it all... just start of String and a length instead but this should be good for now. Remove Objects class since no longer used.",code_debt,dead_code,"e34f3890f32a7e46b659995ad8bf63a27478058a,f38ee24ed0261f0aa0a4dc61f43967c2e43325e8"
hbase,6697,comment_0,"master & region server ports where set to ""random"" in the LocalHBaseCluster class; but not in the test hbase-site. So the pattern used in this test class was not working. Fixed, let's see if it breaks anything on hadoop qa...",code_debt,low_quality_code,55fb6632b9dfb9560510b9c5f319527ed49c5abd
hbase,6806,comment_5,"python, c++ and java: worksforme I had no chance to test php, perl and ruby, they might even have syntactic errors in it...",test_debt,lack_of_tests,7c9d6f34400c8c92297ca55428307d51f66f31e4
hbase,6969,description,"While not an official part of the HBase API, it'd be nice if HBaseTestingUtility allowed for consumer to inject their own instance of the when executing tests. Currently there is no way to control how the ZK instance is started which we've had to hack around when there were ZK version conflicts (3.3 vs 3.4). Or if you want to control which specific port it starts on vs random. Allowing consumers to inject an instance (unstarted) gives them the freedom to implement their own without having to fork the entire HBaseTestingUtility class",design_debt,non-optimal_design,e7f5eea91c296fc80f184404743accc5b849a15e
hbase,6969,comment_2,"As written, at end of tests, won't your passed in zkcluster be shutdown? That is probably not what you want? Should there be a to answer your added Is baseZKCluster necessary? Why not just an internal flag which has whether or not HBaseTestingUtility started the zk cluster? If we didn't start it, we shouldn't stop it on the way out? Otherwise, looks like useful functionality to add. Thanks Micah.",design_debt,non-optimal_design,e7f5eea91c296fc80f184404743accc5b849a15e
hbase,7000,comment_4,patch looks good. nit: insert space before 1:,code_debt,low_quality_code,427684e09a03241e6364da4df345ad66d20df436
hbase,7000,comment_7,The two failed tests are flaky. Integrated to trunk. Thanks for the patch Liang.,test_debt,flaky_test,427684e09a03241e6364da4df345ad66d20df436
hbase,7172,summary,fails when run individually and is flaky,test_debt,flaky_test,b79c62dcbd26aae660b29b3e8c919af857925896
hbase,7172,description,"fails when run individually (run just that test case from eclipse). I've also noticed that it is flaky on windows. The reason is a rare race condition, which somehow does not happen that much when the whole class is run. The sequence of events is smt like this: - we create 1 log file to split - we call in its own thread. - is waiting in since there are no splitlogworkers, it keep waiting. - we delete the task znode from zk - SplitLogManager receives the zk callback from which will call setDone() and mark the task as success. - However, meanwhile the loops sees that remainingInZK == 0, and calls return concurrently to the above. - on return from fails because the znode delete callback has not completed yet. This race only happens when the last task is deleted from zk, and normally only the SplitLogManager deletes the task znodes after processing it, so I don't think this is a production issue.",test_debt,flaky_test,b79c62dcbd26aae660b29b3e8c919af857925896
hbase,7172,comment_4,"Ops, I've found some more flaky tests: I think we can just increase the timeouts a la HBASE-7165. I'll do a v2 patch.",test_debt,flaky_test,b79c62dcbd26aae660b29b3e8c919af857925896
hbase,7172,comment_6,"Attaching v2 patches, which hopefully fixes the remaining flaky tests.",test_debt,flaky_test,b79c62dcbd26aae660b29b3e8c919af857925896
hbase,7212,description,"This is a simplified version of what was proposed in HBASE-6573. Instead of claiming to be a 2pc or 3pc implementation (which implies logging at each actor, and recovery operations) this is just provides a best effort global barrier mechanism called a Procedure. Users need only to implement a methods to acquireBarrier, to act when insideBarrier, and to releaseBarrier that use the ExternalException cooperative error checking mechanism. Globally consistent snapshots require the ability to quiesce writes to a set of region servers before a the snapshot operation is executed. Also if any node fails, it needs to be able to notify them so that they abort. The first cut of other online snapshots don't need the fully barrier but may still use this for its error propagation mechanisms. This version removes the extra layer incurred in the previous implementation due to the use of generics, separates the coordinator and members, and reduces the amount of inheritance used in favor of composition.",design_debt,non-optimal_design,8d117a6117e1b944c6ab4f4aab5da37d23d5fa1b
hbase,7604,summary,Remove duplicated code from HFileLink,code_debt,duplicated_code,bc37ac15538ffa6504bd8d7f9b28fc2cbe4d4ccf
hbase,7604,description,"the static method getReferencedPath() contains almost the same code done internally by the FileLink, and is only used by ExportSnapshot. Remove that code and use the class directly",code_debt,duplicated_code,bc37ac15538ffa6504bd8d7f9b28fc2cbe4d4ccf
hbase,7933,comment_1,"I think the bug is a race condition for the parent znode for the table. deletes parent znode, so that we do not leak znodes for deleted tables.",code_debt,low_quality_code,4d356e24e18a9217b55739ec333ec9b80c9f25ec
hbase,7933,comment_6,TestConstraint is a medium test. Meaning lock manager tests were not run.,test_debt,low_coverage,4d356e24e18a9217b55739ec333ec9b80c9f25ec
hbase,7933,comment_10,"I've run the tests with this once again. The ACL tests seem to be flaky. Master does not wait for _acl_ table to be created before accepting other create table statements. Will open another issue. I want to get this resolved sooner rather than later, since it affects the tests and pre-commit tests.",test_debt,flaky_test,4d356e24e18a9217b55739ec333ec9b80c9f25ec
hbase,7940,comment_1,Thanks for diggingin on this Ted. I changed the top-level pom but did not realize we had hard-coded versions in all submodules. How about this patch. It has us set version once in one place.,code_debt,low_quality_code,05711f755335f9489f7dcff2e312335ca8fa5b95
hbase,8026,comment_3,Could you expand the help to include mention of how one uses RAW/VERSIONS? Also please add a test in should be straight forward.,test_debt,lack_of_tests,f1d1dbfaa4a5ece96b8263bd0cae18ffea11a75f
hbase,8026,comment_4,"1) Usage : Its already present and can be seen once we do help scan on shell. Excerpt that include info on versions and raw : "" Also for experts, there is an advanced option -- RAW -- which instructs the scanner to return all cells (including delete markers and uncollected deleted cells). This option cannot be combined with requesting specific COLUMNS. Disabled by default. Example: hbase"" 2) Test - This jira is not about functionality of RAW and VERSIONS command. Its just the mention of these commands missing in the help. If you meant writing a new unit test for versions and raw command, then I will go through the shell tests to see if there is any corresponding test there and if not present, I can raise a new Jira and can work on it.",test_debt,lack_of_tests,f1d1dbfaa4a5ece96b8263bd0cae18ffea11a75f
hbase,8056,comment_0,A fairly simple patch with lots of comments. I am trying to see if there's a viable way to test the code in this area.,test_debt,low_coverage,88c3d9da6def57a29135438ad4c38f0ec56ae739
hbase,8056,comment_4,Could make a new function to place the added code in StoreScanner and mark that it is used in stripe compactions. I think it would be clear for reader. The change is avaiable to me,design_debt,non-optimal_design,88c3d9da6def57a29135438ad4c38f0ec56ae739
hbase,8056,comment_5,Can you store dropDeletesFromRow and dropDeletesToRow in ScanQueryMatcher and keep the logic local there?,design_debt,non-optimal_design,88c3d9da6def57a29135438ad4c38f0ec56ae739
hbase,8056,comment_6,"Moved code, added tests",architecture_debt,violation_of_modularity,88c3d9da6def57a29135438ad4c38f0ec56ae739
hbase,8324,comment_1,"Talked with , and we found this in the MRAppMaster's logs: This is related to MAPREDUCE-4880 which is in turn fixed by MAPREDUCE-4607 (a race in speculative task execution, fixed in 2.0.3-alpha). Compiling and running against hadoop-2.0.3-alpha fails out even earlier so instead of going that route, I'm going to try an alternate workaround -- disabling mapper and reducer speculative execution.",design_debt,non-optimal_design,0623f4b2a5cd362d54cd50bb35cce5be5280b56c
hbase,8324,comment_9,I don't think this is a one-off -- this would affect all of our MR tests that use more than one mapper/reducer. I don't think speculative execution should be on for our MR tests in general.,design_debt,non-optimal_design,0623f4b2a5cd362d54cd50bb35cce5be5280b56c
hbase,8324,comment_10,"Fair enough. However, that means we are leaving HBase's interaction with this feature untested. Does that mean we will advise users against using it with HBase? If we're not going to test it, that means we're not taking responsibility for it; thus I conclude that we must ""officially drop support"" for the feature, however that's done.",test_debt,lack_of_tests,0623f4b2a5cd362d54cd50bb35cce5be5280b56c
hbase,8527,summary,clean up code around compaction completion in HStore,code_debt,low_quality_code,77a08bbe0a63941afacdf4490b3b4b7f98583637
hbase,8527,comment_5,lgtm. Minor nit: Should there be a comment on when the point of no return is?,code_debt,low_quality_code,77a08bbe0a63941afacdf4490b3b4b7f98583637
hbase,8608,summary,Do an edit of logs.. we log too much.,design_debt,non-optimal_design,bd7b47980740129edf133f6f09023b9ced1e041f
hbase,8608,description,"Since we enabled blooms, there is a bunch of bloom spew.... We also do a bunch of logging around new file open....",design_debt,non-optimal_design,bd7b47980740129edf133f6f09023b9ced1e041f
hbase,8665,description,"Note that this can be solved by bumping up the number of compaction threads but still it seems like this priority ""inversion"" should be dealt with. There's a store with 1 big file and 3 flushes (1 2 3 4) sitting around and minding its own business when it decides to compact. Compaction (2 3 4) is created and put in queue, it's low priority, so it doesn't get out of the queue for some time - other stores are compacting. Meanwhile more files are flushed and at (1 2 3 4 5 6 7) it decides to compact (5 6 7). This compaction now has higher priority than the first one. After that if the load is high it enters vicious cycle of compacting and compacting files as they arrive, with store being blocked on and off, with the (2 3 4) compaction staying in queue for up to ~20 minutes (that I've seen). I wonder why we do thing thing where we queue compaction and compact separately. Perhaps we should take snapshot of all store priorities, then do select in order and execute the first compaction we find. This will need starvation safeguard too but should probably be better. Btw, exploring compaction policy may be more prone to this, as it can select files from the middle, not just beginning, which, given the treatment of already selected files that was not changed from the old ratio-based one (all files with lower seqNums than the ones selected are also ineligible for further selection), will make more files ineligible (e.g. imagine with 10 blocking files, with 8 present (1-8), (6 7 8) being selected and getting stuck). Today I see the case that would also apply to old policy, but yesterday I saw file distribution something like this: 4,5g, 2,1g, 295,9m, 113,3m, 68,0m, 67,8m, 1,1g, 295,1m, 100,4m, unfortunately w/o enough logs to figure out how it resulted.",design_debt,non-optimal_design,03dc97a1f415f09c5a9a850a43803cc2f7908bc4
hbase,8665,comment_5,"W.r.t. property of the store: 1) Current priority is derived entirely from store state. In fact it only depends on number of files and blocking limit, so there's no such thing as important store other than store. 2) It seems to be the obvious cause of this particular issue, see above. When selection is added to queue, its priority is low. As store fills up it's ""real"" priority changes, but we have no way to influence it. Bumping its priority when we queue another one is just a hack around that imho; if it even helps. If blocked store comes into a queue full of non-blocked stores why should we bump them? And how much. 3) Additionally if we pre-select in advance we simply end up with inefficient compactions, for example in this case we could have compacted 6 small files in one go, but would have ended up compacting 3 and 3 if it spent less time in the queue.",code_debt,low_quality_code,03dc97a1f415f09c5a9a850a43803cc2f7908bc4
hbase,8665,comment_6,I was suggesting bumping the priority instead of queueing another. Bumping the compaction request to what would currently be computed seems reasonable. In this case pre-selecting doesn't seem to be a bad thing. Prioritizing a fast compaction over being a little more efficient seems like a trade off that most people would want. So since current implementation biases towards what I would expect users to want maybe we shouldn't change that until we can put those smarts into the compaction selection (plumbing reason the compaction was requested into selection).,design_debt,non-optimal_design,03dc97a1f415f09c5a9a850a43803cc2f7908bc4
hbase,8665,comment_7,"Well, the effect of getting a faster compaction in this case is a pure accident, if there was not a smaller one already queued, it would still compact 6 according to policy. Also, out of many possible faster compactions in this case, bad one (later files) is chosen, so it's not really what user would expect. Policy should make such decisions - if we prefer faster compactions for blocked store, we should have it in the policy, and so last-moment selection would still choose the best one. As for bumping the priority of current to what it would have been, it is actually equivalent to just sorting them by current store priority... I wonder if there's any fundamental reason to divorce selection from compaction? If we introduce compaction-based priority modifiers, not just store based, we could still apply them by doing selection in multiple stores and comparing priorities. Selecting is not that expensive, given how frequently we compact.",design_debt,non-optimal_design,03dc97a1f415f09c5a9a850a43803cc2f7908bc4
hbase,8665,comment_10,"Will submit some simple patch tomorrow, we are hitting this a lot under high load. As soon as I can make the test slightly less ugly.",code_debt,low_quality_code,03dc97a1f415f09c5a9a850a43803cc2f7908bc4
hbase,8665,comment_11,"Here's the patch. It's rather simple, most complexity is in tests. It allows for both pre-selected compactions (from coprocessors and users) and non-pre-selected. When non-preselected compaction makes it to run() it is selected and executed. There are two special cases: 1) Store priority might have decreased since the compaction was queued; in that case it's re-queued with new priority to avoid inversion. 2) Without selecting, we don't know which pool to go to. We go to small pool by default, and if the compaction is large after selection, queue it to the large pool; it can bounce back if it becomes small while stuck in the large pool. Both of these cases can cause compaction to get continuously requeued, or bounced between pools, however it can only happen if store priority decreases (i.e. other compactions happen), or files are removed from store (same); or files are added in some very special pattern that causes policy to select continuously).",design_debt,non-optimal_design,03dc97a1f415f09c5a9a850a43803cc2f7908bc4
hbase,8816,description,"Introducing an optional parameter 'num_tables' into LoadTestTool. When it's specified with positive integer n, LoadTestTool will load n tables parallely. -tn parameter value becomes table name prefix. Tables are created with name in format <tn The motivation is to add a handy way to load multiple tables concurrently. In addition, we could use this option to test resource leakage of long running clients.",design_debt,non-optimal_design,6b26bb0a8e20a7e0c7cde86dbf00df6281080e43
hbase,8816,comment_1,"Should we change argument name concurrent_factor to num_tables? concurrent_factor is not that descriptive. You have changed the parsing of args from being in run(String[]) method to being in main. This breaks the usage model for the Tool interface. We should not do parsing in static main. Maybe you can use the instance which parsed the command line opts as the controller, and spawn all the other LTT instances from that, and join in the end. Can you please update the patch for trunk as well as 0.94. I think this is only for 0.94.",design_debt,non-optimal_design,6b26bb0a8e20a7e0c7cde86dbf00df6281080e43
hbase,8816,comment_4,"Why 64 for upper limit on num_tables? We can do Short.MAX_VALUE. We need a trunk version of the patch as well. Lastly, instead of the command line parsing wizardry for passing a modified version of the args to child classes, maybe you can just construct LoadTestTool instances with the same args in WorkerThread (calling processOptions manually), and then call setNumTables(1), and + ""_"" + i).",design_debt,non-optimal_design,6b26bb0a8e20a7e0c7cde86dbf00df6281080e43
hbase,8816,comment_8,How about passing a comma-separated list of table names to the -tn parameter? That way the one table is simply a special case.,design_debt,non-optimal_design,6b26bb0a8e20a7e0c7cde86dbf00df6281080e43
hbase,8816,comment_9,"Sounds good, but if you want to run with 50 tables, 10 writer/readers each, then passing that parameter will become ugly, no?",design_debt,non-optimal_design,6b26bb0a8e20a7e0c7cde86dbf00df6281080e43
hbase,8904,description,* Clean up the Long overflow (oops) * Clean up the usage of tableName vs tableNameBytes * Add in more than just max time. * Add in tracing.,code_debt,low_quality_code,c56f121117487e07d38cbd74301503d1fa80ab64
hbase,9052,description,"If a region is split/merged, before it's removed from meta, you can still assign it from the HBase shell. It's better to prevent this from happening.",design_debt,non-optimal_design,fd991fe9089afe84a45fa2720d3cd09801cd059f
hbase,9052,comment_1,nit: regionOffline should be renamed setRegionOffline or offlineRegion? I suppose you are following the precedent where this operation is done in a method named regionOffline (reads oddly). I see tightening up of allowed states but how are we prevening assign of MERGE and SPLIT?,code_debt,low_quality_code,fd991fe9089afe84a45fa2720d3cd09801cd059f
hbase,9052,comment_6,Yeah. Its a bad name but should be consistent.,code_debt,low_quality_code,fd991fe9089afe84a45fa2720d3cd09801cd059f
hbase,9158,comment_4,Cleaned up 0.94 patch.,code_debt,low_quality_code,3bc9e2c95c6e45927ad7fb69383fab30748e669e
hbase,9303,comment_6,"Ok, so we have a split parent with the SPLIT marker in the hri in meta. Is this the hri saved off as part of the snapshot manifest, or is it only in the meta hri? Is the problem only that the meta entry has SPLIT as an attribute? Could we have a unit test where we force meta to have the split marker and the restore?",test_debt,lack_of_tests,357fd849bc61d7957593a18f4540157b8e2b1253
hbase,9303,comment_9,"ok, this explanation is really helpful, and I buy it. There are some subtle things going on here, so can we add comments in the code about why there are two mutate calls and why we must first delete and then rewrite (instead of reusing like we did before)?",code_debt,low_quality_code,357fd849bc61d7957593a18f4540157b8e2b1253
hbase,9315,comment_1,This patch alters the test to wait for the cache to stabilize before asserting on the number of evictions run. This may be overkill.,design_debt,non-optimal_design,54d0f9f8000861f5f89a49edccbb5e14755d5a23
hbase,9340,comment_1,nit: no need for that extra newline.,code_debt,low_quality_code,d003a429b4d0924220592f1d205763ba6fad69b0
hbase,9371,summary,Eliminate log spam when tailing files,code_debt,low_quality_code,fa40ff356625d50ea3a2677022b48f81980d0659
hbase,9433,description,OpenRegionHandler uses the timeout monitor checking period as the timeout. It should share the same default as in AssignmentManager.,code_debt,low_quality_code,a6fcaa7f2d9995ebbea9fd0b4f606515bd29f11e
hbase,9461,summary,Some doc and cleanup in RPCServer,code_debt,low_quality_code,ee0c193d536e9587b8b700c03b67fcf0dd1bd7a2
hbase,9461,comment_0,"Here is a start. Adds start of a description of who is doing what to who. Removes thread local that made the RcpServer instance available -- not used (seemingly), ugly.",code_debt,low_quality_code,ee0c193d536e9587b8b700c03b67fcf0dd1bd7a2
hbase,9461,comment_4,"More doc and untangling. More to do but this ok for now. RpcServer had internal CallRunner class. The scheduler was reaching over to use this inner class. Broke it out of RpcServer and made it Standalone. Made it take an RpcServerInterface instead of RpcServer. Undid CallRunner implementing Runnable (confusing -- how it is run is internal function of Scheduler implementation) Cleaned up RpcServerInterface explaining why a start and a startThreads and an openServer. Added a few methods to support CallRunner being outside of RpcServer. Added into CallRunner the managment of the call queue size. As it was, callQueueSize was incremented before we created a CallRunner but internal to CallRunner it was managing the decrement. Shutdown access on CallRunner constructor and CR#getStatus and other methods only for use in this package. Added javadoc all around to explain the cryptic. was like CallRunner, a class used by the Scheduler only it was an inner class of RpcServer. Moved it out. Removed unused thread local SERVER. Add simple test to demo being able to instantiate a CallRunner outside of RpcServer context.a",code_debt,low_quality_code,ee0c193d536e9587b8b700c03b67fcf0dd1bd7a2
hbase,9461,comment_6,"Just asking, if you find the time. Some comments describing the intention (what this class is supposed to do) would be great. For example, the delayed calls, undelayed calls with delayed response are not trivial (are they used?). It's very difficult to understand what these classes are supposed to do and why. Or this low level, with a specific case with '==1' Basically, I wanted to try a Netty based implementation, but there are so many corner cases that even estimating the time needed for a hacked implementation is difficult....",code_debt,low_quality_code,ee0c193d536e9587b8b700c03b67fcf0dd1bd7a2
hbase,9461,comment_7,"Smile. I am in same boat as you. I want to untangle this rats nest so I can understand whats going on and try stuff (I want to try pool of bytebuffers -- direct bytebuffers even -- for incoming requests. Putting in netty too would be sweet). Unless you object, was going to commit this since it some progress. Was going to do more along this line soon but in other issues; I'm afraid the refactors will rot between getting time to work in here. The Delay stuff is unused I think. It was an experiment. Maybe I'll look at that next and purge it if I can.",code_debt,dead_code,ee0c193d536e9587b8b700c03b67fcf0dd1bd7a2
hbase,9493,description,"To make reading code easier, we should rename the CellUtil#get*Array methods to to CellUtil#clone*. This eliminates the possibly confusion between the semantics of these methods (which allocate-copy) and that of Cell#get*Array (which essentially just returns a pointer).",code_debt,low_quality_code,3e25e1b7f571693e4eaa705a12b7c2289f55a0a9
hbase,9493,comment_0,Simple braindead patch. New name makes it clear that allocation is going on (and also clearly points out that we are doing a lot of small array allocations in tests).,code_debt,low_quality_code,3e25e1b7f571693e4eaa705a12b7c2289f55a0a9
hbase,9689,comment_6,"+1 patch looks ok, would be good to have coverage in TestShell for this",test_debt,low_coverage,968f83b2ff45234c756a93cf5bd90f39619ddd53
hbase,9689,comment_7,"Looks good Correct the comment pls. Seems copy paste :) set_get_attributes, set_put_attributes seems same code repeating. We can make this one def?",code_debt,duplicated_code,968f83b2ff45234c756a93cf5bd90f39619ddd53
hbase,9689,comment_9,"There is no way to have a backwards compatible put command that does not take an attributes hash, just a timestamp? It's fine if timestamp has to be specified as part of the attribute hash if one is present.",design_debt,non-optimal_design,968f83b2ff45234c756a93cf5bd90f39619ddd53
hbase,9893,comment_5,"Excellent catch. Attached is a patch that also includes updated test cases. Please let me know if you have other value permutations you'd like to see tested. It'd be nice to have a more thorough test suite around this code, a la the suite Orderly has. From the commit message",test_debt,lack_of_tests,da89846cf78ff1be7f07bbfee40ebacc23eb9ceb
hbase,10001,description,"We have a mockup to test only the client. If we want to include the network, without beeing limited by the i/o, we don't have much tools. This coprocessor helps to test this. I put it in the main code as to make it usable without adding a jar... I don't think it's possible avoid the WAL writes in the coprocessors. It would be great to have it to simplify the test with any kind of client (i.e. w/o changing the durability).",code_debt,complex_code,d9b983bb5ee871387ce5f47d6890cf6ef4b9058d
hbase,10008,summary,is flakey on jenkins,test_debt,flaky_test,eab83c92ed9f101dc981933be8383590491dbff7
hbase,10008,description,"is flakey on Jenkins, this test has failed due to timeout a couple times for me on precommit. Here's the stacktrace.",test_debt,flaky_test,eab83c92ed9f101dc981933be8383590491dbff7
hbase,10213,comment_17,"the intention of this jira is good:-), but by examining the patch: the metric above only reflects the log read/parse rate, not the desired replicating data to peer cluster rate, since the read/parsed log files may contain many kvs from column-families with replication scope=0 which will be filtered out and removed from the entries list before the real replicating to peer cluster occurs... why not use currentSize, the size of all entries which will be really replicated to the peer cluster?",design_debt,non-optimal_design,"f2f316db1fb684a29e051966b7b56201d689ab64,0277f566162f289b1b5c3bab0e34cabf5c8c566c"
hbase,10631,summary,Avoid extra seek on FileLink open,code_debt,low_quality_code,9f86836794cbb7addc3896082ff76ef02d3dd7bc
hbase,10631,description,"There is an extra seek(0) on FileLink open, that we can skip",code_debt,low_quality_code,9f86836794cbb7addc3896082ff76ef02d3dd7bc
hbase,10892,comment_0,"looks ok to me, (I've also tried it a bit) but there are a couple of things I'm not sure about I think that the glob only for ""all tables"" may be confusing, because then you can't do ""tableName*"" you must use ""tableName.*"" but I'm ok having it as a special ""all tables"" case. is and was committed as Deprecated. I know that is used in other places, but we should decide if we want to use that for new code or not and later replace it where is used.",design_debt,non-optimal_design,4d1fc21500f762e5a533af5ae001a763d4b8a41d
hbase,10925,comment_7,"Looks good. This should be a configuration: + long maxRowSize = 10 * 1024 * 1024; On the below: + totalReadSize += kv.getRowLength() + + + + + if (totalReadSize + throw new size of row is :"" + maxRowSize + + "", but the row is bigger than that.""); + } ... it is a bit of a pain having to do the above. I suppose we should add a getSize to CellUtils. Could do that in another patch. Was also thinking what if the Cell is offheap but then above is probably fine still because while it is offheap in the server, in the client it may not be and a 10G rowsize is likely larger than it can swallow w/o OOME.",design_debt,non-optimal_design,c1d32df547c581086c194a203fc3def3c240deed
hbase,10968,summary,Null check in is redundant,design_debt,non-optimal_design,89b60b0374102b9f7b21e04d0e248ff31ee11c6c
hbase,10968,description,"Here is related code: context was dereferenced first, leaving the null check ineffective.",design_debt,non-optimal_design,89b60b0374102b9f7b21e04d0e248ff31ee11c6c
hbase,10968,comment_1,Thanks Matteo. Patch also removes an unused local variable.,code_debt,dead_code,89b60b0374102b9f7b21e04d0e248ff31ee11c6c
hbase,11011,summary,Avoid extra getFileStatus() calls on Region startup,design_debt,non-optimal_design,996ce5211cb228a508470f779e17f76168efe270
hbase,11011,description,"On load we already have a StoreFileInfo and we create it from the path, this will result in an extra fs.getFileStatus() call. In we do a fs.exists() and later a fs.getFileStatus() to create the StoreFileInfo, we can avoid the exists.",design_debt,non-optimal_design,996ce5211cb228a508470f779e17f76168efe270
hbase,11011,comment_3,"In this case is not bad, is an ""expected"" situation, where the RS died before moving the compacted file. Anyway, I think that all the loop can be removed, since is trying to lookup files in and if they are there are already loaded for sure.",code_debt,low_quality_code,996ce5211cb228a508470f779e17f76168efe270
hbase,11011,comment_4,"This might be, but the log message doesn't convey any of that. Taken out of context (so most users reading our logs), it just says a file is missing. I'm trusting you on this one :)",code_debt,low_quality_code,996ce5211cb228a508470f779e17f76168efe270
hbase,11011,comment_6,"none of the above, the removed code was doing nothing (see the comment inside the function). The output files in the entry are simply not useful",code_debt,low_quality_code,996ce5211cb228a508470f779e17f76168efe270
hbase,11229,summary,Change block cache percentage metrics to be doubles rather than ints,design_debt,non-optimal_design,"46e53b089a81c2e1606b1616b1abf64277de50a9,ea9e6576945cc36f5e13a67da285498979411cb2"
hbase,11229,description,"See parent issue. Small changes in the hit percentage can have large implications, even when movement is inside a single percent: i.e. going from 99.11 to 99.87 percent. As is, percents are ints. Make them doubles.",design_debt,non-optimal_design,"46e53b089a81c2e1606b1616b1abf64277de50a9,ea9e6576945cc36f5e13a67da285498979411cb2"
hbase,11511,description,"We used to write COMPLETE_FLUSH event to WAL until it got removed in 0.96 in issue HBASE-7329. For secondary region replicas, it is important to share the data files with the primary region. So we should reintroduce the flush wal markers so that the secondary region replicas can pick up the newly flushed files from the WAL and start serving data from those. A design doc which explains the context a bit better can be found in HBASE-11183.",design_debt,non-optimal_design,"bbe29eb93cc819fbf0287aa2cb343649b72783bf,58727343f2b7ac4093828ca645084901d7ca568c"
hbase,11511,comment_4,"Thanks Stack for taking a look. Good point. We writelock the updatesLock, then start the mvcc transaction, so it should be the case that sync() and sync(trx) should be the same thing I guess. But let me change it to be plain old sync() to be failure-proof. Indeed. The block above is a catch block for catching an ignoring any exception from writing the abort marker to WAL. The call to should still happen in the outer block, resulting in",design_debt,non-optimal_design,"bbe29eb93cc819fbf0287aa2cb343649b72783bf,58727343f2b7ac4093828ca645084901d7ca568c"
hbase,11621,summary,Make MiniDFSCluster run faster,code_debt,slow_algorithm,62d51bb377af46c2fefd3e7a441579878744da13
hbase,11621,description,Daryn proposed the following change in HDFS-6773: With this change in runtime for TestAdmin went from 8:35 min to 7 min,code_debt,slow_algorithm,62d51bb377af46c2fefd3e7a441579878744da13
hbase,11621,comment_4,"Here was 0.98 test suite with patch - on the same host as the previous run: With patch, hbase-server module went from 66 min to 51 min.",code_debt,slow_algorithm,62d51bb377af46c2fefd3e7a441579878744da13
hbase,12030,comment_3,"looks ok to me, it will be nice having a test to cover the path when zk throws an exception.. maybe the easiest way there is mocking zk?",test_debt,low_coverage,a0d9e18ccf3842395a410a6c966f108ab2f55473
hbase,12059,description,Different versions of hadoop have different annotations. We can smooth this out by providing our own.,design_debt,non-optimal_design,a5bd931682c10f34d5e33aa103eb64aeac632e03
hbase,12106,description,Test annotation interfaces used to be under then moved to We should move them to,code_debt,low_quality_code,"3557a3235210b178d9c6cd3355c68d0a7155243e,da440d586eb1e1f0fff8e0f427fd3ff729fd8f51"
hbase,12115,comment_0,Removed unnecessary changes from the commit.,code_debt,dead_code,0075528615763812b57338350af936842ba0dee5
hbase,12238,summary,A few ugly exceptions on startup,design_debt,non-optimal_design,5062edebcfc18329360845988cfee5d25c23a934
hbase,12238,description,"Let me fix a few innocuous exceptions that show on startup (saw testing 0.99.1), even when regular -- will throw people off. Here is one: More to follow...",design_debt,non-optimal_design,5062edebcfc18329360845988cfee5d25c23a934
hbase,12238,comment_0,In standalone mode this is a little disorientating... it shows up frequently:,design_debt,non-optimal_design,5062edebcfc18329360845988cfee5d25c23a934
hbase,12271,comment_6,"ExportSnapshot will skip files that are the same (either via checksum or via name+length comparison). We use this to cut down on transfer time. First we snapshot the table, then export that snapshot to the remote DFS. Next run, we instead start by copying the previously exported snapshot into the current snapshot's destination (all on the remote DFS). We then do the snapshot and export dance again, and it dutifully copies only the changed files. Our (fb) HDFS has hard links (last I heard upstream does not) so we can copy backups around pretty cheaply. In the future we may just throw everything into one directory and let the manifest be the decider of what files are involved, but in the mean time it's copy and differential.",design_debt,non-optimal_design,ba20d4df8ced17e175d6a1d57982559d4dce79cd
hbase,12428,comment_8,"the problem is that is doesn't handle the port correctly. If there isn't one specified, it will throw an exception when it tries to cleanup old files. However, we are also doing the ""... unless port"" check in two different places, which is not as clean as it could be. So this just sets up the port in a single place, if its not set via the command line.",design_debt,non-optimal_design,0c2314b07ab793389d2818b4f06f0defbd7db556
hbase,12428,comment_12,Isn't this (-maybe) a cleaner fix? Keep logic local to the method where it is needed. Sorry to be pedantic. If you prefer your version here's a +1 for that too :),code_debt,low_quality_code,0c2314b07ab793389d2818b4f06f0defbd7db556
hbase,12464,description,"meta table region assignment could reach to the 'FAILED_OPEN' state, which makes the region not available unless the target region server shutdown or manual resolution. This is undesirable state for meta tavle region. Here is the sequence how this could happen (the code is in Step 1: Master detects a region server (RS1) that hosts one meta table region is down, it changes the meta region state from 'online' to 'offline' Step 2: In a loop (with configuable maximumAttempts count, default is 10, and minimal is 1), AssignmentManager tries to find a RS to host the meta table region. If there is no RS available, it would loop forver by resetting the loop count (BUG#1 from this logic - a small bug) Step 3: Once a new RS is found (RS2), inside the same loop as Step 2, AssignmentManager tries to assign the meta region to RS2 (OFFLINE, RS1 = Based on the document ( ), this is by design - ""17. For regions in FAILED_OPEN or FAILED_CLOSE states , the master tries to close them again when they are reassigned by an operator via HBase Shell."". However, this is bad design, espcially for meta table region (it is arguable that the design is good for regular table - for this ticket, I am more focus on fixing the meta region availablity issue). I propose 2 possible fixes: Fix#1 (band-aid change): in Step 3, just like Step 2, if the region is a meta table region, reset the loop count so that it would not leave the loop with meta table region in FAILED_OPEN state. Fix#2 (more involved): if a region is in FAILED_OPEN state, we should provide a way to automatically trigger after a short period of time (leaving any region in FAILED_OPEN state or other states like 'FAILED_CLOSE' is undesirable, should have some way to retrying and auto-heal the region). I think at least for 1.0.0, Fix#1 is good enough. We can open a task-type of JIRA for Fix#2 in future release.",design_debt,non-optimal_design,7eefd0cbed094b188c0e68d3ee99d937fa6942b0
hbase,12464,comment_3,"It's not good for meta to stuck in FAILED_OPEN. Agree we should handle it differently. The patch looks good. Just couples things: 1. Can we add a log (info/debug level may be fine) when we reset the retry count to 0? 2. We also need to prevent meta region goes to FAILED_OPEN at method How about FAILED_CLOSE? It should be fine since the meta region is still available? Is this essentially the same as setting maximumAttempts to a huge number? In many cases, a region may not be able to heal automatically without a pill. Personally, I think a better monitoring system could be better in this case.",design_debt,non-optimal_design,7eefd0cbed094b188c0e68d3ee99d937fa6942b0
hbase,12464,comment_4,"This looks good, except for Jimmy's comments above. Nit. should be renamed to Can you rebase the patch on top of current master code. FAILED_CLOSE should be fine I think. Not much to do there. We default to 10 attempts to assign. Maybe we should bump it up to 30 or so. I did not check how long we are sleeping overall, but for client -> server operations we are retrying 35 times for a total of 10 min before giving up. We can do the similar for region assignment.",code_debt,low_quality_code,7eefd0cbed094b188c0e68d3ee99d937fa6942b0
hbase,12562,comment_2,+1. Looks good to me with some some minor comments: 1) You can break the loop after set canDrop to false 2) Just to check acquiring lock on writestate and memstore are always in this order 3) There maybe no need for the following condition 4. Rename to may be better,code_debt,low_quality_code,4ac42a2f56b91ce864d1bcb04f1f9950e527aab1
hbase,12562,comment_10,v3 should fix findbugs warnings.,code_debt,low_quality_code,4ac42a2f56b91ce864d1bcb04f1f9950e527aab1
hbase,12749,comment_7,"check out HBASE-12332. We don't have a FileStatus to attach to in that particular case initially. In the snapshot case we have the pattern file and get a fileStatus, and in the replicas case we have a file status as well[1] The fs was needed for (and and Probably can just as easily as of a FS in Didn't tackle those pieces yet (they also seem more tightly coupled than ideal) [1]",architecture_debt,violation_of_modularity,30424ec73f7f100b233e27196c0a6a90abd62ad2
hbase,12749,comment_12,"v3 tackles the javadoc, fingbugs, checkstyle and line length issues.",code_debt,low_quality_code,30424ec73f7f100b233e27196c0a6a90abd62ad2
hbase,12833,summary,[shell] table.rb leaks connections,code_debt,low_quality_code,af725a0357efa01f276f2fe15518e6e119b89e45
hbase,12833,description,TestShell is erring out (timeout) consistently for me. Culprit is OOM cannot create native thread. It looks to me like test_table.rb and hbase/table.rb are made for leaking connections. table calls for every table but provides no close() method to clean it up. test_table creates a new table with every test.,code_debt,low_quality_code,af725a0357efa01f276f2fe15518e6e119b89e45
hbase,12833,comment_11,"A couple of comments on this: 1) the interface changes and the managed/unmanaged connection issues are separate concerns. 2) On the mailing list you said: ""Yes, the concept of connection caching, aka, managed connections are going away."" If managed connections are going away, then let's make them go away across the board. If they're staying, then let's keep them and rework the interfaces to support managed connections as a first class citizen. I only see disadvantages for inconsistency in this aspect. That said, this can totally be something that I'm missing. Do you see any advantages to keeping a mix of managed and unmanaged connections around?",design_debt,non-optimal_design,af725a0357efa01f276f2fe15518e6e119b89e45
hbase,12833,comment_13,"On closer inspection, it looks like SecurityAdmin and need updated with this new style as well. ReplicationAdmin appears to manage it's own connection by design, though the rational eludes me.",code_debt,low_quality_code,af725a0357efa01f276f2fe15518e6e119b89e45
hbase,12833,comment_16,"I guess it could be, but won't connections still be leaked when these classes are used? All this shows is our test usage of this part of the client is not as through as others. Whatever you say .",test_debt,low_coverage,af725a0357efa01f276f2fe15518e6e119b89e45
hbase,13341,comment_4,"Oops, disregard the above. (Wrong JIRA). Only two things are nits: 1. Put a period at the end of the usage message addition to be consistent with the other lines. 2. Change the dereference from {{""$ALL"" != ""true""}} to {{""$\{ALL}"" != ""true}}, again, just to be consistent with the rest of the script. +1!",code_debt,low_quality_code,1632cd98ff3c62f02ffbf5774bcb38cfd1f7bb9a
hbase,13395,description,"This class is marked as deprecated, probably can remove it, and if any methods from this specific class are in active use, need to decide what to do on callers' side. Should be able to replace with just Table interface usage.",code_debt,dead_code,50e9825139d9abfd280eb7a930c8c6a96e9e68a6
hbase,13395,comment_5,It is deprecated in 1.0 I think. So we can remove it.,code_debt,dead_code,50e9825139d9abfd280eb7a930c8c6a96e9e68a6
hbase,13528,comment_0,"I think this line is also redundant? The if selectNow is false, then we will not execute throttleCompaction then 'size' is useless Thanks.",code_debt,low_quality_code,e1cb405adc4da68b6bd6ea68b2d5073f6c8fd7c3
hbase,13528,comment_1,"Yes, it's redundant, just like this is OK?",code_debt,low_quality_code,e1cb405adc4da68b6bd6ea68b2d5073f6c8fd7c3
hbase,13710,description,HttpServer makes use of Hadoop's ReflectionUtil instead of our own. AFAICT it's using 1 extra method. Just copy that one over to our own ReflectionUtil.,design_debt,non-optimal_design,353b046d6c5cf028ed509d238eafa46b6b3ec7a3
hbase,13776,comment_8,lgtm : Is it possible to add a unit test so that there is no regression in the future ?,test_debt,lack_of_tests,f35b6c6b77f82499ebcc5eb5843e046014b2f35b
hbase,13776,comment_9,"Can we add the wrong values of min and max versions configured, in the message? Yes pls add a test to cover this scenario. Thanks.",test_debt,lack_of_tests,f35b6c6b77f82499ebcc5eb5843e046014b2f35b
hbase,13871,comment_3,"Do we have to have a class named FirstOnRowFakeCell at top level of our class hierarchy? Is it only used in CellUtil? Could it be an inner class of it? Any reason for this change? 5252 this.isScan = scan.isGetScan() ? -1 : 0; 5252 this.isScan = scan.isGetScan() ? 1 : 0; Otherwise, seems good.",architecture_debt,violation_of_modularity,bf3924ed054a7ee05f1214af9cd695d3c036ec3c
hbase,13871,comment_4,Yes this class need not be top level.. Let me see how we can make it inner. Yes this change is intended. Pls see the compare in isStopROw is changed. So we need this change.,architecture_debt,violation_of_modularity,bf3924ed054a7ee05f1214af9cd695d3c036ec3c
hbase,13871,comment_6,"I will change it in next patch. BTW , we might add some more similar classes in future. This is byte[] backed fake Cell impl and we will have a BB backed one also. Also LastOnRowFakeCell will come in.. JFYI. I will keep this class(es) private audience only even if CellUtil is public exposed. Thanks Stack.",architecture_debt,violation_of_modularity,bf3924ed054a7ee05f1214af9cd695d3c036ec3c
hbase,13905,description,The reruns failed because the test is not idempotent. Perhaps we should have the test startup clean up it's workspace before starting.,design_debt,non-optimal_design,35a9c509fb517ca846e2a376d7e7bdf30344d20f
hbase,13905,comment_9,"Test is still in the flakey category, occasionally timing out with this stack",test_debt,flaky_test,35a9c509fb517ca846e2a376d7e7bdf30344d20f
hbase,14162,comment_5,Using the variable thrift.version instead of hard coding version 0.9.2 in v3.,code_debt,low_quality_code,377bf1937f8af733e40813e5d8bb9c962a8736e0
hbase,14162,comment_7,-1 on v3. the point of the check is to make sure the thrift.version variable doesn't get updated without a review of compatibility. also it's a regular expression so using the variable will be unnecessarily permissive.,code_debt,low_quality_code,377bf1937f8af733e40813e5d8bb9c962a8736e0
hbase,14494,summary,Wrong usage messages on shell commands,code_debt,low_quality_code,4da3c935d4a84968df62bb7e49833ad48a20ed87
hbase,14494,description,"noticed that the grant shell command was missing commas in-between the method arguments in the usage/help messages. After taking a look at some of the others, it's not alone. I'll make a pass over the shell commands and try to find some more help messages which tell the user to run a bad command.",code_debt,low_quality_code,4da3c935d4a84968df62bb7e49833ad48a20ed87
hbase,14494,comment_0,Simple patch which adds some commas to the help message for the follow shell commands: * * grant.rb * * revoke.rb,code_debt,low_quality_code,4da3c935d4a84968df62bb7e49833ad48a20ed87
hbase,14517,description,"In production env, regionservers may be removed from the cluster for hardware problems and rejoined the cluster after the repair. There is a potential risk that the version of rejoined regionserver may diff from others because the cluster has been upgraded through many versions. To solve this, we can show the all regionservers' version in the server list of master's status page, and highlight the regionserver when its version is different from the master's version, similar to HDFS-3245 Suggestions are welcome~",design_debt,non-optimal_design,445dbd8a0e45e45bd4a44dee999a6c134c374dfc
hbase,14517,comment_5,Fix checkstyle errors,code_debt,low_quality_code,445dbd8a0e45e45bd4a44dee999a6c134c374dfc
hbase,14604,description,"The code in MoveCoseFunction: It uses cluster.numRegions + META_MOVE_COST_MULT as the max value when scale moveCost to [0,1]. But this should use maxMoves as the max value when cluster have a lot of regions. Assume a cluster have 10000 regions, maxMoves is 2500, it only scale moveCost to [0, 0.25]. Improve moveCost by use maxMoves) as the max cost, so it can scale moveCost to [0,1].",design_debt,non-optimal_design,60465964039acd05f43f268cdb4f909a150a0f41
hbase,14604,comment_1,Any chance of a unit test ?,test_debt,lack_of_tests,60465964039acd05f43f268cdb4f909a150a0f41
hbase,14604,comment_4,The classes are in the same package. Please make the new methods package private. Patch for 0.98 branch should contain 0.98 in the filename. Extension should be either .txt or .patch. Please attach patch for master branch.,code_debt,low_quality_code,60465964039acd05f43f268cdb4f909a150a0f41
hbase,14941,comment_5,Can we use begin ensure block to ensure we do not leak it ?,design_debt,non-optimal_design,6f8d5e86cee2554ebbe6b4d34d828deff04aa894
hbase,15192,summary,is flaky,test_debt,flaky_test,243e6cc5293dc1e2a4dfd3af4ee29087c84184c8
hbase,15192,description,"fails intermittently due to failed assertion on cleaned merge region count: Before calling the test does: newcount1 is not cleared at the beginning of the loop. This means that if the check for newcount1 <= 1 doesn't pass the first iteration, it wouldn't pass in subsequent iterations. After timeout is exhausted, is called. However, there is a chance that has been called by the Chore already (during the wait period), leaving the cleaned count 0 and failing the test.",test_debt,flaky_test,243e6cc5293dc1e2a4dfd3af4ee29087c84184c8
hbase,15192,comment_6,Looks like there are other flaky sub-tests.,test_debt,flaky_test,243e6cc5293dc1e2a4dfd3af4ee29087c84184c8
hbase,15192,comment_9,"If there is no objection, I plan to resolve this JIRA. There is still flaky subtest, e.g. : which we can track in separate issue(s)",test_debt,flaky_test,243e6cc5293dc1e2a4dfd3af4ee29087c84184c8
hbase,15287,comment_6,Can you modify the following tests to cover your fix ?,test_debt,lack_of_tests,e9211e415a208a3f16b1d6a5d9fc730824e0df92
hbase,15293,comment_9,"Please insert space around 'catch' There're 7 tabs in the patch. Please remove them. Once these are addressed, it should be good.",code_debt,low_quality_code,5d79790c55b6caa0f9cbc77e14a5f39940b02236
hbase,15397,comment_3,It is not used any more.,code_debt,dead_code,664575598ea81fe2a53d8d88ead4ce318ff89d86
hbase,15617,description,"When running in regionserver mode the Canary is expected to probe for service health one time per regionserver during a probe interval. Each time the canary chore fires, we create a which uses (via to enumerate over all table descriptors, find the locations for each table, then assemble the list of regionservers to probe from this result. The list may not contain all live regionservers, if there is a regionserver up but for some reason not serving any regions. To ensure we have the complete list of live regionservers I think it would be better to use and enumerate the live server list returned in the result.",design_debt,non-optimal_design,460b41c80012eff4ba59be95a86037d3b9c736b4
hbase,15617,comment_1,"The new depends on some internal details likely to change. We should treat ZK state as a black box and avoid relying on it. This is a layering violation, really. Why not use the supported Admin API's to get the live server list to enumerate? That unit test failure appears unrelated.",design_debt,non-optimal_design,460b41c80012eff4ba59be95a86037d3b9c736b4
hbase,15640,summary,L1 cache doesn't give fair warning that it is showing partial stats only when it hits limit,design_debt,non-optimal_design,6dd938c20bf4634b5efa1ceca4ded7e54f8d9c0e
hbase,15640,description,"I was baffled looking at L1 Cache because it was stuck at 100k blocks and only 'loading' 3G of data according to the UI. When I looked in log, all the numbers looked write in the Cache summary reported in log. Turns out our buckecache UI template will cut off calculating stats at a limit --which is probably what we want -- but the fact that it has done this should be more plain.",design_debt,non-optimal_design,6dd938c20bf4634b5efa1ceca4ded7e54f8d9c0e
hbase,15640,comment_0,"Here is small patch which ups our limit to 1M blocks from 100k so less likely we'll hit limit and then if we do, puts in place a message in red with how to address the limit (I couldn't figure what the previous message was trying to say... didn't seem to make sense). This could be better.. but will do for now.",design_debt,non-optimal_design,6dd938c20bf4634b5efa1ceca4ded7e54f8d9c0e
hbase,15707,comment_3,Please fix checkstyle warning.,code_debt,low_quality_code,ebb5d421f96b83628cbfc5dd9f41ba714e2adf2b
hbase,15707,comment_5,"Understood the checkstyle warning, will provide a new patch, thanks !",code_debt,low_quality_code,ebb5d421f96b83628cbfc5dd9f41ba714e2adf2b
hbase,15707,comment_7,"Yeah, the function has an anonymous class inline so making it exceeding the 150 line limit. I was considering to separate the anonymous class..., thanks .",design_debt,non-optimal_design,ebb5d421f96b83628cbfc5dd9f41ba714e2adf2b
hbase,15835,description,"When a MiniCluster is being started with the method (most typically in the context of JUnit testing), if a local HBase instance is already running (or for that matter, another thread with another MiniCluster is already running), the startup will fail with a RuntimeException saying ""HMasterAddress already in use"", referring explicitly to contention for the same default master info port (16010). This problem most recently came up in conjunction with HBASE-14876 and its sub-JIRAs (development of new HBase-oriented Maven archetypes), but this is apparently a known issue to veteran developers, who tend to set up the @BeforeClass sections of their test modules with code similar to the following: A comprehensive solution modeled on this should be put directly into HBaseTestUtility's main constructor, using one of the following options: OPTION 1 (always force random port assignment): OPTION 2 (always force random port assignment if user has not explicitly defined alternate port):",design_debt,non-optimal_design,e6d613de70decf2b0f21d61bfa066cca870d5a09
hbase,15835,comment_5,"Curious: It appears that TestFSHLogProvider is (inadvertently, I imagine) instantiating an HBaseTestingUtility with a *null* Configuration (this within the context of kicking off a thread). This might be worthy of further investigation (since it's probably not intended to be this way), but for now I will simply precede my newly inserted logic with a check for a null Configuration, and thus avoid the that my original patch encountered.",design_debt,non-optimal_design,e6d613de70decf2b0f21d61bfa066cca870d5a09
hbase,15835,comment_7,"Hey , can you remove instances of setting the port to -1 in existing tests? I did a quick grep and there looks to be only a handful. Can you add some class-level javadoc about the minicluster ports going random instead of default. Do you think it might be worth a debug-level logging message too? I could image someone setting a value and in one special case having it fail.",code_debt,low_quality_code,e6d613de70decf2b0f21d61bfa066cca870d5a09
hbase,15835,comment_11,"Okay, the above-listed QA-failure prompted me to read in more detail those emails regarding the problem of ""flaky|flakey"" tests. I see that is indeed on the list, and also that testing of HBASE-15876 ran into exactly the same (apparently ""flaky|flakey"") failure. In this case, an awkward aspect is that the changes I made could conceivably have caused this kind of a failure!! So, just for good measure, I did a fresh clone/install of the Master branch and ran the same test: yep, it failed in exactly the same way.",test_debt,flaky_test,e6d613de70decf2b0f21d61bfa066cca870d5a09
hbase,16157,comment_6,lgtm. Is the test stable enough? Looks like it might end up being a flaky test.,test_debt,flaky_test,"5a7c9939cbe115c0d12d9cfe8bf9f3b3d11ac69e,fe75edb556649a101482b974eead2d506f78efe4,d016338e45dca183fc05f254e3e1260d04511269"
hbase,16856,summary,Exception message in SyncRunner.run() should print currentSequence,design_debt,non-optimal_design,278625312047a2100f4dbb2d2eaa4e2219d00e14
hbase,16998,comment_6,".004 latest from rb. Missing InterfaceAudience annotations, constructor cleanup in QuotaObserverChore.",code_debt,low_quality_code,533470f8c879174d023140a80d2b6d548685733e
hbase,17025,comment_7,Is it possible to add some .rb test ? Thanks,test_debt,lack_of_tests,f1066cd7744e185044dcc33aae09623d10ed2631
hbase,17101,description,"As described in the doc (see HBASE-15531), we would like to start with user tables for favored nodes. This task ensures FN does not apply to system tables. System tables are in memory and won't benefit from favored nodes. Since we also maintain FN information for user regions in meta, it helps to keep implementation simpler by ignoring system tables for the first iterations.",design_debt,non-optimal_design,680289d67deb42922dd244daa11496a4c8a38f80
hbase,17101,comment_0,"including a rough patch, will cleanup and upload.",code_debt,low_quality_code,680289d67deb42922dd244daa11496a4c8a38f80
hbase,17184,summary,Code cleanup of LruBlockCache,code_debt,low_quality_code,4a20361f1a235e0365c7923e64d20b28166cfd4b
hbase,17184,description,"Initially I only wanted to fix a badly worded Log message but found a bunch of code convention violations etc. so I decided to clean up the class. It is only whitespace changes with the exception of the one log message (""too many"").",code_debt,low_quality_code,4a20361f1a235e0365c7923e64d20b28166cfd4b
hbase,17192,comment_6,+1. Thanks for debug steps (ugly).,code_debt,low_quality_code,e5dad24a9cb35d831a0ee1cf0eeb14b3719ab7ef
hbase,17338,description,"We have only data size and heap overhead being tracked globally. Off heap memstore works with off heap backed MSLAB pool. But a cell, when added to memstore, not always getting copied to MSLAB. Append/Increment ops doing an upsert, dont use MSLAB. Also based on the Cell size, we sometimes avoid MSLAB copy. But now we track these cell data size also under the global memstore data size which indicated off heap size in case of off heap memstore. For global checks for flushes (against lower/upper watermark levels), we check this size against max off heap memstore size. We do check heap overhead against global heap memstore size (Defaults to 40% of xmx) But for such cells the data size also should be accounted under the heap overhead.",design_debt,non-optimal_design,ab5970773ad187871270bb63f9a37cead4cf2f6a
hbase,17338,comment_3,"In case of on heap MSLAB, all the size calc doing global memstore data size + heap overhead based. That is why was not adding the cell data size in case of on heap. But I think I can do it bit diff way.. So instead of tracking the data size and heap overhead, we will track cell data size and heap size. Pls see it is not heap overhead alone.. This is the total heap space occupied by memstore(s). I can change the calc accordingly. For on heap MSLAB , this heap space count will be the same as what we have before off heap write path work (ie. It will include all cell data size and overhead part). The new accounting ie. cellDataSize will include only the cell data bytes size part. This will be a subset of the former one then. Will do all necessary changes.. This will be a bigger patch then as I will rename all the related area from heapOverhead to heapSize or so. I believe that way will look cleaner. Thoughts? Append/Increment is not adding cells into MSLAB area.. This is to avoid MSLAB wastage. Say same cell is getting incremented 1000 times and cell key+ value size is 100 bytes. If every increment (add to memstore) cell was added to MSLAB, we will overall take ~100KB MSLAB space whereas only 100 bytes is valid at any point.. All the former cells are getting deleted by the addition of a new cell.. The Cell POJO as such is removed from CSLM. But we can not free up that space in MSLAB.. MSLAB is not designed to do this way. That the chunk allocation and offset allocation within a chunk is serially incremented way happening. We can not mark some in btw space as free and reuse.. That will make things very complex for us.. So to avoid these, the simplest way was to NOT use MSLAB for upsert.",design_debt,non-optimal_design,ab5970773ad187871270bb63f9a37cead4cf2f6a
hbase,17338,comment_8,+1 Add more comment on commit to MemstoreSize about diff between heap and data size.... (can be a version of comment that is later in class...) Glad of the simplification. Good stuff @anoop sam john,code_debt,complex_code,ab5970773ad187871270bb63f9a37cead4cf2f6a
hbase,17338,comment_9,Rest is all good. But as discussed internally that isOffheap() addition to MSLAB is better so that we can really avoid adding the dataSize to the MemstoreSize when the memstore is offheap but still the entire Cell is onheap. Now currently we just account for the dataSize also. We can do in another JIRA. +1 otherwise.,design_debt,non-optimal_design,ab5970773ad187871270bb63f9a37cead4cf2f6a
hbase,17338,comment_10,"Thanks for the reviews Stack & Ram.. I have to fix the failed test as those were having assert on heapOverhead size after cell addition to Memstore. Now we track heapSize not overhead. Will add more comments as Stack asked Regarding Ram's comment. I have different opinion. I dont think we need change this way. We track dataSize (irrespective of cell data in on heap or off heap area).. This dataSize been used at Segment level for in memory flush decision, Region level for on disk flushes and globally to force flush some regions. At the 1st 2 levels, it is not doubt that we have to track all the cell data size together. Now the point Ram says is when we have off heap configured and max off heap global size is say 12 GB, once the data size globally reaches this level, we will force flush some regions. So his point is for this tracking, we have to consider only off heap Cells and on heap Cell's data size should not get accounted in the data size but only in the heapSize. (At global level. But at region and segment level it has to get applied). 2 reasons why I am not in favor of this 1. This makes the impl so complex. We need to add isOffheap check down the layers. Also at 2 layers we have to consider these on heap cell data size and one level not. 2. When off heap is enabled, (We have the MSLAB pool off heap in place), we will end up in having on heap Cells when the pool is out of BBs. We will create on demand LABs on heap. If we dont consider those cell's data size at global level, we may reach forced flush level a bit late. That is the only gain. But here the on demand LAB creation is a sign that the write load is so high and delaying the flush will add more pressure to the MSLAB and more and more on demand BBs (2 MB sized) need to get created. One aim of the off heap work is to reduce the max heap space need for the servers. So lets consider the cell data size globally also (how we do now) and make global flushes. Now even if MSLAB is used, the append/increment use cases wont use MSLAB. The cells will be on heap then. But for such a use case, enabling MSLAB (and pool) is totally waste. That is a mis configuration. More and more on demand BB creation, when MSLAB pool is a bad sign. We have a WARN log for just one time as of now.. May be we should repeat this log at certain intervals. (Like for every 100th pool miss or so.) We should be able to turn MSLAB usage ON/OFF per table also. Now this is possible? Am not sure. These 2 things need to be checked and done in another jiras IMO.",design_debt,non-optimal_design,ab5970773ad187871270bb63f9a37cead4cf2f6a
hbase,17338,comment_19,+1 on commit. Lets get up on this new basis. Nice cleanup  (Thanks too for doc edit).,code_debt,low_quality_code,ab5970773ad187871270bb63f9a37cead4cf2f6a
hbase,17480,description,"HBASE-14551 moves the split region logic to the master-side. With the code in HBASE-14551, the split transaction code from region server side became un-used. There is no need to keep region_server-side split region code. We should remove them to avoid code duplication.",code_debt,dead_code,bff7c4f1fda5517c469db7863706140e3c97e9e0
hbase,17480,comment_2,+1 That is a nice lump of code removed. You remove Do we have an explicit test of the splitting path any more post this removal?,code_debt,dead_code,bff7c4f1fda5517c469db7863706140e3c97e9e0
hbase,18085,description,"Parallel purge in ObjectPool is meaningless and will cause contention issue since has synchronization (source code shown below) We observed threads blocking on the purge method while using offheap bucket cache, and we could easily reproduce this by testing the 100% cache hit case in bucket cache with enough reading threads. We propose to add a purgeLock and use tryLock to avoid parallel purge.",code_debt,multi-thread_correctness,d047cc9ecc7e0067c3b3aaeeb1600da8d774b211
hbase,18085,comment_6,Any chance for a micro benchmark tests Yu? Appreciate ur effort to consider all possible options :-),test_debt,lack_of_tests,d047cc9ecc7e0067c3b3aaeeb1600da8d774b211
hbase,18085,comment_9,"Reading the code in I can see it uses a state variable up in the layer in which is declared volatile. So we ourselves using a boolean volatile will be better any way right? Pls check once whether I misread some code path,",code_debt,low_quality_code,d047cc9ecc7e0067c3b3aaeeb1600da8d774b211
hbase,18085,comment_13,"Sorry, don't quite catch you sir, mind clarify? While writing the JMH case, I felt that using AtomicBoolean or volatile boolean we're actually re-implementing the tryLock logic. And I won't be surprised if JIT has any optimization for its native implementation (smile).",code_debt,duplicated_code,d047cc9ecc7e0067c3b3aaeeb1600da8d774b211
hbase,18085,comment_14,"In ur test method for tryLock, there is no logic other than just try lock and release. If that is been removed as dead code by compiler? One way to avoid that is using the return value. See BlackHole in JMH and its usage.. The diff in numbers that reported by JMH benchmark is so huge! The impl of tryLock is having a volatile read and all. So this huge diff in numbers looks strange no? That was my doubt.",code_debt,dead_code,d047cc9ecc7e0067c3b3aaeeb1600da8d774b211
hbase,18092,summary,Removing a peer does not properly clean up the state and metrics,code_debt,low_quality_code,7b40f4f3ec1cdf278bf416db66284d62c4e078e0
hbase,18092,description,Removing a peer does not clean up the associated metrics and state from walsById map in the,code_debt,low_quality_code,7b40f4f3ec1cdf278bf416db66284d62c4e078e0
hbase,18092,comment_4,"Attaching another patch for master. I found a redundant piece of code related to the patch. The only difference between the previous and this patch is {Code} @@ -528,9 +542,7 @@ public class implements ReplicationListener { */ public void src) { LOG.info(""Done with the recovered queue "" + - if (src instanceof ReplicationSource) { - - } + {Code} Also attaching patches for branch-1 and branch-1.3 as they both have diverged a little bit.",code_debt,low_quality_code,7b40f4f3ec1cdf278bf416db66284d62c4e078e0
hbase,18092,comment_13,"Just a nitpick, I think you merged the incorrect patch for master. I was looking at my latest master checkout and see that patch for master that went in is dated May 25, not yesterday. It's not a big deal in this case because the two patches have no functional difference, only that the newer patch got rid of a redundant piece of code.",code_debt,dead_code,7b40f4f3ec1cdf278bf416db66284d62c4e078e0
hbase,18180,summary,Possible connection leak while closing BufferedMutator in TableOutputFormat,code_debt,low_quality_code,ce1ce728c6daee9e294bf2915e91faaa73428f3d
hbase,18180,comment_1,Have a look at TableOutputFormat inside Connection leak problem has already been addressed in this package. Code sample for reference.,code_debt,low_quality_code,ce1ce728c6daee9e294bf2915e91faaa73428f3d
hbase,18180,comment_8,You can attach branch-1 patch alone where TestLockProcedure is not flaky.,test_debt,flaky_test,ce1ce728c6daee9e294bf2915e91faaa73428f3d
hbase,18646,description,The default procedure timeout of 60 sec and pool size (1) may be not optimal for large deployements,design_debt,non-optimal_design,19bb4ef487407cdf59a3f3c5bbef6dd8b917b682
hbase,18646,comment_1,"If the config is specific to log roll, please reflect this in the name of config.",code_debt,low_quality_code,19bb4ef487407cdf59a3f3c5bbef6dd8b917b682
hbase,18646,comment_4,Isn't the above default too short for a large cluster ? Please remove commented out code from patch.,code_debt,dead_code,19bb4ef487407cdf59a3f3c5bbef6dd8b917b682
hbase,18909,comment_6,"Attach a patch to fix the too long line report. But why ruby-lint report ""undefined method java_import""?  Any ideas?",code_debt,low_quality_code,63440a9c7d76e82bb0d90e7db098f0b9b5595244
hbase,18909,comment_14,Does this still reference to deprecated API?,code_debt,low_quality_code,63440a9c7d76e82bb0d90e7db098f0b9b5595244
hbase,19031,comment_6,The deprecation in RemoteHTable was added by this which shipped in version 0.99 tree parent author Enis Soztutar Enis Soztutar <enis@apache.org HBASE-11797 Create Table interface to replace HTableInterface (Carter) ... so the removal is fine. +1 from me on patch.,code_debt,dead_code,482d6bd3a4209b3c642b41ed4e42602a789e8369
hbase,19073,summary,Cleanup,code_debt,low_quality_code,dd70cc308158c435c6d8ec027e2435a29be4326b
hbase,19073,description,"- Remove the configuration - Keep following interface since they nicely separate ZK based implementation: ProcedureMemberRpcs - Replace CSM (interface) + BCSM (unnecessary middle hierarchy) with single CSM interface. - Don't pass whole CSM object around (with server in it which gives acess to pretty much everything), only pass the relevant dependencies. Discussion thread on dev@ mailing list.",design_debt,non-optimal_design,dd70cc308158c435c6d8ec027e2435a29be4326b
hbase,19073,comment_2,"So the only test passing in QA is But it's weird, this is second time am seeing it failing in a precommit, and the last patch was completely unrelated. It's not even in the flaky list. Let me dig.",test_debt,flaky_test,dd70cc308158c435c6d8ec027e2435a29be4326b
hbase,19183,summary,Removed redundant groupId from Maven modules,code_debt,low_quality_code,d4e3f902e6ba5b747295ca6053f34badd4018175
hbase,19183,description,Currently the modules hbase-checkstyle and hbase-error-prone define the groupId redundantly. Remove the groupId from these POMs.,code_debt,low_quality_code,d4e3f902e6ba5b747295ca6053f34badd4018175
hbase,19187,comment_8,Correction of test failures. Removed the on heap BC tests. Fixed one javadoc warn.,code_debt,low_quality_code,bff619ef7b100e8b09f7f5eb0f6e289ca51de096
hbase,19241,summary,Improve javadoc for AsyncAdmin and cleanup warnings for the implementation classes,code_debt,low_quality_code,5c312667ed9b9b3bbd870c7fcf26591479da000d
hbase,19241,comment_5,"+1 after fix the checkstyle warnings. Meanwhile, please add one line change change to trigger hbase-server test. Thanks.",code_debt,low_quality_code,5c312667ed9b9b3bbd870c7fcf26591479da000d
hbase,19241,comment_6,Fix checkstyle and javadoc warnings. Add one line change in hbase-server module to trigger the UTs.,code_debt,low_quality_code,5c312667ed9b9b3bbd870c7fcf26591479da000d
hbase,19373,summary,Fix Checkstyle error in hbase-annotations,code_debt,low_quality_code,07b193ae4f36345c4073023e0220b3e147c61884
hbase,19373,description,Fix the remaining Checkstyle error regarding line length in the *hbase-annotations* module.,code_debt,low_quality_code,07b193ae4f36345c4073023e0220b3e147c61884
hbase,19384,comment_1,"any chance you could share the specifics on how the coprocessors are set up in which you see this? Or, if you really want to go the extra mile, distill it down to a generic test case? Assuming there is a bug anyways, having a clear example of what is going wrong would be good both for identifying the problem as well as preventing a regression later :)",test_debt,lack_of_tests,1856237e2dae6505558bb72ee063839c48ab0e9e
hbase,19384,comment_3,Sure will write test case for this.  Correct. Yes it's because of removal of complete so we are not able to skip running subsequent coprocessors which do not have any implementation for preAppend or preIncrement hooks.,test_debt,lack_of_tests,1856237e2dae6505558bb72ee063839c48ab0e9e
hbase,19478,description,Currently issues one Get per WAL file: This is rather inefficient considering the number of WAL files in production can get quite large. We should use multi-get to reduce the number of calls to backup table (which normally resides on another server).,code_debt,slow_algorithm,cafd4e4ad76f45be912edc9d5021f872de94fd5c
hbase,19531,summary,Remove needless volatile declaration,code_debt,low_quality_code,9d0c7c6dfbcba0907cbbc2244eac570fcc4d58a5
hbase,19633,description,"In the previous implementation, we can not always cleanly remove all the replication queues when removing a peer since the removing work is done by RS and if an RS is crashed then some queues may left there forever. That's why we need to check if there are already some queues for a newly created peer since we may reuse the peer id and causes problem. With the new procedure based replication peer modification, I think we can do it cleanly. After the are done on all RSes, we can make sure that no RS will create queue for this peer again, then we can iterate over all the queues for all Rses and do another round of clean up.",design_debt,non-optimal_design,19707a85dd3287ccd2dc9f70a3a437ece7352722
hbase,19775,comment_4,"yep, @josh is right about the oneliner style - it's much more ruby-esque. {{cause = cause.getCause if cause.is_a? can we add a test in",test_debt,lack_of_tests,6bacb643bc66b994c386b7fd175664802047eef7
hbase,19815,summary,Flakey,test_debt,flaky_test,"53d0c2388d9eb564c20e4231d3cb7ae7be360478,581fabe7b2177a090af33517f2f7cb1cdab2c64b"
hbase,19862,description,"We have temporary (added in HBASE-19007) and concept of CoreCoprocessors which require that whichever they get, it should also implement This test builds mock RegionCpEnv for TokenProvider but it falls short of what's expected and results in following exceptions in test logs Patch adds the missing interface to the mock. Also, uses Mockito to mock the interfaces rather the crude way.",design_debt,non-optimal_design,"61358985b15164cbcdc6e58005208c0906c3830a,1beb687f4e489dff352a746d4544ff1a0a1ed829"
hbase,19862,comment_2,Will cleanup checkstyles on commit. Ping  since you reviewed the related change too.,code_debt,low_quality_code,"61358985b15164cbcdc6e58005208c0906c3830a,1beb687f4e489dff352a746d4544ff1a0a1ed829"
hbase,19939,comment_1,"The flakey-finder fingered the above commit as breaking the split test. Here is when the split test went bad... Unstable Build #1372 (Feb 5, 2018 3:53:40 PM) add description Build Artifacts Changes HBASE-19703 Functionality added as part of HBASE-12583 is not working (detail) Started by timer This run spent: 3 min 39 sec waiting in the queue; 27 min building on an executor; 31 min total from scheduled to completion. Revision: Test Result (6 failures / +3) See how the commit is ""HBASE-19703 Functionality added as part of HBASE-12583 is not working (detail)""",test_debt,flaky_test,3bb8daa60565ec2f7955352e52c2f6379176d8c6
hbase,19939,comment_3,"is already in flaky list so the QA didn't run it for HBASE-19703.- -Ok, It is rather than I misunderstood the test name due to the topic...Let me correct the test name for the topic. The correct test name is rather than",test_debt,flaky_test,3bb8daa60565ec2f7955352e52c2f6379176d8c6
hbase,19939,comment_4,The latest QA in HBASE-19703 is shown below. The is already in flaky.  Do you intent to fix the test totally? I'm +1 to your patch even if the is still flaky.,test_debt,flaky_test,3bb8daa60565ec2f7955352e52c2f6379176d8c6
hbase,19939,comment_5,Thanks for correcting the name ! Let me fix the commit description as well and resubmit the patch. After this fix if the test is still flaky then I will take another look at it.,test_debt,flaky_test,3bb8daa60565ec2f7955352e52c2f6379176d8c6
hbase,19969,summary,Improve fault tolerance in backup merge operation,design_debt,non-optimal_design,d5aaeee88b331e064830a2774f4fed238631457c
hbase,19969,description,"Some file system operations are not fault tolerant during merge. We delete backup data in a backup file system, then copy new data over to backup destination. Deletes can be partial, copy can fail as well",design_debt,non-optimal_design,d5aaeee88b331e064830a2774f4fed238631457c
hbase,19969,comment_2,"Add backup after 'Get' I don't think the above passes checkstyle Do you want to implement in this JIRA ? I don't think the above is right - we use org.slf4j Is the change to public for testing ? Drop commented out code. If we get into the if block, the rename() call below would fail, right ? Drop commented out code. Please address checkstyle, findbugs warnings. Will continue reviewing.",code_debt,low_quality_code,d5aaeee88b331e064830a2774f4fed238631457c
hbase,19969,comment_5,One more comment about the new test : You can store the return value from in a variable. The count would not change in between the log and the assertion. I looped the following tests locally which passed:,design_debt,non-optimal_design,d5aaeee88b331e064830a2774f4fed238631457c
hbase,19969,comment_14,Vlad: Can you address checkstyle warnings ?,code_debt,low_quality_code,d5aaeee88b331e064830a2774f4fed238631457c
hbase,19969,comment_20,", looks like you might have some flaky tests on Hadoop3. Would be good to take a quick look to rule out test issues (the cnxn refused sounds like it might be just be the node itself). Logs are at",test_debt,flaky_test,d5aaeee88b331e064830a2774f4fed238631457c
hbase,19969,comment_21,Josh: See HBASE-20123. Looks like we would need hadoop 3.1.0+ (or 3.0.2+) to make backup tests fully working.,architecture_debt,using_obsolete_technology,d5aaeee88b331e064830a2774f4fed238631457c
hbase,19977,comment_1,Yes . Using AtomicInteger will solve the problem. Will test this in a cluster.,test_debt,lack_of_tests,16f1f5b49424fcabc9b5c10882dab4f5bf7fa84b
hbase,19991,comment_1,"careful debugging revealed a few more places where the jersey deps were leaking in. I also need to add a comment somewhere that upgrading to jersey 2.26 (from our current 2.25.1) will likely need an upgrade to jetty 9.4, so should be done with great care, as that was one of the things i tried here and it didn't work as well as I thought it would",code_debt,low_quality_code,0d6acfa0cf878172ef63d4c5900293f95c0a0fea
hbase,19998,summary,Flakey,test_debt,flaky_test,"2f1b3eab675ac327a6f61b724d5f0bce01ec6e68,50c705dad9825d3095352b997be35a2568bd6190"
hbase,19998,comment_4,Could we add some commit message to remind readers that the commit is for debug? It can help readers to realize that the flaky hasn't been fixed.,test_debt,flaky_test,"2f1b3eab675ac327a6f61b724d5f0bce01ec6e68,50c705dad9825d3095352b997be35a2568bd6190"
hbase,19998,comment_7,"The test failed in flakies a few times last night, lets see how it does.",test_debt,flaky_test,"2f1b3eab675ac327a6f61b724d5f0bce01ec6e68,50c705dad9825d3095352b997be35a2568bd6190"
hbase,20072,comment_1,We don't have a test suite for the book other than building it and we don't have an automated check for building the book. :/,test_debt,lack_of_tests,7889df37118a0593dfb0156c6edf1bb96d4c94b9
hbase,20072,comment_2,filed HBASE-20077 for the lack of a test for building the book. that jira also has a command you can run locally if you want to quickly check the result of this change.,test_debt,lack_of_tests,7889df37118a0593dfb0156c6edf1bb96d4c94b9
hbase,20100,summary,flakey,test_debt,flaky_test,ba063abd2f4b0aaa0622a8665c4ba96ed31eafb8
hbase,20100,description,"Failed in the nightly in interesting way. A subprocedure of enable table is assigning regions. The test is doing a kill of the procedure store to ensure we can handle all state transtions even in face of failure. In this case, the kill of PE framework came in the finish of the assign procedure AFTER we'd updated hbase:meta and the internal AM state moving Region from OPENING to OPEN. Rerunning this step on restoration of the AMv2 WAL Store results in....",test_debt,low_coverage,ba063abd2f4b0aaa0622a8665c4ba96ed31eafb8
hbase,20100,comment_4,Linked to a follow-on issue. The AssignProcedure is too coarsely grained (feedback from  that makes sense). HBASE-20103 is about breaking it up to do finer steps. Will do in follow-on.,design_debt,non-optimal_design,ba063abd2f4b0aaa0622a8665c4ba96ed31eafb8
hbase,20108,comment_5,"looks like jline shows up in the modules now. the rest of the approach looks really good, though! I assume this works both in dev tree and out of a bin tarball?",design_debt,non-optimal_design,2402f1fd43fbe04ecce8bac67d31931251bcac6c
hbase,20595,description,"Regionserver groups needs to specially handle what it calls ""special tables"", tables upon which core or other modular functionality depends. They need to be excluded from normal rsgroup processing during bootstrap to avoid circular dependencies or errors due to insufficiently initialized state. I think we also want to ensure that such tables are always given a rsgroup assignment with nonzero servers. (IIRC another issue already raises that point, we can link it later.) Special tables include: * The system tables in the 'hbase:' namespace * The ACL table if the AccessController coprocessor is installed * The Labels table if the coprocessor is installed * The Quotas table if the FS quotas feature is active Either we need a facility where ""special tables"" can be registered, which should be in core. Or, we institute a blanket rule that core and all extensions that need a ""special table"" must put them into the 'hbase:' namespace, so the test will return TRUE for all, and then rsgroups simply needs to test for that.",design_debt,non-optimal_design,304d3e6fa9f44cb25ab03cb6d949f1c9b74779d1
hbase,20595,comment_0,"The issue I was thinking of was HBASE-20500 which maintains one server in the 'default' group, but that is not the full scope of what we should have. We should guarantee the placement, specifically, of ""special tables"" into a rsgroup that must always have a nonzero number of servers, and not assume that will be the 'default' group. In fact I think we should have two default rsgroups, very similar to how we do namespacing: a ""default"" group into which goes all user level stuff not otherwise specified; and a system group into which goes system/special tables (in namespace terms, akin to the 'hbase' namespace). Special tables should not be allowed to move into rsgroups for user tables.",design_debt,non-optimal_design,304d3e6fa9f44cb25ab03cb6d949f1c9b74779d1
hbase,20595,comment_7,Updated master patch removes the unused imports that checkstyle complained about.,code_debt,low_quality_code,304d3e6fa9f44cb25ab03cb6d949f1c9b74779d1
hbase,20975,description,"Find this one when investigating HBASE-20921, too. Here is some code from executeRollback in You can see my comments in the code above, reuseLock can cause the procedure executing(rollback) without a lock. Though I haven't found any bugs introduced by this issue, it is indeed a potential bug need to fix. I think we can just remove the reuseLock logic. Acquire and release lock every time. Find another case that during rolling back, the procedure's lock may not be released properly: see comment:",code_debt,multi-thread_correctness,a07e755625382d3904c935c21a4f240ede6d2f43
hbase,20975,comment_0,"+1 on removing it for now. We have optimized too much before getting things correct... Let's keep the logic simple first. Also, there are some bad style issues. At least let's remove the space between 'stackTail' and '--', it looks like '-- And do not do assignment in the condition block of if. Let's change to",code_debt,low_quality_code,a07e755625382d3904c935c21a4f240ede6d2f43
hbase,20975,comment_4,Please include a small change in hbase-server module so we can run more tests?,test_debt,low_coverage,a07e755625382d3904c935c21a4f240ede6d2f43
hbase,20975,comment_8,"The failed UTs are related, upload a patch to fix it. My fix reveals a bug which exists all the time... the acquire and release lock logic in procedure is a bit of mess here... For different procedures, we have holdLock() set to true of false, for procedures which holdLock() == true, we need to override hasLock() method to test whether we have the lock... when executing procedure, we acquire the lock and release it immediately. And when the procedure is success, we release the lock again in It works, since for procedure with holdLock = true, we call releaseLock() with force = false the first time, the lock won't release here, but in where we call releaseLock() with force = true, the lock will be released there. For procedure with holdLock = false, it also works since when the first time we call releaseLock() with force = false, the lock will release, and the second time we call relseaseLock in we only release it lock if procedure with holdLock=true. So releasing lock won't called twice But when rolling back, we acquire the lock but only release the lock for procedure with holdLock = true... *Rolling back for procedures with holdLock=false(e.g. most of the Table Procedures) will never release the lock...*",design_debt,non-optimal_design,a07e755625382d3904c935c21a4f240ede6d2f43
hbase,21160,comment_1,"Since the catch block currently re-throws IOException, that means the catch block is no longer needed. Please run with the change locally before attaching patch. Thanks",design_debt,non-optimal_design,1cf920db4395926c634ed0946a57dfa52227472f
hbase,21160,comment_2,Hi I found so many re-throws blocks in the file of . Should we resolve it all?,design_debt,non-optimal_design,1cf920db4395926c634ed0946a57dfa52227472f
hbase,21173,summary,Remove the duplicate HRegion#close in TestHRegion,code_debt,duplicated_code,bea26e98e684d695c440ddcc6d3c62fd3d269d20
hbase,21173,description,"After HBASE-21138, some test methods still have the duplicate HRegion#close.So open this issue to remove the duplicate close",code_debt,duplicated_code,bea26e98e684d695c440ddcc6d3c62fd3d269d20
hbase,21173,comment_1,I think the intention of HBASE-21138 is to let do the cleanup. Can you remove the duplicate region.close() call in these subtests ? Thanks,code_debt,duplicated_code,bea26e98e684d695c440ddcc6d3c62fd3d269d20
hbase,21173,comment_2,"Thanks for working on this JIRA, . In previous discussion, I thought calling on closed/null region is no harm, while deleting the duplicate close may make some tests unhappy. So we were not very strict to make region close only once. Good discussion to revisit this. - The last one in was added after HBASE-21138 and can be removed here. - The in was followed by the assertion to verify that the .regioninfo file is still there. I saw the close-and-assertion happens in the same test method multiple times so I was not sure we could remove the close here. - In {{testSequenceId}} and the pattern in this patch, i.e. ""{{region.close() && region = null}}"", is not correct. The reason is that, it makes the in {{teardown()}} a no-op, leaving WAL not closed. One fix is to not set the null value and leave the test as-is; a better one I think is as suggested, we can replace the {{region.close()}} with and set {{this.region}} null value. - Other places to set {{this.region}} null value after is good to explicitly make the in {{tearDown}} a no-op.",design_debt,non-optimal_design,bea26e98e684d695c440ddcc6d3c62fd3d269d20
hbase,21173,comment_4,"Attach 002 patch as  suggestions.Thanks {{testSequenceId}} - In line 269, Replace region.close() with - In line 285, we need to verify that the value of is consistent before and after region.close(), so we keep region.close() and replace it with - In line 315, replace region.close() with - In line 317, remove duplicate and set this.region to null - In line 578, replace region.close() with and set this.region to null - In line 951, replace region.close() with - In line 1083, replace region.close() with - In line 1281, set this.region to null - In line 1281, set this.region to null - In line 4175, keep region.close() and set region to null as said by Mingliang Liu - In line 6234, remove region.close() Other places where set this.region null value after will be fine as said by .",code_debt,duplicated_code,bea26e98e684d695c440ddcc6d3c62fd3d269d20
hbase,21238,description,Correct way of handling error condition is through return value of run method.,design_debt,non-optimal_design,07e2247d2e0269cbd3f987df579fc0fe78220130
hbase,21589,comment_2,"It is very strange, it never failed in my environment. , can you upload an output or something, I can't find the failing test in jenkins or Flaky test board.",test_debt,flaky_test,68b5df00951d3ee55efaa6068f4530dca17eae1f
hbase,21816,description,"User may get confused, to understanding our HBase configurations which are loaded for replication. Sometimes, User may place source and destination cluster conf under ""/etc/hbase/conf"" directory. It will create uncertainty because our log points that all the configurations are co-located. Existing Logs, But it should be something like, This jira only to change the log-line, no issue with the functionality.",design_debt,non-optimal_design,a155d17fa6b0afab2c21d404af7ea92f195dcef0
hbase,22174,summary,Remove error prone from our precommit javac check,design_debt,non-optimal_design,"86f9443eeb34f089ab10041e80522bdeec6eb796,1233ca7bf873f41e3554b76dc58f0f6d4c0d9534,1233ca7bf873f41e3554b76dc58f0f6d4c0d9534,86f9443eeb34f089ab10041e80522bdeec6eb796,86f9443eeb34f089ab10041e80522bdeec6eb796,86f9443eeb34f089ab10041e80522bdeec6eb796"
hbase,22193,description,"Now the default config is Integer.MAX_VALUE. The ITBLL failed to open the region as HBASE-22163 and retry 170813 to reopen. After I fixed the problem and restart master, I found it need take a long time to init the old procedure logs because there are too many old logs... Code in",code_debt,low_quality_code,"942f8c45cd1a1e0a8956fc10b811dd2add510645,249ac58d4fe10e19e6643a37907a39b75294dbdc,e4e561b37fe5ff623bcf53ad096b30610f36dab2,cfd74a68ad98ae65320d27571e4dc7c5add5cee6,942f8c45cd1a1e0a8956fc10b811dd2add510645,942f8c45cd1a1e0a8956fc10b811dd2add510645"
hbase,22193,comment_1,"Talked with  offline, the problem here is not the retry number, but the retry interval. When a region is failed open, we will try to reassign it ASAP, the intention here is to make the region online soon. But sometimes, the region can not online on any RS because of config error or some other problems, then it is not a good idea to retry immediately as it will lead to so many proc wals... So the first thing is to detect this problem and increase the retry interval... And for a long term solution, I think we need to find out a way to better deal with config error. For now, the will hang there forever and the only way is to use HBCK2 to bypass the procedure and fix the table state, which is a bit difficult. For hbase version before 2.0, I think there is a straight forward way to fix this is to disable the table, fix the schema, and enable it again...",design_debt,non-optimal_design,"942f8c45cd1a1e0a8956fc10b811dd2add510645,249ac58d4fe10e19e6643a37907a39b75294dbdc,e4e561b37fe5ff623bcf53ad096b30610f36dab2,cfd74a68ad98ae65320d27571e4dc7c5add5cee6,942f8c45cd1a1e0a8956fc10b811dd2add510645,942f8c45cd1a1e0a8956fc10b811dd2add510645"
hbase,22203,description,The DemoClient.java currently uses a not consistent formatting and should be reformatted.,code_debt,low_quality_code,"f227eb7aac3310ddcd598dcf134e4943a0b99c09,44be52463bf652213bb395783517af9a09bf6a43"
hbase,22228,description,ThrottlingException was deprecated in 2.0.0 and should be removed in 3.0.0.,code_debt,dead_code,"9c13dde84b494164995ec3f1cd1d308cb78cb5a2,4743efac416812ddc48e80b2550a09676de5dccb"
hbase,22400,comment_1,Please ignore my RR. I was just fed up with the ugly adapter code and can't wait to remove them. :) Please continue.,design_debt,non-optimal_design,"930691a846c217976081a220f6560f94e03726a4,5fddbe04062aead7b30f70ac78c360f37a4d2ad0"
hbase,22424,summary,Interactions in RSGroup test classes will cause and flaky,test_debt,flaky_test,759ee217c52111b45c96809ee77bc62d3b884bea
hbase,22424,description,"When running rsgroup test class folder to run all the UTs together, and will flaky. Because TestRSGroupsAdmin1, TestRSGroupsAdmin2 and TestRSGroupsBalance are all extends TestRSGroupsBase, which has a static variable INIT, controlling the initialize of 'master 'group and the number of rs in 'default' rsgroup. Output errors of is shown in HBASE-22420, and will encounter NPE in because `master` group has not been added.",test_debt,flaky_test,759ee217c52111b45c96809ee77bc62d3b884bea
hbase,22424,comment_1,"nice findings. Just curious, have you ran the flaky tests multiple times (such as 10) and all are passing?",test_debt,flaky_test,759ee217c52111b45c96809ee77bc62d3b884bea
hbase,22656,comment_1,+1 (non-binding) Nice catch. The two method and are never used.,code_debt,dead_code,605f8a15bb7dabb23f1d397ca28ca0696a390497
hbase,22707,comment_0,"Interesting. We need something like this. I like that you hook it into assigns. I'm wary though of re-use of joinCluster. The messaging in logs will look strange. Will say stuff like 'Joining cluster...' and 'Waiting for RegionServers to join;....'. Then we re-add chores, do the unnecessary wait on RS. Should we just add a method that gets the new row in hbase:meta and then does what call does?",design_debt,non-optimal_design,237229cade90f0b138745e1ba4242044e70b671c
hbase,22707,comment_1,"Yeah, using _joinCluster_ was a bit too much of an attempt of reusing existing code without changing it as much as possible, but I agree with all the side effects you had pointed out, . Had done some refactoring to re-use some of the adding a with the changes. Let me know what you think, if you feel this PR approach is fine, I will work on some UTs for it.",design_debt,non-optimal_design,237229cade90f0b138745e1ba4242044e70b671c
hbase,22832,summary,Remove deprecated method in HFileOutputFormat2,code_debt,dead_code,"ef887e52927a391e820d7c22188c16f6ee9853e3,6b10084db3d17ffbc8df34569e7763be3bbf47d7"
hbase,22832,description,The method was deprecated in and should be removed for 3.0.0.,code_debt,dead_code,"ef887e52927a391e820d7c22188c16f6ee9853e3,6b10084db3d17ffbc8df34569e7763be3bbf47d7"
hbase,22837,description,"Currently, explanation about *Custom WAL Directory* configuration is a sub-topic of *Bulk Loading,* chapter, yet this subject has not much relation with bulk loading at all. It should rather be moved to a sub-section of the *Write Ahead Log (WAL)* chapter.",architecture_debt,violation_of_modularity,8c1edb3bba56677b0ee106f340d12b42e999cdaa
hbase,22933,description,"The old implementation is a bit strange, the isStuck method is like this: It can only return true when there are ongoing procedures. But if we have a procedure, then the procedure will try to reassign region. Scheduling a new procedure does not make sense here, at least for branch-2.2+. I suggest we just remove the related code, since the default retry number for assigning a region is Integer.MAX_VALUE. And even if user set this to small value and finally the region is left in FAILED_OPEN state without a procedure, HBCK2 is used to deal with this, it is not necessary to deal it automatically.",design_debt,non-optimal_design,"090c55f3ff40dea807dc7e67240f19dcafb3865f,ee25b92ecc9b554d5db0524c2f41f4c7f87d18c1"
hbase,22981,comment_0,It is better to remove the unused flags instead of just ignoring those.,code_debt,low_quality_code,"51b6ce391b51757472f02cfcbb4b85ebd2d0c50c,6ecf87c7add85ce7d36093c7bc2b757e3678f0d8"
hbase,23087,summary,Remove the deprecated bulkload method in,code_debt,dead_code,"c1a476c49281e4b7fde8f9638918bcebcbde25e3,437bb0665236fb95f49dca07ca50f0a29af9efc2"
hbase,23087,description,The class is IA.Private and it has not been released yet so let's just remove this method to keep the class clean.,code_debt,dead_code,"c1a476c49281e4b7fde8f9638918bcebcbde25e3,437bb0665236fb95f49dca07ca50f0a29af9efc2"
hbase,23646,summary,Fix remaining Checkstyle violations in tests of hbase-rest,code_debt,low_quality_code,"0a9e1f807de0de5ad79708556f36826aa3f1fd05,6f3da9ecd101498915f0f7d80544937e5710ea9c,0042deafd954a00e7b1355eb92b1abd24700cfb0,a0c0821a51e2e65b5c42f2007b6122dfd884ead6"
hbase,23646,description,In {{hbase-rest}} Checkstyle reports a lot of violations. The remaining violations in the tests should be fixed.,code_debt,low_quality_code,"0a9e1f807de0de5ad79708556f36826aa3f1fd05,6f3da9ecd101498915f0f7d80544937e5710ea9c,0042deafd954a00e7b1355eb92b1abd24700cfb0,a0c0821a51e2e65b5c42f2007b6122dfd884ead6"
hbase,23651,description,"HBASE-17178 Add region balance throttling, but it can not be disabled, sometimes we need no throttle and balance the cluster as fast as possible.",design_debt,non-optimal_design,"66d1258aa3a8c48a6001f5ec9d8eaeb64415873e,198f06d2e69f1b28a42775cae2d244f20c2a6fac,4b5f71457dc9771acb17a8c8ad2aa68812bd7aca,37ef9c988605c43c85cb51fb58bea9886b538137"
hbase,23752,summary,Fix a couple more test failures from nightly run,test_debt,low_coverage,"02bd0eca5337a42de47a3b18fe929996eb2ff5e8,792d43aeb5801d75abc9b2fee0fb35c81f8367d1,adcf5c9b1acc077d2067876bf6defd2f002d4275,4e25707bbd53122e5f6477fb32c126a7475cdc81,99afd13743141f35739ff080bdba377ad8ef7d48,e4cbe57225426477d9fd3f6c174c4396fc4b52d6,836bd9495eaa3608ead810b6e1d65a0792971af4,2612bc6931562f3b1f3e41b4bd493c5e3885074e,73569e7d566d356e2dae5d555b5291cb57e16a2e,2d9f88e2008e0bdf4cdd158d5383b4e321972308,334495ef517740d02c6058f62e6a4920294e9142,dfa6755220b564c8c0827adc1b07a76c80c1f3ab,1cca55106e08b984779112930089bef9a1bdd6e3"
hbase,23752,comment_2,"Mind taking a look at this? This is the last (hopefully) patch that fixes all the test failures from the full nightly run. I verified that the two failed tests are flakes, ran them locally.",test_debt,flaky_test,"02bd0eca5337a42de47a3b18fe929996eb2ff5e8,792d43aeb5801d75abc9b2fee0fb35c81f8367d1,adcf5c9b1acc077d2067876bf6defd2f002d4275,4e25707bbd53122e5f6477fb32c126a7475cdc81,99afd13743141f35739ff080bdba377ad8ef7d48,e4cbe57225426477d9fd3f6c174c4396fc4b52d6,836bd9495eaa3608ead810b6e1d65a0792971af4,2612bc6931562f3b1f3e41b4bd493c5e3885074e,73569e7d566d356e2dae5d555b5291cb57e16a2e,2d9f88e2008e0bdf4cdd158d5383b4e321972308,334495ef517740d02c6058f62e6a4920294e9142,dfa6755220b564c8c0827adc1b07a76c80c1f3ab,1cca55106e08b984779112930089bef9a1bdd6e3"
hbase,23789,summary,[Flakey Tests] ERROR [Time-limited test] cannot read rules file located at ' ',test_debt,flaky_test,"9cc7a711e616566ecedb8620a1c8b8a29c3d27b0,3a1a39d40d8a4733c5e657793eaec1c4cf40dde8"
hbase,23789,description,"We can't find the balancer rules we just read in the test in high load conditions Test then goes on to fail with: Instead, have tests write rules to local test dir.",test_debt,low_coverage,"9cc7a711e616566ecedb8620a1c8b8a29c3d27b0,3a1a39d40d8a4733c5e657793eaec1c4cf40dde8"
hbase,23792,summary,[Flakey Test],test_debt,flaky_test,"6ba1df3b3932ce5825cb43511a7483e64f9471f9,ff1b91d153f4322a9342bbf614686993cac2a9c9"
hbase,23792,comment_0,"I can't repro this locally or find a source for the filesystem implementation getting flipped to a distributed fs. However, the only place in Hadoop code where I see this ""Wrong FS"" message thrown as an is in Looking closer at the xml report, I see that the test failed once with the above. Surefire tried to re-run it, but it failed the rerun with which implies to me that when surefire reruns a test method, it does not run the BeforeClass business. I also notice that the test method runs the same code twice, but both times it's using I think one of the invocations is supposed to be calling So. # Survive flakey rerunning by converting the static {{BeforeClass}} stuff into instance-level {{Before}}. # Break the test method into two, one for running over each of the snapshot manifest versions.",test_debt,flaky_test,"6ba1df3b3932ce5825cb43511a7483e64f9471f9,ff1b91d153f4322a9342bbf614686993cac2a9c9"
hbase,23867,summary,[Flakey Test],test_debt,flaky_test,156ababa4e15c9433a209631f4101d2341d7c231
hbase,23867,description,Test has this on it: @Test // Test is flakey. TODO: Fix! Let me cut it down. It runs for a while. It fails nearly every run since HBASE-23865 Up flakey history from 5 to 10 which upped the flakey test so it ran with more fork count.,test_debt,flaky_test,156ababa4e15c9433a209631f4101d2341d7c231
impala,72,comment_0,Hm - we have a test that tests almost exactly this case: We had better check that the test is running as expected.,test_debt,lack_of_tests,222d15c6ca0b1b20338235bcccd768bf97d0e889
impala,74,description,"Impala hdfs-fs-cache values for namenode address and namenode port should be read from Impala's core-site.xml file rather than input as command line args. If these args are not specified when impalad starts, then it defaults to localhost:20500 which can be confusing. Definition: 20500, ""namenode port""); ""localhost"", ""namenode host"");",design_debt,non-optimal_design,84e35d591cdec77ae9945f6447093b29f411ef32
impala,74,comment_0,"Just to let everyone know what happened here: -nn_port and -nn are still around, but if -nn is not specified (and it defaults to blank), Impala will try to read the value of {{fs.defaultFS}} (and if that doesn't exist, from the frontend which has loaded a Hadoop configuration. We will consider removing -nn and -nn_port if this proves to be robust enough. This change will be in Impala 0.6.",design_debt,non-optimal_design,84e35d591cdec77ae9945f6447093b29f411ef32
impala,92,summary,Significant performance difference between LIKE = 'x' AND = 'x',code_debt,slow_algorithm,6c087164392e5734743990d51d197ee833a0be86
impala,92,description,"I'm running the following two queries. The only difference between them is I'm using ""LIKE"" in one case and ""="" in another, though there is no ""%"" in the LIKE, so the effect is the same. I was surprised to see approximately a 10x difference in performance between them. I'm running I've attached the two query profiles. The basic difference is in the execution rate: Obviously I've fixed my query.",code_debt,slow_algorithm,6c087164392e5734743990d51d197ee833a0be86
impala,211,comment_2,"I think it is useful to log it once, but logging it for each impalad every 1/2 second seems excessive.",design_debt,non-optimal_design,b72b711bdb7d9a3a40ead5474d2ae7c1ef846b69
impala,231,summary,Impala HBase scan is very slow,code_debt,slow_algorithm,7aadac236de0cdd7e8fb88d1dcc2adad2662caec
impala,231,description,"Impala's HBase scan is very slow. For scanning 40000 rows from an colocated HBase table, it took ~5.7sec. The query is: select count(*) from (select * from hbasetbl limit 40000); Majority of the time is spent inside I've done some experiment to figure out where we spent the time by adding a few more timers. Here's the runtime profile: HBASE_SCAN_NODE (id=0):(5s714ms 99.98%) - BytesRead: 4.26 MB - 483.594us - 5s710ms - MemoryUsed: 0.00 - MyOwnTimer1: 1s387ms <-- We should trim this time. - MyOwnTimer2: 2s798ms <-- - MyOwnTimer3: 688.179ms - MyOwnTimer4: 0ns - MyOwnTimer5: 0ns - NumDisksAccessed: 0 - 5.14 MB/sec - RowsReturned: 40.00K (40000) - RowsReturnedRate: 7.00 K/sec - ScanRangesComplete: 0 - 0 - 0ns - 0ns - 0ns - 0ns - 0 - 827.604ms <-- we spent only 800ms on fetching from HBase - 775.05 KB/sec I've attached the code with more timers in the attached file When I increase the limit to 80000, the perf is much worse. Here's the profile: HBASE_SCAN_NODE (id=0):(23s027ms 99.99%) - BytesRead: 8.51 MB - 249.40us - 23s018ms - MemoryUsed: 0.00 - MyOwnTimer1: 5s680ms - MyOwnTimer2: 11s401ms - MyOwnTimer3: 2s829ms - MyOwnTimer4: 0ns - MyOwnTimer5: 0ns - NumDisksAccessed: 0 - 2.76 MB/sec - RowsReturned: 80.00K (80000) - RowsReturnedRate: 3.47 K/sec - ScanRangesComplete: 0 - 0 - 0ns - 0ns - 0ns - 0ns - 0 - 3s085ms - 376.52 KB/sec I didn't see any disk activity. So, I dont' think HBase is going to disk. Also, the read time is only 3sec. The big chuck of time is spent within the block of MyOwnTimer1 and MyOwnTimer2. We spent 5x more time there even though we only scan 2x more rows.",code_debt,slow_algorithm,7aadac236de0cdd7e8fb88d1dcc2adad2662caec
impala,231,comment_0,"We have to mark local var using ""DeleteLocalRef"" when the local var is done. This will help reduce mem pressure.",code_debt,low_quality_code,7aadac236de0cdd7e8fb88d1dcc2adad2662caec
impala,321,description,In the summary node in the profile the info strings for start / end times are specified with second granularity. It would be nice to have them specify milliseconds.,design_debt,non-optimal_design,11decfa48a3578d19b50513c3a1a8daae7721ae4
impala,322,description,"It would be nice to get able to get information about errors the query encountered, like corruptions, missing block metadata, integer overflows, and anything you guys think would be interesting, in the profile so it's all centralized.",design_debt,non-optimal_design,d730071dde3193dfacbeb351b8f406e4a632e8dd
impala,330,description,"Web page requests hold for the duration of the callbacks that render the page. This isn't terrible for short-lived requests, but anything that takes a while to render will block other requests. I tried the obvious thing, and Mongoose started behaving very weirdly (rendering the wrong page, rejecting a lot of concurrent requests).",code_debt,multi-thread_correctness,1e88e6d0c07e37a30a3100dadbe36e868c530edc
impala,563,summary,Improve error message on querying an uninitialized Catalog,design_debt,non-optimal_design,31ea219bb400ac6d9e8c7a4ac440490bfdd60498
impala,563,description,"Catalog will fail to initialize when there's a metastore connection issues (mostly misconfiguration). Impala logs the exception, but no one really reads the log. When a user (or admin) connects to Impala, they see an empty database and wouldn't know what wrong. To improve the experience, Impala should return an exception for all queries submitted (except invalidate metadata) if the catalog wasn't initialized. The exception should also contain the original exception message from the failed metastore connection.",design_debt,non-optimal_design,31ea219bb400ac6d9e8c7a4ac440490bfdd60498
impala,636,comment_0,"When we process topic deletion, we just remove an entry from the backend list without removing the entry from backend_map_. We should remove the entry from backend_map_ if the list is empty.",code_debt,low_quality_code,f275c6b951984e1fc3c3d70585f774e473232cc3
impala,710,summary,Cleanup log spew from Planner changes,code_debt,low_quality_code,aa9796817f811413d106d231a979c1ad986118f4
impala,844,description,"We idiomatically do this for Thrift RPCs: Thrift can throw as well (see e.g. {{recv_*}} for any method). We don't catch it, and therefore can abort on the rare occasion it gets thrown. Instead we should catch {{TException}}.",code_debt,low_quality_code,f6a297568d56adda92c5685493799cef14ce1860
impala,979,description,If there is an error starting the statestore subscriber BE (heartbeat server) an Impala service should abort. This helps make it easier to diagnose issues such as port conflicts. This currently doesn't happen because we ignore the Status returned by the heartbeat_server_- From,design_debt,non-optimal_design,38fdda20e4ee98e92fb706c5cc133232b3439e14
impala,1013,summary,is unreasonably slow,code_debt,slow_algorithm,c096d7688102e0570d2461ccc9745abded1b5ae6
impala,1013,description,This function is very simple and goes through the HS2 interface to retrieve a few functions. I believe this used to be instantaneously and now takes hundreds of seconds making this likely unusable.,code_debt,slow_algorithm,c096d7688102e0570d2461ccc9745abded1b5ae6
impala,1022,description,"rows_read < rows_in_file) { We should detect the case where doesn't actually equal the number of rows in the file. If abort_on_error is true, this should fail the query, otherwise we should log something via scan_node_-Such handling did not exist before. Need to decide whether we will read at most as many rows as the metadata or continue reading until there are no more rows in the file. We will need to add tests with parquet files whose metadata is not correct.",test_debt,lack_of_tests,"fe0646f76bf35b525f8f63948a7d3baa481e5bc0,2e2d8ca4a5a5b1aa13b5732eaa31ee54a68a7f13"
impala,1118,comment_2,Fixing this would make query gen testing easier.,design_debt,non-optimal_design,5b4e8b79bf277d093a996d9b1465ab8486cf6dff
impala,1120,description,"We should update our code that fetches and updates column stats to use the new bulk APIs introduced in Hive .13. Instead of fetching a single column at a time, we can now fetch stats for a list of column names.",architecture_debt,using_obsolete_technology,7542d719c941576d967876a89421dfe0d1e51524
impala,1147,summary,View compatibility tests need to be updated to run against Hive .13,test_debt,lack_of_tests,2a59029c2cbac2176df424dfcd01f6653b65b955
impala,1147,description,The view compatibility tests are disabled and need to be updated to run against Hive .13.,test_debt,lack_of_tests,2a59029c2cbac2176df424dfcd01f6653b65b955
impala,1290,description,If an AnalyticEvalNode is the root of a plan tree it should have results ready when Open() returns so that clients do not time out once the query is in a ready state and expect to be able to fetch results. This only happens for unpartitioned analytic evaluation. Should test this in,test_debt,lack_of_tests,f9b60bce43164d3a3598127ae078bc4f33640720
impala,1414,summary,Slow query without codegen,code_debt,slow_algorithm,aedf8e5fb881f05a7be821f9f119280f9f6f5908
impala,1414,description,"The following query on a relatively small input takes ~70s with codegen and ~35 minutes without codegen, which seems unreasonably long. We should make sure we're not doing anything crazy in the non-codegen eval path. Executing with codegen: The query profile when running without codegen is attached. I also used perf (linux profiling tool) to collect a profile (sampled stacks) and printed the aggregated callstacks to the attached file The summary without codegen. Most of the time (28mins) is spent in the hash joins:",code_debt,slow_algorithm,aedf8e5fb881f05a7be821f9f119280f9f6f5908
impala,1430,description,"Currently codegen is disabled for the entire aggregation operator if a single aggregate function can't be codegen'd. We should address this by making it so all aggregate functions can be codegen'd, including UDAs. For UDAs in .so's, the codegen'd function will call into the UDA library. This also affects aggregation operator on timestamp. This perf hit can be especially bad for COMPUTE STATS which is heavily CPU bound on the aggregation and because there is no easy way to exclude the TIMESTAMP columns when computing the column stats (i.e., there is no simple workaround). Even if the portions involving TIMESTAMP cannot be codegen'd it would still be worthwhile to come up with a workaround where codegen for the other types is still enabled. *Workaround* If you are experiencing very slow COMPUTE STATS due to this issue, then you may be able to temporarily ALTER the TIMESTAMP columns to STRING or INT type before running COMPUTE STATS. After the command completed, the columns can be altered back to TIMESTAMP. Note the workaround is only apply to text data, not parquet data. parquet require compatibles data type. TIMESTAMP is INT96, it's not compatible with STRING or BIGINT.",design_debt,non-optimal_design,"d7246d64c777384f29fe0f824ee0036b58e8aa2d,d2d3f4c1a6eefb3f1335da8bd6791fbebd63d98b"
impala,1493,comment_1,"It looks like almost all boost timestamp functions can throw. We can either try catch each of those or do it at a higher level up, e.g. scalar fn call. Putting it higher up is a bit tricky since we codegen scalar fn call.",design_debt,non-optimal_design,"19ba7c92c2587178089c0107458161199961e0f5,2bfb69523f76c589ea05f925b83ea056ed64b28e"
impala,1584,comment_0,"You don't have to do it this time, but next time (and in general) I think we should add the source code snippet of the failed DCHECK along with the relevant backtrace to bugs. That will make it easier to both review the fix (since it'll be easier to understand what went wrong), and also easier to identify duplicates (for all of dev, cce, and support).",code_debt,low_quality_code,e764b29819a54adb457595f08e5e82f73cd665a6
impala,1596,description,Some the builtin decimal functions assume they can always use the val16 union field of a DecimalType. This isn't actually a valid assumption though. We should only initialize and use the smaller fields (val4/val8) for performance if possible.,code_debt,low_quality_code,"31e95e6b440f0317acc8bf3d1ac4d56c8696c376,88add0d40bc902d0f28ecb5e7eab4f0b948986a3,8c6599330305d44c92042773a745e8395067744d"
impala,1598,description,"Some error conditions cause a high volume of output that gets sent to the users, particularly if each backend produces several messages that are aggregated by the coordinator. An example is the Parquet error message when a file spans several blocks: you get one warning per file, all of which are communicating roughly the same thing. Unfortunately we can't just do simple string-matching to de-duplicate the messages, because they're all slightly different (filename changes). We should add an error code to each user-facing message, and use that as the unique key with which to aggregate the messages when there are lots of them.",design_debt,non-optimal_design,b582cdc22b757e929016beed795b9a5d4eff6f59
impala,1651,description,"Currently, CREATE TABLE LIKE 'blindly' copies the source table, including the table's and partitions' parameter maps. The parameter maps may contain information about caching directives that do not apply to the new table. I can see two acceptable behaviors (choose one) if the source table is cached: 1. the new table is also cached; we need to issue new caching directives and store them in the params of the new table as appropriate 2. the new table is not cached and must be cached explicitly",design_debt,non-optimal_design,aed3505c8d17f759be4f73593f6aa290df384cfa
impala,1691,summary,Excessive Memory Usage in Catalogd (without stats),design_debt,non-optimal_design,8c30332237fb6d5a946b80e5fed923e80d064d15
impala,1691,description,"It used to be the case that the entire Catalogd's memory usage won't go over 1~2GB. However, a single table with 1000 columns and 8000 partitions used up 1.3GB of memory in the catalogd. The attached script created such a table. To see the memory usage, start catalogd without background loading. Observe the initial memory usage. Then run ""describe"" to load the table and observe the memory usage again.",design_debt,non-optimal_design,8c30332237fb6d5a946b80e5fed923e80d064d15
impala,1691,comment_1,We should get rid of the cached {{Partition}} object inside {{HdfsPartition}}. I'm pretty sure we can reconstruct it from the partition itself when needed (i.e. when sending updates to the HMS).,design_debt,non-optimal_design,8c30332237fb6d5a946b80e5fed923e80d064d15
impala,1691,comment_2,I just looked at a catalogd instance with a large table loaded in a profiler. This seems to bear out the idea that the arrays of {{FieldSchema}} in the take a lot of memory. take up about 1.1GB of heap even with a relatively small catalog.,design_debt,non-optimal_design,8c30332237fb6d5a946b80e5fed923e80d064d15
impala,1774,comment_2,"This is fixed in however, I'm waiting for us to rebase on the latest Hive bits in order to commit a test for it",test_debt,lack_of_tests,16200b5fe9f5cc4e46e264bd3b4efae7c0d0051d
impala,1929,description,"While running the mem leak test with 4 concurrent clients using the release bits an impalad crashed at with the following stack trace. This can happen in case of RIGHT_OUTER, RIGHT_ANTI and FULL_OUTER joins when the hash table of the partition is NULL. The code has a dcheck that the hash_tbl is not null, which we didn't hit because we were using the release bits.",code_debt,low_quality_code,"05b800e56703f7c4e94914dcf4bee7f955e8a647,05b800e56703f7c4e94914dcf4bee7f955e8a647"
impala,1934,description,"When LDAP authentication is enabled in Impala, impala-shell always prompts for a password when trying to connect to an Impala daemon, which makes it very hard to use impala-shell in a shell script (where to store the password?) Please make impala-shell support reading password from a command line parameter or from a password file, just like what beeline does.",design_debt,non-optimal_design,cca964c3c6d6744b65741c8875ada94cad876042
impala,1963,description,"We have several data sources that return dates in ISO-8601 format. Following IMPALA-648, we are closer to having these convertable to timestamp without effort, but time zones are still not supported: Neither format with a time zone specifier will parse even though they are valid based on my understanding of ISO-8601 and For now we have worked around this by converting with substring to cut off the time zone identifier, but I'm not sure which time zone the dates will be interpreted in. If I cut off the Z, will my dates be parsed in server local time? Ideally, we should be able to cast dates with time zone specifiers to timestamps with cast easily.",code_debt,low_quality_code,"3a962e46c5b31dbbcfe4029fdd17c50e09a4d4df,b0f7dabab3d5ee059e39c63b1f479cb29929e747"
impala,1963,comment_2,"For my current project, I only really need it to handle the Z properly. Anything else (other timezones, full support) would be a bonus. In terms of Tableau/Impala usability - this problem will come up for anyone with a format that doesn't convert automatically, so the more coverage we get the better. However, in terms of release schedules, I would love to have time zones or at least Zulu time working sooner.",design_debt,non-optimal_design,"3a962e46c5b31dbbcfe4029fdd17c50e09a4d4df,b0f7dabab3d5ee059e39c63b1f479cb29929e747"
impala,2068,description,"It is currently difficult for clients connected via load balancers to know which coordinator they're connected to. The TOpenSessionResp has a map of configuration settings which are returned, and we can return the connected coordinator in that map. Clients such as Hue can use that information to find the coordinator and thus the debug web UI pages for submitted queries over that session.",design_debt,non-optimal_design,938158e1c3a5400dda8a65ee75c4969bdcdaf0b5
impala,2076,description,"*Problem Statement* The ExecSummary does not report the time spent on the network. The time reported on the EXCHANGE node is misleading and does not count network time. This makes it impossible to identify the performance bottleneck. *Cause* The ExecSummary reports the non-waiting time spent in the EXCHANGE as the network time. Obviously, the active time spent in the EXCHANGE is zero (or close to zero). It should be replaced by the corresponding Data Stream Sender total time instead. *Workaround* Examine the corresponding time in the Data Stream Sender instead.",design_debt,non-optimal_design,4f18c701ba146130d82cae36455977d7ea4c57d0
impala,2178,comment_1,- that patch doesn't have query tests. Is there a simple repro query that we can add?,test_debt,lack_of_tests,672f040d0334f7bfd7ce6e66299908096a2d395c
impala,2208,comment_1,"Up until now Impala does not use the min/max metadata of Parquet so we don't need to backport it. But it is in our roadmap to exploit them, should that happen we need to be very careful and consider PARQUET-251.",design_debt,non-optimal_design,a0581e1e2d025f81b02b8e0bfe730b0daabcaf4b
impala,2244,summary,HdfsScanNode.java computeNumNodes() can be slow,code_debt,slow_algorithm,750786fee262e5c013026243e07bf4dbab17671c
impala,2244,description,"This function now loops through each scan range and when there are many, is noticeably slow. For example, an EXPLAIN on a query against 1M blocks and 1000K partitions went from around 450 ms to 700 ms with this change.",code_debt,slow_algorithm,750786fee262e5c013026243e07bf4dbab17671c
impala,2295,description,doesn't deep copy collections. This leaves the destination Tuple with pointers into potentially invalid memory.,design_debt,non-optimal_design,"4ac7e5d15d3fba11ae37e5826ca6c7181539804b,4ac7e5d15d3fba11ae37e5826ca6c7181539804b,235a8d08da9fbc1aaa170c952c87f078623cc506"
impala,2341,description,"Ideally, we should make this query work. Alternatively, the error message should be improved. Stack:",code_debt,low_quality_code,84e2c043a4a7c0bbf52bd0786924e2dd86d7c17e
impala,2435,description,"While I was reviewing Dimitris's IMPALA-2369 patch, I realized that in AddBatch() we call Run::Init() without checking the return status of it. That means that Dimitris's patch probably won't do much good :) At Instead of creating a different patch, I will let Dimitris add it to his IMPALA-2369 patch.",design_debt,non-optimal_design,"7d338af63806fc59b7c3c8edf14a1e60940c7c0c,7d338af63806fc59b7c3c8edf14a1e60940c7c0c"
impala,2457,description,"According to the math behind the PERCENT_RANK() value, a row group containing a single row yields a division by zero error and therefore Impala returns NaN in this case. In this example, there is only 1 'Reptile' row and that line in the result set has a NaN: Most RDBMS examples on the web use data with multiple rows per group, so they don't illustrate what's supposed to happen in this case. But I suspect the right return value in this case might be zero. Cf. ""The first row in any set has a PERCENT_RANK of 0."" That statement suggests the first row (and any tied rows I presume) could be special-cased and no calculation performed. Cf. The analytic example doesn't show all of the relevant output, but there are two DEPARTMENT_IDs in the output that have only one relevant row, and in both those cases the PERCENT_RANK result is shown as 0. I didn't find any PERCENT_RANK() in MySQL to try. I don't have a PostgreSQL instance handy.",code_debt,low_quality_code,"f06497e1d61d32b9fffd99a580f56e9a14be3f40,9f5fbdfb9a070455f25443bbd152d0b692f5026e"
impala,2632,description,"Currently impalad needs external LLVM IR files to be compiled separately from the binary and distributed with the binary. This slows down builds, make it easy for the code to get out of sync in dev environments, and complicates distribution. Instead, we could simply link the IR data into the binary file, so impalad is more self-contained. These IR files are also loaded from disk by codegen for each fragment. Linking the IR into the binary would avoid this step, saving some overhead. Could also address IMPALA-799 (.bc file extension instead of .ll extension) at same time.",design_debt,non-optimal_design,47f7f687b9d7090e2222af4b67d9875c9e26ddde
impala,2642,description,"I just noticed this while reading the statestore code: {{OfferUpdate()}} takes if the update queue is full, but in one place that lock has already been taken by the caller: It's not as scary as it sounds, because if the update queue has > 10000 entries there's something wrong anyway, but we should fix this.",code_debt,multi-thread_correctness,f97634fcf9a2e48d307f19595e70bdcee9c1980e
impala,2707,summary,Add FindOrInsert method to hash table to avoid unnecessary probe in aggregation,design_debt,non-optimal_design,42e92abfe017bb074e4bf24b630d646b49e5369e
impala,2707,description,"For each input row the aggregation node uses HashTable::Find() followed by HashTable::Insert() if the grouping key isn't already present in the table. Both of these methods probe the hash table to find the same bucket. If we added a FindOrInsert() method to the hash table that returned a modifiable iterator pointing to the bucket, we could save a significant number of hash table probes. There is already a TODO in the code for this, so I'm creating a JIRA to track the issue. This could speed up aggregations with large output size significantly, e.g. TPC-H query 13 (see IMPALA-2470).",code_debt,slow_algorithm,42e92abfe017bb074e4bf24b630d646b49e5369e
impala,2724,comment_0,"Dan, you seem to be the last person who touched this test. It appears the flakiness of the test had to do with the general non-determinism of impala memory usage. I believe that's the same case for IMPALA-2728.",test_debt,flaky_test,968c61c940fe78faa08e8cc4bd257dd2ec19dc11
impala,3017,description,I saw a private build where an impalad appears to get OOM-killed while executing the below tests. I believe what is probably happening is that a bunch of large allocations happen at the same time.,test_debt,expensive_tests,d2e8881065986ae2a74da9d2dd246e7f92393922
impala,3077,description,"Because of a quirk in the runtime filters implementation, we currently have to disable them when a join spills. The only reason this is necessary is that the filters are constructed as part of the hash table build. But there is no reason that we need to construct the filters at that point: we could instead construct the filters when doing initial processing of the build input, at which point we see all build-side input rows regardless of whether they are spilled or not. This might actually perform better since it would move some of the CPU work from the CPU-intensive hash table build to the less CPU-intensive construction of the input stream.",design_debt,non-optimal_design,"c14a6f11dfa3a3bd66d4868bd6e20f81ba822420,6629e79f328cc7ec6e9ad08d471958708d5fe10e,56654a8364d0da83e9d7d59b072bc9e24096aa94"
impala,3103,summary,Improve efficiency of BloomFilter Thrift serialisation,code_debt,slow_algorithm,0a6954de74eec4cca77d1172cff7f206fbfb2c7e
impala,3103,description,"{{TBloomFilters}} have a 'directory' structure that is a list of individual buckets (buckets are about 64k wide). The total size of the directory can be 1MB or even much more. That leads to a lot of buckets, and very inefficient deserialisation as each bucket has to be allocated on the heap. Instead, the {{TBloomFilter}} representation should use one contiguous string (like the real {{BloomFilter}} does, so that it can be allocated with a single operation (and deserialized with a single copy).",code_debt,slow_algorithm,0a6954de74eec4cca77d1172cff7f206fbfb2c7e
impala,3103,comment_1,The following fix improves serialisation efficiency by 20x (!),code_debt,slow_algorithm,0a6954de74eec4cca77d1172cff7f206fbfb2c7e
impala,3276,description,PrepareForRead() allows callers to use it in two different modes. In one of the modes it does not report pin failures. The code was written assuming that it will have reservations. This can result it hitting a DCHECK or going down some strange code paths:,code_debt,low_quality_code,36c294b55e64b6b9dd1c0fca30205a05db24b120
impala,3329,description,"I believe the {{impalad}} log rotation policy is causing logs to rotate out that we don't have access to. {{impalad}} defaults to keeping 10 log files in the log directory. This means if {{impalad}} restarts 11 times, the first logs will be rotated out. Many of the custom cluster tests restart {{impalad}}, though. This means {{impalad}} logs in the custom_cluster logs subdirectory are missing. To prove this is happening as a result of the log rotation policy, and it's not our inability to properly collect logs, I ran locally, and to get timestamps to show more about when tests were being run, I used this: (0) This gets us timestamps when there otherwise wouldn't be any. Here's the time range for our custom cluster tests: The earliest timetstamp log files are for 20:58. Contrast this to the end-to-end tests' log directory, which only has a handful of logs and span the 2 hours it took to run the test. The quick fix is to set the custom cluster tests to run with a limitless log rotation policy. The more detailed fix is to probably plumb things through such that we don't ever rotate logs. This is safe in Jenkins since we deleted the logs directory before every run (in code): we won't have a workspace accumulating logs. (0) Yup, you need {{gawk}} for {{strftime}} which {{mawk}}, the default {{awk}} on at least Ubuntu, doesn't have.",design_debt,non-optimal_design,50851f8775070fb0c14a0415a1e41ea71ac1f5a1
impala,3344,summary,Simplify and document invariants in Sorter,code_debt,low_quality_code,37ec25396f2964c386655ce3408b32a73f71edf6
impala,3344,description,"The sorter code does not have its internal invariants well-documented and can be tricky to understand when working on it. E.g. what are the valid states for merged versus initial runs, how are end-of-block cases handled. This issue is to clean up the sorter code while documenting important invariants as preparatory work for porting the sorter to the new buffer pool.",code_debt,low_quality_code,37ec25396f2964c386655ce3408b32a73f71edf6
impala,3548,description,"Currently, the FE generates a number of runtime filters and assigns them to plan nodes without taking the value of RUNTIME_FILTER_MODE into account. In the backend, the filters are removed from the exec nodes based on the target node types (local or remote) and the value of the RUNTIME_FILTER_MODE option. This may cause some confusion to users because they may see runtime filters in the output of explain that aren't applied when the query is executed. This operation should be performed in the FE.",design_debt,non-optimal_design,00fd8388c37e1b3207dddf564463b7eeafe3f887
impala,3652,description,"There is a tricky corner case in our resource transfer model with subplans and limits. The problem is that the limit in the subplan may mean that the exec node is reset before it has returned its full output. The resource transfer logic generally attaches resources to batches at specific points in the output, e.g. end of partition, end of block, so it's possible that batches returned before the Reset() may reference resources that have not yet been transferred. It's unclear if we test this scenario consistently or if it's always handled correctly. One example is this query, reported in IMPALA-5456:",design_debt,non-optimal_design,c28bc3e4f3900bfd0b14084ca19e51b36ea4dca7
impala,3652,comment_1,"The required fix is for Reset() to take a RowBatch as an argument, to which memory referenced by previously returned rows can be returned. The logic for Reset() attaching memory needs to be similar to the handling of ReachedLimit(), e.g. this example in NLJNode::GetNext():",design_debt,non-optimal_design,c28bc3e4f3900bfd0b14084ca19e51b36ea4dca7
impala,3671,description,"The immediate motivation for this is to enable better testing of graceful failures when spilling is disabled (e.g. IMPALA-3670). Currently we only have control over this via startup options, so we have to implement these as custom cluster tests, but there's no reason in principle we need to start a fresh cluster. The idea would be to add a query option 'scratch_limit' or similar, that limits the amount of scratch directory space that can be used. This would be useful to prevent runaway queries or to prevent queries from spilling when that is not desired.",design_debt,non-optimal_design,"9313dcdb830b0cd24479ca892988d17defc9ca19,6a2c9049ffe1fc914508c880ab66eaae8bd5f07e"
impala,3718,description,The functional test coverage for Impala on Kudu can be significantly improved by doing the following: * Run TPC-H and TPC-DS against Kudu tables * Add an alltypes test table that covers all the supported Kudu data types and expand existing functional tests to use this table * Add more complex queries in the planner tests that query both Kudu and Hdfs tables * Add more complex expressions (e.g. conditional functions) in the SET portion of UPDATE statements in functional query tests * Improve the test coverage for using and computing stats in Kudu tables,test_debt,low_coverage,"c7fa03286b473a34cdb170f8c89c261fb02d17a6,485022a89921592fdaa12deffe27923832103d84,3e23e40504000dd896fc1862809b659e41d468c1,8d7b01faea6362af675a2a335b462fad3e0caa03"
impala,3742,description,"Inserts into Kudu tables should be partitioned (i.e. rows hashed using the same hash partitioning as the Kudu table) and, at the table sink, sorted on the primary key. This would significantly improve performance. This will require a local sort (IMPALA-2521), and support from Kudu to provide the partitioning.",code_debt,slow_algorithm,"a2a0825b2fb54b1b23e0ed43512230f447fd9038,801c95f39f9de6c29380910274f97748ea8e47a9,014c5603f867907963f3821948f90d526e9a4789,4e17839033f931f98e0c3ec46d99b250b0bb4660,64e28021957f6993aea5ceb3ad626fb577597107"
impala,3780,summary,Uncompressed text scanner is slow when reading strings that significantly exceed the HDFS block size,code_debt,slow_algorithm,3810b7c413f575ad5970d3fb5c53509d81cb0032
impala,3780,description,"I observed after adding logging that the scanner was issuing many small 1024-byte scan ranges: This appears to be based on the constant in the text scanner, which is used to try and find the end of a field that extends into the next HDFS block. We could avoid this in a couple of ways: * Increase the constant. It's unclear why it's so low: I think the cost of reading additional data is probably negligible, at least up to 10's or 100's of KB or so. * Ramp up the read size, e.g. recursive doubling up to 8MB.",code_debt,slow_algorithm,3810b7c413f575ad5970d3fb5c53509d81cb0032
impala,3828,description,"Today the planner creates left deep trees where joins are ordered based on selectivity. The proposal is to add a rule in the planner that traverses the plan after and flip the build and probe sides if the cardinality suggests so, which produces a bushy plan. This optimization should improve queries against normalized schemas with selective joins and multiple fact tables, more ""efficient"" runtime filters will be created as a result of the plan change The query below generates a left deep plan, where are a bush plan should be created query Plan",design_debt,non-optimal_design,1bbd667fd3bb647eed93ff74a9206ad403c1578b
impala,3859,description,"is designed to log the data around a particular scanner parsing error. However, that log output may include sensitive table data, and should be avoided, particularly for uncompressed text formats.",design_debt,non-optimal_design,b00310ca897de325c8c6bd67f430885cddd322da
impala,4027,summary,Memory leak with ExprCtxs not free,code_debt,low_quality_code,e453086171c8069222fb45d5b8372968590834af
impala,4027,comment_2,"Despite this is pretty rare, I mean that variable ""probe_expr_ctxs_"" is added twice while variable ""filter_expr_ctxs_"" is not called by function AddExprCtxsToFree",code_debt,low_quality_code,e453086171c8069222fb45d5b8372968590834af
impala,4231,summary,Performance regression in TPC-H Q2 due to delay in filter arrival,code_debt,slow_algorithm,c7fe4385d927509443a1c4e2c6e9a802d2dcf63b
impala,4231,comment_2,"There are a few things that changed with the codegen'd code in the patch that may be relevant. * A couple of hash functions that were previously shared between the build and probe codegen are generated separately. The final IR should be the same since they're copied during inlining, but there may be some redundant optimisation done before inlining happens. * The function signature for AppendRow() changed so that it returned a status",design_debt,non-optimal_design,c7fe4385d927509443a1c4e2c6e9a802d2dcf63b
impala,4291,description,"For simple queries with one or two functions to codegen, we spent about 23ms or more out of 40ms of codegen time in preparation. We can save some time by not eagerly populating the map of to {{llvm::Function*}} map. This should help with the fixes for IMPALA-3638 when we create the LLVM module unconditionally.",code_debt,slow_algorithm,47b8aa3a9e7682ebb182696901916900d3323039
impala,4291,comment_0,"IMPALA-4291: Reduce LLVM module's preparation time Previously, when creating a LlvmCodeGen object, we run an O(mn) algorithm to map the IRFunction::Type to the actual LLVM::Function object in the module. m is the size of IRFunction::Type enum and n is the total number of functions in the module. This is a waste of time if we only use few functions from the module. This change reduces the preparation time of a simple query from 23ms to 10ms. select count(*) from where l_orderkey > 20;",code_debt,slow_algorithm,47b8aa3a9e7682ebb182696901916900d3323039
impala,4387,description,Haven't seen this before; may be a flaky test.,test_debt,flaky_test,6587c08f701ea4a72d03a36e159f9074a84d5f8f
impala,4548,description,"As shown in IMPALA-4532, the build async thread can still be running after the query has completed. This may lead use-after-free issue in IMPALA-4532. A more appropriate design is for to wait for the completion of the build thread.",code_debt,multi-thread_correctness,692c6a555811a09827dd963d4bf4f1ffd0a3aad4
impala,4617,summary,Remove duplication of isConstant() and IsConstant() in frontend and backend,code_debt,duplicated_code,"88448d1d4ab31eaaf82f764b36dc7d11d4c63c32,a4eb4705c3796b1129b33cc99dd96ee8d8bddafb"
impala,4617,description,"Currently Expr.isConstant() and Expr::IsConstant() in the frontend and backend have duplicate logic to do exactly the same analysis. They need to be kept exactly in sync to avoid problems. We should plumb through the value of isConstant() from the frontend to avoid this duplication. We need to be a little careful about how we do this in the frontend: Alex Behn mentioned that storing state in Exprs was risky, and also that naively calling isConstant() for each expr node was problematic since it could result in traversing the Expr tree many times.",code_debt,duplicated_code,"88448d1d4ab31eaaf82f764b36dc7d11d4c63c32,a4eb4705c3796b1129b33cc99dd96ee8d8bddafb"
impala,4639,description,"Some tests are unable to run on remote clusters because of test setup or infrastructure reasons, and not because of product failure. We should be able to selectively skip tests that can't be set up properly to run against a remote cluster.",design_debt,non-optimal_design,6c5f8e3f5e3bdcb87f65d96b7af35aee8874cfb4
impala,4652,description,Kudu's utility library depends on We need to add the most recent version to the toolchain.,architecture_debt,using_obsolete_technology,60c41c4f0fcfbd241cbf4163b09fbe7b28a98e81
impala,4671,description,"Kudu's {{ServicePool}} uses Kudu's {{Thread}} class to service RPC requests. While this works well, the threads it creates aren't monitored by Impala's {{ThreadMgr}} subsystem. Eventually we'd like to standardise on one thread class between projects, but for now it would be useful to reimplement {{ServicePool}} in terms of our thread class. The reactor threads and acceptor threads will still be Kudu threads, but those are less likely to do substantial work, so having them missing from monitoring isn't a big problem.",design_debt,non-optimal_design,"a94d6068c757912eaa741db82331ef72e5696004,82267a2779da2165802195ba202223d2c205b73d"
impala,4728,description,"Currently Impala uses lazy evaluation for expressions. This can result in a performance overhead when using or reusing expressions in things like a window function order by vs having the expression materialized as a projection from the underlying relation, especially if the expression is used in multiple places.",design_debt,non-optimal_design,6cddb952cefedd373b2a1ce71a1b3cff2e774d70
impala,4801,comment_1,"Yeah I figured out what this is. RuntimeProfile does some cleanup in it's destructor, which includes unregistering some callbacks that periodically touch MemTrackers. So if the MemTracker gets destroyed before the profile, we have a problem. Ideally we should move that out of the RuntimeProfile destructor, but we can fix the immediate problem by fixing the lifetime of the MemTracker.",design_debt,non-optimal_design,"406632640d44c9eb45286d4ed73192b6c16daf2f,406632640d44c9eb45286d4ed73192b6c16daf2f"
impala,4831,description,"If a client unpins some pages, then calls it can leave too many dirty unpinned pages in memory. Similarly, a client can cause mischief by calling AllocateFrom() and ReleaseTo() or by creating child",design_debt,non-optimal_design,62894e323a87f8f48ece7235f2ffb0eac922fbf8
impala,4833,description,"Following on from IMPALA-3748, we should consider combining the resource estimates with the schedule to accurate compute the minimum buffer requirement per daemon. Different daemons may have different resource requirements because not every fragment runs on every daemon (e.g. unpartitioned fragments, fragments with fewer scan ranges than daemons).",design_debt,non-optimal_design,"6c1254656186b62e90674b4fe093a6864ccbbde5,985698bb6f69a61bc2bd88b8fb7e89d476d970a5"
impala,4833,comment_0,There's some relevant code in the Scheduler and Coordinator. The Scheduler does a pass over all fragment instances to determine the set of hosts here: The coordinator also does something similar: It would maybe be helpful if the scheduler produced backend_params_map_ itself to make it easier to compute the per-host aggregates.,design_debt,non-optimal_design,"6c1254656186b62e90674b4fe093a6864ccbbde5,985698bb6f69a61bc2bd88b8fb7e89d476d970a5"
impala,4862,summary,Planner's peak resource estimates do not accurately reflect the behaviour of joins and unions in the backend,design_debt,non-optimal_design,"9a29dfc91b1ff8bbae3c94b53bf2b6ac81a271e0,64fd0115e5691cfaebb730651df003ffee38fd8e"
impala,4862,description,"In the following example the way the peak resource estimate is computed from per-node estimates is wrong. It should be 476.41MB, because the scan node is Open()ed in the backend *while* the concurrent join builds are executing. Another example is this one, where in the backend the aggregations can execute concurrently with the join builds The behaviour for unions also is not accurate - branches of unions within the same fragment are execute serially, but anything below an exchanges is executed concurrently.",design_debt,non-optimal_design,"9a29dfc91b1ff8bbae3c94b53bf2b6ac81a271e0,64fd0115e5691cfaebb730651df003ffee38fd8e"
impala,4933,description,"Thrift currently initializes OpenSSL lazily, when the first connection is created. However, the OpenSSL initialization must happen before the Kudu library is used (because Kudu will not be doing the initialization itself, and it will check that the OpenSSL initialization already occurred), so thrift needs to do the initialization on startup.",design_debt,non-optimal_design,b7a76361bbab7ea4e3c4df7574755cf3f6bc8cae
impala,4970,description,"Although we retain the histogram of fragment instance startup latencies, we don't record the identity of the most expensive instance, or the host it runs on. This would be helpful in diagnosing slow query start-up times.",design_debt,non-optimal_design,"ff26663f90b58153e8086c1a779f8604b2cfe685,b265cb68a229124fdd21427941c3d75ed86e3e2e"
impala,4996,description,"Since we will multi-thread query execution at the fragment level, we should rework KuduScanNode to only use a single thread (the one that's executing the fragment).",code_debt,multi-thread_correctness,5bb988b1c58a2377777312c8e4dd56cbd0dee8a2
impala,5042,summary,"Loading metadata for partitioned tables is slow due to usage of an ArrayList, potential 4x speedup",code_debt,slow_algorithm,6dff90661c07241794de5c24f4f27e7712dca82c
impala,5042,description,"Loading metadata for partitions with custom paths is 4x slower compared to partitions without custom paths, the slow down is due to an N2 lookups to check if a partition already exists. The List should ideally be replaced with a Set. From From Java mission control",code_debt,slow_algorithm,6dff90661c07241794de5c24f4f27e7712dca82c
impala,5130,description,"can run concurrently with There is no synchronisation between the two. I saw a crash with this stack on a development branch, which I believe is caused by this: on this line: This method is not currently used in query execution, but we need to fix this before switching on the buffer pool for query execution.",code_debt,multi-thread_correctness,d0152d424ad1aa21a91122ca874a81793d497720
impala,5273,summary,StringCompare is very slow,code_debt,slow_algorithm,5cab97fd7faced41bbcc2370a400cbe92c3e0128
impala,5273,description,"Replacing StringCompare (which uses SSE4.2 instructions) with a call to glibc's memcmp results in a memcmp on my machine mainly uses sse4.1's ptest, after detecting at run-time that I have sse4.1 instructions available. The StringCompare benchmark is 5 years old and likely out-of-date by now. To replicate:",code_debt,slow_algorithm,5cab97fd7faced41bbcc2370a400cbe92c3e0128
impala,5341,description,"In our planner tests we want to ignore minor differences in reported file sizes to avoid flaky tests, see IMPALA-2565. However, the introduced filter also matches ""row-size="" which we do not want to filter. Relevant code is in TestUtils.java:",test_debt,flaky_test,f9155f0d8185b3344fe16368806b463fe27bc5c7
impala,5481,description,"One of the {{RowBatch}} c'tors copies the row descriptor into the row batch. This leads to a lot of allocation churn since {{RowDescriptor}} contains some vector members, and since the descriptor is usually the same the copies are unnecessary. Instead, we should consider allocating the {{RowDescriptor}} once from an object pool, and sharing it amongst all row batches that need that descriptor. In some tests, {{RowDescriptor()}} shows up as 20% of the tcmalloc allocation time.",code_debt,slow_algorithm,317c413a00bd9b3b29eeaf2efe556c2e924e2d74
impala,5499,description,I suspect this is a rare flaky test failure. Filing a JIRA so that we can see if it reoccurs. This test failed and everything else passed. It looked like for some reason the port was occupied and the process then crashed on teardown. Info log: Error log:,test_debt,flaky_test,df2b5a938c64799a1264540c349e14593aec1544
impala,5525,description,"TestScannersFuzzing currently tests compressed parquet but does not test uncompressed parquet. Uncompressed parquet would help with test coverage because there's more potential to corrupt the actual data in interesting ways, not just the headers. To do this we'd probably need to set do a create table as select to write out some parquet data, then do the fuzz testing on that.",test_debt,low_coverage,d40047aa9b6aff6ef33c10ce5f05ebdac9b37e63
impala,5600,summary,Small cleanups left over from IMPALA-5344,code_debt,low_quality_code,ee640d902ecc96dff3d1f1117c52bd940e5bf990
impala,5612,summary,Join inversion should avoid reducing the degree of parallelism,design_debt,non-optimal_design,e3075c39ac488ebbb59a2483c78ebb106d13d5b1
impala,5612,description,"The degree of inter-node parallelism for a join is determined by its left input, so when inverting a join the planner should be mindful of how the inversion affects parallelism. For example, the left join input may have been reduced by joining with several dimension tables so much that it becomes smaller than the right hand-side (another small dimension table). By inverting such a join the degree of parallelism may be reduced to one or very few nodes, based on how many nodes the right-hand size is executed on.",design_debt,non-optimal_design,e3075c39ac488ebbb59a2483c78ebb106d13d5b1
impala,5618,description,The extra time appears to be in AddRowCustom() via I think allocating the boost::function object is causing the slowdown.,code_debt,slow_algorithm,081ecf01526449c2360d2d702afc1488b57e07fb
impala,5618,comment_1,yeah I *think* that could defer construction of the boost::function until the non-templated slow path function is called. I'm thinking though that the performance is very hard to reason about and it may be better as a matter of policy to avoid lambdas with captured variables in perf-critical code.,design_debt,non-optimal_design,081ecf01526449c2360d2d702afc1488b57e07fb
impala,5636,comment_0,"I think just replacing 2 occurrences of with Encoding::RLE makes sense. I've manually tested with parquet-tools and it seems to work. Since parquet-tools is not in our distribution, I'm not sure how to write a test. Any suggestion?",test_debt,lack_of_tests,"b2dbcbc2d1bb7d57c5f50989ad25eec1783e52b2,c87ad3631a4f3f1854759937ae0f8de63cb6e5dc"
impala,5640,summary,Enable test coverage for Parquet gzip inserts was disabled,test_debt,low_coverage,5b670f49b62db90c0eca28dfc8c5c9dc9c933cd6
impala,5640,description,There's a comment in the code saying that it was disabled because of IMPALA-424. This test coverage seems useful - we should add it back,test_debt,low_coverage,5b670f49b62db90c0eca28dfc8c5c9dc9c933cd6
impala,5688,summary,Speed up a couple of heavy-hitting expr-tests,test_debt,expensive_tests,1653419bd8b3748bbc0e3d5e7ffa1d412bc4b50f
impala,5688,description,"Two tests ({{LongReverse}} and the base64 tests in run their tests over all lengths from 0..{{some length}}. Both take several minutes to complete. This adds a lot of runtime for not much more confidence. If instead we pick a set of 'interesting' (including powers-of-two, prime numbers, edge-cases) lengths, we can get a similar amount of confidence while significantly reducing the runtime of expr-test.",test_debt,expensive_tests,1653419bd8b3748bbc0e3d5e7ffa1d412bc4b50f
impala,5779,description,We do not have any end-to-end tests where we attempt to spill buffers larger than the I/O size. I have tested manually but we need some automated testing.,test_debt,lack_of_tests,bb6b0ce249f561af04ea42752ca874cd93dd63c9
impala,5780,summary,Add missing test coverage for,test_debt,low_coverage,bb6b0ce249f561af04ea42752ca874cd93dd63c9
impala,5780,description,We do not have any end-to-end test coverage with = true. We should add basic tests for the success and OOM cases where querying a table with no stats with = true.,test_debt,low_coverage,bb6b0ce249f561af04ea42752ca874cd93dd63c9
impala,5849,description,"IMPALA-5800, IMPALA-5775 and IMPALA-5743 added TLS configuration to Impala and Squeasel. Since Impala is often built against different versions of OpenSSL (with different TLS capabilities), we used compile-time definitions to avoid using symbols from OpenSSL 1.0.1 that weren't available. This works great if we can ensure that the machine on which Impala is built is the same environment as the one on which it executes, but we have discovered that the installed version of OpenSSL can vary between minor releases of Linux distributions. It appears possible to write the support for TLS1.1+ in terms of symbols that are available in OpenSSL 1.0.0 only. The only downside is that Impala can't then tell whether or not the runtime supports TLS 1.2, and so the error messages won't be quite as clear. However, the benefit of a single binary and Thrift toolchain dependency for all supported versions of OpenSSL is well worth it.",code_debt,low_quality_code,ac8a72f3041688316a20c9a85eda5a5cb9af1578
impala,5923,comment_0,"As far as I understand, you want the unexpected information(i.e. opid) will not be printed. If so, the line 132 is removed and then refine the logging message. I think it's too simple. Please leave more detailed description if there is any missing.",design_debt,non-optimal_design,cbf38dcd03ea5abb92d11a9d35e032e2428d299f
impala,6030,description,Since we introduced the we've forgotten to disable the coordinator specific thread pools on nodes that have only the executor role.,design_debt,non-optimal_design,0290e92da866ac92b4b2fbfa6054f1fa9ad13995
impala,6077,description,"As a follow-on to IMPALA-6076, we should remove support for the encoding in a compat-breaking release so that we no longer have to maintain this code.",code_debt,dead_code,95f16663092af3ffa6dddf745973c7065c164436
impala,6080,summary,Clean up descriptor table handling in coordinator,code_debt,low_quality_code,70919187c64cced4f805cc995a3bcf5e1fc9f02f
impala,6080,description,One of the parts of this patch was cleanup of how descriptor tables were managed. As described in the commit message: That cleanup should be independent of the larger patch so we could get it in separately.,architecture_debt,violation_of_modularity,70919187c64cced4f805cc995a3bcf5e1fc9f02f
impala,6080,comment_1,"I don't think anyone has picked that up due to a lack of time - I agree it would be good to get in but I think parts of that patch require deeper understanding than this subset, which is fairly mechanical cleanup. Separating this out would shrink that patch and I think make it easier to focus on the meat of it.",code_debt,low_quality_code,70919187c64cced4f805cc995a3bcf5e1fc9f02f
impala,6106,summary,test_tpcds_q53 extremely flaky because of decimal_v2 not being reset,test_debt,flaky_test,839c45777b32b50b6d44e60593a39266338754d3
impala,6131,description,Currently we (ab-)use to track the last update time of statistics. Instead we should introduce a separate counter to track the last update. With that we should also remove all occurrences of from and fall back to Hive's default behavior.,code_debt,low_quality_code,5c7d3b12e3aa750e7ab88e3ef1092d5218e53cc2
impala,6223,summary,Gracefully handle malformed 'with' queries in impala-shell,design_debt,non-optimal_design,28162117ad462dfe5fd608d4c09ba02ad213285b
impala,6223,description,"Impala shell can throw a lexer error if it encounters a malformed ""with"" query. This happens because we use shlex to parse the input query to determine if its a DML and it can throw if the input doesn't have balanced quotes. A simple shlex repro of that is as follows, Fix: Either catch the exception and handle it gracefully or have a better way to figure out the query type, using a SQL parser (more involved). This query also repros it:",design_debt,non-optimal_design,28162117ad462dfe5fd608d4c09ba02ad213285b
impala,6285,summary,Avoid printing the stack as part of DoTransmitDataRpc as it leads to burning lots of kernel CPU,design_debt,non-optimal_design,d60eb192a959afd5e1a7062b360ade2ef8a8f4f4
impala,6442,summary,Misleading file offset reporting in error messages,design_debt,non-optimal_design,"dbe0f86e005b07133ef38972905ff1dd2ef35efc,dbe0f86e005b07133ef38972905ff1dd2ef35efc"
impala,6442,description,"has an error message ""File $0 has invalid file metadata at file offset $1."" However, the value reported as ""file offset"" is an offset from the _end_ of the file, not from its _beginning_. This is very misleading, since without explicitly stating that the offset is from the end, it is usually understood to be counted from the beginning. Additionally, although the function name is clearly about a ""footer"", two comments explicitly mention processing the ""header"". This falsely suggests that metadata is at the beginning of the file, when in reality it is at the end.",design_debt,non-optimal_design,"dbe0f86e005b07133ef38972905ff1dd2ef35efc,dbe0f86e005b07133ef38972905ff1dd2ef35efc"
impala,6601,description,"ASAN fails with in during  - Im assigning this to you thinking you might have an idea whats going on here; feel free to find another person or assign back to me if you're swamped. Ive seen this happen in a private Jenkins run. Please ping me if you would like access to the build artifacts. I saw this in builds that also had issues during the e2e tests, so I'm not sure whether this is flaky or reproducibly broken.",test_debt,flaky_test,3302abaa32cf3290cb038fbc1dbd0a821f5cbbc7
impala,6613,description,"Now that KRPC is on by default, we should rename the environment variable to reflect that.",code_debt,low_quality_code,994272b684f1f04a4f27844aa9066e423615a28f
impala,6694,description,"It appears that the buffer pool statistics of exec node is sometimes misaligned in the profile. For instance, the aggregation node's buffer pool appears after the exchange node below: cc'ing",code_debt,low_quality_code,84e30700f1a510e53bda852ec498fcc2690426d8
impala,6709,summary,Simplify tests that copy local files to tables,code_debt,complex_code,e27954a5aa585db23fe3c97726aa89305efa306d
impala,6709,description,"tests/query_test/ contains several tests that use ""hdfs dfs -copyFromLocal"" to fill tables with files that cannot be created with Impala. The current way of doing this looks like ""copy paste"" and adds unnecessary complexity to the test code. The common pattern of ""create a table, copy 1-2 files to it"" could be covered by a single function, or could be moved to the .test files, which support SHELL sections. for an example of SHELL section, see",design_debt,non-optimal_design,e27954a5aa585db23fe3c97726aa89305efa306d
impala,6817,summary,Clean up Impala privilege model,code_debt,low_quality_code,4c1538ab106ec2097927e7586e5d736010706d6a
impala,6847,description,"A scenario we've seen play out a couple of times is this. 1. An Impala admin sets up memory-based admission control but sets the default query mem_limit to 0. This means that admission control will use memory estimates instead of mem_limit. Typically admins want some protection from large queries consuming excessive memory but can't or don't want to set a single mem_limit because the workload is unknown or unpredictable. This configuration has caveats and we recommend against it, but this happens and often works well enough as long as the workload is comprised of relatively simple queries. The caveats include: * There is no enforcement that a query stays within the memory estimate. This means that a query can fail or force other queries to fail. * Memory estimates are often inaccurate (this is unavoidable since they depend on cardinality estimates, which are commonly off by 10x even with state-of-the-art query planners). This means that runnable queries may be impossible to admit without setting a mem_limit. 2. Something changes about the workload, e.g. a new query is added, stats are computed, data size changes, we make an otherwise-innocuous planner change. Then we run into the second problem above and one or more queries cannot be executed, e.g. ""Rejected query from pool root.foo: request memory needed 1234.56 GB is greater than pool max mem resources 1000.00 GB. Use the MEM_LIMIT query option to indicate how much memory is required per node. The total memory needed is the per-node MEM_LIMIT times the number of nodes executing the query. See the Admission Control documentation for more information. "" The last problem is the problem this JIRA is intending to work around. The preferred solutions, which work most of the time, are: # Configure a default query memory limit for the pool # Set a mem_limit for the query # Disable memory-based admission control and do admission control based on num_queries, which is simpler and easier to understand. It would however, be useful to have a third fallback in cases where the first two options are difficult or impossible to apply. The basic requirement is to allow an administrator to configure a resource pool such that queries with very high estimates can run. We probably need enough flexibility that they can run concurrently with other smaller queries (i.e. the big query shouldn't take over the whole pool) and ideally we would also have a mem_limit applied to the query so that we're still protected from runaway memory consumption. The long-term solution to this is IMPALA-6460. This is a short-term workaround that can tide users over until we have a comprehensive solution.",design_debt,non-optimal_design,3ebf30a2a4de675388e8f2237cc0ec0c99458cf5
impala,6850,summary,Print the actual error message to the console when Sentry fails,code_debt,low_quality_code,51cf5b27fc3a5f5c7965d1bf88aebe2a6132b538
impala,6993,description,"We should use Status::Expected() here, just like it's used above. The stack trace isn't interesting and the error is expected once we get down this path. (I'm not sure why we don't just use {{exec_status}} though, but presumably the prefix was added for a reason).",design_debt,non-optimal_design,df2798ea977e1cca87842c524b35ac46a51d5c28
impala,7161,summary,Bootstrap's handling of JAVA_HOME needs improvement,design_debt,non-optimal_design,28b4ad14f61bf6f0195b731195767d3a59e92ed8
impala,7161,description,"installs the Java SDK and sets JAVA_HOME in the current shell. It also adds a command to the to export JAVA_HOME there. This doesn't do the job. tests for JAVA_HOME at the very start of the script, before it has sourced So, the user doesn't have a way of developing over the long term without manually setting up JAVA_HOME. also doesn't detect the system JAVA_HOME. For Ubuntu 16.04, this is fairly simple and if a developer has their system JDK set up appropriately, it would make sense to use it. For example:",design_debt,non-optimal_design,28b4ad14f61bf6f0195b731195767d3a59e92ed8
impala,7161,comment_0,"Additional issue: Suppose a user sets JAVA_HOME in their environment like they are supposed to. Suppose they also have JAVA_HOME in like writes. The two can get out of sync. The JAVA would be from the environment JAVA_HOME, but after setting JAVA, value for JAVA_HOME would overwrite the environment variable and JAVA_HOME would be that value. These could point to two different places.",design_debt,non-optimal_design,28b4ad14f61bf6f0195b731195767d3a59e92ed8
impala,7205,description,"Currently we respond with CANCELLED only when hitting EOS. It seems a bit more robust to always respond with CANCELLED whenever query execution has terminated. That way, if the cancel RPC from the coordinator to a backend fails, the backend will still cancel if it can send status back to the coordinator later on. Without this fix, the query can hang and/or finstances can continue running (until the query is closed, at which point the response to this RPC will be an error).",design_debt,non-optimal_design,"8000c31d14a439a37a5f8dd626763a433f014504,a4ad8f35f7768ac608317e702fdbd3a042140186"
impala,7234,description,"The getMajorityFormat method of the FeCatalogUtils currently returns non-deterministic results when its argument is a list of partitions where there is no numerical majority in terms of the number of instances. The result is determined by the order in which the partitions are added to the HashMap. We need more deterministic results which also considers the memory requirement among different types of partitions. Ideally, this function should return the format with higher memory requirements in case of a tie.",design_debt,non-optimal_design,672a271fd0966bd77f38eda9b6f1e768415bac04
impala,7234,comment_0,"Should this function actually take into account the total byte sizes or counts of ranges or files? In recently looking at this code I couldn't quite make sense of the logic. For example, if we have 10 partitions that are text, each containing one file, and one partition which is Parquet, containing 100 files, maybe it makes more sense to estimate scan range memory usage based on Parquet instead of text?",design_debt,non-optimal_design,672a271fd0966bd77f38eda9b6f1e768415bac04
impala,7349,description,"We should add admission control support for intelligently choosing how much memory to give a query, based on the memory estimate. Currently we require that you set a single mem_limit per pool. Instead it would be better if we allowed configuring min/max guardrails within which admission control chose an amount of memory that will allow good performance. Initially, I think mem_limit will be the same for all backends. Eventually it could make sense to have different limits per backend depending on the fragments.",design_debt,non-optimal_design,fc91e706b4f3b45cdda28d977f652cee3f050e7b
impala,7388,summary,JNI THROW_IF_ERROR macros use local scope variables which likely conflict,design_debt,non-optimal_design,ea615d1d8026dbe1cda3f5c2b048d210b2df435b
impala,7388,description,"The THROW_IF_ERROR macros all use a locally scoped variable ""status"". If they're called with a pattern like: then the status variable inside the macro ends up being assigned to itself instead of the outer-scope variable. This makes it not throw or return, which is quite surprising.",design_debt,non-optimal_design,ea615d1d8026dbe1cda3f5c2b048d210b2df435b
impala,7400,summary,"""SQL Statements to Remove or Adapt"" is out of date",architecture_debt,using_obsolete_technology,dd8a25d4c2976bddbe40cf7b37a2b69447855f83
impala,7400,description,"""Impala has no DELETE statement."" and ""Impala has no UPDATE statement. "" are not totally true - Impala has those statements but only for Kudu tables. ""For example, Impala does not support natural joins or anti-joins,"" - Impala does support Anti-joins via NOT IN/NOT EXISTS or even explicitly like: ""Within queries, Impala requires query aliases for any subqueries:"" - this is only true for subqueries used as inline views in the FROM clause. E.g. the following works: "" Impala .. requires the CROSS JOIN operator for Cartesian products."" - untrue, this works: ""Have you run the COMPUTE STATS statement on each table involved in join queries"". This isn't specific to queries with joins, although may have more impact. We recommend that users run COMPUTE STATS on all tables. ""A CREATE TABLE statement with no PARTITIONED BY clause stores all the data files in the same physical location,"" - unpartitioned tables with multiple files can have files residing in different locations (and there are already 3 replicas per file by default, so the statement is a little misleading even if there's a single file). I think the latest statement about ""Have you partitioned at the right granularity so that there is enough data in each partition to parallelize the work for each query?"" is also misleading for the same reason. ""The INSERT ... VALUES syntax is suitable for setting up toy tables with a few rows for functional testing, but because each such statement creates a separate tiny file in HDFS"". This advice only applies to HDFS, this should work fine for Kudu tables although the INSERT statements are not particularly fast. ""The number of expressions allowed in an Impala query might be smaller than for some other database systems, causing failures for very complicated queries"" - this doesn't seem right - I don't know why the queries would fail. Also the codegen time isn't really specific to expressions or where clauses. There seems to be a point buried in there, but maybe it's just essentially that ""Complex queries may have high codegen time""",design_debt,non-optimal_design,dd8a25d4c2976bddbe40cf7b37a2b69447855f83
impala,7406,description,"Currently the file descriptors stored in the catalogd memory for each partition use a FlatBuffer to reduce the number of separate objects on the Java heap. However, the FlatBuffer objects internally each store a ByteBuffer and int position, so each object takes 32 bytes on its own. The ByteBuffer takes 56 bytes since it stores various references, endianness, limit, mark, position, etc. This amounts to about 88 bytes overhead on top of the actual underlying flatbuf byte array which is typically around 100 bytes for a single-block file. So, we're have about a 1:1 ratio of memory overhead and a 2:1 ratio of object count overhead for each partition. If we simply stored the byte[] array and constructed wrappers on demand, we'd save 88 bytes and 2 objects per partition. The downside is that we'd need to do short-lived ByteBuffer allocations at access time, and based on some benchmarking I did, they don't get escape-analyzed out. So, it's not a super clear win, but still worth considering.",design_debt,non-optimal_design,1dd4403fdcae665b0f343bf3fb7cb644c1875851
impala,7640,description,"Currently, when I execute ALTER TABLE RENAME on a managed Kudu table it will not rename the underlying Kudu table. Because of IMPALA-5654 it becomes nearly impossible to rename the underlying Kudu table, which is confusing and makes the Kudu tables harder to identify and manage.",design_debt,non-optimal_design,4057331fb8cb976ea11364c374183bab01d795da
impala,7682,description,"The method public Set<String> groups, Set<String> users, ActiveRoleSet roleSet) in AuthorizationPolicy needs to be synchronized.",code_debt,multi-thread_correctness,6099255fa69d41c5f3dd581a976fd5be3b027f0a
impala,7688,summary,Spurious error messages when updating owner privileges,design_debt,non-optimal_design,0cbe37afc8f77fd8e0aef7e710e2d1b214c1c5b1
impala,7808,summary,Refactor SelectStmt analyzer for easier debugging,code_debt,low_quality_code,"c7ef48e1ddd1489221a11fe472a72289a6ef3d47,fe47b2352ea9fc13f2d51da8fa9d0f7c70037779,fe47b2352ea9fc13f2d51da8fa9d0f7c70037779"
impala,7808,description,"The analysis steps in {{SelectStmt}} and {{AnalysisContext}} are large and cumbersome. There is ample evidence in the literature that simpler, smaller functions are easier to understand and debug than larger, more complex functions. This ticket requests breaking up the large functions in these two cases into smaller, easier-understood units in preparation for tracking down issues related to missing rewrites of the {{WHERE}} and {{GROUP BY}} clauses. One might argue that large functions perform better by eliminating unnecessary function calls. However, the planner is not performance sensitive, and the dozen extra calls that this change introduce will not change performance given the thousands of calls already made. Experience has shown that the JIT compiler in the JVM actually does a better job optimizing smaller functions, and gives up when functions get to large. So, by creating smaller functions, we may actually allow the JIT compiler to generate better code. And, this refactoring is in support of a possible outcome that the planner can handle rewrites without making multiple passes through the analyzer: that savings will far outweigh the few extra calls this change introduces.",code_debt,complex_code,"c7ef48e1ddd1489221a11fe472a72289a6ef3d47,fe47b2352ea9fc13f2d51da8fa9d0f7c70037779,fe47b2352ea9fc13f2d51da8fa9d0f7c70037779"
impala,7841,summary,"Refactor QueryStmt, other analysis code for easier debugging",code_debt,low_quality_code,fe47b2352ea9fc13f2d51da8fa9d0f7c70037779
impala,7841,description,"IMPALA-7808 started the process of refactoring the Analyzer code for easier debugging. It did so by grouping {{SelectStmt}} code into a nested class, which then allowed breaking up a large function into smaller chunks. This ticket continues that process with two changes: * Follow-on refactoring of {{SelectStmt}} to make some of the newly-created functions simpler. (The first change tried to keep code unchanged as much as possible.) * Apply the same technique to the {{QueryStmt}} base class and to its other subclasses.",code_debt,low_quality_code,fe47b2352ea9fc13f2d51da8fa9d0f7c70037779
impala,7869,summary,Split up for readability and compile time,architecture_debt,violation_of_modularity,07fd332089c262fa8813605f12b927c8602ac0d2
impala,7869,description,suggested reorganising the file to be easier to read on Compile times are also an issue - this file is the longest pole in the Impala compilation at the moment.,code_debt,low_quality_code,07fd332089c262fa8813605f12b927c8602ac0d2
impala,7902,description,"The {{NumericLiteral}} class is a leaf node in the Impala FE AST. In order to address a number of pending issues, we must adjust this class to improve its semantics and fix a number of bugs. See the linked JIRA tickets for the issues that this roll-up ticket addresses. The issues are combined because they are tightly related; makes more sense to offer them as a single patch than as a series of disjoint patches.",design_debt,non-optimal_design,27577dd652554dda5a03016e2d1e3ab66fe6b1f5
impala,7987,comment_0,"A major obstacle here that I'm running into is that ""localhost"" appears in many places in configuration files. Finding those and replacing them with the gateway IP of the docker network isn't sufficient to solve the problem without reloading data, because ""localhost"" makes its way into table definitions and other places (probably sentry?).",design_debt,non-optimal_design,ff628d2b136e9b5ca72a7179294dea06f4cdf0d8
impala,8005,description,"Currently, we use the same hash seed for partitioning exchanges at the sender. For a table with skew in distribution in the shuffling keys, multiple queries using the same shuffling keys for exchanges will end up hashing to the same destination fragments running on particular host and potentially overloading that host. We should consider using the query id or other query specific information to seed the hashing function to randomize the destinations for different queries. Thanks to  for pointing this problem out.",design_debt,non-optimal_design,df6196e064bc7453bee8c7e644bb591391ee3ce2
impala,8073,description,The error in the log (attached) appears to be a connection to the HMS error. Some initial googling suggested that it might be the server closing the connection because of hitting a connection limit.  could you take a look and see if you have any ideas. I wonder if we're leaking HMS connections in this test somehow?,code_debt,low_quality_code,9105c5c5427dd0479caf280b5754b31845df5fc2
impala,8146,comment_3,"I think we should stage this differently - first make it possible to do everything without make_impala.sh, have an interim period where the script is still present so any downstream users have a chance to migrate their tools, then remove the old scripts.",code_debt,low_quality_code,"e8ea4b0525fb3223da9291775eff4b7fbd15c547,11120a9599da986040964bfd672571d97d377c94"
impala,8168,description,"Currently we enable Sentry HDFS sync when running on S3 which sometimes can make the authorization tests on S3 flaky. Since HDFS sync is not supported on S3, we should update our build script to only enable Sentry HDFS sync on HDFS and not on S3.",test_debt,flaky_test,b7df73d3392e485fb7c2ed335a49f8c42a254563
impala,8176,summary,Convert tests with trivial main() functions to use a unified executable,design_debt,non-optimal_design,4cc3ff9c6763de10236df351a47324d662766de3
impala,8176,description,"With the initial commit for the unified backend test executable commited (IMPALA-8071), it should be straight-forward to convert another set of tests to use the unified executable. Any test with a simple main() function (such as IMPALA_TEST_MAIN()) should require only modest changes to convert. Command to find such tests: git grep IMPALA_TEST_MAIN It looks like there are dozens of such tests:",design_debt,non-optimal_design,4cc3ff9c6763de10236df351a47324d662766de3
impala,8181,description,"A recent change added plan node cardinality to the DESCRIBE output using a ""metric"" format: 123.46M instead of 123456789. This makes the output easier to read. Also, since all numbers are estimate, having seven or eight digits of precision is not very helpful. This ticket requests that the same formatting be used for the other places where the DESCRIBE output shows row counts: table stats, extrapolated row count, partition row counts, and so on.",design_debt,non-optimal_design,360f88e20709a4f1447c4997d1dc458e80874389
impala,8187,description,"It's possible for UDF authors to get into a pickle by exporting symbols from UDF/UDA shared objects unintentionally. E.g. if using boost headers, which result in weak symbols in the .so, which then get merged with the (incompatible) symbols in the impalad binary. We should really be keeping symbols hidden by default then exporting the entry points explicitly in the UDF/UDA code",design_debt,non-optimal_design,55b9c899b1c1f5d0e6d7fde5eff677bb015c2e54
impala,8248,summary,Improve Ranger test coverage,test_debt,low_coverage,377471fb4b2794caae63601f937b6c4a48ef2cd3
impala,8248,description,We have authorization tests that are specific to Sentry and authorization tests that can be applicable to any authorization provider. We need to re-organize the authorization tests to easily differentiate between Sentry-specific tests vs generic authorization tests. This also will improve test coverage for Ranger.,test_debt,low_coverage,377471fb4b2794caae63601f937b6c4a48ef2cd3
impala,8473,comment_0,"Can you define ""ImpalaPostExecHook "" as an interface instead an abstract class? I don't see benefits of defining it as abstract class. On the other hand, defining it as interface allows the implementation to extends from another base class. Thanks! Instead of Can you define",design_debt,non-optimal_design,31195eb8119ac6a557486a10dc24692bb0202f85
impala,8485,summary,References to deprecated feature authorization policy file need to be removed,code_debt,dead_code,84addd2a4b74454bd929d6b2ada0f501a2c6b0cb
impala,8485,description,Running the command *git grep authz-policy* produces the following output: ** authz-policy.ini = % WAREHOUSE These references to the *authz-policy.ini* should be cleaned up as the authorization policy file feature is deprecated as of *IMPALA-7918.*,code_debt,dead_code,84addd2a4b74454bd929d6b2ada0f501a2c6b0cb
impala,8534,comment_0,"I'm not going to get to this right away, although I'd like to circle back. If you want to turn it on to get some additional testing, feel free to pick it up - I can provide any pointers if you need them.",test_debt,low_coverage,88da6fd421a9449d372de77aae61a33197f4d3c2
impala,8578,description,"metrics.h and other metric headers are included a lot of places and there is a lot of code in the header that has very few callers. It appears to be pulled into several hundred compilation units, increasing the compile time of each of those and forcing recompilation when the headers are changed. Some ideas: * Move function implementations to .cc files. E.g. ToJson() and ToPrometheus() don't need to be inlined. * Move MetricGroup to its own file * Try to see if we can use forward declarations in more places to avoid including it.",code_debt,low_quality_code,"e78e6f0c260b342c67e055656d239438bca13288,95a1da2d32ea7b28585ad574b22c2bb9dd921029"
impala,8605,summary,Clean up connection/session management,code_debt,low_quality_code,ab908d54c22861967f693428ec7d9f6d7008607f
impala,8605,description,"Following on from IMPALA-8538 and IMPALA-1653, it would be nice to clean up some of the session management logic and add some more tests for the HS2 APIs - we have some testing gaps and a few of the invariants are unclear about what operations are allowed.",code_debt,low_quality_code,ab908d54c22861967f693428ec7d9f6d7008607f
impala,8632,description,"In case of {{INSERT_EVENTS}} if Impala inserts into a table it causes a refresh to the underlying table/partition. This could be unnecessary when there is only one Impala cluster in the system. The existing self-event detection framework cannot identify such events because they are not sending HMS objects like tables and partitions to the HMS. Instead in case of {{INSERT_EVENT}} HMS API only asks for a table name or partition value to fire a insert event on it. We can detect a self-event in such cases if the HMS API to fire a listener event is improved to return the event id. This would be used by EventProcessor to ignore the event when it is fetched later in the next polling cycle. In order to support this, we will need to make a change to Hive as well so that the enhanced API can be used.",design_debt,non-optimal_design,"e0ed7d321c1dca85a0f1482842f8db6db517c909,6fcc758c1aa0b6075738edb76371a0cc43d04388"
impala,8656,description,"Impala's current interaction with clients is pulled-based: it relies on clients to fetch results to trigger the generation of more result row batches until all the result rows have been produced. If a client issues a query without fetching all the results, the query fragments will continue to consume the resources until the query hits is cancelled and unregistered for whatever reasons. This is undesirable as resources are held up by misbehaving clients and other queries may wait for extended period of time in admission control due to this. The high level idea for this JIRA is for Impala to have a mode in which result sets of queries are eagerly fetched and spooled somewhere (preferably some persistent storage). In this way, the cluster's resources are freed up once all result rows have been fetched and stored in the spooling location. Incoming client fetches can be returned from this spooled locations. cc'ing , , ,",design_debt,non-optimal_design,"4ff2cd86e26876607ece8ac705e6dabc1f338c07,49ac55fb691e280ccf061a3fa067aff938614af9"
impala,8857,description,"I believe this is a flaky tests, since there's no attempt to pass the timestamp from the kudu client that did the insert to the impala client that's doing the reading.",test_debt,flaky_test,49cdd785637c82d7ba3f38be7e5c13e2b7272c2e
impala,8884,description,It would be useful for debugging I/O performance problems if we had histogram stats for the time taken for various operations so that we could see if there were slow operations on a particular disk (e.g. because of disk failure) or from a particular remote filesystem.,code_debt,low_quality_code,"ae7a9189081b6ade267198873bd65fcb4bd90001,89b9c93c7ac5f3eb19977290ba5115547120a0a3"
impala,8892,description,Our docker images lack a bunch of tools that help to implement health checks and debugging issues.,design_debt,non-optimal_design,146af97944bb2797d501ecdbc6417a3bc758891f
impala,8912,summary,Avoid calling computeStats twice on HBaseScanNode,design_debt,non-optimal_design,d21f00ef2f2219551e3501e3cf1cf45ff4afdcef
impala,8912,description,"For simple queries on HBase tables that has HBaseScanNode as the root of the SingleNodePlan, will be called twice. Stacktrace for the first call: Stacktrace for the second call: Codes of the second call: Logs for a simple query on an old version of Impala: Such kind of queries are usually point queries and are always expected to return fast. is heavy since it requires RPCs to HBase. We should avoid calling it twice.",design_debt,non-optimal_design,d21f00ef2f2219551e3501e3cf1cf45ff4afdcef
impala,8912,comment_0,"The second call is required since the limit_ may be updated, which may cut the cardinality. I think what we should avoid is sampling twice on hbase.",design_debt,non-optimal_design,d21f00ef2f2219551e3501e3cf1cf45ff4afdcef
impala,8935,comment_2,"So the use-case you're talking about, accessing the minicluster webui in a dev environment from another machine, I think is difficult to make work in all cases. Rather than using ip addresses, we could specify hostnames for these links, but that's not guaranteed to always work - I think its common for people to develop on machines that don't have DNS-resolvable hostnames (eg see IMPALA-8917). And its always going to be the case that the machine's hostname resolves to 127.0.0.1 locally due to I don't think this is too big of a deal - it should work in any sort of real, non-development environment, and of course its possible in a dev environment to access the other webuis by manually specifying the right host:port instead of clicking the link, as its always been. One option if you really want this to work in a dev environment is to specify the flag to some public IP on minicluster startup. This flag is currently broken, but I have a patch out to get it working:",design_debt,non-optimal_design,b3eb2d3c99ad45250866b0ef2a55a795def4df2f
impala,9146,description,"Since broadcast based hash joins are often chosen, we sometimes see very large tables being broadcast, with sizes that are larger than the destination executor's total memory. This could potentially happen if the cluster membership is not accurately known and the planner's cost computation of the broadcastCost vs partitionCost happens to favor the broadcast distribution. This causes spilling and severely affects performance. Although the DistributedPlanner does a mem_limit check before picking broadcast, the mem_limit is not an accurate reflection since it is assigned during admission control (See Given this scenario, as a safety check it is better to have to an explicit configurable limit for the size of the broadcast input and set it to a reasonable default. The 'reasonable' default can be chosen based on analysis of existing benchmark queries and representative workloads where Impala is currently used.",design_debt,non-optimal_design,0f2aa509891642e29763aa24e47f07554932c7d2
impala,9209,summary,is flaky,test_debt,flaky_test,d72fd9a99927045a5b5ef9f4e617b1b6613e1574
impala,9373,comment_0,"Notes so far: * In many cases it recommends including internal headers instead of the public-facing header * It gets confused by ""using"" statements in headers, e.g. it thinks is needed for references to ""string"" * The recommendations are mostly pretty good, but there were various small misfires, e.g. recommendations that didn't work or match our coding standards",code_debt,low_quality_code,"04fd9ae268d89b07e2a692a916bf2ddcfb2e351b,da5b498c18ba1a22b122682da73481af633a3398"
impala,9467,summary,Impala Doc: Improve Impala shell usability by enabling live_progress in the interactive mode,design_debt,non-optimal_design,42dcaabf7c7f5e8ba5b98ce7c039574516d9e65d
impala,9530,description,"In some cases pre-aggregations can balloon up and consume lots of memory, forcing the merge aggregation to spill. This is often OK as long as the preaggregation is reducing the input sufficiently, since it reduces the amount of data shuffled over the network. However in some cases it's preferable to be more conservative with memory and just cap the size of the preaggregation to prevent it ballooning too much. It would be useful to add a query option to directly limit the memory consumption of the preaggregations.",design_debt,non-optimal_design,ca53f68525504b6a1a64bb35c7edb57d030916d7
impala,9543,summary,Reduce duplicate code in thrift CMakeLists.txt,code_debt,duplicated_code,24518467970f29d2a33f56c782ab52a049d9335a
impala,9543,description,"Reduce duplicate code in thrift CMakeLists.txt. And if in future, we change hive to version 4 or higher. This can adapt automatically.",code_debt,duplicated_code,24518467970f29d2a33f56c782ab52a049d9335a
impala,9560,description,"When working on the Impala 3.4 release, we changed the version on branch-3.4.0 from 3.4.0-SNAPSHOT to 3.4.0-RELEASE. now fails with the following error: The output is expecting a cardinality of 17.91K, but instead the cardinality is 17.90K. The RELEASE version has one character fewer than the SNAPSHOT version. The version gets embedded in parquet files, so the parquet file is slightly smaller than before. The test is estimating cardinality by looking at the size of the parquet file. Apparently, this is right on the edge. This test should tolerate this difference.",design_debt,non-optimal_design,e9dd5d3f8c1d533bc5ae94c7e0677820fcd851aa
thrift,37,summary,add some missing new lines to fprintfs,code_debt,low_quality_code,2be87f3ab036183d49ad7a9af274c797839ef342
thrift,48,description,"The patch adds the unix_socket parameter for TServerSocket. The patch fixes an error message, because it's confusing to see the message ""Could not connect to localhost:9090"" if you try to connect to /tmp/unix_test.",code_debt,low_quality_code,e29995e75be58281975def90935c5894e3eb98c7
thrift,48,comment_0,"Line 74 of the patch, please edit the comment to say ""We need remove the old unix socket if the file exists and nobody is listening on it."" Line 95 of the patch, I personally prefer leaving the parens here. Line 103 of the patch, please use ""is not None"" instead of ""!="" Other than those nitpicks, I'm fine with this. bmaurer, you wrote the original Python Unix-domain code. What do you think of it?",code_debt,low_quality_code,e29995e75be58281975def90935c5894e3eb98c7
thrift,56,description,"When displaying the caller of a deprecated class/module, any thrift library code should be skipped. This really only affects (as it called each_field, which would trigger the deprecation). The given patch is actually 2 patches (suitable for use with git-am), one for this specific issue, the other fixes the specs to handle deprecation warnings a bit better. I bundled the second with this because the issue with the specs only shows up once the first patch is added.",code_debt,low_quality_code,"f2e4d107af58ad806556cc289befa3b2cf694d3c,28744ab833688ffe95eb4634aae33ad6f57a9054"
thrift,191,comment_3,"Here is a first patch. For the time being the metadata structure only contains the field name and is an inner class just like Isset, but unlike Isset this one will be the same for all classes, so may be it should be placed somewhere else. Let me know your thoughts.",code_debt,low_quality_code,b936ffdbd1740fda3686cd71bdcb1f865b709d07
thrift,191,comment_4,"I like the map declaration, but I think that the MetaData class should live outside of the generated code. It's not actually a class that will vary by structure, so it should just live in c.f.thrift.",architecture_debt,violation_of_modularity,b936ffdbd1740fda3686cd71bdcb1f865b709d07
thrift,211,description,"Add a client option that causes clients to monitor their creators and terminate when the creator dies. This makes it possible to prevent client leaks without linking, because the latter causes application code to be killed when a transport error occurs and exits are not trapped.",code_debt,low_quality_code,"5e530af5878ce7650e94ee662951b49358100984,bb97fd90a501ce7a7b37170295c4d02c719a2150,1e1a6976723b37c44b1eff5cc3f4df1a52b95e0b,fad60657a3b64894b56b56f7c23b2dcfa8cf4647"
thrift,211,comment_2,"1/ Unify the setup of {Starter, Opts} so that you don't have to do the monitor keysearch twice. 2/ Consider checking the source of the DOWN message ... having an unverified DOWN message could lead to some very hard-to-trace bugs. Lg otherwise. Also, what do you think about converting all the Erl unit tests to use eunit at some point (now that it's standard as of R12B) ?",design_debt,non-optimal_design,"5e530af5878ce7650e94ee662951b49358100984,bb97fd90a501ce7a7b37170295c4d02c719a2150,1e1a6976723b37c44b1eff5cc3f4df1a52b95e0b,fad60657a3b64894b56b56f7c23b2dcfa8cf4647"
thrift,211,comment_3,"Replaced tabs with spaces (oops). Fixed #1. Is this unified enough to you? It eliminates the double search. It is not super-unified, but unifying them more would be kind of gross. For 2, I did consider it, but who would forge a 'DOWN' message? Verifying it just requires more state. I think converting to eunit would be great.",code_debt,low_quality_code,"5e530af5878ce7650e94ee662951b49358100984,bb97fd90a501ce7a7b37170295c4d02c719a2150,1e1a6976723b37c44b1eff5cc3f4df1a52b95e0b,fad60657a3b64894b56b56f7c23b2dcfa8cf4647"
thrift,211,comment_6,"OK. This is probably good enough to check in. I don't remember if erlang:monitor has a monopoly on DOWN messages, so hence there's some reason to at least add a guard clause (is_reference() on the monitor_ref) to make sure that it's in the right format.",code_debt,low_quality_code,"5e530af5878ce7650e94ee662951b49358100984,bb97fd90a501ce7a7b37170295c4d02c719a2150,1e1a6976723b37c44b1eff5cc3f4df1a52b95e0b,fad60657a3b64894b56b56f7c23b2dcfa8cf4647"
thrift,221,summary,Make java build classpath more dynamic and configurable,design_debt,non-optimal_design,"249d7cb199b5c08e7a7a51189a733cc8fef12cf9,249d7cb199b5c08e7a7a51189a733cc8fef12cf9"
thrift,221,description,"The way that the current Java library and test builds are set up, they rely on an external jar that's assumed to be at some specific path. This isn't where the jar resides on my machine, so every time I want to do some building, I have to hand edit the file, and then when I want to generate a diff for posting on jira, I have to revert the changes. We should make it so the class path is configurable in a way that doesn't require checkins. There are a couple different ways to do this, but a properties file with the classpath is probably one of the easiest.",design_debt,non-optimal_design,"249d7cb199b5c08e7a7a51189a733cc8fef12cf9,249d7cb199b5c08e7a7a51189a733cc8fef12cf9"
thrift,240,description,"Now that we have a deep copy constructor, TBase should implement cloneable.",design_debt,non-optimal_design,4371038cd1e1a0a0ef5283bfd540c7623fbe7514
thrift,241,summary,Python __repr__ is confusing and does not eval to the object in question,code_debt,low_quality_code,888f88b05d25eb7b07541ed916f05234e1229138
thrift,241,description,"Having the repr return the repr of __dict__ is confusing, because the object is not a dict and does not act (much) like one. 100% of the 3 python developers I have seen who are new to thrift were confused at first why the repr looked like a dict but obj[attributename] raised KeyError. Additionally, this violates the repr guideline that where possible eval(repr(obj)) == obj. Finally, specifying a __str__ equal to __repr__ is redundant, since str() will use __repr__ if no __str__ is given. Here is a patch: $ diff -u compiler/cpp/sr\  2008-12-24 16:36:54.000000000 +0000 +++ 2008-12-24 16:49:54.000000000 +0000 @@ -614,11 +614,10 @@ // Printing utilities so that on the command line thrift // structs look pretty like dictionaries out << - indent() << ""def __str__(self):"" << endl << - indent() << "" return str(self.__dict__)"" << endl << - endl << indent() << ""def __repr__(self):"" << endl << - indent() << "" return << endl << + indent() << "" L = ['%s=%r' % (key, value)"" << endl << + indent() << "" for key, value in << endl << + indent() << "" return '%s(%s)' % ', '.join(L))"" << endl << endl; // Equality and inequality methods that compare by value",code_debt,low_quality_code,888f88b05d25eb7b07541ed916f05234e1229138
thrift,241,comment_0,Sorry about the messed up formatting. Clean patch attached.,code_debt,low_quality_code,888f88b05d25eb7b07541ed916f05234e1229138
thrift,255,description,is a thin wrapper around TFDTransport that simplifies the process of opening a file.,design_debt,non-optimal_design,abb56a4a0dbeaedf0e71cf206db8e7e16932ab6b
thrift,255,comment_2,1) const std::string& for path 2) does specifying default params in just the .cpp file really work? seems like the compiler would have trouble understanding that. 3) Document using O_APPEND. Maybe allow overwriting as another option,code_debt,low_quality_code,abb56a4a0dbeaedf0e71cf206db8e7e16932ab6b
thrift,275,summary,Remove deprecated classes from Ruby library,code_debt,dead_code,e8ae5d3728de07e880d6477db0f63ed9a7c65415
thrift,275,description,"There are a lot of weirdly named files in the Ruby Thrift library in order for backwards compatibility to work. I think this is of dubious value, and is really confusing. The deprecation code is also very complicated. I think we should just get rid of the deprecated stuff. We're pre-version 1.0, so better to get all the big changes in now.",code_debt,dead_code,e8ae5d3728de07e880d6477db0f63ed9a7c65415
thrift,277,summary,Abstract Transport in Ruby #read method should throw,code_debt,low_quality_code,11727d2445573eab9fa86824ffcde9a3f5441b46
thrift,277,description,"It's really an abstract method, so not overriding it is an error. We should throw an error when someone calls it inappropriately. The same can probably be said for #write. #open and #open? might also want to change, though that's not quite as certain.",code_debt,low_quality_code,11727d2445573eab9fa86824ffcde9a3f5441b46
thrift,278,summary,#validate exceptions should contain the offending value,code_debt,low_quality_code,97592b40b1ae6d00911ab5e76dd951b011c63863
thrift,278,description,"If you have an enum field, and its value isn't in the VALID_VALUES of your enum, the exception thrown says that it's not found, but it doesn't say what the value is.",code_debt,low_quality_code,97592b40b1ae6d00911ab5e76dd951b011c63863
thrift,278,comment_2,"Does this work? Maybe I'm reading it wrong, but: {{+ indent(out) << ""throw new field '"" << field- Looks like it just prints the field name as the value. Got a test?",test_debt,lack_of_tests,97592b40b1ae6d00911ab5e76dd951b011c63863
thrift,298,comment_3,"Well, if you can believe it, I don't get what's going on here, either :). I think this code is a little too complicated. For now, I think I'll just apply your patch, and hopefully one day it won't be a problem, as I refactor my way through the Ruby libraries.",code_debt,complex_code,dc9092ab1a2953c282b1add4a2500467eb9e3c9d
thrift,353,summary,"Service generation needs to make service name capitalized, since Ruby modules need to be constant.",code_debt,low_quality_code,c289608e1928cc8c6db482aafe7262a428b6f32e
thrift,353,description,"service generator needs to capitalize service names, since services are ruby modules and ruby modules are constants.",code_debt,low_quality_code,c289608e1928cc8c6db482aafe7262a428b6f32e
thrift,372,comment_0,"How's this? I cleaned up some missed dead code related to deprecation, as well.",code_debt,dead_code,8852720e70e77669fa5ffff494dd641f6fff06f2
thrift,397,summary,remove unnecessary redefinition of generate_program(),code_debt,dead_code,db3a83a799de34c85fd1330b9833c7ca0af2c359
thrift,397,description,This is a holdover from the OCaml generator and is unnecessary in Haskell.,code_debt,dead_code,db3a83a799de34c85fd1330b9833c7ca0af2c359
thrift,418,description,"Currently the ruby struct code sorts the field ids of each struct every time it is serialized, which is an unnecessary drag on performance.",design_debt,non-optimal_design,"df8a0e6fca0dd0513a1b89a2feaf03f9a0056416,d1df20a20d1f23321bfdd9ca06ab03a71ceba51d"
thrift,418,comment_0,Attaching patch. There are 2 changes: 1) Modified t_rb_generator.cc to generate a FIELD_IDS constant for each struct + a struct_field_ids() accessor method. 2) Use instead of to iterate through struct fields id-sorted order. Updated the native code as well. All tests pass. Should be targeted for next major release as the change is not code-compatible with 0.6-generated files. Can be re-implement without a compiler change if we want to keep full compatibility between 0.6 gen-rb files and 0.7 library.,design_debt,non-optimal_design,"df8a0e6fca0dd0513a1b89a2feaf03f9a0056416,d1df20a20d1f23321bfdd9ca06ab03a71ceba51d"
thrift,418,comment_3,Reopening as I've thought about it and a patch which doesn't change the compiler or generated code ends up being much cleaner.,design_debt,non-optimal_design,"df8a0e6fca0dd0513a1b89a2feaf03f9a0056416,d1df20a20d1f23321bfdd9ca06ab03a71ceba51d"
thrift,427,comment_1,"The patch looks good, and everything compiles and the tests pass still. It'd be nice if this code was exercised through at least a test .thrift file though.",test_debt,lack_of_tests,2bcf3995ef7ff3db554c550b2d760d870a64d389
thrift,447,summary,Make an abstract base Client class so we can generate less code,design_debt,non-optimal_design,0fd37f08716758b283010abfa5162eb2c1aee2ad
thrift,447,description,"The Java generator currently uses the generator to create all of the contents of the myService.Client class, including boring stuff like the constructor and instance variables. It seems like we could just factor this common base stuff out into a BaseClient that lives in the library and simplify the generator accordingly.",code_debt,complex_code,0fd37f08716758b283010abfa5162eb2c1aee2ad
thrift,447,comment_0,"This patch restructures the code generator to rely on some abstract base classes for clients, processors, and process functions. It makes the generated code marginally smaller and a lot cleaner.",code_debt,complex_code,0fd37f08716758b283010abfa5162eb2c1aee2ad
thrift,471,summary,Generated exceptions in Python should implement __str__,code_debt,low_quality_code,4f3192093d819d7933342d9dd2560b00b95c55c0
thrift,471,description,"When the python generator makes an exception class since THRIFT-241, it does not include a __str__ method. This is problematic since raised exceptions don't include any useful information that might be part of the exception struct. Without __str__: With __str__: Clearly the latter is way more useful. Patch to follow if no one objects.",code_debt,low_quality_code,4f3192093d819d7933342d9dd2560b00b95c55c0
thrift,471,comment_0,"yeah, the python docs say that __repr__ should be used if there is no __str__ and every other case I have seen follows that rule but the exception printing apparently does not. +1",code_debt,low_quality_code,4f3192093d819d7933342d9dd2560b00b95c55c0
thrift,503,summary,"""make check""-enabled C++ tests should be in lib/cpp/test",architecture_debt,violation_of_modularity,"f261dd795c4004c72c7efd66c4b09f66299bfc83,351e22b52afcbe9b40e0933f80b06b479dff50f4"
thrift,525,description,"The C# Library Solution fails to compile because of problems with the PreBuildEvent for the ThriftTest Project - Thrift.exe is called with the outdated -csharp instead of --gen csharp - All of the existing commands in the PreBuildEvent will fail if there's a space in any directory name in the directory tree (such as ""My Documents"" or ""Visual Studio 2008""). - The recursive forced rmdir command will match other directory trees that share a common root (e.g. c:\test\Work\ will be recursively removed if the project is located in c:\test\Work for Thrift\test\csharp) I have attached a patch that replaces the PreBuildEvent in the ThriftTest project file to fix these problems and work with directory trees that contain spaces. As the thrift.exe compiler does NOT accept paths with spaces when surrounded by quotes, my script generates MSDOS 8.3 pathnames to pass to the thrift compiler and it appears to work just fine. This has only been tested on my machine with VS.net pro 2008 and a current thrift checkout",architecture_debt,using_obsolete_technology,e8f38f47c8bfdb8738e016222c45a1cb08335299
thrift,544,description,"The current generator produces multiple -define statements with the same name, which isn't valid erlang code (and also isn't valid semantically if we want two different values). produces: In the patched version, it produces this:",design_debt,non-optimal_design,04650afb872fbf62dc31426d7278261bf178fc4f
thrift,544,comment_5,"I think the patch is correct, but the argument for correctness is very complicated because constants_[name] is a mutating operation that creates an entry if the ""name"" key is not present. I'd much prefer the condition be != constants.end()""",code_debt,complex_code,04650afb872fbf62dc31426d7278261bf178fc4f
thrift,554,summary,Perl improper namespace check for exception handling and writeMessageEnd missing on processor calls,code_debt,low_quality_code,"4184e2be546d19ece83c8132a03f08a5de6656a6,8fc413fbfd721080d5e8e0300d1094387cdeda15"
thrift,554,description,"There are two more issues affecting the functioning of a Perl service server: 1. Failure to prepend the Perl namespace to the exception name when checking the exception type from a eval'ed method call. 2. writeMessageEnd() should be present after a method call writes its result. I'm attaching a patch which addresses these issues, in addition to the following more minor changes: 1. Tried to make indentation and line breaks more consistent to ensure readability of the generated code. 2. Added a few best practice ideas to improve the code in minor ways. 3. Added a readAll() function to the as the one found in Thrift::Transport uses a while loop to consume the data, which results in a endless loop.",code_debt,low_quality_code,"4184e2be546d19ece83c8132a03f08a5de6656a6,8fc413fbfd721080d5e8e0300d1094387cdeda15"
thrift,597,summary,Python THttpServer performance improvements,code_debt,slow_algorithm,d6a02ff99bcee734d5335c91f5ada41d7322ecb4
thrift,597,description,"This class was originally meant for functional testing only, so performance wasn't a concern. But now I'm using it for load testing. :) Two patches here. The first enables buffered I/O. The second allows the http server class to be specified, which allows users to use the ThreadingMixin.",code_debt,slow_algorithm,d6a02ff99bcee734d5335c91f5ada41d7322ecb4
thrift,597,comment_1,"One query: I want to use THttpServer in my python project but from initial performance review for a single client single request (No ThreadingMixIn required at this point), it shows that HttpServer is always 2 to 4 times slower than NonBlocking server in tutorial Calculator client server app and same was observed in my product testing as well. Is there a way to make the performance equal for THTTPServer as compared to NonBlocking? Heres the code for NonBlocking client and server in python and its performance on ubuntu. single machine.  In my project I observed that on client side, its the generated code send_<api Will really appreciate if someone can shed some light on how to improve performance for THttpServer in thrift 0.8.0.",code_debt,slow_algorithm,d6a02ff99bcee734d5335c91f5ada41d7322ecb4
thrift,612,comment_0,+1 Looks good to me. Do you have test so that we make sure there are no regressions in the future?,test_debt,lack_of_tests,034c49f0fcf95f13982de528f19760f5ed01617e
thrift,612,comment_2,That's why I was asking for a test before committing the patch.,test_debt,lack_of_tests,034c49f0fcf95f13982de528f19760f5ed01617e
thrift,628,description,"It turns out that Enums in Java don't have good hashcode behavior. It uses object ID, which can be inconsistent between different invocations of the same application, which breaks things like Hadoop partitioning. We should use the hash of the actual thrift field id instead of the hash of the enum version of the field.",design_debt,non-optimal_design,e476480691d114ad1f1a9794bd10b34df6554ad2
thrift,653,description,"Now that enums are actually enums, their toStrings are acceptable in the overall struct toString. We should remove the special case we built in for this in the past.",code_debt,dead_code,e533bace3d462ebad7c65d7de4fd167a0ffbe740
thrift,673,summary,Generated Python code has whitespace issues,code_debt,low_quality_code,1316ed9d1644234106035d76f48d6362c41bf914
thrift,673,description,The Python code generator produces code with a number of whitespace issues: - Trailing whitespace at the end of lines. - Multiple blank newlines at the end of files. Both of these issues cause problems if the code is used in a Git repository with `git diff --check' in a hook. The attached patch corrects the code generation to address these issues.,code_debt,low_quality_code,1316ed9d1644234106035d76f48d6362c41bf914
thrift,673,comment_0,Updated patch which catches one more case of trailing whitespace.,code_debt,low_quality_code,1316ed9d1644234106035d76f48d6362c41bf914
thrift,678,summary,HTML generator should include per-field docstrings,code_debt,low_quality_code,9bfacd36800f052e33ffa78a06bc9a4f600e5f3d
thrift,685,summary,Direct buffer access to improve deserialization performance,design_debt,non-optimal_design,becaf536211a699f1fb936752262fdb7bcd36126
thrift,685,description,"After poking around a bit and comparing how Thrift performs versus Protocol Buffers, I think we should change our transports and protocols to support optional direct buffer access behavior. Basically, the way this works is that if the transport is backed by a buffer, then it can give access to that buffer to the protocol. The protocol can then do things like read a byte without instantiating a new one-byte array or decode a string without an intermediate byte[] copy. In my initial testing, we can reduce the amount of time it takes to deserialize a struct by at least 25%. There are probably further gains to be had as well.",design_debt,non-optimal_design,becaf536211a699f1fb936752262fdb7bcd36126
thrift,685,comment_0,"+1. The template code that Chad and I worked on for C\+\+ (almost ready for trunk) gets the complier to do something similar for us, and we saw a really significant boost.",design_debt,non-optimal_design,becaf536211a699f1fb936752262fdb7bcd36126
thrift,685,comment_1,"Here's my initial effort. This produces a pretty noticeable speedup in deserialization time when using the compact protocol and TDeserializer. This patch notably does not yet address the binary protocol implementation. I'd love some people to review it and give me feedback. Ideally, we'd commit what I have here as a starting point and then continue to improve other parts.",design_debt,non-optimal_design,becaf536211a699f1fb936752262fdb7bcd36126
thrift,695,comment_1,"This seems like a very faithful translation of the Java version, which has benefits and drawbacks. I'm okay committing it, but the patch seems corrupt (""%ld"" where there should be a line number).",design_debt,non-optimal_design,6acc269d1883c822595ab5f346c6e4c1e1b2724c
thrift,695,comment_3,"Here's a simpler to use, more pythonic version.",code_debt,complex_code,6acc269d1883c822595ab5f346c6e4c1e1b2724c
thrift,701,description,"In glancing at one of our jars of generated Thrift class files, I noticed that it was pretty huge. Further digging showed that while certainly most of the bulk was attributable to the number of class files we have, part of it was due to lots of unnecessary internal classes. This is because we use a specific syntax for defining some maps inline in structs and enums. Since this is only syntactic sugar, I think it would make sense to write a tiny bit of helper code and avoid the need for a dynamic internal class.",design_debt,non-optimal_design,15b7b287947dd348e2801ead6b1a33b2a2d6b31b
thrift,710,description,TBinaryProtocol can probably take even more advantage of direct buffer access than TCompactProtocol. Identify the relevant spots and clean it up.,design_debt,non-optimal_design,a268019e85ac34b5b7a1c89d59288ecead5e6d97
thrift,710,comment_0,"This patch makes TBinaryProtocol use direct buffer access in the relevant methods. My performance testing was somewhat rudimentary, but I think it may have as much as doubled performance. Obviously your performance boost will be really dependent on the contents of your struct, but this seems pretty great. As a side effect of this issue, I refactored the TCompactProtocol test so that we could exact TBinaryProtocol to the same bevy of test cases.",code_debt,slow_algorithm,a268019e85ac34b5b7a1c89d59288ecead5e6d97
thrift,714,description,"THsHaServer instantiates its ThreadPoolExecutor with a That behavior is documented in java as: There are three general strategies for queuing: ... 2. Unbounded queues. Using an unbounded queue (for example a LinkedBlockingQueue without a predefined capacity) will cause new tasks to wait in the queue when all corePoolSize threads are busy. Thus, no more than corePoolSize threads will ever be created. (And the value of the maximumPoolSize therefore doesn't have any effect.) This may be appropriate when each task is completely independent of others, so tasks cannot affect each others execution; for example, in a web page server. While this style of queuing can be useful in smoothing out transient bursts of requests, it admits the possibility of unbounded work queue growth when commands continue to arrive on average faster than they can be processed. therefore changing maxWorkerThreads (passed as maximumPoolSize) has no effect. The parameter should probably just be removed and minWorkerThreads renamed to numThreads, since setting minWorkerThreads does have an effect and is a workaround.",design_debt,non-optimal_design,76c2dcc4d3d4497a2ec11fefb4a76087fc5fbf79
thrift,714,comment_1,"After reading up on it some more, I think this change is what we actually want. Up until the min number of threads, new ""core"" pool threads will be created. Once we pass this limit, new threads will be created up to the max number of threads, at which point the server will start rejecting executions. After (default) 60 seconds of idleness, the pool will start to kill threads, but go no lower than the min number of threads. This patch is probably incomplete, as if we get a rejected execution exception when trying to queue an invocation, we should respond to the client with an error immediately.",code_debt,low_quality_code,76c2dcc4d3d4497a2ec11fefb4a76087fc5fbf79
thrift,714,comment_3,I committed a patch to remove maxWorkerThreads and rename minWorkerThreads to workerThreads. This will keep the behavior the same and remove the ineffectual parameter.,code_debt,dead_code,76c2dcc4d3d4497a2ec11fefb4a76087fc5fbf79
thrift,716,summary,Field names can conflict with local variables in code for unions,code_debt,low_quality_code,4e7cf25b9574eb439b329070fd57e854948144df
thrift,716,description,"Try creating a union with the field name ""value"", and the code won't compile. In writeFields for the generated class, you'll have something like the following: <code case VALUE: String value = return; </code ""String value"" conflicts with the parameter ""Object value"".",code_debt,low_quality_code,4e7cf25b9574eb439b329070fd57e854948144df
thrift,758,comment_3,"Oh sorry, could you include a test case for this under lib/perl/test please? Thanks!",test_debt,lack_of_tests,ee8255d0867da74510574afe634c692603c7d7aa
thrift,758,comment_4,"Jake, I unfortunately have no clue how to generate a with the provided ThriftTest. I didn't find a way to run the tests automatically either. I manually executed: perl -Iblib/lib -Ilib -Itest/gen-perl test/processor.t Thanks, Yann",test_debt,lack_of_tests,ee8255d0867da74510574afe634c692603c7d7aa
thrift,813,comment_0,"It could be in Java's TJSONProtocol implementation. I'm not super familiar with that implementation, so I don't know where to look off hand, but maybe you could add a test case that exercises this?",test_debt,lack_of_tests,127909c1c948e9248f7ed98f0b5e3a7088d89ca3
thrift,813,comment_2,"I have no idea - TJSONProtocol.java is a big file, and I didn't write it. personally, I'd just grep around for any occurrence of the @ sign or something that indicates URL-encoding is going on. Sadly, not really. It's a bit of a jungle in there. The very first thing to do in every case is to open a JIRA ticket so that we have a record of the bug. From there, either get out your machete and safari hat and dig in, or try to find someone who can fix it for you.",code_debt,complex_code,127909c1c948e9248f7ed98f0b5e3a7088d89ca3
thrift,813,comment_4,"Hi Jordan, This too is most a browser side encoding issue. The js implementation of writeString uses I suppose this isn't consistent, with the non javascript protocol readers. but the js readString uses We could implement our own encoder to encode only JSON special chars. -Jake",design_debt,non-optimal_design,127909c1c948e9248f7ed98f0b5e3a7088d89ca3
thrift,840,summary,Perl protocol handler could be more robust against unrecognised types,code_debt,low_quality_code,a55f51d8709582e1d054844c17c554e8d124b394
thrift,840,description,"The Perl protocol implementation can loop arbitrarily on corrupt data because Protocol::skip() skips nothing if it doesn't recognise a type. It might be nice if it threw an exception instead. For example, the loop to skip a map will keep looking at the same invalid bytes for every iteration if it hits an unrecognised type. If corrupt data has resulted in a bogus huge map size, then the loop goes on for a long while, rather than dying quickly after running out of available data. I recognise that input validation probably isn't a priority for Thrift (usually communications are well-formed!), but wonder if you'd consider the attached patch which dies if skip or skipBinary encounter types that they don't know what to do with. It probably needs more thought than I've given it, as I'm not sure what the correct behaviour should be for valid types not handled by the existing code: in the patch I'm throwing an exception for those saying that they cannot be skipped. An additional TType constant of MAX_TYPE might be helpful in writing a better solution. I know it's a bit weird to be suggesting this; I'm using Thrift for serialisation among other things, and with an data store. This ""infinite"" loop was the only instance in which your existing error handling didn't adequately flag up bad data. Clearly I should also be doing checksums on the data beforehand, but I just thought I'd suggest this to you. Thanks, Conrad",design_debt,non-optimal_design,a55f51d8709582e1d054844c17c554e8d124b394
thrift,840,comment_0,Throw exception instead of silent return on unrecognised type.,code_debt,low_quality_code,a55f51d8709582e1d054844c17c554e8d124b394
thrift,840,comment_1,"I'm no Perl guy, but it seems like there's no reason to have two different kinds of ""die"" cases. The one with the if seems to be subsumed by the one without - am I wrong?",design_debt,non-optimal_design,a55f51d8709582e1d054844c17c554e8d124b394
thrift,840,comment_3,I committed this with the redundant parts removed. Thanks for the patch!,code_debt,duplicated_code,a55f51d8709582e1d054844c17c554e8d124b394
thrift,873,description,"All of the tests run in the same JVM, and it seems like is leaking sockets. As a quick fix, dropping that to use only 200 clients instead of 500, and changing each unit test to run in its own JVM instead of sharing them. Also allowing the port used for binding the test servers to be configured from the command line",code_debt,low_quality_code,90ec5bfcd0acd9314b0c02df25802fe9e26de4e3
thrift,873,comment_1,"I like all of this patch except for the separate JVMs. I know that's its probably a more responsible way to do things, but it makes the test run slower. Are you certain that this is what is necessary in order to make the test pass? Wouldn't it be possible for us just to clean up after ourselves in the test for",test_debt,expensive_tests,90ec5bfcd0acd9314b0c02df25802fe9e26de4e3
thrift,873,comment_2,"On my box it goes a bit slower, but still pretty fast. With forkmode=""once"" I did indeed see failures that went away with the separate JVMs. It also made it harder to figure out which test was actually at fault, since the problem with AsyncClientManager caused a bunch of later tests to fail too.",test_debt,expensive_tests,90ec5bfcd0acd9314b0c02df25802fe9e26de4e3
thrift,897,description,"Through looking at THRIFT-544 and THRIFT-895, it's come to my attention that we currently register each of every enum's values as a global (and scoped) constant. This allows you to do things like: This is handy, insofar as you might want to use the values of an enum in constant or default circumstances. However, this behavior is unstable - if you have two enums with values that have the same name, all constant references will point at the last occurrence of the name. Further, in order to allow this to go on, we must not check if any constant has been declared twice, which means you can get stupid, detectable errors in your IDL very easily. I propose that we stop allowing this method of access, and instead require the enum values referenced in constant context to be prefixed with the enum type's name. For instance:",design_debt,non-optimal_design,"9f0a786dad1d6d0d737b89f96ba7ec90bf9096e2,cb0218f5c8fcc5e14016ea97f9cfc51009472246,6e05e25c8977953a46251f5abfaf111a3ae9607b,76ecb91c3d7034c939d6df9316f571cbf9f7ce09"
thrift,897,comment_4,"I think the smart resolution thing is only worthwhile if we really want to keep backwards compatibility. I'm not crazy about breaking people's IDL, but I think that's preferable in the long term to supporting a more convoluted syntax. If we were starting from scratch today, I think that we'd want to use the fully-qualified approach, and the cost of fixing the breakage *should* be reasonably low, especially because you're getting a consistency improvement at the same time. This is certainly my preference.",design_debt,non-optimal_design,"9f0a786dad1d6d0d737b89f96ba7ec90bf9096e2,cb0218f5c8fcc5e14016ea97f9cfc51009472246,6e05e25c8977953a46251f5abfaf111a3ae9607b,76ecb91c3d7034c939d6df9316f571cbf9f7ce09"
thrift,897,comment_6,"LGTM. In the main.cc part, you could possibly check the part before the dot matches the name of the enum (possibly with the scope qualifier). Not critical.",code_debt,low_quality_code,"9f0a786dad1d6d0d737b89f96ba7ec90bf9096e2,cb0218f5c8fcc5e14016ea97f9cfc51009472246,6e05e25c8977953a46251f5abfaf111a3ae9607b,76ecb91c3d7034c939d6df9316f571cbf9f7ce09"
thrift,906,description,"The current haskell type mappings are awkward and error prone to work with: binary -string -byte -i16 -i32 -i64 - This patch updates the mappings to the canonical types of the correct length in Haskell: binary -string -byte -i16 -i32 -i64 - THIS BREAKS EXISTING CODE. It is, however, very arguably broken already. For convenience of patching, this patch is a superset of THRIFT-743. Thoughts?",design_debt,non-optimal_design,75a33e858fdeb2171a4ee973ee2e53d39e891d27
thrift,931,description,"Right now, slf4j-simple is used when the tests are run. Unfortunately, the ""simple"" logger doesn't support debug level messages at all, which is somewhat of a pain. It would be good to switch to slf4j-log4j for the tests so we have debug logs available.",design_debt,non-optimal_design,"fcaa8f5364931155ba5675db7a02d898abf07998,84a7c2a901ee11433ca755edad1c278172ba7644"
thrift,959,summary,TSocket seems to do its own buffering inefficiently,code_debt,low_quality_code,"8166073e7fc4b4a13bbbcc464328f03d0a89b577,44b2bb6bb591ee106f1bcbbea328a736a70fdc91"
thrift,959,description,"I was looking through TSocket today while reviewing THRIFT-106 and I noticed that in TSocket, when we open the socket/stream, we wrap the input/output streams with objects and use those for reading and writing. Two things stand out about this. Firstly, for some reason we're setting the buffer size specifically to 1KB, which is 1/8 the default. I think that number should be *at least* 8KB and more likely something like 32KB would be better. Anyone have any idea why we chose this size? Secondly, though, is the fact that we probably shouldn't be doing buffering here at all. The general pattern is to open a TSocket and wrap it in a TFramedTransport, which means that today, even though we're fully buffering in the framed transport, we're wastefully buffering again in the TSocket. This means we're wasting time and memory, and I wouldn't be surprised if this is artificially slowing down throughput, specifically for multi-KB requests and responses. If we remove the buffering from TSocket, I think we will probably need to add a TBufferedTransport to support users who are talking to non-Framed servers but still need buffering for performance.",design_debt,non-optimal_design,"8166073e7fc4b4a13bbbcc464328f03d0a89b577,44b2bb6bb591ee106f1bcbbea328a736a70fdc91"
thrift,959,comment_1,I just committed a tiny fix for this. I found that removing the buffer improved the distribution of latencies by a small but notable percentage.,design_debt,non-optimal_design,"8166073e7fc4b4a13bbbcc464328f03d0a89b577,44b2bb6bb591ee106f1bcbbea328a736a70fdc91"
thrift,959,comment_2,This caused a big performance regression see THRIFT-1121,code_debt,slow_algorithm,"8166073e7fc4b4a13bbbcc464328f03d0a89b577,44b2bb6bb591ee106f1bcbbea328a736a70fdc91"
thrift,992,summary,Naming convention in C# constructor is not consistent with other fields causes compile errors,code_debt,low_quality_code,be87ab28b06323f46835983ce2501b952978bda2
thrift,992,description,"While updating my FluentCassandra project using the latest Thrift-0.5.0.exe generator against the 0.7 Beta3 release of cassandra.thrift I experienced a generation problem that caused compiler errors when compiling the C# code. It appears the constructor fields are using the old naming convention for the fields, while every other part of the class seems to be using the new naming convention for fields with underscores. You can see a diff of the files I had to manually edit here: The paths starting with should be the ones that you need to worry about for this bug.",code_debt,low_quality_code,be87ab28b06323f46835983ce2501b952978bda2
thrift,1003,summary,Polishing c_glib code,code_debt,low_quality_code,c101092ea742e1252207b6e8f680bf392292c916
thrift,1003,description,"attached patch contains following changes: * Added Apache headers to c/h files * Use gtester for running tests. We don't need -wrapper script anymore * Use one-line macros G_DEFINE_TYPE instead of 15-line class definition * Keep formatting closer to glib-like style (one line class definition macroses/remove trailing spaces) Given changes are mostly fixing low hanging fruits. It does not change any logic/api. There are more chages needed, such as * using CLASS_TYPE_new functions instead of * stop using _set_property (aka reflection) in constructors * check more careful about _ref and _unref handling but this requires more careful refactoring so it will be later in a separate patch.",code_debt,low_quality_code,c101092ea742e1252207b6e8f680bf392292c916
thrift,1055,description,"resulted in TFramedTransport being improved so it wasn't necessary to disable Nagle. For the case of TServerSocket and TSocket, a simple server case, performance is still very poor, ~30 msg/sec vs. ~11k msg/sec with NoDelay = true. Java and cpp disable nagle in their TSocket and TServerSocket implementations and performance is MUCH improved with nagle disabled for csharp so I'm proposing that csharp should also disable nagle.",design_debt,non-optimal_design,2d9dfdb88e58ab5b961726a5506a4f3e0653b807
thrift,1065,summary,Unexpected exceptions not proper handled on JS,code_debt,low_quality_code,"a8738b5560db8216c06d0a8cea116b7f29255e8e,b3b07d6de4fd673fd8acd1484daa8bf9002d91cc,76d55f635784aa9dfae8ce52ce3eb49ba7f90a40"
thrift,1065,comment_1,remove some misplaced code (probably copied from another implementation?),code_debt,dead_code,"a8738b5560db8216c06d0a8cea116b7f29255e8e,b3b07d6de4fd673fd8acd1484daa8bf9002d91cc,76d55f635784aa9dfae8ce52ce3eb49ba7f90a40"
thrift,1065,comment_3,"apparently the java server is not encoding undeclared exceptions ""properly"" (it could be my impl., please see patch), but it works fine on C++ Incoming content: [cpp] Outgoing content: is a [java] Outgoing content: Error:This is a",design_debt,non-optimal_design,"a8738b5560db8216c06d0a8cea116b7f29255e8e,b3b07d6de4fd673fd8acd1484daa8bf9002d91cc,76d55f635784aa9dfae8ce52ce3eb49ba7f90a40"
thrift,1100,description,"The python TSSLSocket.py module has TSSLSocket and TSSLServerSocket for outbound and inbound SSL connection wrapping. This ticket is for a patch that makes several improvements: * adds Apache license at top of file * for outbound sockets, SSL certificate validation is now performed by default ** but may be disabled with validate=False in the constructor ** instructs python's ssl library to perform CERT_REQUIRED validation of the certificate ** also checks to make sure the certificate's {{commonName}} matches the hostname we tried to connect to ** raises when the certificate fails validation - tested using google's www.gmail.com (doesnt match) versus mail.google.com (matched cert commonName) ** puts a copy of the peer certificate in self.peercert, regardless of validation status ** sets a public boolean self.is_valid member variable to indicate whether the certificate was validated or not * adds a configurable server certificate file, as a constructor argument {{certfile}} ** allows runtime changing of server cert with setCertfile() on the server, that changes the certfile used in subsequent ssl_wrap() calls ** exposes a class-level variable SSL_PROTOCOL to let the user select ssl.PROTOCOL_TLSv1 or other versions of SSL, instead of hard-coding TLSv1. Defaults to TLSv1 though. * removes unnecessary sys.path modification * adds lots of docstrings In a somewhat unrelated change, this patch changes two lines in TSocket.py where self.handle is compared to None using {{!=}} instead of: {{is not}}.",code_debt,low_quality_code,5040911bfab39b5c9f2a0d715cea0ee9012f7450
thrift,1103,description,"New implementation of zlib compressed transport for python. The attached patch provides a zlib compressed transport wrapper for python. It is similar to the TFramedTransport, in that it wraps another transport, implementing the data compression as a transformation layer on top of the underlying transport that it wraps. The compression level is configurable in the constructor, from 0 (none) to 9 (best) and defaults to 9 for best compression. The way this works is that every write() to the transport appends more data to the internal cStringIO write buffer. When the transport's flush() method is called, the buffered bytes are then passed to a zlib Compressor object and flush()ed with zlib.Z_SYNC_FLUSH. Because the thrift API calls the transport's flush() after writeMessageEnd(), this means very small thrift RPC calls don't get compressed well. This transport works best on thrift protocols where the payload contains strings longer than 10 characters. As with all data compression, the more redundancy in the uncompressed input, the greater the resulting compression. The TZlibTransport class also implements some basic statistics that track the number of raw bytes written and read, versus the decompressed equivalent. The getCompRatio() method returns a tuple of where ratio is computed using: (So 10 compression is 0.10, meaning smaller numbers are better.) The getCompSavings() method returns the actual number of which might be negative when the compression of non-compressible data ends up expanding the data. So hopefully, anyone who uses this transport will be able to tell whether the compression is saving bandwidth or not. I will add the patch in a few minutes. I haven't tested this against the C++ TZlibTransport, only against itself.",test_debt,lack_of_tests,1606659171d9ee8885d5806d6030ec39399b3b08
thrift,1103,comment_2,"Thanks for comitting THRIFT-1094, I'll update the patch this evening (and attach as _v2) to include testing TZlibTransport wrapping in the code (as a cmdline --zlib argument to both scripts). FYI, currently the hudson(jenkins) build seems to be failing on the javascript jslint tasks, stopped the build tests from progressing past the 'test/js' directory...",test_debt,lack_of_tests,1606659171d9ee8885d5806d6030ec39399b3b08
thrift,1103,comment_3,"I updated the test suite to include running every valid combination of server, protocol and wrapping transports (both ssl and zlib). For python2.4, this is 30 combinations and runs in about 24 seconds. For python2.7, there is an extra server type (TProcessPool which uses the multiprocessing module) and the SSL transport (unavailable in py2.4), whichadds up to 66 combinations of tests, running in ~95 seconds. The 4 nested for-loops significantly expands the code test coverage. In addition to everything in the _v1 of this patch, the _v2 version also has: Updated test code: * added testing of TSSLServer, an alternate socket transport * added testing of TZlibTransport, a wrapping transport * added a self-signed cert in with a cautionary .readme to allow testing of the TSSLServerSocket (it needs a certificate file) * fixed -q (quiet) and -v (verbose) options to to lower and raise the verbosity Fixed two problems in and one enhancement: * fixed confusing parameters to both client and server constructors, removing the overly ornate \*args and \*\*kwargs which made the constructor behave poorly when used with just (host,port) as arguments. The constructors better match the TSocket and TServerSocket constructor parameters now. * fixed logic in TSSLServerSocket parameter checking, if validate=True and ca_certs=None, now it raises an exception like the docstring claims it should. * made TSSLServerSocket more robust on failed SSL handshake by closing socket connection and returning None from accept() call, which is better than terminating the entire server in some cases I will attach the _v2 patch in a moment.",code_debt,low_quality_code,1606659171d9ee8885d5806d6030ec39399b3b08
thrift,1130,comment_0,"Not sure if this is the best fix, but seems to be rather minimalistic By the way - this is my first attempt at contribution here, please let me know if there is some other procedure should be followed.",design_debt,non-optimal_design,6c928f3f5f91afa0917f4625f39b4048865a6027
thrift,1130,comment_2,"I haven't tested this patch yet, but my concern is that by adding these tokens, you may possibly have broken string literals or identifiers named ""true"" and ""false"". Can you add some examples to ThriftTest.thrift that exercises these corner cases?",test_debt,lack_of_tests,6c928f3f5f91afa0917f4625f39b4048865a6027
thrift,1130,comment_3,"This was my concern too, and my threshold for passage was to ensure that it will provide an error, obviously its not the same error as it originally did. At the end of the day, this seems to be pretty much like a c-pre-processor macros: #define true 1 #define false 0 The the items NOT in my negative tests that WOULD fail are attempts to use true/false as field numbers (for example) What you are asking for is a negative test (which I performed locally), I only saw one but it is commented out (so its not in any automated test) Is this how you would like to see it? If not can you point me at an example of a negative test in ThriftTest.thrift? This is what I tried and what I had in mind ...",test_debt,lack_of_tests,6c928f3f5f91afa0917f4625f39b4048865a6027
thrift,1141,comment_0,"patch did not apply cleanly.. fixed that, the wrong libthrift.jar file location and updated the changelog. => committed",architecture_debt,violation_of_modularity,f3a51e6b03679fa3b1b14a7616f57fc62be05b90
thrift,1174,comment_2,got this all working and published to the staging repo with no issues. Generates the pom and swc files based on the current java version. Artifacts uploaded where I'm cleaning up the and as3 build files to share common components and reduce code duplications. Will check the updates into trunk shortly,code_debt,duplicated_code,c023d90e3ba48f6d77b27719aab5bc58c926f7db
thrift,1176,summary,Thrift compiler global leakage js,code_debt,low_quality_code,5860f8850e049a22e69022697a899958aa00b534
thrift,1176,description,Below I added the var qualifier on the declare field function. This is fairly trivial and fixes the global scope leak.,code_debt,low_quality_code,5860f8850e049a22e69022697a899958aa00b534
thrift,1199,summary,Union structs should have generated methods to test whether a specific field is currently set,test_debt,low_coverage,63c2d378c53ab7340466495b49451e68a7411c6c
thrift,1199,description,"For example, in the following union it would be nice to be able to do something like {{boolean test = as an alternative to {{boolean test = ==",code_debt,low_quality_code,63c2d378c53ab7340466495b49451e68a7411c6c
thrift,1202,comment_1,Could you reuse Xtruct and Xtruct2 instead of adding new definitions foo and foo2 to ThriftTest.thrift? and use something like: Thanks Roger,code_debt,low_quality_code,"9d8e8f87ed0648c48357c76bf6abccb6e2e964a6,a1c416fbbd46d632376bd47d5dca908904e8cba8,a1c416fbbd46d632376bd47d5dca908904e8cba8,0680a836344a4d3927131d26c1b38c2d7b379517"
thrift,1202,comment_2,"should solve the problem with maps, at least for i32 and strings... Cheers! ps: I removed the old foo struct test, this is already tested by ThriftTest.Xtruct anyway",code_debt,dead_code,"9d8e8f87ed0648c48357c76bf6abccb6e2e964a6,a1c416fbbd46d632376bd47d5dca908904e8cba8,a1c416fbbd46d632376bd47d5dca908904e8cba8,0680a836344a4d3927131d26c1b38c2d7b379517"
thrift,1217,description,"As part of an effort to use the Non-Blocking server (using libevent) on Windows, it was necessary to remove the use of ""pipe"" for the notification mechanism to signal end-of-task. We propose to use evutil_socketpair instead (tested with libevent 2.0.12). Patch included. Please see for more details.",design_debt,non-optimal_design,"30aae0ca877c9f5863ff881b29edc6a38df9d85a,266b1737a5ce7ecdafdf5274f66bd46315f7dfde"
thrift,1217,comment_1,"I really like to have Windows port within next Thrift release. The suggestion by [David and to use APR on Windows and do minimal changes on the current implementation is a key factor to bring this up and running. However, this patch depends on pthreads for windows and as mentioned on the dev mailing list or within Jira = Is APR feasible for your? Do you really need thet libevent version? Most GNU/Linux distro still provide 1.4.x and I would really like to keep capability to build and use thrift without lot of extra versions of libraries not within a standard distribution.",architecture_debt,using_obsolete_technology,"30aae0ca877c9f5863ff881b29edc6a38df9d85a,266b1737a5ce7ecdafdf5274f66bd46315f7dfde"
thrift,1217,comment_2,"@Roger: to avoid some confusion: - the attached patch does not depend on pthread. The suggestion here is merely to use evutil_socketpair instead of pipe. - evutil_socketpair is part of libevent 1.4 on ubuntu 10.10, although I did not test it. All the other changes related to win32 in the attached patch attached are not to take literally (Winsock2.h, static cast...). About THRIFT-1031: - my understanding is that THRIFT-1031 does not support async/libevent server on Windows (??) - libevent+thrift server seems VERY fast on win, and is needed for my project (blocking server won't do) - APR was not strictly necessary for the Non-Blocking server win32 port (i.e. although I tried the THRIFT-1031 patch and observed it made the port a bit cleaner, so I would not refrain from adding APR to thrift-C++ I decided after reviewing the win32 patches, it was better to open a separate issue, since the change here is atomic and should bare little consequences on linux. I was hoping that (naively) the win32 delta would get smaller by using the compatible call. All that said, I'd be really happy to contribute to THRIFT-1031 for it to go through. For this to happen, I would hope to have access to a shared implementation on top of 0.6.1 (the way I provided it on github), so I can test it regularly. Let me know if I can contribute somehow! As a conclusion, I think this patch is somewhat unrelated to THRIFT-1031, but it will help a future port of the NB server on win32 (assuming also someone will replace the pthread dependency by boost, which does not seem trivial at all).",test_debt,lack_of_tests,"30aae0ca877c9f5863ff881b29edc6a38df9d85a,266b1737a5ce7ecdafdf5274f66bd46315f7dfde"
thrift,1231,summary,Remove bogus include,code_debt,low_quality_code,fd39193aa00d2098184b452bd955bd60ae39f86d
thrift,1231,description,"TAsyncChannel.h includes TTransportUtils.h, but doesn't really need/use it.",code_debt,dead_code,fd39193aa00d2098184b452bd955bd60ae39f86d
thrift,1241,description,Patch is based mainly on but some more improvements: namespace can be specified with dots '.' like for every other language for example: namespace php com.onego.thrift would generate in php file namespace com\onego\thrift to generate php files with namespaces use: thrift --gen php:namespace53 example.thrift,code_debt,low_quality_code,de8d1857e8492f8d25abfb11a68ba9c90a49d99a
thrift,1241,comment_0,"Thanks Darius, initial work looks great. Will want to change from namespace53 to just namespace since php5.3+ will have this option available and php5.4 is in rc right now so no sense in defining a specific version in the option. Can you attach this as a patch with asf inclusion please.",design_debt,non-optimal_design,de8d1857e8492f8d25abfb11a68ba9c90a49d99a
thrift,1243,description,"[Context: This patch is part of a larger patch to use thriftnb on Windows/VisualC++. See for more details.] When compiling using Visual C++ 2010, the compiler chokes on casting some callbacks bool(*)() to void(*)(). Although probably valid and supported by gcc, this is further complicated by the fact those casting seem unnecessary: for each of the callbacks returning bool in TAsyncChannel.h: 1. the returned value is never checked 2. they always return true Attached is a trivial patch based on 0.7.0, tested on Ubuntu 11.04 and Visual C++ 2010.",design_debt,non-optimal_design,"285cfaa19943c168f7400424a8abde37da2e143f,08077bf9d8c6c212f5ff384c94423b6f76892358"
thrift,1243,comment_2,"Saw this a bit late but I'm concerned about this change. With this change, there is no way for implementors of to signal to the callers that something unexpected happened or any other error conditions. Just because the current implementation ignores the return values doesn't mean they're not useful. Is the recommendation now to throw a TException instead?",design_debt,non-optimal_design,"285cfaa19943c168f7400424a8abde37da2e143f,08077bf9d8c6c212f5ff384c94423b6f76892358"
thrift,1243,comment_3,"@diwaker: Thanks for your comments, I share your concerns and am working currently on a patch for 0.8 that will use exception handling, and more generally handle/propagate libevent errors, log errors... For example I'll propose to remove 'abort' calls in TEvhttpServer.cpp, and replace them by exceptions. Also one significant issue I plan to cover, is to avoid propagating exceptions to libevent (which is C code): see THRIFT-1222 for example.",design_debt,non-optimal_design,"285cfaa19943c168f7400424a8abde37da2e143f,08077bf9d8c6c212f5ff384c94423b6f76892358"
thrift,1243,comment_4,"Included is a patch on 0.8 that adds more error handling to the libevent based server. It also fixes 2 minor bugs (handle failure of evhttp_bind_socket and prevent exceptions to reach libevent in As far as I can tell, the question of @diwaker is still valid, there is no good way currently to have the callback of to be passed the exact failure. However I don't think returning a bool helps, because libevent is the one asynchronously calling, so the error will have to be handled by the client *within* the callback. This callback will invariably throw a when a disconnect or a 404 happens. I added to this patch the ability to log such failures to stderr, but it would take more changes (compiler?) to actually pass the error to the client, so that a call will throw the correct error (and *not* EOF). Finally I removed all abort calls, trying to replace them by appropriate exceptions instead. Thanks!",code_debt,low_quality_code,"285cfaa19943c168f7400424a8abde37da2e143f,08077bf9d8c6c212f5ff384c94423b6f76892358"
thrift,1248,summary,pointer subtraction in TMemoryBuffer relies on undefined behavior,design_debt,non-optimal_design,6077481139933b927397c7da0088aa4678f9fb3c
thrift,1248,description,"The ensureCanWrite function in TMemoryBuffer currently ""rebases"" the buffer pointers by subtracting the original buffer pointer from the pointer newly returned by realloc. While this seems to work fine on my linux setup(I couldn't force a reproducer), pointer subtraction between pointers that we allocated separately is an undefined operation. I have run into problems with this on platforms other than linux. I am attaching a patch that removes the undefined operation.",design_debt,non-optimal_design,6077481139933b927397c7da0088aa4678f9fb3c
thrift,1269,summary,thrift: handle undeclared exceptions in the async,design_debt,non-optimal_design,a94d2e5921b6bba6c0a677955edc310fb700be14
thrift,1269,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Wed, 6 Jan 2010 02:10:19 +0000 Subject: [PATCH 01/33] thrift: handle undeclared exceptions in the async error cob Summary: This updates the generated TAsyncProcessor code to correctly handle when an undeclared exception type is passed into an error callback. It now sends back a T_EXCEPTION message to the client, just like the non-async code does. Previously the exception wouldn't be handled, causing the TEventWorker thread to exit. Test Plan: Tested changing the async sort tutorial to pass in a generic TException rather than a SortError, and verified that the client correctly received an exception. Conflicts:  | 38 1 files changed, 32 insertions(+), 6 deletions(-)",design_debt,non-optimal_design,a94d2e5921b6bba6c0a677955edc310fb700be14
thrift,1275,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Thu, 4 Mar 2010 00:53:37 +0000 Subject: [PATCH 07/33] thrift: always prefix namespaces with "" ::"" Summary: Thrift always refers to namespaces using their full name. Therefore, it should prefix them with ""::"" to avoid accidentally matching a name defined in one of the current namespaces, rather than at the top-level. For example, if ServiceB is in namespace bar, and inherits from ServiceA in namespace foo, all code emitted for ServiceB now refers to ServiceA as ::foo::ServiceA instead of just foo::ServiceA. This allows the code to compile even if a namespace ::bar::foo also exists. An extra leading whitespace is also emitted, which is needed in cases when the name is used as the first template parameter, so that the emitted code contains ""< ::"" instead of ""<::"". Test Plan: jsong reported a build problem because of this name lookup error, and this change fixed his build. I also tested building [internal fb thing] and [internal fb thing], and they both built successfully. Tags: thrift  | 11 +++++++++-- 1 files changed, 9 insertions(+), 2 deletions(-)",code_debt,low_quality_code,0b7eeb59750fef5be018e6ec406da2da5be13c1b
thrift,1290,summary,thrift: TNonblockingServer: clean up state in the,code_debt,low_quality_code,37874ca8486cdce5a4b7f87c9c0b2fb3516aee5b
thrift,1290,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Tue, 6 Apr 2010 20:43:23 +0000 Subject: [PATCH 17/33] thrift: TNonblockingServer: clean up state in the destructor Summary: Implement the TNonblockingServer destructor, so that it closes the listen socket, destroys the event_base, and deletes any TConnections left in the connectionStack_. However, TNonblockingServer doesn't keep track of active TConnection objects, so those objects are still leaked. As part of this, I also changed the code to use event_init() rather than event_base_new(). This way we won't set the global event_base inside libevent, and we can be sure that no one else will be using it after the TNonblockingServer is destroyed. I grepped through all of [fb code base] to check for any other direct uses of event_set(), and didn't see any places that weren't also using event_base_set(). Therefore it seems like it should be safe to stop initializing the global event_base pointer. Test Plan: Tested with the test code in [a fb unittest], which creates, stops, and then deletes several Ran it under valgrind, and now it only complains about any active connections being leaked. Revert Plan: OK  | 4 ++-- 1 files changed, 2 insertions(+), 2 deletions(-)",code_debt,low_quality_code,37874ca8486cdce5a4b7f87c9c0b2fb3516aee5b
thrift,1290,comment_0,"Hm, this patch doesn't apply cleanly for me.",code_debt,low_quality_code,37874ca8486cdce5a4b7f87c9c0b2fb3516aee5b
thrift,1314,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Mon, 19 Apr 2010 19:09:16 +0000 Subject: [PATCH 25/33] thrift: add TProcessorFactory Summary: Adds a new TProcessorFactory class, and update all server classes to store a TProcessorFactory instead of a TProcessor. is called once for each newly accepted connection. This will allow a separate processor to be created for each connection, if desired. For now, everything always uses so all connections still end up using the same TProcessor. Some of the servers don't handle it well if getProcessor() throws an exception. TNonblockingServer may leak the fd and a TConnection object. TSimpleServer will exit and stop serving. However, this is no worse than the existing behavior if Test Plan: Ran the test code from [a fb unittest]. Revert Plan: OK Conflicts:  | 39 | 12 ++++++-- | 31 | 9 +++++- | 7 ++++- | 5 +++- 6 files changed, 89 insertions(+), 14 deletions(-)",code_debt,low_quality_code,6dd9cd0e3bb0bac0d4a70594956d035b75d4d7c8
thrift,1349,summary,Remove unnecessary print outs,code_debt,low_quality_code,4fb2706ecf74f533f71fa4ceab15db984fd13244
thrift,1349,description,There are a couple of spurious calls which can lead to lots of output when talking between slightly different versions of generated code. For instance launching a server which returns a new field to a client will result in the client printing out information about an unknown fid. This leads to lots of logging which you probably don't want. I'd like to remove these unless anyone in dev objects.,design_debt,non-optimal_design,4fb2706ecf74f533f71fa4ceab15db984fd13244
thrift,1393,summary,thrown from THttpClient contain superfluous slashes in the Exception message,code_debt,low_quality_code,711c70c786abf91dc784a002edc4bdab468f3d8e
thrift,1393,description,"This is a very minor issue, but should be addressed nonetheless. The THttpClient class ensures the $uri_ property has a slash prefixed by appending one if needed in the constructor. However in THttpClient::read, there are 2 exceptions thrown where a slash is concatenated between the port and uri. This results in a superfluous slash in the TTransportException message. Example: ""THttpClient: Could not read 184549154 bytes from",code_debt,low_quality_code,711c70c786abf91dc784a002edc4bdab468f3d8e
thrift,1431,comment_1,committed. Could you please create the patch from thrift source root directory? This makes it much easier to handle. If you have some spare time... we need a test suite for node.js THRIFT-1134 ;-),test_debt,low_coverage,289cbb2e4c3550caf1011548bda1996383100c7c
thrift,1440,comment_0,"Thanks for Reporting that, we like to apply your patches to improve packaging and build procedure in general. The Build Job for Debian Packages is here: currently we only hav a amd64 at Apache Build Infrastructure... Do some simply tricks or cross compile approaches exist to build more architectures on that build server?",design_debt,non-optimal_design,8516f58b77a7911c42561d7cc53024fbfab9cea9
thrift,1452,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Mon, 21 Jun 2010 20:24:50 +0000 Subject: [PATCH 3/5] generate a swap() method for all generated structs Summary: Andrei mentioned it would be convenient if thrift generated swap() methods for all C++ thrift types. Apparently the ads team manually writes swap() functions for many of their thrift data types, but have to keep updating them by hand when new fields are added to the thrift interface. This updates the thrift compiler to emit swap() methods for user-defined types. For now, I decided not to emit swap() methods for the internal XXX_args, XXX_pargs, XXX_result, and XXX_presult types. Test Plan: Tested compiling serveral internal projects. I didn't actually test the generated swap() functions, but they look okay. DiffCamp Revision: 124773 Reviewed By: aalexandre Commenters: dreiss, edhall CC: davidrecordon, achao, dreiss, kholst, aalexandre, simpkins, edhall, thrift-team@lists Revert Plan: OK git-svn-id: fb thing]@30392  | 70 1 files changed, 67 insertions(+), 3 deletions(-)",design_debt,non-optimal_design,"3318201c2f6d7dd6c9d10b120556bc9567184f18,e1d2458f1a84c1e975d8b73260324d7ca823bf75"
thrift,1452,comment_0,"Line 1514 of t_cpp_generator.cc in the function has an unused variable ttype, any reason for this or can it be removed? t_type *ttype =",code_debt,dead_code,"3318201c2f6d7dd6c9d10b120556bc9567184f18,e1d2458f1a84c1e975d8b73260324d7ca823bf75"
thrift,1452,comment_1,Nope no reason it looks unused in our codebase as well.,code_debt,dead_code,"3318201c2f6d7dd6c9d10b120556bc9567184f18,e1d2458f1a84c1e975d8b73260324d7ca823bf75"
thrift,1480,summary,"python: remove tabs, adjust whitespace and address PEP8 warnings",code_debt,low_quality_code,6972041392314d526584e733781ca382a960b295
thrift,1480,description,"The python library files have some inconsistencies (different indent levels and docstring placement) and the pep8 linter produces dozens of warnings. There are also several places where tabs are used instead of spaces, which is not good. This patch addresses almost all of the pep8 issues with as little modification of the code as possible. This patch: * converts 3 instances of tabs into the correct number of spaces * removes unnecessary trailing semicolons and backslashes * changes None comparisons to be identity based, 'x != None' becomes 'x is not None' in a handful of places * removes unnecessary '== True' in one if statement * wraps lines at 80 characters and removes trailing whitespace * corrects a handful of grammar problems in docstrings (mostly to help with 80 char line wrap) * converts all the docstrings to use """""" (instead of ''' or "") and makes placement consistent * fixes pep8 warnings about missing spaces around operators, e.g. (a-b) becomes (a - b) * adjusts ordering of stdlib imports to be alphabetical (could be better still) * correct internal indent depths of methods when they switch from 2 to 4 spaces There's a mix of files that use 4-spaces for indentation, versus the majority which use 2-spaces for indentation. This patch doesn't change that. I wanted to get the code as pep8 clean as possible and touch as few lines as possible to get it there. The TType constants defined in Thrift.py have some nice vertical whitespace that isn't pep8-happy, but it looked too clean to touch so I left it unchanged. After this patch, the pep8 utility only reports two warnings: # ""indentation is not a multiple of four"" for most files (no biggie) # ""multiple spaces before operator"" in Thrift.py for the TTypes class constants The unit tests all pass with this patch.",code_debt,low_quality_code,6972041392314d526584e733781ca382a960b295
thrift,1504,summary,Cocoa Generator should use local file imports for base Thrift headers,code_debt,low_quality_code,ba021466824299c6122e26b8850759f0a17314c0
thrift,1533,summary,Make TTransport should be Closeable,design_debt,non-optimal_design,c9f4a35c30cfff8c98ee767dbba0f7afe62997cf
thrift,1533,description,should implement the {{Closable}} interface. Doing so will allow users to perform,design_debt,non-optimal_design,c9f4a35c30cfff8c98ee767dbba0f7afe62997cf
thrift,1583,summary,c_glib leaks memory,code_debt,low_quality_code,c75797d9060e049692c5db1617aa9560aec939c8
thrift,1583,description,The generated code has memory leaks if you create and destroy an object. I've fixed the main problem on this git repo on branch c_glib_0.8.x but commits can be moved to trunk easily. There are still some errors to control on generated code (there are comments in the code). I'm planning to fix in a short term if you validate the current patches. This git repository has also the patches sent on issues 1582 and 1578 that are also related:,code_debt,low_quality_code,c75797d9060e049692c5db1617aa9560aec939c8
thrift,1595,description,The implementation of the java test server should be the same as the languages so our cross platform tests can work.,design_debt,non-optimal_design,f42ce2a8f49cf09e695974e6cd3c434b8dda61ab
thrift,1624,comment_2,"Here's a theory: maybe t_struct::is_union_ is just an uninitialized variable? The t_struct constructor doesn't initialize it. thrifty.yy line 724 will initialize it for struct/union declarations, but I'm guessing that codepath is skipped for service declarations.",code_debt,low_quality_code,d62032850caf9c76f5069e5c53d34e433b05da58
thrift,1624,comment_4,"Hey Bryan, would you consider merging this patch? Nathaniel never replied to confirm that it fixed his issue, but fixing an uninitialized variable couldn't hurt.",code_debt,low_quality_code,d62032850caf9c76f5069e5c53d34e433b05da58
thrift,1745,comment_0,"Frederic, Thank you for your work on this. Would you please attach the patch for this to the ticket. We can not accept gist or pull requests from github due to ASF licensing requirements. Once the test cases are added i'll gladly review and commit this patch.",test_debt,lack_of_tests,"85fb6de7f4c1ea6260f98bc24401593e8c974bc7,05ab89a1286049567e8d6ada1833a7d75179a365"
thrift,1799,description,"Improvements to HTML Generator * Removed HTML page name at <a href=""..."" * Option added to embed CSS inline instead of style.css. Both improvements are quite helpful in some situations, e.g. when the HTML is accessed in a way other than via the original file name, or when the separate CSS file cannot be provided along with the HTML for some reason.",design_debt,non-optimal_design,c880b44c5d81ade7ceb897fd55af94c0a8e5b31a
thrift,1799,comment_2,"Cleaned up patch, no other changes.",code_debt,low_quality_code,c880b44c5d81ade7ceb897fd55af94c0a8e5b31a
thrift,1800,comment_1,"Can you change test/DocTest.thrift to demonstrate the change that you made? I'm not able to get it to properly escape the characters (instead, it is adding additional bogus characters).",code_debt,low_quality_code,63e3c6307806f58a0325a1fe895e7c7f6b73d6f3
thrift,1810,description,"we need ruby as part of the cross language test suite, currently just a shell script (test/test.sh) running on Linux. references THRIFT-1766",test_debt,lack_of_tests,"c95d5dfb76631af655f2d905e5e514d5db6078d5,9aa08a9f6849f5c71aa3e7d1ba493521a11d026c"
thrift,1810,comment_2,initial patch without cross language test,test_debt,lack_of_tests,"c95d5dfb76631af655f2d905e5e514d5db6078d5,9aa08a9f6849f5c71aa3e7d1ba493521a11d026c"
thrift,1813,description,Why? # A lot of static analysis tools understand the annotation and treat those classes differently. # Setting the date field with the date & comments field with the version of the thrift compiler might prove to be valuable for tracing the origins of the generated file. The overheads should be minimal as the retention policy of the annotation is purely at a source level,code_debt,low_quality_code,1ee7bb645d1ca5b54198d77cdc9f0517e509cc39
thrift,1813,comment_3,"I rebased Arvind's patch, removed the duplication between autogen_comment and autogen_summary, and added a date as I previously suggested. GitHub pull request also available at:",code_debt,duplicated_code,1ee7bb645d1ca5b54198d77cdc9f0517e509cc39
thrift,1829,description,"If you attempt to build the cpp unit tests using 'make -j x' the build will fail as there's a race between the unit test code targets depending on the thrift-generated code targets. I think the real fix would be to use one of the approaches described in the 'Multiple Outputs' section of the automake manual (see I experimented with one of them and it seemed to help but never got it quite working. However an easier workaround is to simply disable parallel builds by using the "".NOTPARALLEL"" special target which forces make to run serially. from .NOTPARALLEL If .NOTPARALLEL is mentioned as a target, then this invocation of make will be run serially, even if the -j option is given. Any recursively invoked make command will still run recipes in parallel (unless its makefile also contains this target). Any prerequisites on this target are ignored. That's the approach I ended up taking as I'm short on time.",design_debt,non-optimal_design,"c095919087adc9508300ec6e9cdcd58cf147a207,81a1f996bc055c6833c829beaf9e5549db2a774d"
thrift,1842,summary,Memory leak with Pipes,code_debt,low_quality_code,b64a774b2fbfab034c0b7fff1641a46d8123d19f
thrift,1873,description,"The binary protocol factory takes strict read/write flags as (optional) arguments, but did in fact never use them.",code_debt,dead_code,5cb0d22a03c709ec9f581a615b9274ab765cea26
thrift,1924,summary,Delphi: Inconsistency in serialization of optional fields,design_debt,non-optimal_design,0f8acc5697d2ad251fccf469cad5141887626b2d
thrift,1932,description,"The Compilation of thrift-0.9.0 ended with the following warnings: * QA Notice: Package triggers severe warnings which indicate that it * may exhibit random runtime failures. * warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing] * warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing] When looking into ... 711 = 712 713 if == 4) { 714 // 0 length event indicates padding 715 if (*((uint32_t == 0) { 716 // T_DEBUG_L(1, ""Got padding""); 717 718 continue; 719 } ... it becomes obvious that the four values casted into a pointer were previously read from the input stream by ::read() in line 661: 661 = ::read(fd_, readBuff_, readBuffSize_); Same issue here: 725 readState_.event_ = new eventInfo(); 726 While it is a two minute job fix this problem, the method looks so fragile that a clean rewrite appears to be more appropriate.",code_debt,low_quality_code,7f1df992479fdcad208889e53b8b982e2428d250
thrift,1932,comment_1,"Hi Roger, here is the patch you requested. It does, however, only suppress the type-punned pointer warning. Since I'm new to Thrift, I'm quite unsure if the sequence of bytes read from file handle 'fd_' is already in host byte order. If this is not the case the patch is still wrong",design_debt,non-optimal_design,7f1df992479fdcad208889e53b8b982e2428d250
thrift,1982,description,"vsnprintf on Windows (visual) return -1 on failure instead of amount of bytes needed. so line like this: int need = STACK_BUF_SIZE, message, ap); won't work. Solution: ugly conditional compilation which will use microsoft specific functions... See patch for details.",code_debt,low_quality_code,d65216df190b0ff1522098c8a552594ce29feb3d
thrift,1999,description,"""unused arguments"" warning. suppressed by (void) x; in patch",code_debt,low_quality_code,cd54ec62492aa3f01c68910e5c942388d21c7379
thrift,2017,summary,Resource Leak in thrift struct under,code_debt,low_quality_code,09b97c78de58fea61b5dc90bd56095515bdd4f02
thrift,2017,description,"In file class t_program : public t_doc { 59 public: 60 path, std::string name) : 61 path_(path), 62 name_(name), 63 out_path_(""./""), 64 { 65 scope_ = new t_scope(); 66 } 67 68 path) : 69 path_(path), 70 out_path_(""./""), 71 { 72 name_ = program_name(path); 73 scope_ = new t_scope(); 74 } In Above code at line number 65 and 73 Resource leaks happens as 1. Allocating memory by calling ""new t_scope"". 2. Assigning: ""this-3. The constructor allocates field ""scope_"" of ""t_program"" but there is no destructor in the code. destructor should deallocate memroy for t_scope. which sometimes causes Resource Leak. Possible patch: There should be destructor to release the resources allocated dynamically. ~t_program() { delete scope_; scope_ = NULL; }",code_debt,low_quality_code,09b97c78de58fea61b5dc90bd56095515bdd4f02
thrift,2020,summary,Thrift library has some empty files that haven't really been deleted,code_debt,dead_code,d5f617f6a338fb61608f7a3f6659e05e980b3374
thrift,2020,description,"A lot of windows specific files have been ""removed"" recently, but git clone still creates empty versions of those files.",code_debt,low_quality_code,d5f617f6a338fb61608f7a3f6659e05e980b3374
thrift,2021,summary,Improve large binary protocol string performance,code_debt,slow_algorithm,fd64c15c4fa5ab092ecdda713bae142c05aafd72
thrift,2021,description,"Currently, TBinaryProtocol reads into a temporary buffer, then copies the results into the resulting std::string. For large strings (like a megabyte), the copy can be a performance issue. The attached patch reads directly into the std::string instead.",code_debt,slow_algorithm,fd64c15c4fa5ab092ecdda713bae142c05aafd72
thrift,2021,comment_2,"+1 Applies after fixing EOL style (win=>*nix), looks good to me.",code_debt,low_quality_code,fd64c15c4fa5ab092ecdda713bae142c05aafd72
thrift,2031,comment_0,"Ack... doesn't apply cleanly, because of other unsubmitted patches. I'll clean this up shortly.",code_debt,low_quality_code,19244ed87f7e6e8e997391e5ab7cb862c73449b2
thrift,2031,comment_1,"Seems outdated, the -1 has been replaced with in the meantime.",code_debt,low_quality_code,19244ed87f7e6e8e997391e5ab7cb862c73449b2
thrift,2032,summary,C# client leaks sockets/handles,design_debt,non-optimal_design,102bca47f754d9c9ed6ce341c7f8f106bd2719d7
thrift,2032,description,"The C# client code does not correctly clean up the transport used, so the programmer has to take care on his own about this. This may even lead to a program hang in certain scenarios. Furthermore, the generated client should support IDisposable. Note that in contrast, the server side handles this automatically without any explicit manual coding. TODO: * modify generated code to add IDisposable support * modify TProtocol to add IDisposable support * update the tutorial code accordingly",design_debt,non-optimal_design,102bca47f754d9c9ed6ce341c7f8f106bd2719d7
thrift,2032,comment_0,Patch ] does also fix a unnecessary double assignment to result2 in AcceptImpl().,code_debt,low_quality_code,102bca47f754d9c9ed6ce341c7f8f106bd2719d7
thrift,2032,comment_1,"Meanwhile I'm in doubt if IDisposable for the Iface is really such a good idea. To the class yes, but the Iface breaks Server code due to the missing method. The only reason for this was the using() use case, but maybe that's not enough of a reason here.",design_debt,non-optimal_design,102bca47f754d9c9ed6ce341c7f8f106bd2719d7
thrift,2032,comment_2,"Just to clarify, you are talking about the Client and the Processor; the IDisposable for the Client's Iface makes sense, but the IDisposable for the Processor's Iface doesn't. Unfortunately, they are the same Iface, so we need to find the correct middle ground. I'm much more in favor of having the Iface not be IDisposable, as it lowers the maintanence, with the client being explicitly IDisposable. I'm unsure of how we currently clean up state from the processor, but I don't think that IDispoable makes the most sense.",design_debt,non-optimal_design,102bca47f754d9c9ed6ce341c7f8f106bd2719d7
thrift,2032,comment_3,"Exactly. So lets drop the idisposable from the iface. Dont want to put too much effort in here, just need to address the leak.",code_debt,low_quality_code,102bca47f754d9c9ed6ce341c7f8f106bd2719d7
thrift,2124,comment_1,after moving test to subdir to allow for distclean to run missing following in tutorials folder csharp/tutorial.sln d/async_client.d d/client.d d/server.d erl/client.erl erl/client.sh erl/json_client.erl erl/README erl/server.erl erl/server.sh gen-html/index.html gen-html/style.css go/server.crt go/server.key hs/HaskellClient.hs hs/HaskellServer.hs java/build.xml js/build.xml js/src/Httpd.java js/tutorial.html ocaml/CalcClient.ml ocaml/CalcServer.ml ocaml/_oasis ocaml/README perl/PerlClient.pl perl/PerlServer.pl php/PhpClient.php php/PhpServer.php php/runserver.py shared.thrift tutorial.thrift,architecture_debt,violation_of_modularity,102c600bbb64b23679a3b143cdf9815733bdcb55
thrift,2210,description,"TSimpleJSONProtocol can emit JSON with maps whose keys are not string (which is not allowed is the JSON spec). This happens if the key in a map is anything other than a String (int, enum, etc) For example, it can emit JSON like this: which should be: I have a path that fixes this, I'll upload it shortly (still trying to get my dev environment to run the tests). Also AFAICT there is no unit test for TSimpleJSONProtocol -- I'll try and add one to the patch. Thanks! Alex",test_debt,lack_of_tests,38b453be5a015b7aaefcd91b4e261e53e0e211c2
thrift,2210,comment_8,Unfortunately the change caused the Hive object not able to be serialized into JSON with Here is the struct: struct SkewedInfo { 1: list<string 2: list<list<string 3: map<list<string} The field is defined as a map with list as the key. The structure can be serialized and deserialized correctly with libthrift 0.9.1. This change regressed the logic. People do use TSimpleJSONProtocol to serialize Hive objects for audit logs or such. This is a blocker for thrift upgrading. Could we remove the check for the case when the key is a 'list'? Thanks!,design_debt,non-optimal_design,38b453be5a015b7aaefcd91b4e261e53e0e211c2
thrift,2225,description,destroy the SSLContext before cleanup the openSSL to avoid recreating some openssl internal resource.,design_debt,non-optimal_design,301dfa94d6465244d5970e2abdc0650b386468d5
thrift,2227,summary,Thrift compiler generates spurious warnings with Xlint,code_debt,low_quality_code,4ccc24f6214f3041af5e564322382df1d84bf935
thrift,2246,description,"if enum is not set, ToString() returns a first enum value, but it should indicate that field is empty enum Distance { DISTANCE_1 = 0, DISTANCE_2 = 1, } struct RaceDetails { 1: optional Distance distance, } c#: new result: expected: )""",code_debt,low_quality_code,"088c26b40ccf747eaa5200727c9bacdc9288fb35,0ec155e1608c2909183b7c5e0b08a4a80579b4bd"
thrift,2246,comment_3,Some edge cases still produce warnings.,code_debt,low_quality_code,"088c26b40ccf747eaa5200727c9bacdc9288fb35,0ec155e1608c2909183b7c5e0b08a4a80579b4bd"
thrift,2263,description,Previously Thrift only generated high-quality hashCode implementations when configured due to the dependency on commons-lang3. After THRIFT-2260 we no long have this dependency and can unconditionally give the better implementation.,design_debt,non-optimal_design,acdac816659c88e7b8b601b4ad42dc43bf7d48e2
thrift,2279,description,"TSerializer copies from the transport into a 1024 length byte array and returns the array, which will effectively clips off everything after the first 1k. The attached patch instead returns a copy of the underlying memory buffer used by the transport.",design_debt,non-optimal_design,7949447efdcb2b355d3140a0d1a765e98a9a9e68
thrift,2285,comment_6,"I'm re-opening this issue to get some attention. Please close it again, if it's not important enough or I'm breaking some workflow with this. The change done to PHP's library doesn't do anything: the variable {{$ESCAPE_CHARS}} is not used anywhere. Instead json_encode and json_decode built-in functions are used. Now JAVA can understand PHP and this is good enough for now, but ideally, the JSON should be the same whatever language's library has generated it.",code_debt,dead_code,797595049274b9085385de5ef402075f6fc3de56
thrift,2285,comment_7,"Hi , feel free to submit a patch cleaning up the code. I have only modified things that already existed. If the stuff is not used at all, it should be removed. And it should probably be a new ticket.",code_debt,dead_code,797595049274b9085385de5ef402075f6fc3de56
thrift,2293,comment_4,"Hi, Thanks community for finding the fix for this. While testing and reviewing the patch, we noticed the createSSLContext() allows for the condition where both the keystore and truststore is set: if && { null); } For the case above where both the keystore and truststore parameters are set, the truststore file input stream should still leak (depend on GC) since it was not implicitly closed before the reference ""fin"" is reused for the keystore. We got around this by allocating a different file input stream reference for the keystore and truststore and closed both in the finally block. Best Regards, T.",code_debt,low_quality_code,53db7cc486a9f1f0e000d977d853a6811238a9e6
thrift,2328,summary,Java: eliminate all compiler warnings,code_debt,low_quality_code,"567df43e80b46bf8537875c1ac817c8f9af6277b,98d9ef2bd675e16dde9304061f71b6435caa5cf8"
thrift,2328,description,I don't like compiler warnings such as these: patches are welcome! -roger,code_debt,low_quality_code,"567df43e80b46bf8537875c1ac817c8f9af6277b,98d9ef2bd675e16dde9304061f71b6435caa5cf8"
thrift,2328,comment_0,committed fix removing lint check for unchecked casts. compile output,code_debt,low_quality_code,"567df43e80b46bf8537875c1ac817c8f9af6277b,98d9ef2bd675e16dde9304061f71b6435caa5cf8"
thrift,2328,comment_2,"@  we should keep this check: ""Some input files use unchecked or unsafe operations."" ;-r",code_debt,low_quality_code,"567df43e80b46bf8537875c1ac817c8f9af6277b,98d9ef2bd675e16dde9304061f71b6435caa5cf8"
thrift,2344,comment_1,"+1 ""--compiler-only"" is perhaps more readable? but it doesn't matter really Cheers!",code_debt,low_quality_code,"c92cda0d99820af884fb79c85ba9c738d0c7210f,3bf62d1d89de63ed3d4b9d14eb62e61d3143cbfe"
thrift,2344,comment_3,"should be --with-libs to match current convention, otherwise +1",code_debt,low_quality_code,"c92cda0d99820af884fb79c85ba9c738d0c7210f,3bf62d1d89de63ed3d4b9d14eb62e61d3143cbfe"
thrift,2351,description,"There are two typos in that prevent correct decoding of any message. They are on consecutive lines: First $seqId does not match case of argument $seqid so this is not set PHP is not case sensitive for class names but IS for variable names. Second and more importantly, on the following line $name has the decoded length assigned to it instead of $result. This means the method name ""hello"" ends up being decoded as ""6"" (the length of the string plus length prefix) and hence any processor trying to dispatch the event will fail. I can submit a patch/pull request but its only 6 chars to change and clearly there are no automated tests for this protocol implementation as that should have been caught before. I guess not many other people are using PHP as a thrift service processor with this protocol.",test_debt,low_coverage,47b89b9584d224a7e0053768b6c7c954eb42bf33
thrift,2351,comment_4,"no test case and *make cross* integration, we can close this if you like.",test_debt,lack_of_tests,47b89b9584d224a7e0053768b6c7c954eb42bf33
thrift,2375,comment_3,Hi Jens Thanks so much! Here is one more testcase where {{<a>}} tags are not processed correctly,code_debt,low_quality_code,e3ab0bf507d1f3b6e898df702869331ee8ac8e12
thrift,2404,summary,emit warning on (typically inefficient) list<byte>,code_debt,low_quality_code,6fe77e8e660139dbe7ad2b52e5ca3d0e5a0de7ca
thrift,2405,comment_0,"This patch repairs the Node.js Multiplex server implementation. The full node.js make check now passes. As far as I can tell from walking the code history, the client side Multiplex code never worked for multiple servers. The node.js code base dumps several pages of jsHint warnings right now. The three files attached here are now jsHint clean, but there's still more to do. It would be great if we could require new JavaScript code to jsHint clean and include working tests prior to commit.",code_debt,low_quality_code,7201c0d38ffb1505fdddcc9b65b16621f7e493c3
thrift,2415,description,"The performance of the named pipes Delphi server is sub-optimal. Furthermore, BYTE message modes should be used (instead of MESSAGE)",design_debt,non-optimal_design,e9651367c550a6dd72b5a67a3e5c487bd299eac8
thrift,2416,comment_1,"You could fix if for the platforms that you care about, and continue to let it error elsewhere. i386 and x64 are the main platforms to worry about, with ARM following behind that. A distant fourth would be IA64.",defect_debt,uncorrected_known_defects,f83c0e13b7db684b9cef2a50e1bcc0bfbcef0c9c
thrift,2416,comment_3,"The attached patch looks fine to me (though I haven't tested). +1. When the MSVC + ARM developers complain, they can put the appropriate code behind an _M_ARM guard.",test_debt,lack_of_tests,f83c0e13b7db684b9cef2a50e1bcc0bfbcef0c9c
thrift,2435,comment_1,"Proposed The idea is, to reference the enum types at these places always via full namespace to prevent ambiguities.",code_debt,low_quality_code,"d06957bf914a025fbf65d7625e1edc78c4bc11ef,cf18e91831507f20ac777262174288d64e3f5bbe"
thrift,2449,comment_1,"Hey Jens, I also think this is a good distinction to have manifest at the compiler level. Haven't tested the code but the patch looks good to me. +1 -Randy",test_debt,lack_of_tests,d000b241a43e50157938e056b9fa0f7d88f099df
thrift,2511,comment_0,This patch adds the compact protocol to the Node.js lib. It includes various ThriftTest tests in This patch also repairs a map of maps bug in the node.js test driver and repairs all jshint warnings in There are however still many (valid) jshint warnings in thrift compiler generated node.js code. Will create a new issue for that.,code_debt,low_quality_code,20aeba3e384f6590770dc3b4343e6d5dfcbd3ce1
thrift,2555,description,produces which starts to become annoying the larger the gaps between the numbers are.,design_debt,non-optimal_design,eb1e1d5567cabe4a4f7f58c380f651325cdba77e
thrift,2568,description,"Enhance the C# TLS-Transport with a paramater to give it a specific certificate handler function. This enables to easely accept untrusted certificates or accept a specific certificate, which can be useful while debuging.",design_debt,non-optimal_design,"7b11fec0c53b3231a472e008dfbb285d1aac44df,7b11fec0c53b3231a472e008dfbb285d1aac44df"
thrift,2589,description,Compiling a thrift file containing const definitions for basetype variables results in static properties {{public static whatsits}} being generated. Should generate const properties {{public const whatsits}}. Current version generates this from The code should instead look like this. A patch for this change is supplied. As i don't really know yet how the testing for these kind of changes is done i haven't supplied one. But generating all the .thrift files in test with a before and after version of the compiler and comparing the output of both looked good to me.,test_debt,lack_of_tests,c20eeaae8296fa9d9de34b07fe8b21cf509c8884
thrift,2590,comment_2,Patch that adds the three specified files to the project file. Please verify if it's working as i haven't installed the necessary dependencies to compile the c++ library so i couldn't really test it. But it should work.,test_debt,lack_of_tests,"9a4f40de1949e45d0b4e76f93ba737186b4a474d,41312c7e928ca9f29b1e132114c56c890a60870c"
thrift,2599,summary,Uncompileable Delphi code due to naming conflicts with IDL,code_debt,low_quality_code,12ee88170a30c025962513b993f8f2c421b5f08c
thrift,2605,comment_0,patch attached fixes both TSocket and TServerSocket also - removed mixed line endings in TSocket,code_debt,low_quality_code,3e50a9a1d01950f356242aaab0cbf5fae778b81c
thrift,2607,comment_0,just removed unused field,code_debt,dead_code,9a242c02f6bb1d6c851d75d38522527ebdfbc12c
thrift,2609,summary,TFileTransport.h unused field warning (clang 3.4),code_debt,dead_code,5f61d29a26b7d3988c0ca7ec3bbac160c0cb7511
thrift,2609,comment_0,"field used - this field could be removed, but by using it code is more consistent",code_debt,dead_code,5f61d29a26b7d3988c0ca7ec3bbac160c0cb7511
thrift,2636,description,"The class has ""type"" and ""message"" fields, but these are not exposed as GObject properties. Instead clients are expected to modify the object's member variables directly&mdash;a bad practice. The attached patch exposes the two fields as GObject properties (and adds relevant test cases), allowing clients to set and access these fields in a conventional manner.",design_debt,non-optimal_design,909f186fa6a68b0ec187aa6b0588ea2b899ea59b
thrift,2666,description,"The hash function introduced with THRIFT-2621 breaks with Python older than 3.2, because PYTHONHASHSEED has been introduced with 3.2. The proposed solution is to define a constant PYTHONHASHSEED as a workaround, but only if PYTHONHASHSEED is not available.",design_debt,non-optimal_design,d0bd17e7263cb8f92c21d3e1dad2ee5b5e9f79e5
thrift,2666,comment_0,Replaced {{PYTHONHASHSEED}} by a literal. Any better solution is welcome.,design_debt,non-optimal_design,d0bd17e7263cb8f92c21d3e1dad2ee5b5e9f79e5
thrift,2768,summary,Whitespace fixups,code_debt,low_quality_code,"d5436f5cf7a100d89abb3d125d8f241ca7dc925e,aaa8947f3a149d38dd4bfd395573860c2f18ea93,79f988c27ac97b7a89e6c78a64da2f3a2f65d916,196c5afbb1dcd9f815c055e76b3603bb9acccbb4"
thrift,2768,description,"Of course I had to make some content changes. The C# patch file is 354 KB, a lot of files are considered completely replaced. The Delphi patch file has ""only"" 282 KB. The ""content"" changes are (1) one indentation alignment in a batch file, and (2) one missing ASF header in a .cs file which I added. Anything else is just TABS, SPACEs and CRs. I'm going to commit that. For the records, here are the patch files.",code_debt,low_quality_code,"d5436f5cf7a100d89abb3d125d8f241ca7dc925e,aaa8947f3a149d38dd4bfd395573860c2f18ea93,79f988c27ac97b7a89e6c78a64da2f3a2f65d916,196c5afbb1dcd9f815c055e76b3603bb9acccbb4"
thrift,2781,comment_1,"The idlgen feature is not strictly required for Thrift. So as a workaround, the patch simply removes it from the build run.",design_debt,non-optimal_design,18502ee177e137d54dba520418d6e9208dc04ba7
thrift,2791,description,"There is currently no way in a go server to use buffered sockets. Failing to do so decreases performance significantly in my tests. I added an option on TServerSocket to set the buffer size to use. This will default to 1024 bytes, but can be disabled if desired to get back to the original behavior by setting BufferSize to 0. Github pull request: Patch",code_debt,slow_algorithm,cc15dff1274eebb8306e131530ef74e910f32ae9
thrift,2791,comment_5,"This is not the appropriate way to add buffering to the server. When you create your server, you can do something like transport, protocolFactory) and it will buffer the sockets it creates. After this patch, my code as above is creating a double layer of buffers, and that is not a good change. Can we please revert this patch and encourage creating buffered server sockets the correct way?",design_debt,non-optimal_design,cc15dff1274eebb8306e131530ef74e910f32ae9
thrift,2851,summary,Remove strange public Peek() from Go transports,code_debt,low_quality_code,1e7971cfff020be14bd30114f0dfe264797f259a
thrift,2851,description,"I've been seeing this public Peek() function in the GO library for a while, but still cannot figure out any sense to it. If there are useless, we can remove them right? This PR removes the public Peek() from the implemented transports. All tests still pass. PR:",code_debt,dead_code,1e7971cfff020be14bd30114f0dfe264797f259a
thrift,2851,comment_0,"It may not be the best way how it is implemented, but {{Peek()}} is a standard functionality for all Thrift transports. Throughout the library the core concepts and the implementations should be as consistent as possible. Even though {{Peek()}} is not listed (note that the list is marked as not exhaustive), IMHO making it better = @all: Other opinions welcome, though.",design_debt,non-optimal_design,1e7971cfff020be14bd30114f0dfe264797f259a
thrift,2851,comment_1,"Ok I see the point now. I stumbled upon it again, because in THRIFT-2502 the Peek function for iostream_transport got commented out. There seem to be no support from GO to provide such a function to most of the transport. IMHO Only barely used memory_buffer could get a real implementation. So the question is if we should carry along the Peek() function even though there will be no real implementation for quiet some time. Currently the implementation doesn't seem to follow a strict line. E.g. the transport interface doesn't carry a Peek method and not all transport implements the Peek function. I vote for remove, as we gain nothing, and appreciate the differences of having different language platforms with different abilities.",code_debt,low_quality_code,1e7971cfff020be14bd30114f0dfe264797f259a
thrift,2851,comment_2,"After sleeping a night about it, I think you are right. In its current form Peek() is not only next to useless, but more important misleading to the less careful observer. So unless someone comes up with a better implementation, we should throw it out. Committed.",code_debt,low_quality_code,1e7971cfff020be14bd30114f0dfe264797f259a
thrift,2868,description,The Go client doesn't do proper error checking. E.g. it doesn't check whether the received method name is correct nor if the message type has the expected value. The following PR enhances the Go client error handling by the following: - Check if method name is correct - - Check if MessageType is thrift.REPLY or EXCEPTION - - Checking the sequence id is done before checking the message type Includes test cases for every error case.,code_debt,low_quality_code,1f42d315759a725f9c2846b0996d03128e2f1887
thrift,2874,summary,"TBinaryProtocol member variable ""string_buf_"" is never used.",code_debt,dead_code,"37b7a0afe081c04402d011b652eca4ba39cdf69b,6794698a12b32d15e2bb7c627d813514c41be0b2"
thrift,2874,description,string_buf_ and string_buf_size_ are never used. Removing these will also resolve THRIFT-2465.,code_debt,dead_code,"37b7a0afe081c04402d011b652eca4ba39cdf69b,6794698a12b32d15e2bb7c627d813514c41be0b2"
thrift,2907,summary,'ntohll' macro redefined,code_debt,low_quality_code,38bf23404c8caad07766023d2861d1316be23080
thrift,2907,description,The {{ntohll}} macro is already defined in on Mac OS X.,code_debt,low_quality_code,38bf23404c8caad07766023d2861d1316be23080
thrift,2932,description,"I've been investigating using the Node.js client in a project however it seems like there are instances which don't follow Node.js best practices. In particular http_connection.js and connection.js throw errors during callbacks. This is considered an anti-pattern in Node because it both removes the Exception from the context of the callback making it hard to associate with a request as well as throwing it in the context of the EventEmitter code which can cause inconsistencies in the Node process. This means under some error conditions an uncaught exception would be thrown or at least an 'error' event on the singleton client (again removing it from the request context). Both transport receivers share the same copy-pasta code which contains: I'm working on a patch, but I'm curious about some of the history of the code. In particular the exception based loop flow control and the using the seqid to track the callback which makes it hard to properly associate it with exception handling.",design_debt,non-optimal_design,a7270074d31a25cd5e3965db7013446ac5d21c52
thrift,2932,comment_1,"Both of these libs predate my involvement in Thrift but when first exposed to them a couple years ago they were primordial and supporting only very limited functionality. The browser lib only did JSON/XHR/HTTP and Node only did Binary/Socket so they could not talk to each other (browser was/is tested against Java). No npm or bower support at that time either. The libs are now out on npm and bower (which is a mess due to the whole repo download thing). Compact and JSON protocols were added to Node. Many things that didn't work at all were fixed and lots of tests were added. Other than the tests (which were needed just to get things working more broadly) all of this work has been feature oriented and incremental. People tend to provide patches to fix or add things they need, not clean up stuff that is a mess (partly because it might break existing code, for example changing the call back interface in the browser to add error objects). So we have some baggage... That said, you are not alone. Many would like to see the kind of improvements mentioned here and in other tickets. There's still time to make a big push before Thrift v1.0. Patches welcome!",design_debt,non-optimal_design,a7270074d31a25cd5e3965db7013446ac5d21c52
thrift,2932,comment_3,"Hey Andrew, Thanks for the consolidated patch. Several nice improvements here. I am concerned about the new sequence id modification though. It propagates the sequencing data into the transport. This may seem reasonable in isolation but the transport should not be involved at this level of abstraction. In Apache Thrift, Protocols are responsible for the physical layout of bits (on the wire, on disk or in memory). This includes where, how and if sequence numbers are packaged. Transports are responsible for transmission of the data only. This may involve packetizing, framing, encrypting or what have you, but the key is that the Protocol and the Transport are isolated. The Transport receives/returns an opaque block of bits from/to the Protocol. The current node implementation is not clean in this regards and ultimately needs some structural refactoring. For example the multiplexing read implementation is a hack (paying no attention to the service name in the payload and rather wiring everything to seqid lookups). I think we should try to migrate things in the direction of the generalized Apache Thrift model. This is non-trivial due to the pure async approach of JavaScript (a late model option in the reference CPP and Java implementations). The heart of the node problem here is that Apache Thrift is a layered system. Lower layers provide services to upper layers and services do what you tell them, they dont call you with demands. The node implementation wires callbacks from the bottom up (with several call chains back down from proto to trans in the process). These conflicting approaches are merged with unsatisfactory results in my opinion. To respect the Apache Thrift layered approach the data event needs to be handled by the client (or by a much more focused connection helper) and this piece of code needs to then call down the layered stack like all other languages (client- In short, getting the node client side call return in order is a fair size chunk of work and should fall into a separate issue.",design_debt,non-optimal_design,a7270074d31a25cd5e3965db7013446ac5d21c52
thrift,2937,comment_1,I'm ok with this. However we should set it also to the same level(256MB) as we did for TNonblockingServer with THRIFT-1337,design_debt,non-optimal_design,0d964d8e520067c461f9dcef9f7654d43c8fba7f
thrift,2969,comment_0,"These patches together represent a major refactoring of the nodejs library testing code, eliminating most of the duplication. The testAll.sh script now tests every possible implementation. There is room for parallelizing these tests for speed, but that can be the subject of another patch. I'm trying to move as fast as possible to improve the nodejs code. The sooner this and my other patches can be reviewed the faster I can contribute more patches before we hit 1.0.0.",code_debt,duplicated_code,3b9ff4de6e38eb2e0e17eacd22865e4b8ce27e37
thrift,2969,comment_5,"Hey Andrew, Great patch. Big improvement. I had to remove a spurious require xtend from server.js but otherwise as submitted. -Randy",code_debt,low_quality_code,3b9ff4de6e38eb2e0e17eacd22865e4b8ce27e37
thrift,2972,summary,Missing backslash in,code_debt,low_quality_code,208738a6e34c7153908bad77210c6d194ea9d3ab
thrift,2972,description,It seems that I failed to escape a line ending in THRIFT-2910 fix. For some reason it doesn't break {{make check}} right now but only causes a bootstrap (or configure ?) warning and processor_test executable is missing because of this.,code_debt,low_quality_code,208738a6e34c7153908bad77210c6d194ea9d3ab
thrift,3027,summary,Go compiler does not ensure common initialisms have consistent case,design_debt,non-optimal_design,1d1bca2738febb87bf132d041a73cd8da5a6328b
thrift,3027,description,"In Go, as per words in names that are initialisms or acronyms should have a consistent case. For example, if you have a struct like: One would expect it to compile to: Rather than:- It would be pretty difficult to handle all cases of initialisms in the Go compiler of course, but there is a set of common initialisms that have been identified by the authors of Golint and could be handled relatively easily:-",design_debt,non-optimal_design,1d1bca2738febb87bf132d041a73cd8da5a6328b
thrift,3027,comment_1,"Hi , the patch introduces a new dependency to boost into the Thrift compiler. Can we avoid that? Have a look into there are some similar functions that may serve as model. Next, the {{to_upper_copy()}} function expects a locale, the default is {{std::locale()}} which is ... Not sure if that is what we want. The generated code should not depend on a particular system locale.",design_debt,non-optimal_design,1d1bca2738febb87bf132d041a73cd8da5a6328b
thrift,3040,comment_1,"Committed, thanks for the patch! Note, our Bower solution is a mess (cloning the entire thrift repo) and may change in the future. Also the src dir thrift.js may end up broken into multiple files, with thrift.js only being available in lib/js/dist after a grunt build. In the mean time this patch is a needed fix.",code_debt,complex_code,aad5de793097307e67be78a8d029d3170e229b91
thrift,3047,description,"indent_down was called one extra time which gave us an indentation level of -1, then to combat this, we indented the server implementation an extra level. This appeared as if everything was fine unless you generated two services in one thrift file, in which case the indentation would get progressively worse.",code_debt,low_quality_code,3b61971c8e8401e09919b3fb40b8e42bd27f9c71
thrift,3047,comment_1,I didn't include this in THRIFT-3041 because it made the generated sources exceedingly difficult to compare.,design_debt,non-optimal_design,3b61971c8e8401e09919b3fb40b8e42bd27f9c71
thrift,3088,summary,TThreadPoolServer with Sasl auth may leak CLOSE_WAIT socket,code_debt,low_quality_code,b1a35da9168cca5a7524ab9814161f024da145df
thrift,3088,description,"Start TThreadPoolServer to server with as transportFactory. While using nc to test the specified port whether reachable, it will leak CLOSE_WAIT socket.That's because nc will close socket at once while successful connect TThreadPoolServer, but the server still try using sasl protocol to build an inputTransport which of course failed at once. However inputTransport is null which makes it can't close socket properly which lead to CLOSE_WAIT socket.",code_debt,low_quality_code,b1a35da9168cca5a7524ab9814161f024da145df
thrift,3114,summary,Using local temp variables to not pollute the global table,code_debt,low_quality_code,811d279d581c7daffcee846492f5efca12fda3db
thrift,3114,description,"Should prefix the field deserialization statements with ""local"" whenever the output is a temporary variable, for example this compiler output: _elem46 = iprot:readI32() should be changed to: local _elem46 = iprot:readI32().",code_debt,low_quality_code,811d279d581c7daffcee846492f5efca12fda3db
thrift,3140,description,"After running *make check* for JS or *ant test* in {{lib/js/test}}, you can always find several stack traces in I don't know if it's even a bug since it does not cause any test failures. It distracted me from solving real cause of failures for a while through. Cause: Threading issue. Synchronization is needed because all instances of MimeUtil2 seems to share single state.",code_debt,multi-thread_correctness,fbc6977381a58ae018567492399c7ba8130d1b84
thrift,3157,description,Is there any reason this isn't the case? The wildcarding doesn't appear to give us anything extra. I've been told that it may help with uses of the Comparable. Can anyone think of a use?,design_debt,non-optimal_design,c2d4c77c5aec372bd4f265d6bff6a55c05c6e7c1
thrift,3191,comment_0,"Linking a few things together. Given the lack of test infrastructure for perl, I will be fixing these at the same time. I am updating ""make cross"" for THRIFT-3053 to support perl client and server, ssl or no ssl. Getting that working required me to solve THRIFT-3189 and THRIFT-3191 along the way.",test_debt,lack_of_tests,f5f1b35a7d1ce819bdfdc966741399605b051c92
thrift,3197,summary,keepAliveTime is hard coded as 60 sec in TThreadPoolServer,code_debt,low_quality_code,0b8132d20ea691c56f0fe973072a58086999a4d8
thrift,3197,description,"While creating ThreadPoolExecutor in TThreadPoolServer, keepAliveTime is hard coded as 60 sec. It should be",code_debt,low_quality_code,0b8132d20ea691c56f0fe973072a58086999a4d8
thrift,3241,comment_1,"It is by design, and is considered a production issue. 1. Run with X memory. 2. Max out the load, and observe memory use. 3. Use less memory the next run if there is significant unused memory, more if they are OOM'ing. 4. If you cannot raise memory, lower the concurrency per server. Steps 1-3 are completely automated in Google production systems and elsewhere. So in Thrift what's important is exposing the concurrency limits in all pool/async implementations. In Go LimitListener is useful.",design_debt,non-optimal_design,ca8469ec578b13524e387782e7ee72d4150ab542
thrift,3276,comment_1,"I haven't, but neither TBase64Utils.java nor TJSONProtocol.java have changed in any relevant way since 0.9.1. I suspect this was just never tested against a client that sends padded base64 strings.",test_debt,lack_of_tests,a175437f66fa1a0b36233e7dd40b061d471276ff
thrift,3280,summary,Initialize retry variables on construction,design_debt,non-optimal_design,5445e3fdee66a93e04d342bc2d5d1881427be885
thrift,3280,description,"Currently retry variables are only initialized after a connection has been successfully established. When the initial connection fails the retry logic is broken since the state has not been properly initialized. To solve this, we need to initialize the retry state before the initial connect() request.",design_debt,non-optimal_design,5445e3fdee66a93e04d342bc2d5d1881427be885
thrift,3283,summary,c_glib: Tutorial server always exits with warning,code_debt,low_quality_code,c76a9ecdd29ee7c542b276c975c2731687900224
thrift,3283,description,"When terminating the C (GLib) tutorial server with Ctrl-C, this message is output to the console: The server should instead exit quietly without reporting an issue.",code_debt,low_quality_code,c76a9ecdd29ee7c542b276c975c2731687900224
thrift,3364,description,"Ruby JSON protocol uses pack('m') method to encode Base64 string. It seems that it inserts a ""\n"" character every 60 characters. You can refer to these pages for this behavior. This has been making it impossible to send long binary field data to other languages. I fixed this by using alternative encode method that is added in Ruby 1.9 (which should be OK). After the fix, I had to add Ruby namespace to to avoid name collision of ""Base64"" symbols that is used for new encode method and also as DebugProtoTest message name. I also removed extraneous double quote in encoded binary fields that resulted in invalid JSON.",code_debt,low_quality_code,123258ba60facd8581d868c71a543487b2acff3c
thrift,3391,summary,Wrong bool formatting in test server,code_debt,low_quality_code,fa0796d33208eadafb6f42964c8ef29d7751bfc2
thrift,3391,description,The go test server printf()s bools with wrong formatting: while this is expected:,code_debt,low_quality_code,fa0796d33208eadafb6f42964c8ef29d7751bfc2
thrift,3409,description,There's at least 3 problems in NodeJS binary field. # API are inconsistent across binary/comact/JSON protocols # compact/JSON wire format is imcompatible with other languages (JSON : THRIFT-3200) # size of compact wire format is 2x of the original binary I propose following changes to fix this # Change JSON protocol return value from string to Buffer (same as binary/compact) # Change JSON protocol wire format to Base64 (same as other languages) # Change compact protocol wire format to plain binary (same as other languages),code_debt,low_quality_code,8a4d06febe8bc2e1bd84f955b1c2f0149665a0be
thrift,3413,comment_0,"The attached files and are enriched test cases, as there are some more issues.",test_debt,low_coverage,"86284da8495bcaeca9d9632374ada63cbf388ead,3aa617a48341341feb767280eb87da6ea3d05417"
thrift,3415,comment_1,I'll do some scans with clang's iwyu and clean a little bit around in THeader* related includes,code_debt,low_quality_code,517aa1491b1e16c88d17d6d83dcc7ef83bc85164
thrift,3416,description,"While poking into the IDL syntax, I found that hidden gem: While thinking whether I should do sth about it, I came to the conclusion that the time ist right to ""_Get rid of this_"" since today probably really ""_everyone is using the new hotness_"", as the comments say. Opinions? If there are no objections, I'll provide a patch to convert the warnings into an error and remove the rest of it.",code_debt,low_quality_code,"7388037fa17e1253b264bb0afea3c51aaa4b40d9,52de5cafbdec0feb9e0d16531b28f8f0654b8780"
thrift,3440,summary,Python make check takes too much time,code_debt,slow_algorithm,cacce2f1d503b7e98842308852237af53180fd87
thrift,3440,description,"It runs every combination of I want to reduce this keeping the same code coverage, so that we can run it more often.",test_debt,expensive_tests,cacce2f1d503b7e98842308852237af53180fd87
thrift,3495,summary,Minor enhancements and fixes for cross test,design_debt,non-optimal_design,9b35a7c021d06b9322e208e466a0f2aac1e95212
thrift,3495,description,"h4. fixes * fix python mapmap test * fix problematic regex compilation in connection retry logic in cross test runner (was compiling already-compiled regex object) h4. enhancement * add some C++ edage-case tests (unicode, double values, empty collections) * flush C++ client stdout so that we have better diagnostics when it crashed/hanged * add go binary test * add ruby unicode string test",code_debt,low_quality_code,9b35a7c021d06b9322e208e466a0f2aac1e95212
thrift,3535,description,The default Dart compiler behavior is to generate a new library for each thrift file. Add an argument to generate files and imports that are usable in an existing library. This produces a file structure like:,design_debt,non-optimal_design,217a44b9dcd3ae199571fe584cb13ad8528d6814
thrift,3559,summary,Fix awkward extra semi-colons with Cocoa container literals,code_debt,low_quality_code,4b7abedb0463fb55a15b389fb9f12b77cf5194f8
thrift,3559,description,Changes made in THRIFT-3545 introduce and awkward semi-colons and extra new lines.,code_debt,low_quality_code,4b7abedb0463fb55a15b389fb9f12b77cf5194f8
thrift,3572,comment_0,"I have a fix, though I think there must be a simpler one. In any case, it does amend the build; I'll make a pull request after testing with THRIFT-3573 to make sure nothing new pops up.",code_debt,complex_code,ccd998a04e8b82a56b1788aa13167e84aa2126d1
thrift,3596,summary,Better conformance to PEP8,code_debt,low_quality_code,10308cb975ac090584068d0470b81e41555b2f35
thrift,3596,description,"py coding_standards.md states it follows PEP8 but currently it does not do so at all. So a typical experience of a first-time (potential) py contributor would be to see red warnings all over the display and then has to either adjust their editor's settings or stop there. On the other hand, a huge downside of global reformat is git-blame experience but will be mostly mitigated by git blame -w in this case.",code_debt,low_quality_code,10308cb975ac090584068d0470b81e41555b2f35
thrift,3605,comment_2,"A few comments: - A few generators did not document all switches, some did not dcoument any switches at all. I fixed what I found. - The Python generator stood out form the crowd by being the least maintainable one. I tried hard to no break anything and to mimic the existing behaviour to the best of my knowledge, but it was a hard task. If, despite of my efforts, I still managed to overlooked some obscure edge case combination, please bear with me.",code_debt,complex_code,4733c4c53520259a002b9b173a7f5407499311c7
thrift,3617,comment_1,"Does not have much to do with the ticket itself, but is there any specific reason why we should not sort that list alphabetically? Makes it much easier to compare things (like forgotten targets). $0,02 JensG",code_debt,low_quality_code,41e8cbf6866bf9b8ec6d5bbc157a1fb970d8c3cc
thrift,3634,summary,Fix Python TSocket resource leak on connection failure,code_debt,low_quality_code,1c8b5cb1528d91be98c3652baade99e406417e5f
thrift,3634,description,Same as THRIFT-3615. It turned out that the problematic part of TSSLSocket originated from TSocket. I took this opportunity to remove the duplicate code.,code_debt,duplicated_code,1c8b5cb1528d91be98c3652baade99e406417e5f
thrift,3642,summary,Speed up cross test runner,test_debt,expensive_tests,59310f5dd065681db9dc2ab13fda289d8fa41922
thrift,3681,description,It was fragile against parallel make because it was erroneously invoking pub twice in a same directory.,code_debt,low_quality_code,f2952847e65c998b6f543f1cc63a4e173e10b66a
thrift,3731,summary,Perl multiplex test is flaky,test_debt,flaky_test,33331a32790726d78b50fa09d2b2f7238fc46f01
thrift,3733,description,The socket timeout handling has room for improvements,design_debt,non-optimal_design,"6f6aa8a4060e3e8a0c1250fc571da97da3e4f330,30ed90e0650e30734c9d728c2935d461671a0dc9"
thrift,3744,summary,The precision should be 17 (16 bits need after dot) after dot for double type.,code_debt,low_quality_code,7f6ea4e7fe6fc15955438e00335398424cf0fca4
thrift,3744,description,"The precision is lost when converting double to string. E.g: double PI = 3.1415926535897931; string value = format(""%.16g"", PI); The value will be '3.141592653589793' and last 1 is lost after format operation. But expected value should be Solution: string value = format(""%.17g"", PI);",code_debt,low_quality_code,7f6ea4e7fe6fc15955438e00335398424cf0fca4
thrift,3760,description,"* Perl: /usr/usr/local -* Python: /usr/local - perl fix for ""local"" part is frankly bad but at least it works. The patch also removes .pyo files from python package that should be generated by install process instead. This resolves most of lintian warnings.",design_debt,non-optimal_design,93bbdc8127cb6ebdd34350fbba1b3a0a4e8e4111
thrift,3839,summary,Performance issue with big message deserialization using php extension,code_debt,slow_algorithm,dd9885e3225180cc12cdfb7bfddc4b3cdbd405fe
thrift,3839,description,"I have found performance issue when tried to deserialize big thrift binary message with enabled thrift_protocol php extension. Messsage size was 10 mb and it took about 30 seconds to deserialize it. When i have done debug of php extension and php library i have found that issue is because small read buffer is used in TBufferedTransport and i cannot change it from method. So i have added parameter $buffer_size to function from php extension. And also this parameter i have added to method from php library. And i extended class by method putBack, so this class will be used for desearilization without TBufferedTransport warapper. After these changes it takes less than a second to deserizlize message with 10 mb size id read buffer 512 kb. Here is the pull request",code_debt,slow_algorithm,dd9885e3225180cc12cdfb7bfddc4b3cdbd405fe
thrift,3868,summary,Java struct equals should do identity check before field comparison,design_debt,non-optimal_design,d6bcb265bb45917ddefac155ae71cf17ea60f9bc
thrift,3868,description,"The identity check is cheap and should be done before comparing fields of a struct. Idiomatic equals methods always include this check especially if the field by field comparison can be expensive. Check to add: if(that == this) return true; 1864 out << indent() << ""public boolean equals("" << tstruct-1865 indent_up(); 1866 out << indent() << ""if (that == null)"" << endl << indent() << "" return false;"" << endl; INSERT IDENTITY CHECK HERE 1867 1868 const vector<t_field*1869 vector<t_field*1870 for (m_iter = members.begin(); m_iter != members.end(); ++m_iter) { 1871 out << endl; 1872",design_debt,non-optimal_design,d6bcb265bb45917ddefac155ae71cf17ea60f9bc
thrift,3905,description,"In Dart it is desirable to initialize bool (false), int (0), and double (0.0) required properties (those that are not marked optional), instead of leaving them with the value of null.",design_debt,non-optimal_design,aa4312ef5ff8ae4965cc779fe73d2375aba0c2dc
thrift,3907,description,"Previously we were able to reuse Docker layers from prebuilt images pulled from docker hub. This has been reducing total build time by 3 hours out of 7~8 hours total. After Docker 1.10 or so, it is no longer possible and we tend to easily saturate entire Apache's 30 jobs on Travis-CI. Standard solution as of now is to use docker save/load. This typically requires automated file upload on CI to some external storage. Unfortunately it cannot be done with our current Travis-CI account settings. To workaround this, we can put Dockerfile itself to Docker image and see if it's modified after the prebuild time and skip fresh builds if unchanged.",code_debt,slow_algorithm,"93fb7eadd093e561e3c7122dc9eb084ca033047d,ddc53c32486cc23dfa63ed4e5abb19923b8d13e6"
thrift,3944,summary,TSSLSocket has dead code in checkHandshake,code_debt,dead_code,bede86a032789ea9d8ed7a7d9c684d3fc86ade5c
thrift,3944,description,"There is a block of code in checkHandshake that attempts to set read/write memory bios to be nonblocking. This code doesn't do anything: Here's what this code looks like, and the problems: - creates a new memory BIO. Not sure why. - BIO_set_nbio() executes BIO_ctrl(..., BIO_C_SET_NBIO, ...). This errors out and return 0 because mem_ctrl does not have a case for BIO_C_SET_NBIO. See: - SSL_set_bio() sets the SSL* to use the memory BIOs. - SSL_set_fd() creates a socket BIO, sets the FD on it, and uses SSL_set_bio() to replace the memory BIOs. As far as I can tell, this block of code does nothing and will not change functionality. If there's a reason that it's there, it needs to be re-implemented.",code_debt,low_quality_code,bede86a032789ea9d8ed7a7d9c684d3fc86ade5c
thrift,4014,description,The meta data in AssemblyInfo.cs are inconsistent and do not reflect the ASF properly in all cases.,code_debt,low_quality_code,19066b75d014487d5ba6731910edd524aac6aaf6
thrift,4043,summary,thrift perl debian package is placing files in the wrong place,architecture_debt,violation_of_modularity,bd257f1b9058c1ea7fc12b0a312fe7fa3de86a7e
thrift,4069,summary,"All perl packages should have proper namespace, version syntax, and use proper thrift exceptions",code_debt,low_quality_code,177c37ce8516e21b8093fbd7c8047037f794a5f8
thrift,4069,description,"Currently our perl package module files contain multiple packages. We should break each package out to an individual file (or at least make sure everything is in the Thrift namespace) and properly version it. Package versioning was introduced in Perl 5.10 so: 1. Update the minimum required perl to 5.10. This is based on indicating that perl version object was added to perl in 5.10. 2. For each package use the {{perl MODULE VERSION}} perlmod syntax, where VERSION is {{v0.11.0}}. This is based on 3. Each module not under the Thrift namespace must be moved there TMessageType, TType). This will be a breaking change, but necessary for proper packaging of the library. Currently if you inspect the Perl PAUSE version metadata for Thrift's sub-modules only the 0.9.0 modules from gslin have version identities. For example if you look at Thrift and in the CPAN list of packages at you will see: There are some anomalies, for example packages defined in Thrift.pm come out at the top level namespace like: So technically if you do 'install I would expect you might get thrift. This is wrong and should be fixed. needs to be inside Thrift, not at the top level. Also we should pull in relevant changes from the patch in THRIFT-4059 around improving packaging. Also we should actually use TProtocolException and TTransportException instead of just TException everywhere.",architecture_debt,violation_of_modularity,177c37ce8516e21b8093fbd7c8047037f794a5f8
thrift,4129,summary,C++ TNonblockingServer fd leak when failing to dispatch new connections,code_debt,low_quality_code,75386db8c0eaba39ec5ad374cba27e039d2493e2
thrift,4129,description,"In THRIFT-2789 the error handling for connections where notify fails leaks a file descriptor. This was reported and fixed in a pull request without an Apache Jira entry: When failing to dispatch new connections to other IO threads other than the number 0, we returned these connections for reuse without closing them, so the corresponding fds were leaked forever. We should close these connections instead.",code_debt,low_quality_code,75386db8c0eaba39ec5ad374cba27e039d2493e2
thrift,4130,description,"There is a connection leak in the THttpClient when using the Apache HttpClient with the Without calling releaseConnection on the HttpPost object, the connections are never returned to the pool. Under heavy load, this can lead to both failures for subsequent calls to be able to get a connection from the pool and connections being held by the underlying OS, eventually resulting in the inability to grab another client port for outgoing connections. Per the Apache HttpClient ""In order to ensure correct deallocation of system resources the user MUST either fully consume the response content or abort request execution by calling This might have not been an issue when using the 3.x version of the HttpClient, but it's definitely an issue in the 4.x line. See for more details.",design_debt,non-optimal_design,6c08ac72c6ce9faf66bf28ee75cbb87413daa6aa
thrift,4136,summary,Align is_binary() method with is_string() to simplify those checks,code_debt,complex_code,971d077807d4497ff841519103577de52fbcd3a1
thrift,4136,description,Similar to {{is_string()}} the {{is_binary()}} method should be virtual and implemented at {{t_type}}. This simplifies the code and reduces possibilities for making technically wrong casts.,code_debt,complex_code,971d077807d4497ff841519103577de52fbcd3a1
thrift,4164,description,"In a project where thrift is used, i was investigating a core in an assertion in (pthread variety). The mutex in question was one of the locking mutexes that thrift gives to openssl. The core occurred in where the mutexes are destroyed (on the last line). I suspect that we might be changing the locking callbacks too early in the cleanup process; perhaps one of the other cleanup calls that follows it would have released a mutex in some situations? In any case, this needs to be investigated and I am assigning it to myself.",design_debt,non-optimal_design,7f5a8c28bc58011abef0cede10915c2071efbe41
thrift,4231,summary,TJSONProtocol throws unexpected on null strings,code_debt,low_quality_code,c55fdb95340417a4ba2dda41e9e872a4bcc63459
thrift,4231,description,"When a required (e.g. string) field is not set, the C# code may throw a non-Thrift nullptr exception, which at this point is a bit unexpected. Happened to me with TJSON but is in fact a problem of required fields not being checked properly in the generated struct.Write() code.",code_debt,low_quality_code,c55fdb95340417a4ba2dda41e9e872a4bcc63459
thrift,4245,description,"if p.transport.Write fails, p.buf will not be truncated, which leads to thrift client's memory increasing forever. Is it more reasonable to truncate p.buf when write to transport fails? here are my pull request, i'm new in github&jira, if more details are needed, please tell me, thx.",code_debt,low_quality_code,49e5103920e75a149d02c7d487496c8c79154a6c
thrift,4308,comment_2,"Switching to ld.gold exposed other issues (some good, some bad) and I'm not sure I can use it on everything yet.",design_debt,non-optimal_design,f338d576b715ddf1384eedbe301169eea74ea073
thrift,4316,summary,TByteBuffer.java will read too much data if a previous read returns fewer bytes than requested,code_debt,low_quality_code,d862c2fd7b379a52f0fe9e63220a785f4c2bcc20
thrift,4316,description,The TByteBuffer read() method is using the wrong length variable inside the processing loop and may read more than it should.,code_debt,low_quality_code,d862c2fd7b379a52f0fe9e63220a785f4c2bcc20
thrift,4362,summary,Missing size-check can lead to huge memory allocation,design_debt,non-optimal_design,a39ba7f2946c08fa59dd0928e9c608a70ca52529
thrift,4362,description,"In some cases the method size)}} gets called with a ""size"" parameter that has not been validated by the existing method size)}}. This is true if the method is called by of the same class. The method {{readString()}} checks the size correctly before calling size)}}. Since the methods size)}} and are public, there may be other callers who don't check the size correctly. We encountered this issue in production several times. Because of this we are currently using our own patched version of libthrift-0.9.3. The patch is attached, but it is surely not the best solution, because with this patch the size may be checked twice, depending on the caller.",design_debt,non-optimal_design,a39ba7f2946c08fa59dd0928e9c608a70ca52529
thrift,4362,comment_1,"Do you think I can submit this patch as-is or do you have any suggestions for improvement? Edit: As mentioned, with my patch the size may be checked multiple times. So I think it's a good idea to remove the other calls to the size-check method.",design_debt,non-optimal_design,a39ba7f2946c08fa59dd0928e9c608a70ca52529
thrift,4416,description,"The perl library has a few files and a shell script designed to automate the production of a perl package for CPAN. It mostly works, however: 1. CPAN cannot parse the package version references we use when we point to lib/Thrift.pm from other packages. This is fixed by adding a provides{} stanza to META.json. 2. Users of CPAN don't care about anything but obtaining the ""Thrift"" package that contains all the others, so identifying the others (like is not necessary. This is done by adding a provides{} stanza to META.json. 3. The CPAN package no longer needs the META.yml file, as META.json is sufficient. 4. These changes were made by hand to the 0.11.0 package before uploading to CPAN. This needs to be automated.",design_debt,non-optimal_design,"8101f00b0966deebd36a6ba658aa59d718453345,cea50758b9e5165b14508c3ded3834480e97f2b8"
thrift,4419,description,"Related to THRIFT-4390 Description copied form there: While working on improving test coverage and fixing busted cross tests I reworked the cpp test client to send binary in at size 0, 1, 2, 4, 6, 16, ..., 131072 and after 4096 the rust server gave up.",test_debt,low_coverage,b7084cbc0d377c59e045553add6f61215ed77854
thrift,4437,summary,JS WebSocket client callbacks invoked twice on parallel requests,design_debt,non-optimal_design,0a84eae1db28abb5e3ee730e8fa40a154c6e1097
thrift,4437,description,"When using a WebSocket Transport and doing two service calls immediately, without waiting for the first to return, e.g. like this: The callback to the first invocation is called twice, and the second never, i.e. console shows: instead of the expected I suspect this bug was introduced with the patch for where for some reason the callback registered twice when set:",design_debt,non-optimal_design,0a84eae1db28abb5e3ee730e8fa40a154c6e1097
thrift,4437,comment_1,"This issue would have been caught by the `ThriftWS` test suite, which is not enabled for some reason. I will add a commit to my PR that enables these tests.",test_debt,lack_of_tests,0a84eae1db28abb5e3ee730e8fa40a154c6e1097
thrift,4446,summary,JSONProtocol Base64 Encoding Trims Padding,design_debt,non-optimal_design,d066fa850a43859d829ff2f4a1678404cc44596d
thrift,4446,description,"In the C# and .NET Core libraries, the JSONProtocol's Binary Encoding to Base64 trims padding from the user provided byte arrays before encoding into Base64. This behavior is incorrect, as the user provided data should be encoded exactly as provided. Otherwise, data may be lost. Fixed by no longer trimming padding on encode. Padding must still be trimmed on decode, in accordance with the Base64 specification. For example: * Before this patch, encoding the byte array [0x01, 0x3d, 0x3d] yields [0x01] upon decode. This is incorrect, as I should decode the exact data that I encoded. * After this patch, it yields [0x01, 0x3d, 0x3d], as expected. I have submitted a pull request",design_debt,non-optimal_design,d066fa850a43859d829ff2f4a1678404cc44596d
thrift,4485,description,"If a read or write operation on pipes reaches the set timeout, the read/write operation is not properly cancelled. However, the overlapped struct gets freed when leaving the method, which essentially leaves the pending read or write operation with an undefined pointer. Easily reproducible with buffered transport over pipes, a combination that does not work at all anyways. The workaround for both problems is to not use buffered transport with pipes (use framed instead), and some sane tinemouts (not too short).",code_debt,low_quality_code,00645162ba1e73ea4fd6e7a47cecf910a29b3281
thrift,4546,comment_3,"Part of the reason that the thrift.apache.org site is so woefully under-maintained is that it uses a content manage system from the last century. I have to recommend that we move all our development related content to markdown files within the GitHub repository so that we have better control over it. We should be able to teach GitHub and/or the CI build systems that if the only changes are to a documentation directly, not to do a build.",architecture_debt,using_obsolete_technology,"0d8da22dba430c379de04ff48e507e7277f4ea21,858809fad01dba7318c33dc30f6cc92a6e2ac7b1,a139082755369611ed243376ec3c4161452369fc,106ea0bdd4458532e86717162a084bb4f874b353"
thrift,4559,summary,TSSLServerSocket incorrectly prints errors,code_debt,low_quality_code,"b33130f67964813169003fcbb4b7acc535082b5a,cd48acd11fdad7d450aad3fdf1cfde7fd9b86e9b"
thrift,4559,description,"Tested on both 0.11.0 and master. C++ Server, Python Client. SSL sockets. SSL works correctly and communication is successful, however when the client disconnects the server always prints the following message: {{Thrift: Tue Apr 17 15:43:36 2018 TConnectedClient died: SSL_read: error code: 0 (SSL_error_code = 5)}} {{Deeper diving shows that SSL_error_code 5 is SSL_ERROR_SYSCALL. Documentation says to check both errno and the SLL error stack, however upon inspection both return 0 (no error). I believe this message is printed incorrectly.}} Upon inspecting the code for handing SSL_read, it appears that reading is done in a while-loop, which if no error is found is broken out of. At some point a switch-case was added, but the single level of break statements remained, leaving non-errors to break out of the switch instead of the while. A potential fix can be seen here:",code_debt,low_quality_code,"b33130f67964813169003fcbb4b7acc535082b5a,cd48acd11fdad7d450aad3fdf1cfde7fd9b86e9b"
thrift,4604,description,When using the Node.js libs in the browser (via browser.js) Int64 isn't exposed the same way as it is in a Node environment. This just adds Int64 to the exports in browser.js to make consuming the libs more consistent with how we do in an actual Node environment.,code_debt,low_quality_code,"fdd735eec34d7ddd7a4a8821bc92a445c92db87c,6b03da72617f450bba33158616a598fb25898700"
thrift,4715,description,"`union` option for netcore generator was fixed in 0.12, but what it generates doesn't seem very user-friendly. Following thrift: Generates: Usage: Is there a reason for the `public abstract object Data`? If we get rid of that and instead generate a strongly-type getter we don't need to cast `Data`: I could have sworn it worked like that with the ""csharp"" generator in 0.11.0 but it generates the same now. Is the intended usage different from what I'm doing?",design_debt,non-optimal_design,"f5de98aab38d3460ee7339c47b1bcf3467e1fd9e,d5dc338f0c42e40e1b5151696548b07d6be6c353"
thrift,4745,summary,"warning C4305: 'initializing' : truncation from '""__int64' to 'long'",code_debt,low_quality_code,"53bd0e6295547e76ab00913cfd62f8d040bd996c,e2105426e3779202e450b4c89224b72e3752044e,593e1d6a3c0501cb550228411efb6ad1e0b942e5,b072f3d7c1490d6b036c32827ffee7f882334e89,d9f70cc5e719e793e5f233130c2332b6a3868297,0740421d4ebcc4e1e7fe8913d3ad748580a81ede,8afa9473c5916a142a2e51e1a7aedffef608b3b6,2fa751db3db3c9c6c11f7ab55bb99147ee0764c3,86ef177db97083bc8888ccf5059a46440d10651a"
thrift,4822,description,"Certain CTORs accept two boolean flags {{public SomeTransport( arg1, arg2, ..., bool useBufferedSockets = false, bool useFramedTransport = false)}} The only valid combinations here are in fact (false,false), (true,false), (false,true) - the forth combination does not make sense because framed by design already acts as a buffer. Not to mention, that multiple boolean arguments are usually less coder-friendly. Therefore, the parameterlist should be shortened to the more readable, maintainable and concise style like so (proposal):",design_debt,non-optimal_design,bf27637960aca6a6c3c5eb700a677e57b88b2409
thrift,4830,description,ITNOA I think it useful and have some performance benefits to generate to_string function for enum beside of operator<< overloading.,code_debt,slow_algorithm,"6a61dfabbf6ae2fa9fbbc3996590ebdbe38e569f,48e0069ad086013495483810f1bfbcd3e5e7fceb,a9a5571093498e704ffe298ba482253fb3f640dd,8dc4639ba1e82f9c1981b52d027e5419397d19a8,609a7fab21f67b950a0497ceb8478aa4473b8e7f"
thrift,4851,description,Remove all calls to and ensure that everything is sent through the logging system so that all logging goes to the same place.,code_debt,low_quality_code,cc13de4f87bda658d138b4f9b47f55202be4ac51
thrift,4857,summary,Java field hash code implementation inconsistent with equals.,code_debt,low_quality_code,"6b6a8279aba29a67f005f5e498e88519cdb85049,b261f3c0f114be31ef0f9a103dc4d2baa7c4fc3f,419e25b37b181c6add0693415370a71383755a65,7b06e31043d148b018232923a7223ea5d62cd749,419e25b37b181c6add0693415370a71383755a65,7b06e31043d148b018232923a7223ea5d62cd749"
thrift,4857,description,"The {{TField}} hash code implementation is inconsistent with equals, which is a breaking bug. If you know what hash codes are and are familiar with the Java {{Object}} API, then you already know what I'm talking about. Basically Java _requires_ that, if you overriden {{hashCode()}} and {{equals()}}, then for any two objects that are equal they _must_ return the same hash code. The {{TField}} class API contract isn't clear about what is considered equality, but according to the {{TField.equals()}} implementation, fields are equal if and only if: * Both objects are a {{TField}} (I'm generalizing here; there's another subtle bug lurking with class checking, but that's another story). * The fields both have the same {{type}} and {{id}}. In other words, fields are equal _without regard to name_. And this follows the overall Thrift architecture, in which field names are little more than window dressing, and the IDs carry the semantics. Unfortunately includes the name in the hash code calculation! This completely breaks the {{Object}} contract. It makes the hash code inconsistent with equality. To put it another way, two fields {{foo}} and {{bar}} could have the same type and ID, and {{foo.equals(bar)}} would return {{true}}, but they would be given different hash codes!! This is completely forbidden, and means that with this bug you cannot use a {{TField}} as the key in a map, for example, or even reliably keep a {{Set<TField This is simply broken as per the Java {{Object}} API contract.",code_debt,low_quality_code,"6b6a8279aba29a67f005f5e498e88519cdb85049,b261f3c0f114be31ef0f9a103dc4d2baa7c4fc3f,419e25b37b181c6add0693415370a71383755a65,7b06e31043d148b018232923a7223ea5d62cd749,419e25b37b181c6add0693415370a71383755a65,7b06e31043d148b018232923a7223ea5d62cd749"
thrift,4857,comment_3,"All right, I just created a [pull Let me know if I missed anything. The contribution instructions weren't clear whether the language indication (e.g. {{java}}) in the pull request should be uppercase or lowercase. I saw both in historical commits. As the official contribution instructions showed e.g. {{perl}}, I guess that you want to follow the token form used in the {{thrift --gen}} CLI, so I went with {{java}} rather than {{Java}} in the commit message. I have my own opinions about approaches to generating hash codes (doing it manually is tedious and error-prone), and I could quibble about the approach to checking for class equivalence. For example, the equality check compares exact classes, which the author apparently thought would be more efficient, but this will break if anyone ever subclasses {{TField}}. Either someone should make {{TField}} a {{final}} class, or use a normal {{instanceof}} in comparison. But these issues are ancillary to the core bug here, so I tried to make my change as surgical as possible, especially as this is my first contribution. Let me know what you think!",design_debt,non-optimal_design,"6b6a8279aba29a67f005f5e498e88519cdb85049,b261f3c0f114be31ef0f9a103dc4d2baa7c4fc3f,419e25b37b181c6add0693415370a71383755a65,7b06e31043d148b018232923a7223ea5d62cd749,419e25b37b181c6add0693415370a71383755a65,7b06e31043d148b018232923a7223ea5d62cd749"
thrift,4862,comment_0,"NB. Slight compilcation comes from the rather unfortunate (but documented) fact, that [Delphi generates RTTI info only for some specific types of Hence, in some cases the values can be printed as names, but in others we still have to print the numeric value.",design_debt,non-optimal_design,"8f7487e1086d8da6baff3376679436e526dd8fd0,85431d9c6a4695c5fbdeccc34e60de6c6ecf7225,225646b554e6c37de31657e11d907bf35d50679d"
thrift,4863,summary,better indication of WinHTTP errors,code_debt,low_quality_code,14a9a120a859c2e101bcd5f529693139bf9aef7a
thrift,4863,description,"A failing call to WinHTTP that leads to an NULL handle should be properly reported with the original error code. The NULL handle is reported, but thats only a symptom, not the real problem.",code_debt,low_quality_code,14a9a120a859c2e101bcd5f529693139bf9aef7a
thrift,4886,description,"The WinHTTP transport method {{CreateRequest()}} needs more detailed error information. Otherwise it is pretty hard to guess at which step exactly the problem arises. Additionally, the method should wrap any into Furthermore, the error codes retrieved via GetLastError should be enriched by a textual representation of the error code.",code_debt,low_quality_code,"19fdca82c2e61bd42f92a502a91a07b9dc74b5d2,433a649a36ce9c78793128391e8466e9e96a9482,62238d1a6c2746d9afa3900bf83d50896e5fed20"
thrift,4949,description,"I am currently improving the HTTP/1 test case. I found that the servlet 2.5 dependency package is relatively old, and users need to create the http environment themselves, so I added the embedded package of tomcat, Second, i improved the test case of the http application layer, so the client and server can communicate with each other and print the string.",design_debt,non-optimal_design,"6e4c581fddae9106c2c5a59c4d0bfbe6ad3e4560,af7d7633ea35cd5b36418fb5e5a8f6f98108509a,9c9962b3e9f2410a922ed01b232dca2ceb5d4d29,e529c8155d188514628af0c3a9d0f6e08a723ae9,c881b2479f9a8583ba79ff7f646b4d733bad25a8,19227423388364d09d931c02adade5d4bb151eb3"
thrift,5010,comment_0,"If it is wrong, we need a test case and should mark it as ""bug"". If it works and ""improvement"" is correct, then claim is wrong (""should do X"") because it obvuiously does that already.",test_debt,lack_of_tests,9d958a3caf5c82b25a00b0b6cd4ab431bc1fed18
thrift,5010,comment_2,"Test case reque RISC-based processor computer :) It small improverment to make avaible run code on other processor architectures, then x86 & ARM. Class avaible as extension for netstandard 2.0 in nuget package System.Memory.",test_debt,lack_of_tests,9d958a3caf5c82b25a00b0b6cd4ab431bc1fed18
