project,issue_number,issue_section,text,classification,indicator
camel,17,summary,Support Business Activity Monitoring (BAM),non_debt,-
camel,17,description,Provide a declarative way to correlate the multiple process actions related to business activity and generate monitoring events when pocessing constraints are violated.,non_debt,-
camel,17,comment_0,initial framework has been implemented.,non_debt,-
camel,35,summary,Provide a spring remoting implemenation to camel.,non_debt,-
camel,35,description,Provide a spring remoting implemenation that can invoke Camel PojoExchanges and expose Spring pojos as via PojoExchanges on the camel router.,non_debt,-
camel,35,comment_0,implemented.,non_debt,-
camel,52,summary,keep the HTML extracted from the website for debugging,non_debt,-
camel,52,description,"sometimes we get some strange stuff inserted into the generated docbook. e.g. the first part of contains some bogus stuff... Everything between <section> and <para> looks bogus (or at least generates strangeness in the PDF). I wonder if might help if we keep the HTML that is downloaded from the site so we can see, is this a wiki issue or XSL issue etc. As right now I've no idea! :)",documentation_debt,low_quality_documentation
camel,52,comment_0,refactored a bit the output source location. docbook source are now extracted by default to : wiki source are now extracted by default to : /docbkx/wiki-source rev: 550675,non_debt,-
camel,60,summary,support spring's ApplicationEvent model for sending or consuming events,non_debt,-
camel,60,description,None,non_debt,-
camel,60,comment_0,See the documentation here:,non_debt,-
camel,61,summary,provide the ability to move or delete a processed file,non_debt,-
camel,61,description,None,non_debt,-
camel,61,comment_0,See here for more details:,non_debt,-
camel,112,summary,JMSXGroupID is not copied across ActiveMQ endpoints,non_debt,-
camel,112,description,See thread:,non_debt,-
camel,112,comment_0,See test case,non_debt,-
camel,115,summary,Provide access to the Exchange in,non_debt,-
camel,115,description,I ran into a problem where I need to move some data from a Header into the body. It would be nice to supply the exchange to the type converter o this can be done in type conversion rather than as an explicit processor,non_debt,-
camel,115,comment_0,Need access to metadata about charset to use for encoding etc. So while working on a clever way of getting hold on the Exchange you could also think about how to get hold of other kind of metadata for the future.,non_debt,-
camel,115,comment_1,"This is important, scoping it for 1.4 release",non_debt,-
camel,115,comment_2,"It'd be nice to be able to pass in context parameters, like Exchange or Message as another parameter to the converter. e.g. public InputStream toStream(File file, Exchange context) {....}",non_debt,-
camel,115,comment_3,Hadrian see also the chat log for yesterday. James and I had a little chat about this one. But it would really be great for Camel 1.5 to get this one. We need this especially for byte converters that need encoding configuration.,non_debt,-
camel,115,comment_4,Closing all 1.5.0 issues,non_debt,-
camel,129,summary,Support preserving the original message QoS options when a message gets re-sent to a JMS destination,non_debt,-
camel,129,description,None,non_debt,-
camel,129,comment_0,The lack of support for doing this was first noticed on this thread:,non_debt,-
camel,129,comment_1,The JMS endpoint now supports a a option. This option is used by the endpoint producer when sending a message to JMS to determine if it should use the QoS options that are in the message that it is given when sending. example usage:,non_debt,-
camel,129,comment_2,fixed in trunk in rev 570851,non_debt,-
camel,149,summary,Add a thread(5) DSL method and assoicated ThreadProcessor,non_debt,-
camel,149,description,Brian McCallister had this great idea.. add a thread() method to the DSL instead of using the seda component to do async processing. We should be able to: or ThreadPoolExecutor pool = ...,design_debt,non-optimal_design
camel,149,comment_0,Fixed.,non_debt,-
camel,168,summary,ognl classes not bundled in snapshot,non_debt,-
camel,168,description,camel-ognl has a typo in its packaging. This causes the classes to be not included in the snapshot,documentation_debt,low_quality_documentation
camel,168,comment_0,Patch applied with thanks!,non_debt,-
camel,184,summary,support InOut with JMS so we can use JMS to do proxy and export of services (Spring Remoting),non_debt,-
camel,184,description,None,non_debt,-
camel,184,comment_0,"If response of jms server and consumer ist very fast, i get an error that correlationId isn't registered and the wait of response end with a timeout. I moved in JMSproducer.java the lines: long requestTimeout = FutureTask future = requestTimeout); before the send: new MessageCreator() { Now all works fine.",non_debt,-
camel,184,comment_1,great catch - reopening unti this is fixed,non_debt,-
camel,184,comment_2,"Patch appled Axel, many thanks!",non_debt,-
camel,184,comment_3,"Hi James, i have another problem wit jms INOUT messages. On some Client computers there are problems. When i send a request to the ActiveMQ server it will be shown in the log of the ActiveMQ, but my consumer (A AsyncProzessor with Spring) doesn't receive the message. In the class JmsConfiguration in method createInOutTemplate in case of inout message the explicitQosEnabled is set to true If i remove this line all work fine with all clients. Have you any idea? Regards Axel",non_debt,-
camel,201,summary,allow expressions such as XQuery to be used as a transformer,non_debt,-
camel,201,description,e.g. This basically means renaming the DSL's setOutBody() to be,non_debt,-
camel,201,comment_0,"I finally got around to doing this one up. There are no XQuery specific tests (mainly because of not knowing where to put them ;)), but it is generic enough to work for any expression language. Let me know if you have any questions!",test_debt,lack_of_tests
camel,201,comment_1,Johathan. The patch looks great. Only two issues with the ident of the code. One @override was not idented properly. And one method parameter was on a new line instead of singleline. Just nitpicking. Would love to have it documented on the wiki that we got this new transform DSL now. And we should remember to add it to the release notes that setOutBody() is depreacted and replaced with transform() And since setOutBody() is to be replaced with transform. Could we have a unit test that verifies a setOutBody() test that is done by transform render the same OUT body?,documentation_debt,outdated_documentation
camel,201,comment_2,"Hey Claus, Nitpick away :) I fixed up the indentations and added a test that verifies the deprecated setOutBody method still behaves. Yeah, the wiki will need to be updated once this feature gets in.",documentation_debt,outdated_documentation
camel,201,comment_3,"Patch applied. Excellent contribution, thanks Jon. I will leave this issue open until we update the documentation.",documentation_debt,outdated_documentation
camel,201,comment_4,"Updated the docs for this at: Unfortunately, it doesn't look as nice as it should because confluence is barfing all over the place with the following message: ""An error occurred: Connection refused. The system administrator has been notified.""",documentation_debt,low_quality_documentation
camel,201,comment_5,"Jonathan: btw there's some XQuery tests in where the XQuery implementation lives, for example",non_debt,-
camel,201,comment_6,"IIRC one of my issues for putting a spring transform test in camel-saxon was that the public camel xsd did not contain the transform element *yet* and so it failed to resolve. The camel-spring module has some magic in there to use a locally built xsd, so the transform element could be tested there fine. Since putting an XQuery test in camel-spring is a bad idea (circular dependency!), I opted for a test case using the ""simple"" expression language. Make sense?",test_debt,low_coverage
camel,201,comment_7,"Got it. BTW when running unit tests using the Spring XSD, Spring will use the XSD thats on the classpath by default. So the XSD will validate in camel-saxon as it'll use the latest/greatest XSD generated by camel-spring. The only time it won't validate is in your IDE when editing the XML :) But then you can fudge your IDE to take the XSD from your generated XSD in :)",non_debt,-
camel,201,comment_8,"Oh, neat! I could swear I hit the case where it wasn't using the locally built xsd though... perhaps I'm going crazy :) I was just gonna try it out again but it seems apache svn is down... grumble grumble...",non_debt,-
camel,201,comment_9,You've gotta make sure your project is using the latest/greatest built camel-spring jar though! Sometimes IDE based maven projects have a tendency to pick a different version :),non_debt,-
camel,201,comment_10,"Yeah, I know... because of this I still don't trust Eclipse based results when it comes to multi-module maven projects. When multiple modules are involved I always run the maven build from the command line just to be sure!",non_debt,-
camel,231,summary,Broken link on wiki page for Type Converter,documentation_debt,low_quality_documentation
camel,231,description,"I tried to figure out how i could edit the wiki page myself - i created an account but do not have edit rights, or I could not find the edit button. Type converts has a broken link: In the chapter ""Discovering Type Converts"" the link for @Converter is missing the h in http so the link is broken.",documentation_debt,low_quality_documentation
camel,231,comment_0,BTW see this FAQ entry on [how to edit the,non_debt,-
camel,231,comment_1,Fix applied - thanks Claus! I've also granted you confluence karma - so you should be able to edit the wiki now :),non_debt,-
camel,234,summary,support the auto-reloading of Camel routing rules from XML,non_debt,-
camel,234,description,If we support something like we could auto-detect if rules.xml changes and auto-reload 'em so that folks could dynamically change the routing rules,non_debt,-
camel,234,comment_0,One solution to the problem is just to reuse the [ServiceMix 4 which supports auto-reloading of OSGi bundles in jar or expanded form. We could make the Camel distro come with theServiceMix Runtime so Camel rules can be easily hot deployed and redeployed as folks change the Java / Spring XML,non_debt,-
camel,234,comment_1,"hey, this feature would be a great benefit for all the people using activemq as message broker and want to use the eip without restarting a core component or running another java service. in my case i could integrate activemq into a rails application and do not care about activemq being java, cause i can configure eip's in xml, update the rules.xml in the lifecycle of my application deployments and have just on running, reliable service. that would be nice! greets, jochen",non_debt,-
camel,234,comment_2,So using OSGi (e.g. the ServiceMix Kernel) this works already today. Also the [Web Console for now has a highly RESTful API along with a web UI so you can routes restfully - via your browser or scripted.,non_debt,-
camel,234,comment_3,Yeah I think we should let containers handle this and not Camel itself. Isnt jetty:run capable of reloading web apps if you change files?,non_debt,-
camel,234,comment_4,"Since Camel is not designed to be a server by itself (it relies on a container), I think the container should take care of that. ServiceMix does this pretty well.",non_debt,-
camel,234,comment_5,"Yeah this is out of scope for Camel itself. People can update routes from XML files using the API on CamelContext. We have an example in the cookbook documentation page. A server such as Tomcat, Jetty, ServiceMix which can hot-deploy an app is the better choice. A Camel route could have updated java code, reference to other beans, and whatnot, that Camel itself cannot update on-the-fly.",non_debt,-
camel,251,summary,Misc. RemoteFileProducer improvements,non_debt,-
camel,251,description,"displays the file instead of directory (a boolean) automagically attempt to reconnect if from their destinations. This should probably be shared with Consumers, and could maybe done more cleanly using camel's try/catch features, but there you go. Smarter directory/file building/handling. This lets you handle URI's like similarly, avoid putting files in '/' on servers that expose the full filesystem, etc. More verbose logging. Stuff like ""what file went where,"" reconnection attempts, etc.",code_debt,low_quality_code
camel,251,comment_0,Re-attached patch w/ ASF license granted (it'd be nice if this was an option when creating a JIRA issue),non_debt,-
camel,251,comment_1,"D'oh! That patch completely broke FtpConsumer and SftpConsumer, and a few unit tests. I'll have something that fixes those shortly.",non_debt,-
camel,251,comment_2,Reconnect logic was added to was re-written to more thoroughly check defaults and file/path parsing.,non_debt,-
camel,251,comment_3,"Patch applied. Excellent work, thanks Aaron!",non_debt,-
camel,284,summary,add a JMX wiki page for how to work with JMX with Camel,non_debt,-
camel,284,description,Maybe with a screen shot of jconsole in action etc. e.g. like the ActiveMQ page...,non_debt,-
camel,284,comment_0,CAMEL-508 introduces a new feature for JMX - a system property to alter behavior of which MBeanServer to use,non_debt,-
camel,284,comment_1,Added a wiki page,non_debt,-
camel,284,comment_2,This is a really cool jmx page you have added Willem. However the jconsole image could use to be cropped so its size isn't so large.,non_debt,-
camel,284,comment_3,done. It may need to wait a while to sync the wiki page.,non_debt,-
camel,302,summary,Added support for the management of the Http headers,non_debt,-
camel,302,description,"The HttpProducer checks if in the map of the message there is the property named This property must contain a map of String, that will be added to the header of the message Http. Then, in a component of Camel it will be possible to write: public void process(Exchange exchange) throws Exception { Map<String,String headerHttpMap); ""testAction""); }",non_debt,-
camel,302,comment_0,This issue is very similar to CAMEL-254 we already have. The question is if we prefer just Camel headers as HTTP headers or your approach (specific Camel header holding HTTP headers map). My choice would be the first one (described in CAMEL-254) as we can easily use DSL to set those headers.,non_debt,-
camel,302,comment_1,"Could you look at the solution I made in CAMEL-254 and say if it is OK for you? We could have also your solution implemented (as by default in CAMEL-254 I skip all headers that start with 'org.apache.camel') but I think it could be too much ;) If it works for you, then lets close the issue.",code_debt,low_quality_code
camel,302,comment_2,In CAMEL-254 you add all headers except in the HTTP Header. In this way it is more difficulty to exclude some headers. It is better perhaps to use a special map for these headings. Does thing think of it?,code_debt,low_quality_code
camel,302,comment_3,HttpBinding has features to evaulate if a header should be included or not. Also you can set a list of headers to ignore etc. Donatello please see if this is not sufficient for your needs.,non_debt,-
camel,302,comment_4,Roman we might need a for end users to easier adding their own headers to ignore instead of clearing the already default ignored ones.,design_debt,non-optimal_design
camel,302,comment_5,And we would love a java snippet example how to exclude custom headers on the wiki component page ;),non_debt,-
camel,302,comment_6,"Hi Claus. I have given a glance to the HttpBinding and I believe whether to add a list of headers to ignore is not a bad idea. But I think that perhaps the choice to say what to ignore is not winning in many cases. For example, in my case I use the map of the message to exchange about ten information among a component to the other, while I would be wanting to send as header http only ""SOAPAction."" Therefore instead of saying what to exclude is it would be better to say what to include is. Does thing think of it?",non_debt,-
camel,302,comment_7,Donatello are ten information in your component specific to this component? Have you considered to store them on properties on the exchange itself and not on the in message itself. Properties on the exchange are ment for long lasting information. See RomKals reponse in the thread: However your request for a reverse strattegy is a valid use-case and a good idea to include in Camel if that really helps your situation. So something like:,non_debt,-
camel,302,comment_8,I dont think this use case is needed. Anyone have any thoughts of this one?,non_debt,-
camel,302,comment_9,"Please comment if you have any updates to this one, but currently we dont plan to do anything about this for camel 1.5",non_debt,-
camel,302,comment_10,Closing old tickets,non_debt,-
camel,323,summary,spring-2.5.1.jar is twice in the camel distro,non_debt,-
camel,323,description,I downloaded the latest snapshot from The .zip file contains spring-2.5.1.jar twice \camel\lib \camel\lib\optional Is Spring both mandatory and optional? And btw it includes an old version of v1.1 of commons logging is out.,non_debt,-
camel,323,comment_0,"Is there any reason we have two of these? If not, I say we jettison the optional copy.",non_debt,-
camel,323,comment_1,Patch applied with thanks!,non_debt,-
camel,345,summary,get log4j exclude working in camel-core pom test-jar,non_debt,-
camel,345,description,None,non_debt,-
camel,345,comment_0,Applied the patch with thanks to Jonathan.,non_debt,-
camel,345,comment_1,Closed all 1.3 tickets,non_debt,-
camel,353,summary,Distribution assemblies generated from 1.3.0-RC2 tag are missing,non_debt,-
camel,353,description,"The Distributions are missing this core tests jar. THe spring tests jar is included, but depends in turn of the core tests jar. This is due to the core tests dependency being set with test scope. Removing the scoping includes it correctly. A patch is attached. rgs, ste",non_debt,-
camel,353,comment_0,"Hi stephen, Can you grand the ASF licence for you patch? Or I can't apply it . Thanks, Willem",non_debt,-
camel,353,comment_1,"What do you need for me to do? Is it enough for me to grant it here? If so, duly granted! If you need a new patch with some extras just shout. thx ste",non_debt,-
camel,353,comment_2,Didn't see the selector the first time round. Good now.,non_debt,-
camel,353,comment_3,conformed the issue and applied the patch.,non_debt,-
camel,353,comment_4,Closed all 1.3 tickets,non_debt,-
camel,383,summary,Exchange.getOut() not returning (blocking till timeout) when used as well as message headers and exchange properties not passed when a Camel Mina used.,non_debt,-
camel,383,description,"Exchange.getOut() not returning (blocking till timeout) when used. Additionally, message properties and exchange headers does not seem to be passed when using MINA TCP communication. In direct pojo or bean, everything seems working. I have attached the classes needed to resolve the issue. No modifications are made to interfaces (method signatures). Classes affected are: new class introduced: Junit test attached:",non_debt,-
camel,383,comment_0,"George, thanks for the code. I am currently looking into it now.",non_debt,-
camel,383,comment_1,A patch with the following changes: - copyright headers for missing files - removed an unused import - new feature to transfer exchange objects using TCP protocol see how to use it The new feature only works for the TCP protocol and for the default codec (= object). So do not use textline=true as option.,code_debt,low_quality_code
camel,383,comment_2,I think this ticket should be changed from bug to new feature. Also anyone got a better name for the option (transferExchange)? I considered the option bodyOnly also but then I needed to make sure the defaults would be bodyOnly=true even when the parameter is not specified.,code_debt,low_quality_code
camel,383,comment_3,TODO: Remember to update wiki component documentation with the new option if patch is accepted.,documentation_debt,outdated_documentation
camel,383,comment_4,"Hi Claus, I just committed your patch. Here is one issue for the since we do not mashal or unmashal the exchange's exchangeId, so I comment out the process assertion for the exchangeId to pass the test . BTW, the document for the transferExchange properties is highly demanded, and I like to use the transferExchange as the parameter name since it is more clear for the user :) Regards, Willem.",test_debt,lack_of_tests
camel,383,comment_5,"Hi Willem I am on vacation at my parents at the time being and there I have not access to the net. So its a bit challenge to work on Camel using maven in offline mode. I decided in last minute to remove the transfer of exchangeid, but I forgot to remove it in the unit tests that still could pass if the generated ids are the same. Today I have access to the net for a few hours so I am trying to catch up on the mail and the camel commits to merge with a few changes in the mina component I have improved during the last 3 days. I will update the wiki documents on sunday when I am back home. I am also working on providing a camel-mina-example sample for the distribution.",documentation_debt,outdated_documentation
camel,383,comment_6,Basic documentation added to the mina component on the wiki.,non_debt,-
camel,383,comment_7,Closed all 1.3 tickets,non_debt,-
camel,390,summary,"if folks overload xmlns to use camel, then define beans with <bean ...> Camel should generate an error with a useful error message",non_debt,-
camel,390,description,See this thread...,non_debt,-
camel,390,comment_0,"I am not sure this is a valid issue yet. I suspect the xml is not correct but I only have briefly looked into it. If I understand correctly this is related to the use of refs. It's ok for the xml doc to have one default namespace and than one element (<camelContext But then again, I may have misunderstood the issue completely. I'll look into it soon though.",non_debt,-
camel,390,comment_1,I think this bug was raised to try help folks figure out issues caused by the xmlns being redeclared. e.g. something like this I can't remember if the xmlns redeclaration of the default namespace on the <camelContext/> disappears again after the </camelContext>. Does something like the above work? If not can we generate some kinda useful error message somehow?,non_debt,-
camel,390,comment_2,Isn't doing exactly this successfully? The only difference is the missing. And there is obviously no element with the id:,non_debt,-
camel,390,comment_3,you're right. I wonder if I was thinking of nested <bean lets just close this issue for now :),non_debt,-
camel,390,comment_4,not an issue AFAIK,non_debt,-
camel,390,comment_5,Closing all 1.5.0 issues,non_debt,-
camel,457,summary,Camel should raise an exception if parsing of xslt file fails,non_debt,-
camel,457,description,"Consider a Camel route that involves an xslt component. When Camel sets up the route, the XsltBuilder ignores any errors coming from the xerces xml parser and continues constructing the route even though the xslt transformer is not initialized. Later when the route is executed Camel correctly checks if the transformer is initialized and raises an error if not: No transformer configured! However in case of a parse error in the xslt file, the whole route becomes unusable and the setup routine that constructs the route should fail with an appropriate error message. A sanity check needs to be added in method",non_debt,-
camel,457,comment_0,JUnit testcase,non_debt,-
camel,457,comment_1,JUnit testcase. See included README.txt,non_debt,-
camel,457,comment_2,proposed patch,non_debt,-
camel,457,comment_3,"Torsten, thanks a lot for the patch",non_debt,-
camel,457,comment_4,Torsten the bug must be a ServiceMix only. In pure Camel I do get a compile stylesheet error during creating of the routes. Using this simple unit route builder where invalid.xsl is your xsl file with no changes.,non_debt,-
camel,457,comment_5,Applied the patch to trunk with thanks to Torsten,non_debt,-
camel,457,comment_6,Closing 1.4 issues,non_debt,-
camel,478,summary,allow any Camel Endpoint to be used as a Spring Integration so folks can use the Spring Integration bean approach with Camel components,non_debt,-
camel,478,description,"e.g. folks use SI annotations on their beans, but inject Camel endpoints, adapted to the SI APIs",non_debt,-
camel,478,comment_0,"The patch is in the trunk now, so we need to add some document to show how to use the CamelSourceAdpater and CamelTargetAdpater.",documentation_debt,low_quality_documentation
camel,493,summary,camel-mail add support for NNTP protocol,non_debt,-
camel,493,description,The camel mail component registers nntp as a supported protocol but there is no javacode or documentation how to use it. It is currently not support out-of-the-box in Camel. This feature has been requested by an end-user by private mail correspondence.,documentation_debt,outdated_documentation
camel,493,comment_0,Is somebody working on this issue or is it planned an nntp protocol support? It will be good to know since we like to add this feature but don't like to duplicate work efforts.,non_debt,-
camel,493,comment_1,Sebastian. Nobody has started working on this yet. However the person from the private mail said he would like to give it a go and maybe contribute it back to Camel. We love contributions. And if you supply a patch granted Apache license rights then I has a very high chance that it will be accepted and committed. I took a quick glimpse to look for a good NNTP open source library to use under the covers and only found Apache Commons Net. Is there a better option or is it the best candidate to use? What feature should the NNTP support have? What are your requirements Sebastian? And btw voting for the tickets will give people a better chance to work on the more popular ones than not.,non_debt,-
camel,493,comment_2,"Claus, I am aware of: - Apache Commons Net - Topcoders nntp client: - JScape: But I think apache commons is the way to go. IMHO the Camel component basic features must include: - Caching: Retrieving only new articles from server and save them locally - Stateful: For each routing configuration it can remember the messages already received. I don't know if it's too ventured to say this, but I think search engines like Nutch must migrate to an ESB/EIP concept, so if you want to build an nntp search engine you just connect your nntp connector to a system indexer.",non_debt,-
camel,493,comment_3,"Sorry I dont see currently a demand for this ticket. We love contributions so if anyone in the community want to step up and create such a component, then that would be great.",non_debt,-
camel,516,summary,Camel routes only work when log level is set to DEBUG,non_debt,-
camel,516,description,"In the file if I don't set the log level to DEBUG I get SAXParseExceptions. For example, if I leave the log level to INFO as specified",non_debt,-
camel,516,comment_0,Mike can you - attach a zip file with your sample project that fails. - what version of ServiceMix are you using? - and how do you start your sample and see the bug It would get us much faster up to speed how to see the bug,non_debt,-
camel,516,comment_1,This issue has been split into two issues: CAMEL-519 and CAMEL-520,non_debt,-
camel,516,comment_2,Closing 1.4 issues,non_debt,-
camel,554,summary,upgrade spring integration to 1.0.0M4,non_debt,-
camel,554,description,"Spring Integration 1.0.0.4 m4 is released, we need to catch up it.",non_debt,-
camel,554,comment_0,The patch is in the svn trunk.,non_debt,-
camel,612,summary,Exchange should end in error when no choice in a ChoiceType matches,non_debt,-
camel,612,description,"When you define a route with a choice() and no matching when() clause is found, the Exchange just ends successfully without doing anything. In my mind, it should fail by default in this case (or we should at least have an easy way to get this behavior).",design_debt,non-optimal_design
camel,612,comment_0,Gert I do think that the choice() should *always* have an otherwise() so there always is a match. If the otherwise() is missing on the choice() then Camel should thrown an exception. Maybe checked during the route creation stuff.,design_debt,non-optimal_design
camel,612,comment_1,"Claus, I actually do think this use case for a choice() block does make sense. I definitely do not want all my unmatched exchanges to go to any third customer, so I usually code something like: ... to get my RouteBuilder's error handler to pick it up. We should at least make this behavior easier to implement or maybe even make it the default when no explicit otherwise() was given. Gert",non_debt,-
camel,612,comment_2,-1 [edit: retracting my +1] In other words if there is no explicit otherwise() to go the errorHandler (no redelivery of course). Thinking more about this I think the behavior of choice should be the way it already is. If no clause matches and there is no otherwise it should silently succeed with a noop. Users are familiar with this from switch() statements and is i think a reasonable expectation. I think it's perfectly reasonable to add the otherwise (as one would add a default: ) to make it explicit that the exchange should fail if there is no match: What I would do though is adding a log (below INFO) in the if otherwise == null that would say that exchange is not processed.,code_debt,low_quality_code
camel,612,comment_3,"Hadrian good comments. How can we archive this, the DeadLetterChannel is probably already ""chaining"" the routing? Would be good to have it documented somehow. I do think the wiki needs a section where each DSL types is documented one-by-one. That is a however a extensive work to do.",documentation_debt,outdated_documentation
camel,612,comment_4,"OK, I wasn't aware of the {{throwFault()}} method that allows a user to easily make the message exchange fail instead of having to use the {{Processor}} trick. Reducing the priority of the issue, because I guess I only need to document the default, messsage filtering behavior of the choice() on the site to make people more aware of it.",documentation_debt,outdated_documentation
camel,612,comment_5,I added a warning to the logging in and I changed the documentation at,non_debt,-
camel,633,summary,SpringTestSupport - should support spring xml files loaded from file system,non_debt,-
camel,633,description,"Now you must use ClassPath based xml files as Camel expects a The code below is not possible, but desired for unit testing camel projects where spring configuration resides in WEB-INF and not on the classpath etc. There must be a common spring interface we can return in instead of the classpath based.",non_debt,-
camel,633,comment_0,You can do Lets see in Camel 1.5 if we can just let it accept an ApplicationContext,non_debt,-
camel,633,comment_1,The workaround above doens't work.,non_debt,-
camel,633,comment_2,Looking into this.,non_debt,-
camel,633,comment_3,With this patch you can use file based application contexts like:,non_debt,-
camel,633,comment_4,Applied patch with thanks to Jonathan.,non_debt,-
camel,633,comment_5,Closing 1.5 issues,non_debt,-
camel,643,summary,Camel JMX useJmx=false does not work,non_debt,-
camel,643,description,None,non_debt,-
camel,643,comment_0,See patch in CAMEL-637,non_debt,-
camel,649,summary,create ant build files for the new examples,non_debt,-
camel,649,description,The two new examples in 1.4 needs support for ANT build files. loan-broker spring-jms,non_debt,-
camel,649,comment_0,The existing examples is failing. the build files must be changed.,non_debt,-
camel,649,comment_1,loan-broker already had ANT files,non_debt,-
camel,649,comment_2,"Oh, there is no ant file in loan-broker example. I will add one for it.",non_debt,-
camel,649,comment_3,Oh my eyes must have played a trick with me. I had to run it from the ANT from the RC2 disto exploded zip to let it find the libs needed. Okay William you are the best to add the ANT for your great demo,non_debt,-
camel,649,comment_4,The loan broker build.xml is in the svn repository.,non_debt,-
camel,662,summary,add an errorHandlerRef attribute to <camelContext/> and all route nodes like <route/> or <pipeline/> so that you can customize the error handler on a per context or route or step basis,non_debt,-
camel,662,description,None,non_debt,-
camel,704,summary,Replace Group Chat with Multi User Chat in xmpp component,non_debt,-
camel,704,description,According to XEP-0045: Multi-User Chat standard Multi User Chat is an old protocol which is supported for backward compatibility. It is better to use Multi User Chat because most servers support it.,non_debt,-
camel,704,comment_0,Good spot. Patch applied with many thanks!,non_debt,-
camel,704,comment_1,Closing 1.4 issues.,non_debt,-
camel,706,summary,#NAME?,non_debt,-
camel,706,description,See the parent issue.,non_debt,-
camel,706,comment_0,When an exception occur and the TX is rolled back and redelivered then this happends in a new thread and Camel is stateless so we can not 100% determine what the previous condition was for the exchange. So currently we can only support end-users can configure the All other options is not supported in TX mode.,non_debt,-
camel,706,comment_1,"I am considering changing to not use the RedeliveryPolicy for TX mode, or introducing an with fewer options to resemble what end-users can use. eg: only has redeliveryDelay as an option, and the delay is fixed. Any thoughts?",non_debt,-
camel,706,comment_2,RedeliveryPolicy extends DelayPolicy and DelayPolicy only have 1 setting for a fixed delay.,non_debt,-
camel,706,comment_3,"code committed, need to update wiki",documentation_debt,outdated_documentation
camel,706,comment_4,Updated wiki as well. See transactional client wiki page.,non_debt,-
camel,706,comment_5,Closing 1.5 issues,non_debt,-
camel,707,summary,JMX Instrumentation - Nodeid can be null,non_debt,-
camel,707,description,The processors that is registered in JMX can have null in their id. So the JMX path is processor - Either we should replace null with unknown as we do for endpoints. Or try to fix that the id is generated.,non_debt,-
camel,707,comment_0,See screenshot,non_debt,-
camel,707,comment_1,"The instanceid is not needed, will be removed. The process id is unique.",non_debt,-
camel,707,comment_2,"nodeid is now mandatory and the instanceid has been removed, as nodeid is unique.",non_debt,-
camel,721,summary,Log component should use ExchangeFormatter for formatting log output,code_debt,low_quality_code
camel,721,description,"Currently log component only logs the payload. We have a nice ExchangeFormatter that can format an exchange with all kind of options. The options could be enabled on the log component so you can customize your logging. Also there should be a *multiline* option to the exchange formatter so it can log all the stuff on multi lines if for instance there are many options, they get very long.",code_debt,low_quality_code
camel,721,comment_0,See nabble:,non_debt,-
camel,721,comment_1,Updated wiki as well.,non_debt,-
camel,763,summary,CamelContext's setErrorHandler is not effect when the jmx agent is disabled,non_debt,-
camel,763,description,"The example applications from Camel 1.4 source distribution work correctly on my machine. After comparing the sample Spring XML configuration and my Spring configuration, I noticed there is no jmxAgent element in the sample XML where my configuration has this element. The problem I noticed was that when the disabled attribute of the jmxAgent element is set to true, the redelivery policy does not work.",non_debt,-
camel,763,comment_1,Closing all 1.5.0 issues,non_debt,-
camel,789,summary,pom.xml - maven repositories - clean up,build_debt,build_others
camel,789,description,When downloading artifacts we have a many repos. Several of them are specific for a single/few components. We should move these repo settings to these targeted components and let the uber pom.xml have only the major public maven repos. Maybe some of the repos is out-of-date and not used anymore.,architecture_debt,violation_of_modularity
camel,789,comment_0,Having faster builds sounds good... I'll take a look at this.,non_debt,-
camel,789,comment_1,Cheers Jonathan. And I recall we have a ticket as well to use the spring amazon repo for their jars instead of what we have configued in our pom.xml. So you might as well check up on the repos where we get the spring stuff from. For instance we have a spring-integration project that might need to use the amazon stuff for the milestones while regular spring can also be retrieved from the major apache maven repos also. Okay good luck and I as well would love faster builds without checking 10 repos each time ;),non_debt,-
camel,789,comment_2,"This patch pushes down... well... all of the repositories that were in the root pom lower in the build tree. This should speed up builds quite a bit! :) Note that if you build from a clean repo, the build will fail trying to find the mina ftpserver. This patch does not introduce this failure however. See for more details about this failure.",build_debt,build_others
camel,789,comment_3,"Thanks Jonathan. I am wondering if you are not able to ""assign"" ticket to yourself? Would be handy to know if you are working on it ;) You should get the credit for all the hard work you do!",non_debt,-
camel,789,comment_4,I think being able to assign JIRAs is for official committers only. Hopefully I will be able to do this some day ;),non_debt,-
camel,789,comment_5,Closing 1.5 issues,non_debt,-
camel,808,summary,http and jetty - improve documentation based on parent fix (dynamic options in URI) and some more samples,documentation_debt,low_quality_documentation
camel,808,description,Just a ticket for a reminder for myself to improve the wiki documentation a bit for these two components.,documentation_debt,low_quality_documentation
camel,808,comment_0,Closing 1.5 issues,non_debt,-
camel,823,summary,add HL7 DataFormat to the Java and XML DSLs,non_debt,-
camel,823,description,None,non_debt,-
camel,823,comment_0,Committed revision 686463. Updated the wiki documentation also.,non_debt,-
camel,823,comment_1,Closing all 1.5.0 issues,non_debt,-
camel,856,summary,Typo in system property name to enable tracing in DefaultCamelContext,documentation_debt,low_quality_documentation
camel,856,description,In the system property should read camel.trace and not canel.trace (I guess from the project name which is Camel and not Canel). public boolean getTrace() { final Boolean value = getTracing(); if (value != null) { return value; } else { return } },non_debt,-
camel,856,comment_0,Thanks Carl for reporting this. It was already fixed in 1.5 ;),non_debt,-
camel,856,comment_1,Closing all 1.5.0 issues,non_debt,-
camel,857,summary,DeadLetterChannel - maximum redelivery is not corrent and redelivery counter is wrong when failure handled,non_debt,-
camel,857,description,"The unit test from CAMEL-794 demonstrates a few issues with the DLC in Camel - the maximum redelivery is not reached - eg. setting it to 2 will only perform 1 normal attempt + 1 redelivery, and not as expected 2 redeliveries - when the exchanged could *not* be redelivered at all then the redeliverycounter has already been incremented and thus is off by +1",non_debt,-
camel,857,comment_0,"Okay is fixing this now. Camel will now as before do by default: 1 regular attempt + 5 re-deliveries = a total of 6 times to process an exchange before moving to the dead letter channel. Camel does this in 1.4 or older, just the counter was wrong when you was reading it from the dead letter channel after it was moved there.",non_debt,-
camel,857,comment_1,Also the redelivery boolean flag was always TRUE even though there was no attempt of redelivery. This is also fixed now.,non_debt,-
camel,857,comment_2,Closing 1.5 issues,non_debt,-
camel,870,summary,camel-jms - transfer exchange,non_debt,-
camel,870,description,"If end-users uses JMS queues as DeadLetterChannel it only send the original exchange body and primitive headers. The original caused exception is lost. The camel-jms component could benefit from the *exchangeTransfer* option that we introduced to camel-mina to send the exchange itself over the wire. However end-users must make sure if using remote queues that the consumer of the message can deserialize the exchange object. Also end-users have requests to be able to set the kind of JmsMessage that is used (Text, Byte, Object, etc.)",non_debt,-
camel,870,comment_0,The end-user with the problem,non_debt,-
camel,870,comment_1,Committed revision 756685. Updated wiki as well.,non_debt,-
camel,870,comment_2,Closing all 2.0M2 tickets,non_debt,-
camel,872,summary,remove generics from classes,non_debt,-
camel,872,description,None,non_debt,-
camel,872,comment_0,"This is the third time I attempt this and the change set becomes pretty large, making it quite hard to deal with failures. I will make the fixes in a few stages and this will impact other developers with potential need for merge. It would be great if you could avoid commits for the next 24-48 hours or so (if it's not too much to ask).",code_debt,low_quality_code
camel,872,comment_1,"I disabled the DataSetSedaTest. Must be reenabled before closing this issue. It used to fail on my randomly in the past, but now it's more frequent, hopefully it will help identify the cause.",non_debt,-
camel,872,comment_2,"There is one thing remaining, but it's more of a side effect of this issue and is tracked in CAMEL-1078.",non_debt,-
camel,872,comment_3,Closing 2.0m1 tickets,non_debt,-
camel,882,summary,SftpEndpoint ignores custom ssh port,non_debt,-
camel,882,description,SftpEndpoint ignores custom ssh port. It works only if server runs on port 22.,non_debt,-
camel,882,comment_0,This fixes the custom ssh port problem. Call the getSession with the port.,non_debt,-
camel,882,comment_1,Applied patch with thanks to Bela.,non_debt,-
camel,882,comment_2,Closing all 1.5.0 issues,non_debt,-
camel,887,summary,Update to a released version of Google Guice,non_debt,-
camel,887,description,We currently depend on a snapshot dependency of Google Guice 1.1. This most likely won't be released before we want Camel 1.5 to go out so we need to create a release ourselves :) I've put one together here: Will update build shortly with this fix.,non_debt,-
camel,887,comment_0,Fixed in rev 693569.,non_debt,-
camel,887,comment_1,Closing all 1.5.0 issues,non_debt,-
camel,903,summary,camel-ftp - last polltime should be off by default and only used for test scenarious,non_debt,-
camel,903,description,"The camel-ftp component uses stored as last poll time. The remote file timestamp is used for comparing against last poll time if its new and thus a candidate for download. As timestamps over FTP is not reliable we should not use this feature by default, but turn it off. It should only be there for test or experimental usage. Most FTP servers only sent file timestamp as HH:mm (no seconds). And as a bonus we avoid timezone issues as well. Instead end-users should use a different strategy for ""marking files done"" such as: - delete consumed files - rename consumed files See nabble:",design_debt,non-optimal_design
camel,903,comment_0,Its actually a critical bug,non_debt,-
camel,903,comment_1,The last poll timestamp feature is @deprecated. Will be removed in Camel 2.0. In Camel 1.5 the last poll timestamp will be *disabled* by default. You enabled it with a new option flag.,non_debt,-
camel,903,comment_2,Commited fix. The flag is named: its a boolean that is *false* by default. Set it to *true* to enable it as it works in Camel 1.4 or older.,non_debt,-
camel,903,comment_3,TODO: Need to document it in wiki,documentation_debt,outdated_documentation
camel,903,comment_4,Updated wiki with a warning and added the option.,non_debt,-
camel,903,comment_5,Closing 1.5 issues,non_debt,-
camel,910,summary,loan broker example - feedback and minor review,non_debt,-
camel,910,description,"First of all I think we need to promote this great example some more. Maybe it should be easier to find on our wiki site. I will post findings in this ticket: #1 Link to EIP book loan broker sample doesnt work #2 I think the 2 parts in the introduction should be listed as bullets (one for JMS, one for webservice) #3 spelling (comman in the sentence below) credit agency , and banks) #4 Maybe the exchange pattern InOnly, InOut is not easily understood by new users and non JBI/ServiceMix end-users. Maybe use terms such as sync/async instead (and list the MEP in parathes) #5 Could ""test-jms"" component name be renamed to jms or activemq or something with non test? #6 multicast().to() ah clever if its really using a multicast ;) I didn't know that we have the .to on multicase. Are you sure its working as expected? and it should not be multicase(""bank1"", ""bank2"", ...) without the to? #7 Use the getHeader with the expected type as 2nd param, to avoid plain java type cast String ssn = #8 The aggregator. I am wondering if we have or should have a counter build in Camel so you can use a build in header instead of ""remebering"" to code this yourself old + 1); will continue...",documentation_debt,low_quality_documentation
camel,910,comment_0,#9 its not easily understood how the client can send using the template. Maybe divide the server and client into each section. The client just need much less code than the server. #10 webservice example. It is not clear that its the *true* parameter that turns the multicast into parallel mode. This is not documented to well in the code.,code_debt,low_quality_code
camel,910,comment_1,"Finished #1, #2, #3, #4, #5, #7, #9, 10 For #6, yes it works. For #8, yes we could add counter in the MulticastProcessor for completedPredicate.",non_debt,-
camel,910,comment_2,#8: Yes I think we should provide this out of the box in Camel. In many cases the completed predicate is when you have received the X expected number of messages. BTW: Do we have a timeout setting as well for the multicast?`So you don't wait forever.,non_debt,-
camel,910,comment_3,#8 should be resolved in CAMEL-928.,non_debt,-
camel,912,summary,camel-cxf component is not passing request context through correctly,non_debt,-
camel,912,description,"is not setting up the request context correctly during createCxfMessage, so later in that method propagateContext has no request data to propagate. Also, we need the exchange properties to be added to the request context, so that non-camel components can pass properties into the cxf request context. I have coded a fix for this, along with a unit test of course. I'll attach the patch to this JIRA.",non_debt,-
camel,912,comment_0,Applied the patch with thanks to Mike.,non_debt,-
camel,912,comment_1,Closing all 1.5.0 issues,non_debt,-
camel,946,summary,improve package searching in Spring DSL,non_debt,-
camel,946,description,We should improve the <package Also when doing unit testing it would be great to be sure only YOUR route is loaded and not all in the same package.,design_debt,non-optimal_design
camel,946,comment_0,There is somekind of routebuilder ref you can add to ref to a single class. However we could consider restrucutre the DSL a bit so the package / routebuilderef or what we call it is in a configuration element. We could move all the jmx and other stuff in that one too. so you have a <configuration> element to start with where we can add all the weird camel stuff. And then have a more clean root element with fewer high level tags to select among.,architecture_debt,violation_of_modularity
camel,946,comment_1,"Yeah, I like the configuration element idea. I was thinking this would be nice to have when I was putting the data formats into a dataFormats element. I held off making those changes because it would be a big change for users upgrading from 1.4. For 2.0 though, its fair game! :)",non_debt,-
camel,946,comment_2,We will keep the current configuration as is.,non_debt,-
camel,946,comment_3,Closing all 2.0M2 tickets,non_debt,-
camel,948,summary,review & improve the PDF that comes with the release,non_debt,-
camel,948,description,"e.g. we should include the tutorials, maybe some of the cook book and am sure there's other useful documentation to include - like the recent changes to XPath / XQuery and other languages. Maybe some reformatting is needed too - missing images and so forth. The various sections of the book are here in the wiki here is the entire book...",documentation_debt,low_quality_documentation
camel,948,comment_0,1) Added cookbook and tutorials into the book page. 2) Fixed the missing images error 3) Added the Languages Appendix page,documentation_debt,low_quality_documentation
camel,948,comment_1,Closing all 1.5.0 issues,non_debt,-
camel,949,summary,"Support for multi-threaded, reliable resequencing.",non_debt,-
camel,949,description,"Support for multi-threaded, reliable resequencing has been discussed in",non_debt,-
camel,949,comment_0,Martin is working on a patch.,non_debt,-
camel,949,comment_1,"Martin, if possible this feature should also supprt the aggregaor as well. They both use BatchProcessor, so I guess it should be possible",non_debt,-
camel,949,comment_2,The Resequencer EIP is due to an overhaul due community ask for support for persistence support. Then we can take a look at see what it takes to support something along the lines of the features we added to the aggregator EIP,non_debt,-
camel,990,summary,TypeConverter support with @Converter meta-annotation and not only @Converter,non_debt,-
camel,990,description,"Today TypeConverter detection is based on annotation. Allow user to override this by using meta-annotation. This can be useful to hide camel dependencies, when extending the framework. ie, this can be done: package ElementType.METHOD }) public @interface { } import public class FileToXXXConverter { public static XXX file) throws IOException { return new XXX(); } }",non_debt,-
camel,990,comment_0,Patch With License,non_debt,-
camel,990,comment_1,We must remember to update the wiki with this new feature (if patch is committed) Do you mind writing a snippet what such documentation should be? As accepting a patch is only really useable if we also updates and improves the related documentation as well. However I feel your idea is great. Maybe there should be a that just contains the few annotations needed and with your strategy end-users can use their own annotations and thus imports.,documentation_debt,outdated_documentation
camel,990,comment_2,"Claus, I'll try to write a snippet, and will think about the mechanism you suggest. Will come back when done.",non_debt,-
camel,990,comment_3,BTW - its not really a big deal to use annotations from any particular framework in your code as annotations are a soft dependency. e.g. adding Camel annotations to your code doesn't add any direct dependency on camel. You can then use your code totally fine without any camel jars on the classpath. So there's not a huge need to 'hide' dependencies on Camel annotations - as you only need them to compile your source code. But using indirect annotations can be useful; e.g. you can have a foo.jar which defines a @Foo annotation which builds itself with camel - then folks can just add foo.jar to their classpath to get camel goodness without adding camel jars to the build. So you could introduce a kinda macro annotation that includes various framework annotations on it (say spring and camel and guice annotations :),design_debt,non-optimal_design
camel,990,comment_4,"The original need is the following: Let's say I build a Routing System based on Camel. Of course my System will depend on Camel. But I don't want the end user of my System (who will be responsible for adding routes, converters...) to depend on any camel jar. Your solution is better (and also more time consuming than my simple metaannotation patch :)), will think about it.",design_debt,non-optimal_design
camel,990,comment_5,"James, can you give more details on how your solution would work exactly ?",non_debt,-
camel,990,comment_6,"Patch applied with many thanks. I made a few manual modifications, because the code changed a bit from the version the patch was created against.",non_debt,-
camel,990,comment_7,Closing all 1.5.0 issues,non_debt,-
camel,1014,summary,ExpressionType is missing in Spring DSL,non_debt,-
camel,1014,description,None,non_debt,-
camel,1014,comment_0,Damm in ExpressionType everything is more or less @XmlTransient,non_debt,-
camel,1014,comment_1,"Is this really needed? Don't we use ExpressionType in the Spring DSL already? For instance, I just used it in the ExceptionType... Maybe I'm missing something obvious here :)",non_debt,-
camel,1014,comment_2,"Yeah sort of, you had to add a new class HandledPredicate to get it in the XSD for the onException. If we need it in other areas as well we have to add new classes there as well. There might be something we can do generally. If not then .. well then we gotta add the needed classes.",non_debt,-
camel,1014,comment_3,"Ah, I think I get it. You'd like to have a reusable JAXB class for creating an expression in an element, like: instead of including say the expression type directly Make sense?",non_debt,-
camel,1014,comment_4,I just committed a fix for this. Now to get something like this You need to add this to your model class:,non_debt,-
camel,1014,comment_5,Closing 1.5 issues,non_debt,-
camel,1018,summary,camel http Uri from the message header,non_debt,-
camel,1018,description,"Imagine a use case when the target systems, where http messages have to be sent to, are maintained in a database. In that case the http uri is cannot be set in the camelcontext but it should come from the header of the message. So the http client should check first if there is an uri in the header and if not than use the one given in the camelcontext (like for querystring).",non_debt,-
camel,1018,comment_0,Patch replaced with the one based on the latest svn revision.,non_debt,-
camel,1018,comment_1,Applied patch with thanks to Balazs. Also updated the wiki page.,non_debt,-
camel,1018,comment_2,Closing 2.0m1 tickets,non_debt,-
camel,1068,summary,file component - option to set a ref for a java.io.FileFilter,non_debt,-
camel,1068,description,We have some URI options to set some regexp for filtering files. But good old java.io.FileFilter should be there as well for end-users to use. And also add an option for using Camel predicate's for filtering file names. This only applies for the file consumer of course.,non_debt,-
camel,1068,comment_0,Commited by CAMEL-1112 Sending Sending Adding Sending Transmitting file data ..... Committed revision 721719.,non_debt,-
camel,1068,comment_1,Closing 2.0m1 tickets,non_debt,-
camel,1070,summary,- hasAttachments is buggy,non_debt,-
camel,1070,description,I must use if Instead of if { As the latter always returns false. Or at least returns false even though the size is > 0,non_debt,-
camel,1070,comment_0,Closing 2.0m1 tickets,non_debt,-
camel,1107,summary,camel-http - does not remove httpClient.xxx URI options,non_debt,-
camel,1107,description,"When using the options to configure httpClient using URI option, they should be removed from the uri that is left over to the HTTPProducer. should remove the httpClient.xxx so it's",code_debt,dead_code
camel,1107,comment_0,Will be fixed in a patch for other issues,non_debt,-
camel,1107,comment_1,I have also added httpClient.XXX options configuration of camel-jetty as well,non_debt,-
camel,1107,comment_2,Closing 2.0m1 tickets,non_debt,-
camel,1112,summary,add orderby to file/ftp/sftp component to get files in a reliable order (e.g. created date),non_debt,-
camel,1112,description,"when reading files from file,ftp/sftp component, it is often neccesarry to get the files in a predifined order (e.g. created date or filename). to solve this, an extra uri parameter could be used, to set the order of files beeing processed. ordering can be done after reading the file-list. i know this can also be solved by applying a resequencer pattern in the route, but i think for large files it would be hard to apply this. also a processing in an created date order would be more natural.",design_debt,non-optimal_design
camel,1112,comment_0,I had sort of planned to do some refactorings of the file consumer in 2.0 so it would gather the list of files to consume before consuming. This would allow to resort the list based on certain out-of-the-box settings and of course also allow end users to provide their own sorting implementation. And also filter and finnaly to have a headers with total number of files and the current file number. In summary: - sorting - filtering - headers with index and total number of files,non_debt,-
camel,1112,comment_1,"Wiki documentation needed for file component: - idempotent options *DONE* - headers with current index and total *DONE* - important camel 2.0 changes *DONE* - file filter options *DONE* - file sorter options *DONE* - out of box file sorters for: by name, by timestamp, etc. *DONE* - idempotent sample *DONE* - file filter sample *DONE* - file sorter sample *DONE* - file sortBy sample *DONE* - sort by: ignore case option *DONE*",documentation_debt,outdated_documentation
camel,1112,comment_2,The new idempotent options is CAMEL-1099,non_debt,-
camel,1112,comment_4,Revision #721798 Committed by davsclaus at 30-11-08 12:36:04 CAMEL-1112: Added file language expression based sorting incl. grouping (quite fancy),non_debt,-
camel,1112,comment_5,Revision #721801 Committed by davsclaus at 30-11-08 12:53:27 CAMEL-1112: Added file language expression based sorting incl. grouping (quite fancy),non_debt,-
camel,1112,comment_6,Closing 2.0m1 tickets,non_debt,-
camel,1123,summary,commons-csv contains SNAPHSOT dependency,non_debt,-
camel,1123,description,This could be bad if 3-SNAPSHOT ever gets removed (the build will break). ServiceMix has a version of commons-csv without this SNAPSHOT dependency. I'm going to try switching to that one.,non_debt,-
camel,1123,comment_0,Closing 2.0m1 tickets,non_debt,-
camel,1138,summary,Memory leak in FileConsumer,code_debt,low_quality_code
camel,1138,description,"Using a memory profiler, we've identified what appears to be a substantial memory leak in FileConsumer in Camel 1.5.0. It appears that the noopMap is constantly having items added to it, but nothing performs a remove on it when the file is consumed. This causes a very large amount of string data to be accumulated in the heap. In our application, this was a leak of several hundred megabytes and is a showstopper. Considering the apparent severity of this issue, it would really be nice if a fix could be incorporated into a 1.5.1 version.",code_debt,low_quality_code
camel,1138,comment_0,This is *fixed in 2.0* as we have removed all this troublesome code. For 1.5.1 I suggest to replace the noopMap with the LRUCahce Map so it keeps up till 1000 files and nothing more.,non_debt,-
camel,1138,comment_1,These maps is part of some code logic to determine if the file has been changed using timestamp and filesize checks. All this code has been @deprecated and removed in 2.0. It leads to unexpected behavior and is hard to test. And shouldn't generally be used. If you use file connectivity then you should either delete or move files after they are processed and not keep then around. As a fix for this in 1.5.1 I have added the LRUCache so the maps will contain at most 1000 elements.,design_debt,non-optimal_design
camel,1138,comment_3,Hadrian is the current quick fix okay?,non_debt,-
camel,1138,comment_4,"I think so. It actually solves the leak problem. The side effect is that if a file is not moved/renamed after 1000 other entries/files it will disappear from the lru cache and look new again and get reprocessed, which may also lead to an infinite loop (that would make another one of those 1000+ files new/out of cache, and so on). However I don't think this scenario is a valid one as it has other flaws, such as the fact that the state is lost after a camel restart, so files would get processed again in that case too (a comment in the code mentions that, actually). I would add a warn in the wiki that not renaming/moving files is a dangerous scenario and should not be used.",design_debt,non-optimal_design
camel,1138,comment_5,I think we can close this issue.,non_debt,-
camel,1143,summary,Upgrade to Spring Integration 1.0 GA,non_debt,-
camel,1143,description,It's GA now so Williem could you upgrade our integration to use the GA version,non_debt,-
camel,1143,comment_0,applied the patch into trunk and camel-1.x branch.,non_debt,-
camel,1143,comment_1,Closing 2.0m1 tickets,non_debt,-
camel,1165,summary,"can we add a front page for the manual specifying Camel User Guide, Version XX?",documentation_debt,low_quality_documentation
camel,1165,description,None,non_debt,-
camel,1165,comment_0,I've put up a manual with simple cover page at let me know what you think. I still need to put in something that will dynamically insert the version number. That is for tomorrow though :),documentation_debt,low_quality_documentation
camel,1165,comment_1,I've put in changes that will dynamically insert the version number in rev 725302. Resolving this task now.,non_debt,-
camel,1165,comment_2,awesome stuff! :),non_debt,-
camel,1165,comment_3,Closing 2.0m1 tickets,non_debt,-
camel,1173,summary,Need example of scatter-gather pattern,documentation_debt,low_quality_documentation
camel,1173,description,See We should be able to do this in Camel already... just need to cook up an example/doco for it.,documentation_debt,low_quality_documentation
camel,1173,comment_0,Adding Transmitting file data . Committed revision 724833. This is my first stab at the pattern. Not completely happy with it at the moment... maybe we need to put better support into Camel somewhere for this kinda thing.,documentation_debt,low_quality_documentation
camel,1173,comment_1,Put in an improved scatter-gather example in revision 740056. The problem with the first example was that it was not dynamic enough. Also added wiki docs here,code_debt,low_quality_code
camel,1173,comment_2,This can be done in Camel 1.5/2.0,non_debt,-
camel,1173,comment_3,Closing all 1.5.0 issues,non_debt,-
camel,1196,summary,MockEndpoint - sleep for empty test doesnt work,test_debt,lack_of_tests
camel,1196,description,See nabble:,non_debt,-
camel,1196,comment_0,r727713 for trunk r727718 for 1.x,non_debt,-
camel,1196,comment_1,Closing 2.0m1 tickets,non_debt,-
camel,1228,summary,Clean up the OSGI bundle profiles,code_debt,low_quality_code
camel,1228,description,To make sure the Class.forName() can get what it wants Also need to support to set the bundle activator .,non_debt,-
camel,1228,comment_0,Closing 2.0m1 tickets,non_debt,-
camel,1256,summary,Clean up camel-cxf,code_debt,dead_code
camel,1256,description,"The camel-cxf component is dued for some code cleanup and refactoring. We can clean some of of the interfaces and redundant code, etc.",code_debt,dead_code
camel,1256,comment_0,"A fix has been submitted. Here is some highlights. * The fix is for 2.0 only since it involves some APIs changes that is an overkill to keep it backward compatible. Will update wiki. * It depends on CXF 2.2-SNAPSHOT now. Will upgrade CXF once the next release of CXF is available. * CxfBinding, Bus [CAMEL-1239], can be looked up from registry by the ""#"" notation. * Decouple CXF Message from Camel Message. That is, users are no longer required to cast Camel's message body to CXF Message in order to access SOAP headers and body in PAYLOAD. In PAYLOAD mode, Camel message body now returns a new type CxfPayload which contains SOAP headers and body. With CxfPayload being the body in PAYLOAD mode, It makes using Camel converter to convert to other types pretty easy and transparent. * Cleaning up the old CxfBinding and ""invoking context"". Both tried to do Camel/CXF message binding. CxfBinding is now an interface that an custom impl can be set on each endpoint. impl can also be set on each endpoint. * [CAMEL-1254] support serviceClass=#bean * some major refactoring to make code cleaner.",code_debt,low_quality_code
camel,1256,comment_1,"Nice. I am looking forward to the PAYLOAD. My last gripe with camel-cxf is really that a manual convertBodyTo is needed. BTW I get this maven issue now [INFO] A required plugin was not found: Plugin could not be found - check that the goal name is correct: Unable to download the artifact from any repository Try downloading the file manually from the project website. Then, install it using the command: mvn Alternatively, if you host your own repository you can deploy the file there: mvn deploy:deploy-file -Durl=[url] -DrepositoryId=[id] from the specified remote repositories: central from the specified remote repositories: central",non_debt,-
camel,1256,comment_2,"Ah, since we have the snapshot dependency on the CXF 2.2 , we need to add the snapshot maven repository in our pom. I will did a quick fix it .",non_debt,-
camel,1256,comment_3,"@Clause I'm deploying a new version of CXF 2.2 snapshot (there is a patch for camel-cxf), you should have a successful build on camel-cxf two hours later :)",non_debt,-
camel,1256,comment_4,Closing 2.0m1 tickets,non_debt,-
camel,1257,summary,persistence.xml should not be in camel-jpa jar,non_debt,-
camel,1257,description,should be deleted from camel-jpa.jar,non_debt,-
camel,1257,comment_0,Closing 2.0m1 tickets,non_debt,-
camel,1270,summary,Starting Camel using Main from camel:run or Main.class - countdown latch is hanging,non_debt,-
camel,1270,description,The countdown latch in MainSupport is not completed when Main is stopping. Then we have a hanging thread. Can bee seen using ctrl + \,code_debt,multi-thread_correctness
camel,1270,comment_0,trunk: 735421 1.x: 735424,non_debt,-
camel,1270,comment_1,Closing 2.0m1 tickets,non_debt,-
camel,1298,summary,Resolve the mvn -Psetup.eclipse profile,non_debt,-
camel,1298,description,"mvn -Psetup.eclipse is not working as expected. The idea is that it builds a nice workspace with correct formatting rules as per project standard and performs a logical eclipse:eclipse on all projects. it behaves differently to eclipse:eclipse however, dieing when it encounters (something about) new projects (such as a new component, perhaps not built yet ? is jat not in the repo ? )",non_debt,-
camel,1298,comment_0,Willem have fixed it just recently,non_debt,-
camel,1304,summary,Add a GridComponent to integrate GridGain,non_debt,-
camel,1304,description,"Hi all, i'd like to have some feedback about the idea to develop a GridGain component for Camel. This Component will permit to invoke jobs to compute (Map & Reduce pattern) on a grid system very lightweight based as GridGain. Working and Contributing on Mule and Globus enterprise environments (no light :) ) i've understand the power of Camel and just now i'd like to use it with a GridGain. GridGain is squarelly focused on one thing - providing the best Java software middleware that allows to develop complex grid applications on the cloud infrastructure in a simple and productive way. Integrating them it will be possible obtain Scalability feature in application with high computing throughtput requirements using Apache Camel.",non_debt,-
camel,1304,comment_0,Sound awesome We love contributions,non_debt,-
camel,1304,comment_1,Thanks a lot for your feedback. I read contributions guidelines but I 'd like to know if i can firstly begin develop this component and only after try to propose it to community. it is possible? I will follow discussion and issue tracker but just on development time about my component. thanks in advance again.,non_debt,-
camel,1304,comment_2,Yeah of course Yes thats very good for you to start and when you think you have something ready to show and want feedback on then come back again.,non_debt,-
camel,1304,comment_3,ok. Thanks a lot. I will remain this issue opened until i will submit something to show . :),non_debt,-
camel,1304,comment_4,"Actually I talked with Nikita Ivanov about this in Raleigh a few months back. GridGain is also interested in this, to the point where, quite a while ago, they changed the licensing for their api and made it APL2.0 per James' request (see GridGain is actually a pretty cool piece of technology, but I would be concerned about testing. Afaik, their server side technology is still LGPL licensed only, so while we would be able to compile against GG, I am not sure how we will test.",non_debt,-
camel,1304,comment_5,"Ok, i'd like understand better the question related to Licences. Can i continue to develop the component? I suspended it a few month ago. why? Thanks a lot.",non_debt,-
camel,1304,comment_6,Any updates on this?,non_debt,-
camel,1304,comment_7,"I need to find another team member to' continue work . Il giorno 27/ott/2009, alle ore 21.11, ""Claus Ibsen (JIRA)"" <jira@apache.org",non_debt,-
camel,1304,comment_8,"Hi, Actually I toyed with this a while ago and it's not hard to implement. It would help in a few ways, one being distributing our own tests on multiple nodes and reduce the ridiculously long building time (but hey, we have some 4000 unit tests, something to brag about). The issue is that gridgain repackaged and distribute their api under ASL2.0 at our own's jstrachan request quite a while ago. The issue is that the nodes are still GPL so I have no idea how we could test such a component unless they would make available a mock or something as ASL2.0 as well. I am willing to bet that they won't relicense the whole thing. I spoke to Nikita some 6 months ago about this and it looks like the GG crowd is not quite happy about the prospect. One alternative would be to implement and host it at the fuse forge and camel extra and depending on the community interest see if we could work something out with GG and move the component later at apache.",code_debt,slow_algorithm
camel,1304,comment_9,"Ok Hadrian. Just now it is possible close this issue and host it on camel extra. Please let me know if you agree with me. Inviato da iPhone Il giorno 28/ott/2009, alle ore 01.05, ""Hadrian Zbarcea (JIRA)"" <jira@apache.org",non_debt,-
camel,1304,comment_10,"Ok, let's leave it as future and plan to put it in a separate repo for now like camel-extra. I will assign this to me then.",non_debt,-
camel,1304,comment_11,Closing ticket which is not progressing.,non_debt,-
camel,1304,comment_12,Closing all resolved tickets from 2010 or older,non_debt,-
camel,1317,summary,Claim Check EIP,non_debt,-
camel,1317,description,We can definitely do this in Camel already... just need an example/docs for it.,documentation_debt,outdated_documentation
camel,1317,comment_0,Added example for this in rev 741254. Also added wiki docs here,non_debt,-
camel,1317,comment_1,Closing 2.0m1 tickets,non_debt,-
camel,1320,summary,Create GZip Data Format,non_debt,-
camel,1320,description,Let's create gzip data format. Would be useful for http trafic that can have Content-Encoding: gzip,non_debt,-
camel,1320,comment_0,Roman How is this related to the zip data format recently added?,non_debt,-
camel,1320,comment_1,"It is very similar to zip data format - it even uses the same deflate algorithm. The point is, that zip data format uses deflate algorithm directly, while gzip would use gzip file format, that is (I believe more popular). The problem with zip data format is that is is not a zip really - zip is a format that compresses set of files, while our zip data format doesn't - it just compresses data using compression algorithm of zip. On the other hand gzip is a format that compresses data (not files), uses the same algorithm, but has its own headers, that has to be interpreted. I even believe that our zip data format should be renamed to 'deflate' data format as it is the name that describes what it really is.",design_debt,non-optimal_design
camel,1320,comment_2,Created gzip data format,non_debt,-
camel,1320,comment_3,Great. You probably need to add it to the jaxb file as well in the model package. Otherwise it wont show up in Spring XML.,non_debt,-
camel,1320,comment_4,"And we need to add it to the wiki page as well, under data formats.",documentation_debt,outdated_documentation
camel,1320,comment_5,Both things done. Documentation to be found at:,non_debt,-
camel,1320,comment_6,Closing all 2.0M2 tickets,non_debt,-
camel,1387,summary,we should add List<Component> getComponents() and List<Language> getLanguages() to the CamelContext API for tooling,non_debt,-
camel,1387,description,which for now can just return the known instances,non_debt,-
camel,1387,comment_0,Committed revision 752893. Added getXXXNames() to return a List<String> with the name.,non_debt,-
camel,1387,comment_1,Closing 2.0m1 tickets,non_debt,-
camel,1389,summary,"be able to render a single route definition or collection of routes in other languages (e.g. Java, Scala, Groovy, Ruby etc)",non_debt,-
camel,1389,description,We could do with an interface to be able to turn a RouteType or RoutesType into a textual representation. Maybe something like The XML one is quite simple - we'd just use JAXB. For other languages (Java / Scala / Groovy / Ruby) we'd need some kind of visitor pattern for navigating the Routes/Route tree to turn each node into some neat language expression; so folks could extend some visitor thingy and include the specific Scala (say) code,non_debt,-
camel,1389,comment_0,Lets keep the camel-web simple and have the XML render only,non_debt,-
camel,1389,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,1400,summary,Restlet default binding does not acknowledge Fault message in the exchange when creating a response,non_debt,-
camel,1400,description,More details can be found in this mail thread.,non_debt,-
camel,1400,comment_0,committed to trunk: Committed revision 748392. committed to 1.x: Committed revision 748393.,non_debt,-
camel,1400,comment_1,Closing 2.0m1 tickets,non_debt,-
camel,1447,summary,"DSL name change - tryBlock, handleBlock instead of handle, and maybe add a finallyBlock",non_debt,-
camel,1447,description,"See article, listening 2 We should consider renaming *handle* to *catchBlock* so its using the same name as try .. catch. And we could add a finallyBlock so end user got them all.",code_debt,low_quality_code
camel,1447,comment_0,Renaming it to be consistent in Java DSL and Spring DSL - doTry - doCatch - doFinally in both Java DSL and Spring DSL,non_debt,-
camel,1447,comment_1,trunk: 766057,non_debt,-
camel,1447,comment_2,Forgot Scala: rev 766062,non_debt,-
camel,1447,comment_3,Closing all 2.0M2 tickets,non_debt,-
camel,1481,summary,should use the regular classloader if the osgi bundle classloader can't find any class,non_debt,-
camel,1481,description,"Since ServiceMix4 also supports the old JBI service unit and the classloader is not osgi bundle class loader, the OsgiResolverUtil should use the regular classloader if the osgi bundle classloader can't find any class.",non_debt,-
camel,1481,comment_0,trunk camel 1.x branch,non_debt,-
camel,1481,comment_1,Closing all 2.0M2 tickets,non_debt,-
camel,1496,summary,Using request parameters in the feed url will result in for Unknown parameters,non_debt,-
camel,1496,description,While configuring a route like: <route <from <to camel throws an exception with: Failed to resolve endpoint due to: There are 1 parameters that couldn't be set on the endpoint,non_debt,-
camel,1496,comment_0,"Added new patch, which solves issue with file based URI's, which should not keep the request parameters, but http and https should.",non_debt,-
camel,1496,comment_1,trunk: 761536 1.x: 761538,non_debt,-
camel,1496,comment_2,Thanks!,non_debt,-
camel,1496,comment_3,Closing all 2.0M2 tickets,non_debt,-
camel,1507,summary,"Allow sending MIME messages with both a plain-text and text/html body, and allow sending images inline",non_debt,-
camel,1507,description,"To send a email you have to follow a pretty specific course. This adds a property (which is poorly named in this patch) to the MailConfiguration that names the header that contains the plaintext version of the email, and adds a property where you can embed images inline. If an attachment has a filename starting with ""cid:"" then this will add the ""Content-ID"" header to that multipart body - which will allow the email client to put the image in the appropriate place when it is viewed. (i.e. the html email has something like <image src=""cid:0001"" /",code_debt,low_quality_code
camel,1507,comment_0,might need some cleanup - but I have tested it and it works for me,code_debt,low_quality_code
camel,1507,comment_1,"This contains a very simple maven project that has a main method that will send out an email. It's definitely NOT intended for inclusion as-is (but I'll check the box since you can include anything in it if you really WANT to) I just modified the default archetype and built a simple attachment processor that will attach the logo.gif to the message and send out the email. If you want to use this, it might save you the 5 minutes it would take you to do something similar to test how this works with sending out actual emails to test it. (It will work with gmail accounts that allow SMTP - just modify the YOURNAMEHERE and YOURPASSWORDHERE appropriately... I almost submitted it with my name and password in plaintext - which would have been rather dumb :) )",non_debt,-
camel,1507,comment_2,"Do you want me to tweak the patch to rename the alternate body header tag andif move it to the MailConstants? If you have any other things you don't like about it, let me know and I can work on it tomorrow.",non_debt,-
camel,1507,comment_3,"@Rayan, Don't worry about the MailConstants issue, I already did a quick fix for that.",non_debt,-
camel,1507,comment_4,I renamed the constant so its according to the Camel 2.0 syntax: rev 761182 on trunk,code_debt,low_quality_code
camel,1507,comment_5,"Ryan/William We also need to update the mail wiki page with this feature. At least document its possible and give some hints how to do it. And it would be great with a sample how to do it, and you can use the SNIPPETS in the unit test to put it on the wiki pages.",documentation_debt,outdated_documentation
camel,1507,comment_6,Applied the patch in trunk and camel-1.x branch with thanks to Ryan trunk camel-1.x,non_debt,-
camel,1507,comment_7,Closing all 2.0M2 tickets,non_debt,-
camel,1528,summary,id method needs to be re-instated from being deprecated,non_debt,-
camel,1528,description,Please re-instate the .id method of so that JMX nodes can be easily identified from within the Java DSL. In addition please provide an option for Camel to only add JMX route nodes where there is an id - this should then reduce the clutter of JMX consoles.,non_debt,-
camel,1528,comment_0,See nabble:,non_debt,-
camel,1528,comment_1,removed @deprecated in 1.x: r765885,non_debt,-
camel,1528,comment_2,And added option to JMX Agent to only register processors if they have a custom id set. Wiki page updated with the option name This only applies for *Camel 2.0* trunk: 768333.,non_debt,-
camel,1528,comment_3,Closing all 2.0M2 tickets,non_debt,-
camel,1539,summary,Upgrade to Jackrabbit 1.5.3,non_debt,-
camel,1539,description,JackRabbit 1.5.x JARs are OSGi bundles so upgrading to those to make the component more OSGi-aware.,non_debt,-
camel,1539,comment_0,Fixed as part of CAMEL-1526 (cfr.,non_debt,-
camel,1539,comment_1,Closing all 2.0M2 tickets,non_debt,-
camel,1564,summary,Add an option to specify Ethernet network Interface to listen on for UDP (Unicast & Multicast) and TCP.,non_debt,-
camel,1564,description,"If you specify to listen thus : + inPort + mina assumes ""eth0"" for both from and to addresses. This is keeping us from using Camel for a critical pieces of our architecture ; Filters & adapters. :-( Suggest that a new option be created for Camel Mina with a name like ""nic"" or ""NetworkInterface"" with a default value of ""eth0"". This value will have to be passed into and added to the MinaConfiguration, MinaEndpoint,",non_debt,-
camel,1564,comment_0,Andy can you look at the Apache Mina project how to do this with Mina? With your findings we can quicker get this added to Camel.,non_debt,-
camel,1564,comment_1,[18:19] <cibsen[18:19] <cibsen[18:19] <jgenender[18:20] <jgenender[18:20] <jgenender> cibsen: So associate an IP address to a specific interface and it will work that way,non_debt,-
camel,1564,comment_2,Andy could you check with Netty as we are adding camel-netty in Camel 2.3 and it may have more features than the old Mina 1.x/2.0 seems to have.,non_debt,-
camel,1564,comment_3,Closing old tickets. This is not possible,non_debt,-
camel,1576,summary,Add and for better supporting of the OSGi integration test,non_debt,-
camel,1576,description,Add some common util class which extends the CamelContextSupport to wrapper the Options of PAX-Exam and setup the bundleContext for CamelContext to use.,non_debt,-
camel,1576,comment_0,trunk,non_debt,-
camel,1576,comment_1,Closing all 2.0M2 tickets,non_debt,-
camel,1581,summary,ClassCastException when using camel-cxf and camel-jms in the InOut same flow,non_debt,-
camel,1581,description,"As per thread on FUSE forum [[ I have a camel route that listens on a CXFEndpoint (for MESSAGE data as DOMSource) and routes the message to a JMS queue. Something like: <route<from My understanding is that the JMS producer will realize that the exchange is InOut, create a temporary queue for the response, and then wait for the response to arrive on the temporary queue before sending the original CXFExchange on along the pipeline. Indeed, this is what I observe! However, I?ve discovered a little nasty. When the JMSProducer gets in the incoming message, it sets the Out message of the original CXFExchange to a JmsMessage. Then, the pipeline processor calls exchange.getOut on the CXFExchange and BANG! we get a class-cast exception. Turns out that the method always casts the message to a CxfMessage. Ouch. Surely this means then that camel-cxf can?t talk to any producer that does not produce a CXFMessage - very limiting. ]] The response on the form from davsclaus was: [[ Well spotted. If just the damn Exchange was final so we had one implementation of it. But I guess we have to thank Java generics for that so the first API had generics for per component. Could you create a ticket in JIRA for this? As a workaround you can add a processor where you have full access to the Exchange so you can set a CxfMessage as the payload. ]]",non_debt,-
camel,1581,comment_0,"Since we removed the CxfMessage and CxfExchange in Camel 2.0, we will not get into this trouble in Camel 2.0. I just committed a integration test in the camel-trunk to confirm it.",non_debt,-
camel,1581,comment_1,Fixed the test in camel 1.x branch,non_debt,-
camel,1608,summary,Fix pom.xml files to support nexus based release process,non_debt,-
camel,1608,description,"When profiles are explicitly activated in maven using -P, the <activeByDefault However there is one more blocker I will have to take care before 1.6.1 can be released. During the release:prepare phase the poms are regenerated and the <?xml> decl, the apache license notice is lost and there are issues with white spaces. This seems related to the fact that the <project> element is not on one line as per the new maven release guide. Fixing this may take a day or two as we have so many poms but I'll do it as the highest prio. Once I'll get it to work and release 1.6.1, I'll tackle 2.0-M2. The good news is that once we have this fixed, the release process will be much simplified.",design_debt,non-optimal_design
camel,1608,comment_0,"Hello Hadrian In Maven, when you put the: <project declaration on multiple lines, Maven will flatten it into one line and throw out the preceding comments -in this case the copyright. I guess it also messes up all the spacing. On the OW2 projects, we solved the issue by putting the declaration on one line. It doesn't look very good that way, but it works :) Cheers S. Ali Tokmen",design_debt,non-optimal_design
camel,1608,comment_1,1.x branch already fixed and release 1.6.1 accomplished. 2.0-M2 to follow.,non_debt,-
camel,1608,comment_2,Closing all 2.0M2 tickets,non_debt,-
camel,1614,summary,"Warning ""Disabling JMSReplyTo as this Exchange is not OUT capable"" on false positives in JBossAS5",non_debt,-
camel,1614,description,see:,non_debt,-
camel,1614,comment_0,trunk: 774678 1.x: 774687 Frank feel free to test the fix,non_debt,-
camel,1614,comment_1,Closing all 2.0M2 tickets,non_debt,-
camel,1622,summary,camel-osgi doesn't support to look up the fallback converter,non_debt,-
camel,1622,description,camel-osgi module doesn't support to lookup the fallback coverter from the active bundles. And I got the user complain in the support forums,non_debt,-
camel,1622,comment_0,trunk camel 1.x branch,non_debt,-
camel,1622,comment_1,Closing all 2.0M2 tickets,non_debt,-
camel,1641,summary,camel-ftp is not thread safe,requirement_debt,non-functional_requirements_not_fully_satisfied
camel,1641,description,See nabble for discussion: Basically FTPClient should be created as a new instance for each created producer or consumer (like http component does),non_debt,-
camel,1641,comment_0,trunk: 778354. When/if CAMEL-1644 is implemented then this bug is already fixed. However remember to add the unit test in java. There is a TODO.,test_debt,lack_of_tests
camel,1641,comment_1,trunk: 778419 And fixed by CAMEL-1644.,non_debt,-
camel,1641,comment_2,Closing all 2.0M2 tickets,non_debt,-
camel,1648,summary,Bespin editor mangles route XML,non_debt,-
camel,1648,description,The Bespin editor in the web console takes nice route XML like and spits out which will blow up Camel when we try and save it.,non_debt,-
camel,1648,comment_0,Just disabled the Bespin editor in revision 778465 until we get this fixed. Its back to the good ol ugly textarea :),non_debt,-
camel,1648,comment_1,The bespin editor has been removed from 2.5 onwards,non_debt,-
camel,1648,comment_2,Closing all resolved tickets from 2010 or older,non_debt,-
camel,1656,summary,Properties file component - And allow simple language to access the properties,non_debt,-
camel,1656,description,People using Spring XML as DSL is challenged with the fact they dont have a real programming language under the cover. And people do not want to hardcode values/options in the DSL directly. And spring property placeholder support is limited in what it can do. So we need for now our own properties component that can load a properties file from classpath. Leverage the spring resource endpoint that can do this. And let the simple language be able to access these properties as well so people can use it in expressions And the properties file is could be define something like: And the file content Need to think a bit about how to define the properties file. If we can allow you to define it as a url or as a ref. For instance the simple expression could be: The above is for quick and dirty.,non_debt,-
camel,1656,comment_0,And I think James Strachan have reported something similar.,non_debt,-
camel,1656,comment_1,Got a basic properties component up and running.,non_debt,-
camel,1656,comment_2,You can now do something like this Where #{ } is the property placeholder syntax to lookup properties. Using #{ } does not clash with ${ } for simple language and it also resembles the # lookup we support on parameters. Notice how you can define placeholders as only part of the uri or for parameters etc. There is also a properties component you can use Which also supports having only part of the endpoint uri as a placeholder. Notice how {{mock:}} is fixed below And it support a locations parameter so you can use non default locations,non_debt,-
camel,1656,comment_3,"And you can have placeholders in the properties file itself, which Camel will resolve as well (think recursive)",non_debt,-
camel,1656,comment_4,And simple language can not leverage properties as well With properties,non_debt,-
camel,1656,comment_5,Closing all resolved tickets from 2010 or older,non_debt,-
camel,1660,summary,A lite AOP feature in the DSL,non_debt,-
camel,1660,description,"We could use a lite AOP in the DSL, allowing you to easy invoke an endpoint before/after the route. In the route above we have defined an AOP *around* where we invoke the before and after endpoints. This DSL only supports setting the *before* and *after* as a endpoint URI. We have the following DSL operations - before - after - around - afterFinally - aroundFinally The finally version will invoke the after in a finally block to ensure its always executed even if an exception occurred.",non_debt,-
camel,1660,comment_0,This is a spring XML sample Any thoughts on the DSL? Could we do it better or more elegant?,non_debt,-
camel,1660,comment_1,The patch with this implementation,non_debt,-
camel,1660,comment_2,And the special *finally* variation,non_debt,-
camel,1660,comment_3,trunk: 794544.,non_debt,-
camel,1660,comment_4,Updated wiki as well,non_debt,-
camel,1660,comment_5,Closing all 2.0M3 tickets,non_debt,-
camel,1693,summary,Should be able to set custom MailSender implementation for the default smtp endpoint,non_debt,-
camel,1693,description,The SMTP component uses JavaMailSenderImpl to send the mail. But many a times we need finer control over sending the mail and so wold like to use our own MailSender implmentation. This should be a property that can be specified via the endpoint URL.,non_debt,-
camel,1693,comment_0,Lets get this done for 2.0,non_debt,-
camel,1693,comment_1,trunk: 791068. Added new option {{javaMailSender}} to use a custom mail sender implementation instead of the default one,non_debt,-
camel,1693,comment_2,Closing all 2.0M3 tickets,non_debt,-
camel,1694,summary,Add a DSL for polling consumer that is similar to enrich but just for polling consumer instead,non_debt,-
camel,1694,description,"The {{enrich}} DSL works for producers only. eg it sends the exchange to the given endpoint uri and aggregates the response. We should have a similar DSL for polling consumer {{poll}} could be a good name. It should accept same kind of parameters, but also an optional timeout value for the receive method on the consumer.",non_debt,-
camel,1694,comment_0,"Wanted to get this in 2.0 as some users have requested it. The DSL name is {{pollEnrich}} to indicate its like {{enrich}} but for polling, eg using a PollingConsumer to obtain the additional data.",non_debt,-
camel,1694,comment_1,trunk: 789294. updated wiki,non_debt,-
camel,1694,comment_2,Closing all 2.0M3 tickets,non_debt,-
camel,1701,summary,Let camel spring auto detect any InterceptStrategy spring beans and auto use them on startup,non_debt,-
camel,1701,description,"We have other beans you can just add to spring context and Camel will autodetect them by type and use them. For instance: Tracer, Debugger, Registry and whatnot. We should add InterceptStrategy to that to allow Camel to easily use global interceptors for all camel contexts you may define with spring XML files.",non_debt,-
camel,1701,comment_0,trunk: 784101. And started a wiki page about it,non_debt,-
camel,1701,comment_1,Closing all 2.0M3 tickets,non_debt,-
camel,1734,summary,Progress OpenEdge Component,non_debt,-
camel,1734,description,"Supports the CHAR, DECIMAL, INTEGER, LOGICAL and TEMP-TABLE types and depends on the Progress OpenEdge Java Open Client (tested with all versions up to 10.1C).",non_debt,-
camel,1734,comment_0,"Hi Fernando, Thanks for the contribution. I just have quick review of your code, there is no unit test, can you add them ? Since Progress OpenEdge is a commercial product , I don't know if we could download the jar from public mvn repository and use it freely, can you clarify it ? If not , we need to find a way to accept your code. BTW, the codes have no Apache License declaration, you can take the other camel component module as an example. Willem",test_debt,lack_of_tests
camel,1734,comment_1,"Regarding the dependency on Progress OpenEdge, it is only available to their customers, which will need instructions for wrapping and deploying it in SMX. I will work on the unit tests and Apache License issues right away.",test_debt,lack_of_tests
camel,1734,comment_2,Since folks at Apache won't be able to even compile this (as they won't have the jars) we might wanna move this to a separate project such as at FuseSource where there's a bunch of Camel components for commercial projects (with a private repo so things can be built using CI etc),non_debt,-
camel,1734,comment_3,"I don't have any problem with that, let me know how to proceed. Updated the submission with the Apache License (are we going to stick to it?), no unit tests yet.",test_debt,lack_of_tests
camel,1734,comment_4,I've spun up a little Forge project to host this code; then we can use the private mvn repo to do CI builds against OpenEdge and so forth...,non_debt,-
camel,1759,summary,"Tracer - when using delay, throttle the to node keeps displaying the parent",non_debt,-
camel,1759,description,".delay(100) does affect the Tracer log and do not output correctly in the to destination See Also Throttler, Split. Looks like sub routes in general is not displayed correctly in tracer",non_debt,-
camel,1759,comment_0,Tracing with interceptor is letting interceptor shadow the original node that was intercepted.,non_debt,-
camel,1759,comment_1,"trunk: 788621, 788663, 788766.",non_debt,-
camel,1759,comment_2,"trunk: 788786. This was a tough one to crack as onCompletion, onException and intercept needed special care. But the tracer is now fine grained and shows much better the traced routing.",non_debt,-
camel,1759,comment_3,Closing all 2.0M3 tickets,non_debt,-
camel,1773,summary,JMSReplyTo is not set when explicitly set by user,non_debt,-
camel,1773,description,"By default camel sets the JMSReplyTo for InOut messages. A user should be able to customize the JMSReplyTo property. Camel always overrides the JMSReplyTo and sets it to a temp-queue regardless user settings. =========== code Snippet ============= package import import import import import import import import import import public final class App { public static void main(String args[]) throws Exception { CamelContext context = new ConnectionFactory connectionFactory = new final String TEST_QUEUE_1 = final String TEST_QUEUE_2 = final String REPLY_QUEUE = RouteBuilder() { public void configure() { from(TEST_QUEUE_1) .process(new Processor() { public void process(Exchange exchange) throws Exception { again""); REPLY_QUEUE); } }) .to(TEST_QUEUE_2); from(TEST_QUEUE_2) .process(new Processor() { public void process(Exchange exchange) throws Exception { String body = (String) got process"" + body.toString()); World""); // the reply destination is set as a property on the exchange while we process it String ReplyTo = if { //assert throw new is not set correctly: got:"" + ReplyTo + "" Expexting:"" + REPLY_QUEUE); } }}); }}); ProducerTemplate template = context.start(); Thread.sleep(2000); String requestBody = (String) ""Hello world"", String.class); } } ======== Log ============== 2:19:41,559 INFO - JMX enabled. Using 22:19:41,656 INFO - Apache Camel 2.0-M2 is starting 22:19:42,251 INFO - Apache Camel 2.0-M2 started 22:19:44,453 DEBUG JmsProducer:202 - Using JMS API v1.1 22:19:44,515 DEBUG - Executing callback on JMS Session: ActiveMQSession 22:19:44,519 DEBUG JmsBinding:370 - Using JmsMessageType: Text 22:19:44,525 DEBUG JmsProducer:188 - sending JMS message: ActiveMQTextMessage {commandId = 0, responseRequired = false, messageId = null, originalDestination = null, = null, producerId = null, destination = null, transactionId = null, expiration = 0, timestamp = 0, arrival = 0, brokerInTime = 0, brokerOutTime = 0, correlationId = replyTo = persistent = false, type = null, priority = 0, groupID = null, groupSequence = 0, targetConsumerId = null, compressed = false, userID = null, content = null, = null, dataStructure = null, redeliveryCounter = 0, size = 0, properties = null, readOnlyProperties = false, readOnlyBody = false, droppable = false, text = Hello world} 22:19:44,527 DEBUG - Sending created message: ActiveMQTextMessage {commandId = 0, responseRequired = false, messageId = null, originalDestination = null, = null, producerId = null, destination = null, transactionId = null, expiration = 0, timestamp = 0, arrival = 0, brokerInTime = 0, brokerOutTime = 0, correlationId = replyTo = persistent = false, type = null, priority = 0, groupID = null, groupSequence = 0, targetConsumerId = null, compressed = false, userID = null, content = null, = null, dataStructure = null, redeliveryCounter = 0, size = 0, properties = null, readOnlyProperties = false, readOnlyBody = false, droppable = false, text = Hello world} 22:19:44,564 DEBUG - consumer receiving JMS message: ActiveMQTextMessage {commandId = 5, responseRequired = true, messageId = originalDestination = null, = null, producerId = destination = queue://test1, transactionId = null, expiration = 1246303204528, timestamp = 1246303184528, arrival = 0, brokerInTime = 1246303121068, brokerOutTime = 1246303121099, correlationId = replyTo = persistent = true, type = null, priority = 4, groupID = null, groupSequence = 0, targetConsumerId = null, compressed = false, userID = null, content = null, = null, dataStructure = null, redeliveryCounter = 0, size = 0, properties = null, readOnlyProperties = true, readOnlyBody = true, droppable = false, text = Hello world} Hello world 22:19:44,853 DEBUG JmsProducer:202 - Using JMS API v1.1 22:19:44,853 DEBUG - Executing callback on JMS Session: ActiveMQSession 22:19:44,854 DEBUG JmsBinding:370 - Using JmsMessageType: Text 22:19:44,854 DEBUG JmsProducer:188 - sending JMS message: ActiveMQTextMessage {commandId = 0, responseRequired = false, messageId = null, originalDestination = null, = null, producerId = null, destination = null, transactionId = null, expiration = 0, timestamp = 0, arrival = 0, brokerInTime = 0, brokerOutTime = 0, correlationId = replyTo = persistent = false, type = null, priority = 0, groupID = null, groupSequence = 0, targetConsumerId = null, compressed = false, userID = null, content = null, = null, dataStructure = null, redeliveryCounter = 0, size = 0, properties = null, readOnlyProperties = false, readOnlyBody = false, droppable = false, text = Hello again} 22:19:44,855 DEBUG - Sending created message: ActiveMQTextMessage {commandId = 0, responseRequired = false, messageId = null, originalDestination = null, = null, producerId = null, destination = null, transactionId = null, expiration = 0, timestamp = 0, arrival = 0, brokerInTime = 0, brokerOutTime = 0, correlationId = replyTo = persistent = false, type = null, priority = 0, groupID = null, groupSequence = 0, targetConsumerId = null, compressed = false, userID = null, content = null, = null, dataStructure = null, redeliveryCounter = 0, size = 0, properties = null, readOnlyProperties = false, readOnlyBody = false, droppable = false, text = Hello again} 22:19:44,895 DEBUG - consumer receiving JMS message: ActiveMQTextMessage {commandId = 7, responseRequired = true, messageId = originalDestination = null, = null, producerId = destination = queue://test2, transactionId = null, expiration = 1246303204855, timestamp = 1246303184855, arrival = 0, brokerInTime = 1246303121399, brokerOutTime = 1246303121433, correlationId = replyTo = persistent = true, type = null, priority = 4, groupID = null, groupSequence = 0, targetConsumerId = null, compressed = false, userID = null, content = null, = null, dataStructure = null, redeliveryCounter = 0, size = 0, properties = null, readOnlyProperties = true, readOnlyBody = true, droppable = false, text = Hello again} direct:a got processHello again {JMSXGroupID=null, JMSType=null, JMSDeliveryMode=2, JMSPriority=4, 22:19:44,905 ERROR - JMSReplyTo is not set correctly: JMSReplyTo is not set correctly: Caused by: JMSReplyTo is not set correctly: ... 8 more 22:19:44,906 WARN - Execution of JMS message listener failed JMSReplyTo is not set correctly: ====",non_debt,-
camel,1773,comment_0,Related to CAMEL-1689,non_debt,-
camel,1773,comment_1,You have to set the {{JMSReplyTo}} as a *header* and not as a property to expect Camel to use it. Use,non_debt,-
camel,1773,comment_2,Closing all 2.0M3 tickets,non_debt,-
camel,1784,summary,"Don't put null string in the string generated for CSV, KeyValuePair messages",non_debt,-
camel,1784,description,None,non_debt,-
camel,1784,comment_0,Code committed,non_debt,-
camel,1784,comment_1,Closing all 2.0M3 tickets,non_debt,-
camel,1842,summary,Add more OSGi test with camel-mail and camel-cxf components,non_debt,-
camel,1842,description,"camel-cxf and camel-mail are the popular components in camel, so we need add some OSGi integration test for them.",test_debt,lack_of_tests
camel,1842,comment_0,trunk,non_debt,-
camel,1846,summary,throwFault (and other things?) are missing from the xsddoc,non_debt,-
camel,1846,description,"This affects at least version 1.5.0 of the docs. If I look through the xsddocs on the website (starting from the Camel Xml Reference page) for the throwFault element, it's not there even though it _is_ present in the actual xsd. I guess something went wrong with the generation of those pages and it makes it rather difficult to figure out what's valid and what isn't.",documentation_debt,low_quality_documentation
camel,1846,comment_0,"Try finding what you need in the pdf manual for the version you are using The online docs is autogenerated and could often be overwritten with latest version, eg at this time Camel 2.0.",non_debt,-
camel,1846,comment_1,If you click on this link you got the 1.5.0 XSD i.e. click on the 2nd column from this page,non_debt,-
camel,1846,comment_2,"ugh, I didn't notice those links all went to the same place. Can those be fixed to point at the pdf, or at least removed to avoid confusing anyone else?",documentation_debt,low_quality_documentation
camel,1846,comment_3,"Well, the pdf manual doesn't have the details about the xml elements, so I guess the only option at the moment is to try to read the xsd directly, or figure out how to generate the xsddoc pages. :(",documentation_debt,outdated_documentation
camel,1846,comment_4,Fixed the links. Sadly we cannot maintain easily a v1.x and 2.0 branch of the online docs as part of build process etc. The xsddoc do not bring much value so I am looking into removing it as well.,documentation_debt,outdated_documentation
camel,1846,comment_5,Patch to remove xsddoc from the maven poms,non_debt,-
camel,1846,comment_6,The maven reports is just getting to old and intermixed with 1.x and trunk releases. We should just remove them as they dont bring much value anyway. The xsddoc has been removed from the build files. So its now a matter of removing from the apache http site (people with credentials is needed),documentation_debt,outdated_documentation
camel,1851,summary,Route JMX attributes not being updated,non_debt,-
camel,1851,description,"Some routes are not getting instrumented using JMX and we get the following WARN for those on startup; Route has not been instrumented for endpoint: ... It turns out that its only those routes that have an onException handler as the first processor in the route, e.g. <route id=""per-msg-route"" <from uri=""jms:MSG_IN""/ <onException <exception <redeliveryPolicy <handled </onException <bean ref=""Formatter"" More info and proposed fix at",non_debt,-
camel,1851,comment_0,trunk: 797926. Mark you can test by trying latest SNAPSHOT or get the source and build a new .jar yourself,non_debt,-
camel,1851,comment_1,Confirmed fixed in 2.0-SNAPSHOT Thanks!,non_debt,-
camel,1863,summary,camel-hl7 - add option to disable HAPI parser validator,non_debt,-
camel,1863,description,None,non_debt,-
camel,1863,comment_0,see nabble,non_debt,-
camel,1863,comment_1,trunk: 798902. TODO: HL7 segment that fails validation and add it to the validate unit test,test_debt,low_coverage
camel,1863,comment_2,trunk: 798934. HL7 test with validation error,non_debt,-
camel,1881,summary,camel-irc consumer only listens to messages on a channel and ignores direct messages,non_debt,-
camel,1881,description,"While camel-irc listens to events on a channel very nicely, if you send a camel-irc consumer a message directly it ignores it.",non_debt,-
camel,1881,comment_0,here's a patch.,non_debt,-
camel,1881,comment_1,"@Stan, maybe you want to add a unit test? Many thanks for the patch!",test_debt,lack_of_tests
camel,1881,comment_2,"yup, will do...",non_debt,-
camel,1881,comment_3,Patch with unit test...,non_debt,-
camel,1881,comment_4,Thanks a lot Stan for the patch. I have applied it to trunk. trunk: 802379.,non_debt,-
camel,1895,summary,camel-file - Consume and pollEnrich another file should work transparent,non_debt,-
camel,1895,description,See nabble,non_debt,-
camel,1895,comment_0,trunk: 802661,non_debt,-
camel,1895,comment_1,trunk: 802679 Added in Camel 2.0 a pre check that will deny polling if both starting and poll endpoint are file based as its not supported in Camel 2.0.,non_debt,-
camel,1895,comment_2,trunk: 988146.,non_debt,-
camel,1895,comment_3,Closing all resolved tickets from 2010 or older,non_debt,-
camel,1901,summary,DataSet - Add ability to be timer based,non_debt,-
camel,1901,description,"The DataSet component is number of message based, so you can only do testing by sending in eg. 50000 messages. What is needed as well is a period/time based instead, to say it should run for 60 seconds.",non_debt,-
camel,1901,comment_0,Closing all resolved tickets from 2010 or older,non_debt,-
camel,1913,summary,camel-jetty - Should support multipart/form posted data in the message body better,non_debt,-
camel,1913,description,See CAMEL-1801 It does not work as expected as the posted data should be in the *body* and not as it does currently stored as a message header with the body as a key and with an empty value! For instance sending a file using curl You want Camel to provide the file content in the body and the other as headers. But what you get in the body is,non_debt,-
camel,1913,comment_0,Whether it should be stored in the body directly or as an attachment I really dont know.,non_debt,-
camel,1913,comment_1,You could for instance upload multiple files in same POST so I wonder what is the correct solution with Camel More details at,non_debt,-
camel,1913,comment_2,"While understanding that it should stay as it is, conforming to the spec, I still believe it should be in the body, the next endpoint could be a file, with an implicit converter which will pull the uploaded file from the request in the case there are multiple attachments, the same implicit converter would be able to detect this fact, and produce the files.",non_debt,-
camel,1913,comment_3,"Any new ideas? In my opinion, it's not possible to put the content of the file in the body, because the user could upload more than one file in one request. The sample from the spec is: When I try to map all of these information to the message exchange, I came to the following result: The submitted form-data could be mapped into the header. We could create an attachment for each uploaded file. The file name is mapped to the name of the attachment. The content type is set as the content type of the attachment (which is an instance of DataHandler). This would be result in the following sample code: I don't know, if Camel already uses/implements some classes we should use... What do you think? Regards, Christian",non_debt,-
camel,1913,comment_4,"The Message API already supports attachments. See addAttachment, getAttachments etc.",non_debt,-
camel,1913,comment_5,"@Claus, I know this. My last post was my proposal how we could map the uploaded files into the message. I think the body is not the right place (or my knowledge about the Camel TypeConverter system is to bad). :-) Regards, Christian",non_debt,-
camel,1913,comment_6,"By leveraging the Jetty's MultiPartFilter, it's easy to implements this feature. I also updated the wiki page for this feature.",non_debt,-
camel,1913,comment_7,Closing all resolved tickets from 2010 or older,non_debt,-
camel,1923,summary,OnException - Configuring route scoped should be limited to be configured on the route node itself,non_debt,-
camel,1923,description,"OnException, OnCompletion and the others in this cross cutting concerns should be configured on the route itself and not later during the route. See nabble:",non_debt,-
camel,1923,comment_0,*Idea:* We could introduce a that is a base defintion and then let ProcessorDefinition extend this. And then have context scoped build methods on the former and have route stuff only on the latter. eg onException only on CPD so you cannot use it later in the route.,non_debt,-
camel,1923,comment_1,This will break backwards comparability in case some people have defined their routes with cross cutting concerns to far down their route.,non_debt,-
camel,1923,comment_2,I think we should address this in Camel 2.5 or some release that could have a minor breakage,non_debt,-
camel,1923,comment_3,Lets keep current behavior.,non_debt,-
camel,1944,summary,Revamped IRC component event adapter logging,non_debt,-
camel,1944,description,Currently only the camel-irc producer installs an event listener on the IRC connection to log incoming messages from the IRC server. This patch revamps the logging so that the component installs an event adapter to do logging before the connection is even established so it's easier to debug various issues that could be occurring with the connection to the IRC server.,code_debt,low_quality_code
camel,1944,comment_0,Patch Applied with thanks to Stan,non_debt,-
camel,1944,comment_1,Closing old resolved issues,non_debt,-
camel,1996,summary,Overhaul of JMX - Add managed error handler,non_debt,-
camel,1996,description,None,non_debt,-
camel,1996,comment_0,This is a tough one to crack as error handlers can be quite flexible how its configured and how its woven into the routes.,non_debt,-
camel,1996,comment_1,First first cut of managed error handler trunk: 812410.,non_debt,-
camel,1996,comment_2,"trunk: 812426, 812460",non_debt,-
camel,1996,comment_3,trunk: 812464,non_debt,-
camel,1996,comment_4,Closing old resolved issues,non_debt,-
camel,2035,summary,new Component: camel-snmp,non_debt,-
camel,2035,description,I have ported servicemix-snmp as a camel-component. Please review and integrate if needed.,non_debt,-
camel,2035,comment_0,added patch,non_debt,-
camel,2035,comment_1,added another patch,non_debt,-
camel,2035,comment_2,added missing delay for the OIDPoller Consumer,non_debt,-
camel,2035,comment_3,Committed to trunk: 818024. Thanks a lot Lars for the contribution.,non_debt,-
camel,2035,comment_4,Lars contributed wiki documentation as well. Thanks a lot Lars. Good work.,non_debt,-
camel,2035,comment_5,Closing old resolved issues,non_debt,-
camel,2046,summary,Fix the restlet and scacle osgi bundle issue,non_debt,-
camel,2046,description,"Current camel-features uses restlet and scacle bundle don't work, we need to replace them with a workable bundle.",non_debt,-
camel,2046,comment_0,trunk restlet trunk scala camel 1.x branch restlet camel 1.x branch scala,non_debt,-
camel,2046,comment_1,Closing old resolved issues,non_debt,-
camel,2058,summary,File Component - Rename operations fail sometimes on certain filesystems (Windows),non_debt,-
camel,2058,description,"On Windows (don't know if there are other platforms that suffer from this problem), the file component fails to successfully rename files with the File.renameTo operation. It fails when the rename is performed immediately after closing a file. On Windows this usually indicates that some other process has the file open. This occurs due to things like Virus scanners which keep the file open for very short periods of time. Given a slight pause the rename would succeed. This is a serious problem which effectively makes useless options like ""tempPrefix"" in the File Producer and with the FTP Consumer. Workarounds like ""disable your virus scanner"" don't cut it for everyone (me specifically) as I'm system privilege restricted from doing so, and even then, there's no guarantee that other windows processes might not do similar things (file indexers, etc). The Java spec doesn't define the behavior of the rename operation and specifically says that this can vary from implementation to implementation / filesystem to filesystem. Second, rename doesn't say why it fails, it merely returns false which is very unhelpful. A couple ways to fix: 1). Provide an option to disable this optimization. ie, a or something (clean, simple, easy). This would be a simple fix. More or less just a few clauses/tests in GenericFileProducer around any ""is local"" checks.... 2). Attempt a copy instead if the rename fails - maybe after a brief pause - maybe even after a number of rename attempts - maybe watch to see the file is closed prior to a rename attempt. Rename operation failures may affect other things as well like certain locking schemes.",non_debt,-
camel,2058,comment_0,Michael can you try 2.1-SNAPSHOT as we have fixed improved this. Fixed already by CAMEL-1965.,non_debt,-
camel,2058,comment_1,"For the record, I did try my route in 2.1-SNAPSHOT before filing the report. But apparently I tried it just for and not for {{tempPrefix}}. 2.1-SNAPSHOT does seem to be fixed for {{tempPrefix}} (although I think I still found an issue there), but +NOT+ fixed for an option of the FTP component. \\ h3. Issue #1: In both 2.0 and 2.1snap, this has the following behavior. # After creating the ""inprogress"" file, it fails renaming (only makes a single attempt) to the final name in the working directory. # After failing rename, it proceeds to consider the message ""delivered"" and does not attempt to reconsume the file on the next poll (this seems wrong to me, but I'm a newbie). \\ h3. Issue #2: {{tempPrefix}} You are right this is fixed with CAMEL-1965 as you indicated. _However_, when using {{tempPrefix}} with the {{fileExist}} option does not behave as expected. Instead, the {{fileExist}} options are applied to the ""temp"" file and not to the target file. || Option Value || Expected Behavior || Actual Behavior when using {{tempPrefix}} || | {{Override}} (Default) | If the target file exists, overwrite it | Will overwrite the temp file if it exists but will fail if the target file exists because the rename operation fails | | {{Fail}} | Fail the exchange immediately if the target file exists | Will fail the exchange immediately if the temp file exists. Will not fail immediately if the target file exists. The exchange still fails ultimately though due to the failing of the rename operation. | I did not test {{Append}} or {{Ignore}} values but they probably have similar problems. Also, I did not test other File component options that could involve rename operations such as {{preMove}}, or {{moveFailed}}. Let me know if you want me to file a separate issue for #2. Given the number of potential issues with rename and the Java implementation of it (other filesystems?), I'm still thinking that an eventual option might be a good idea.",non_debt,-
camel,2058,comment_2,*Issue #1* 1. I am fixing the rename to be aligned with other code by doing retries to cater for Windows platforms. *DONE* Commited: 826686.,non_debt,-
camel,2058,comment_3,Can you post a snippet of the route that you are using to test this?,non_debt,-
camel,2058,comment_4,"*Issue #2* I guess it depends on the OS. On my Mac the rename operation will succeed even if the target file exists. So that could be an Windows OS issue, that wont allow renaming a file when the target filename already exists. But Camel should of course use the {{fileExist}} option in any case",non_debt,-
camel,2058,comment_5,Okay looks like I got the hang of this and have fixes being tested now,non_debt,-
camel,2058,comment_6,trunk: 828994. Michael can you test again?,non_debt,-
camel,2058,comment_7,"Sorry, I haven't got back to help you out with this as you requested. I just haven't been able to. I'll try to hit the new changes this weekend.",non_debt,-
camel,2058,comment_8,"I produced a test to test the issue with the {{tempPrefix}} use on Windows when the temporary file pre-exists and {{fileExist}} behavior is set to 'Fail'. This test fails in 2.0.0 and now passes in 2.1-SNAPSHOT. I also tested that when the _actual_ target file pre-exists that it fails correctly (and not failing due to a ""rename"" failure). I'd produce the test code here but it is currently intermixed with proprietary stuff. This looks totally resolved to me!",non_debt,-
camel,2058,comment_9,Thanks Michael for reporting and testing it. We can add your test code to Camel when you have the proprietary removed.,non_debt,-
camel,2058,comment_10,Closing old resolved issues,non_debt,-
camel,2071,summary,Camel Bean component should support methodName and arguments,non_debt,-
camel,2071,description,"When using the Bean component methodName stops working if you have overloaded methods with that name. However in the case where the argument type of one of the overloaded forms matches the body, then this should be used in preference to a method with a different name but same argument type. Currently the following error is thrown <to / Ambiguous method invocations possible: [public final void public final InstructionSet on the exchange: Exchange[Message: XXX] interface MyBean { public void someOtherMethod(XXX x); public InstructionSet x); public InstructionSet y); }",non_debt,-
camel,2071,comment_0,Which version of Camel are you using?,non_debt,-
camel,2071,comment_1,2.0.0,non_debt,-
camel,2071,comment_2,I am looking into this about the overloaded method and picking the one with the best matched type.,non_debt,-
camel,2071,comment_3,"trunk: 826591. Now Camel is better at picking methods. Specifying parameters is not implemented as its not the right solution. What should happen if you specify parameters Camel cannot map, and whatnot.",non_debt,-
camel,2071,comment_4,Closing old resolved issues,non_debt,-
camel,2094,summary,CamelContext should support autoStartup,non_debt,-
camel,2094,description,We now have option {{autoStartup}} on routes. Lets also have this on the camel context itself so it replaces the option that is a bit confusing. For example the code above will not start Camel on startup. You can then later start camel manually by And then all runs. This also means that we will remove the option so if you use this option you must migrate to the autoStartup instead.,design_debt,non-optimal_design
camel,2094,comment_0,trunk: 828332.,non_debt,-
camel,2094,comment_1,Closing old resolved issues,non_debt,-
camel,2141,summary,log component - show by default not output Future bodies,non_debt,-
camel,2141,description,This prevents Camel having to wait for the body to be ready by the Future task. There should be an option on the logger so you can turn that on if you want {{showFuture}},non_debt,-
camel,2141,comment_0,trunk: 833360.,non_debt,-
camel,2141,comment_1,Closing old resolved issues,non_debt,-
camel,2145,summary,camel-http - creating http uri may contain special endpoint options,non_debt,-
camel,2145,description,For example sending to an endpoint as follows Could potentially added as a parameter but its an endpoint parameter not a HTTP parameter. This issue is for components that build on top of camel-http.,non_debt,-
camel,2145,comment_0,trunk: 833419,non_debt,-
camel,2145,comment_1,1.x: 833423.,non_debt,-
camel,2145,comment_2,Closing old resolved issues,non_debt,-
camel,2163,summary,camel-jdbc - Add option to split rows,non_debt,-
camel,2163,description,If we add an option on camel-jdbc + camel-sql it would be easier for people to either work on the entire ResultSet or on a row by rows basis. See similar options on the RSS / Atom component.,non_debt,-
camel,2163,comment_0,Stan do you want to try a stab at this one?,non_debt,-
camel,2163,comment_1,See nabble:,non_debt,-
camel,2163,comment_2,"This seems to be the only way I could find to do this other than implementing this component as a polling consumer, let me know though if that'd be preferred.",non_debt,-
camel,2163,comment_3,"Stan, yeah I can see the pain now. The original code is a producer which doesnt easily allow to spawn exchanges",design_debt,non-optimal_design
camel,2163,comment_4,Stan lets add an example to camel-jdbc and camel-sql wiki pages how to use the split(body()) to process each row one by one.,documentation_debt,outdated_documentation
camel,2163,comment_5,I added a wiki example how to use the splitter EIP to the camel-jdbc component,non_debt,-
camel,2163,comment_6,Closing old resolved issues,non_debt,-
camel,2194,summary,RestletProducer should support native async request/response semantics,non_debt,-
camel,2194,description,RestletProducer should support native async request/response semantics like the recent changes made to the camel-jetty producer.,non_debt,-
camel,2194,comment_0,We take a look at this after restlet 2.0 has been released and published to maven repos.,non_debt,-
camel,2213,summary,Configure project to allow deployment on OSGI servers (depends on RESTfull api modification),non_debt,-
camel,2213,description,None,non_debt,-
camel,2213,comment_0,Could you elaborate what you mean?,non_debt,-
camel,2213,comment_1,"Please report tickets with more details, so we know what the issue is.",non_debt,-
camel,2213,comment_2,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2217,summary,Feature file of camel does not include cxf-rs for RESTful services,non_debt,-
camel,2217,description,Feature file of camel does not include cxf-rs for RESTful services,non_debt,-
camel,2217,comment_0,"I'm not sure that we need additional bundles. Here is what servicemix feature file has defined : <feature name=""cxf-jaxrs"" Camel cxf feature file already includes : - (1.3.0) - xmlbeans (2.4) - jettison (1.1) - abdera",non_debt,-
camel,2217,comment_1,"cxf bundle is an all in one bundle, it includes cxf-rs and cxf-soap implementation. camel-cxf features has all the required jars.",non_debt,-
camel,2309,summary,"""preMove"" option creates unwanted .camel directory",non_debt,-
camel,2309,description,"Given the following Camel context to copy a text file to a JMS queue <camel:camelContext id=""camel"" <camel:route id=""file-to-jms"" <camel:from <camel:to </camel:route I noticed that the ""preMove"" attribute puts the file in: instead of what I expected: the ""move"" attribute works how I expect, and places the file in",non_debt,-
camel,2309,comment_0,I suspect its the XML variation that messes up with ${. It should work when using regular Java DSL route,non_debt,-
camel,2309,comment_1,Works for me on trunk with OS X. Also the Spring XML variation. Have you tried on both OS X and Windows 7? Can you try with 2.2-SNAPSHOT? even though I cannot recall any important fixes in this area.,non_debt,-
camel,2309,comment_2,I have created an unit test using your preMove and it works for me,non_debt,-
camel,2309,comment_3,"I have tried on both Windows 7 and OS X 10.6 and issue still occurs. (Have not tried 2.2 snapshot yet) I have reviewed the unit tests Claus wrote and they are the same as mine. Changing JMS endpoint to other type makes no difference. I switched on debug logging (on Windows) and noticed that the file appears to be moved to /before directory but then later moves it to /before/.camel by the Here is the debug log: 2009-12-23 09:35:53,152 - DEBUG - Renaming file: to: 2009-12-23 09:35:53,152 - DEBUG {FileUtil} - Tried 1 to rename file: to: with result: true 2009-12-23 09:35:53,153 - DEBUG {FileConsumer} - About to process file: using exchange: with body: 2009-12-23 09:35:53,179 - DEBUG - Registered MBean with objectname: <...JMS Stuff here2009-12-23 09:35:53,301 - DEBUG - Done processing file: using exchange: with body: hello from file1-SIMPLE] 2009-12-23 09:35:53,302 - DEBUG {FileUtil} - Tried 1 to delete file: with result: true 2009-12-23 09:35:53,303 - DEBUG - Renaming file: to: 2009-12-23 09:35:53,304 - DEBUG {FileUtil} - Tried 1 to rename file: to: with result: true",non_debt,-
camel,2309,comment_4,"The {{preMove}} is just to move it during processing. The {{move}} option still kicks in, and it will by default move it to a {{.camel}} sub directory. I fail to see a problem.",non_debt,-
camel,2329,summary,Add chunked option for camel-jetty component to turn off HttpStream,non_debt,-
camel,2329,description,"Currently even with a simple route like this: we'll return a response with chunked transfer encoding. This is fine, however there doesn't appear to be an easy way to disable chunked encoding which can be necessary to support legacy clients that do not support HTTP 1.1 properly. Ideally we should add an option for camel-jetty to support turning off chunked encoding, in which case we would revert to using the Content-Length header, this would also effectively disable HTTP streaming.",non_debt,-
camel,2329,comment_0,trunk Also updated wiki page of camel-jetty.,non_debt,-
camel,2354,summary,Examples running with ANT need commons-managment on the classpath,non_debt,-
camel,2354,description,See nabble,non_debt,-
camel,2354,comment_0,trunk: 898654,non_debt,-
camel,2377,summary,failing on build on trunk,non_debt,-
camel,2377,description,"Unit test failing on mvn install on latest head from trunk. mvn -e output T E S T S Running 22, 3, Errors: 0, Skipped: 0, 17.012 sec <<< FAILURE! Results : Failed tests: 22, 3, Errors: 0, Skipped: 0 [INFO] [ERROR] BUILD FAILURE [INFO] [INFO] There are test failures. Please refer to for the individual test results. [INFO] [INFO] Trace There are test failures. Please refer to for the individual test results. Method) Caused by: There are test failures. Please refer to for the individual test results. ... 17 more Surefire Output Test set: 22, 3, Errors: 0, Skipped: 0, 17.013 sec <<< FAILURE! 0.431 sec <<< FAILURE! expected:<[AB]C 0.206 sec <<< FAILURE! expected:<[AB]C 0.176 sec <<< FAILURE! expected:<[AB]C",non_debt,-
camel,2377,comment_0,Only fails on some boxes.,non_debt,-
camel,2377,comment_1,trunk: 900669.,non_debt,-
camel,2377,comment_2,verified fixed!,non_debt,-
camel,2413,summary,Upgrade to ftpserver 1.0.3,non_debt,-
camel,2413,description,upgrade to ftpserver 1.0.3 and moved the dependecy to parent pom because anf uses different versions,non_debt,-
camel,2413,comment_0,provide the patch,non_debt,-
camel,2413,comment_1,trunk: 903876. Thanks for the patch,non_debt,-
camel,2426,summary,"CXF Header ""ResponseContext"" cannot be filtered by",non_debt,-
camel,2426,description,None,non_debt,-
camel,2426,comment_0,Committed revision 904624.,non_debt,-
camel,2430,summary,throwing exception,non_debt,-
camel,2430,description,When I execute mvn jetty:run the following exception is thrown: I have edited the exception to make it shorter.,non_debt,-
camel,2430,comment_0,camel trunk,non_debt,-
camel,2446,summary,Proposed updates to context files in examples,non_debt,-
camel,2446,description,"I have attached patches of various changes that I have made to context files. The changes fall into following categories: * using p: for properties rather than full xml * using u:list for lists rather than full xml * using in order to reduce amount of xml required * updating files to use camel:camelContext schema rather than <camelContext Please let me know if I need to tweek any of the changes. I have smoke tested the examples, but is there a general unit test that will validate that the context file?",test_debt,lack_of_tests
camel,2446,comment_0,Good work Chris. Is it possible to remove the as its not commonly use and the XML files should help new users to understand what happens. So its best to be a bit more verbose than playing tricks with auto wiring.,design_debt,non-optimal_design
camel,2446,comment_1,And maybe you can create one big patch files so its easier to apply,non_debt,-
camel,2446,comment_2,"Will do. I use autowire all the time, but I understand that non-spring weanies may be a bit confused",non_debt,-
camel,2446,comment_3,Closing old ticket,non_debt,-
camel,2446,comment_4,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2449,summary,Schema files relative to WSDL file break CxfEndpoint,non_debt,-
camel,2449,description,"I have a WSDL (see files attached) that imports a schema file. I create a Camel-CXF route from a SOAP endpoint to a logger : <c:camelContext <c:route <c:from <c:to uri=""log:test""/ There are 2 problems: 1. If the XSD is imported like this (without ""./"" in schemaLocation): <xs:schema <import </xs:schema then it is possible to create a CXF Endpoint using an implementation class (see files attached, the Test class is an implementation os the AlarmServices SEI): <cxf:cxfEndpoint if I use the Service Provider API it will fail: <cxf:cxfEndpoint /with the following exception: Exception in thread Failed to create service. ... Caused by: WSDLException (at Problem parsing (The specified file cannot be found) at Source) at Source) at Source) at Source) at Source) at Source) at Source) ... 28 more 2. Now if I change my WSDL to use a relative location for the XSD import: <xs:schema <import </xs:schema In *both* cases, I get the following, very strange error: Caused by: Failed to create route route10: - ... Caused by: Failed to resolve endpoint: due to: cannot be cast to ... Caused by: cannot be cast t o ... Using a debugger I checked in the CxfSpringEndpoint init() method that the CamelContext instance retrieved is really a SpringCamelContext. In the Manifest.MF of my bundle I have imported nearly all Camel packages (including but it does not change anything.",non_debt,-
camel,2449,comment_0,"Do you wrap the camel-context as the ServiceUnit? Or just deploy the camel-context as osgi bundle? There may have more than one version of camel in the servicemix, please uninstall the useless one.",non_debt,-
camel,2449,comment_1,"I deploy the camel-context inside an OSGi bundle (files to come, I'm cleaning up things). As for Camel and cxf bundles, here is the list: [ 43] [Active ] [ ] [ ] [ 60] camel-core (2.1.0) [ 44] [Active ] [ ] [ ] [ 60] camel-stream (2.1.0) [ 67] [Active ] [ ] [ ] [ 60] Apache CXF Bundle Jar (2.2.5) [ 88] [Active ] [ ] [ ] [ 60] camel-jms (2.1.0) [ 89] [Active ] [ ] [ ] [ 60] camel-jaxb (2.1.0) [ 108] [Active ] [ ] [ ] [ 60] camel-spring (2.1.0) [ 109] [Active ] [ ] [ ] [ 60] camel-osgi (2.1.0) [ 110] [Active ] [ ] [ ] [ 60] camel-spring-osgi (2.1.0) [ 125] [Active ] [ ] [ ] [ 60] camel-cxf (2.1.0) [ 128] [Active ] [ ] [ ] [ 60] activemq-camel (5.3.0) Note: I removed all the servicemix components because of the integration bug with Camel.",non_debt,-
camel,2449,comment_2,A minimum test case to reproduce the problem,non_debt,-
camel,2449,comment_3,I must add that this it seems to be a regression. In another environment with the environment below it works perfectly. This was done with the Servicemix 4.1.0-SNAPSTHOT from 2009/12/03 (before Camel 2.1.0 was released). The Servicemix was patched to circumvene the deployment problems with teh web deployers. karaf@root[ 82] [Active ] [ ] [ ] [ 60] camel-core (2.1.0.SNAPSHOT) [ 85] [Active ] [ ] [ ] [ 60] camel-spring (2.1.0.SNAPSHOT) [ 86] [Active ] [ ] [ ] [ 60] camel-osgi (2.1.0.SNAPSHOT) [ 170] [Active ] [ ] [ ] [ 60] camel-spring-osgi (2.1.0.SNAPSHOT) [ 187] [Active ] [ ] [ ] [ 60] camel-cxf (2.1.0.SNAPSHOT) [ 188] [Active ] [ ] [ ] [ 60] activemq-camel (5.3.0) [ 189] [Active ] [ ] [ ] [ 60] camel-jms (2.1.0.SNAPSHOT) [ 190] [Active ] [ ] [ ] [ 60] camel-jaxb (2.1.0.SNAPSHOT) [ 191] [Active ] [ ] [ ] [ 60] camel-stream (2.1.0.SNAPSHOT) karaf@root[ 137] [Active ] [ ] [ ] [ 60] Apache CXF Bundle Jar (2.2.5) [ 138] [Active ] [ ] [ ] [ 60] ServiceMix :: CXF Binding Component [ 166] [Active ] [ ] [ ] [ 60] Apache CXF Runtime JBI Binding (2.2.5) [ 167] [Active ] [ ] [ ] [ 60] Apache CXF Runtime JBI Transport (2.2.5) [ 168] [Active ] [ ] [ ] [ 60] ServiceMix :: CXF Service Engine [ 169] [Active ] [ ] [ ] [ 60] Apache ServiceMix Example :: CXF OSGi (4.1.0.SNAPSHOT),non_debt,-
camel,2449,comment_4,"Hi Vincent, I tried to build the test case on my box by fixing the jmx-ri issue, and jaxb, jaxws issue ( I'm using JDK 1.5). Now I'm blocked with this error Can you submit a newer test case?",non_debt,-
camel,2449,comment_5,"Did you try to use the Fuse ESB, which should be will tested and there is no version conflict between servicemix , camel, cxf. You can find the latest 4.1.0-psc-01-RC1",non_debt,-
camel,2449,comment_6,"OK, it seems to fix the problem. By the way I was not able to reproduce my problem in a genuine SMX4 build, so it seems my environment has become somewhat corrupted. Closing the bug for now. Willem, did you have a chance to discuss with JB Onofr about the version conflict problem ? Seems that you have fixed it in the Fuse version and I think it would be great having it in the genuine SMX4 distro.",non_debt,-
camel,2449,comment_7,Closing the bug unless I find how the environment became corrupted.,non_debt,-
camel,2481,summary,Update camel-scala to match aggregator overhaul,non_debt,-
camel,2481,description,Changes to aggregator need to be reflected in camel-scala so the component builds again.,non_debt,-
camel,2481,comment_0,Few more changes necessary than I anticipated... Sending Sending Sending Sending Sending Transmitting file data ..... Committed revision 911127.,non_debt,-
camel,2481,comment_1,"Stan, While the commits adds the missing pieces to the Scala DSL, it also seems to remove the option of just using Regards, Gert",non_debt,-
camel,2481,comment_2,@Gert Yes we have removed that as a default aggregator as its just too confusing for new users. We want people to force to provide their own aggregation strategy so they learn how to use them and merge the data themselves.,design_debt,non-optimal_design
camel,2481,comment_3,"Claus, Thanks for clarifying that -- completely missed that change Regards, Gert",non_debt,-
camel,2481,comment_4,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2503,summary,Add sysenv to Simple language so you can get environment variables,non_debt,-
camel,2503,description,See nabble,non_debt,-
camel,2503,comment_0,trunk: 916669,non_debt,-
camel,2503,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2506,summary,Upgrade to Spring 3.0.1,non_debt,-
camel,2506,description,Camel will still by default use 2.5.6. But the Spring 3.x profile should be upgraded to Spring 3.0.1.RELEASE.,non_debt,-
camel,2506,comment_0,trunk: 917142,non_debt,-
camel,2506,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2512,summary,Camel-soap jar does not contain the generated classes,non_debt,-
camel,2512,description,Seems I forgot to add the generated soap package to the exported packages. So these classes are not packed into the jar.,non_debt,-
camel,2512,comment_0,Added patch for this issue. It also contains some improvement to the tests I found when doing the wiki documentation.,non_debt,-
camel,2512,comment_1,Applied patch with thanks to Christian.,non_debt,-
camel,2535,summary,Get ride of the cxfsoap component,code_debt,dead_code
camel,2535,description,"As we don't use the CxfSoap component any more, it's time to clean it up.",architecture_debt,using_obsolete_technology
camel,2535,comment_0,"As the SoapBinding's methods are used across the camel-cxf component, so I renamed the class to CxfMessageHelper. Other camel soap components relates file are removed in revision 921743.",non_debt,-
camel,2535,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2546,summary,Simple language - Add lightweight OGNL to invoke methods on objects,non_debt,-
camel,2546,description,"Just a very lightweight alternative to using mvel, ognl etc. For example if you need to invoke getName() on a User object stored in the body of a Camel Message. But we should support bean properties And chained method calls",non_debt,-
camel,2546,comment_0,"Got it rolling, except for the getter/setter style as we need to make that as convention in Camel bean component",non_debt,-
camel,2546,comment_1,Support ?. so we can let end user decide if its mandatory or optional,non_debt,-
camel,2546,comment_2,trunk: 922808. Both simple and bean language now supports a OGNL like notation. Still to do is the shorthanded notation for getter style methods,non_debt,-
camel,2546,comment_3,"We should also support OGNL when using <to and also in the <bean ref=""foo""",non_debt,-
camel,2546,comment_4,Should also work with @Header and @Body annotations.,non_debt,-
camel,2546,comment_5,Shorthand is now supported trunk: 922880.,non_debt,-
camel,2546,comment_6,Added special {{last}} keyword to get the last value from a list. trunk: 922885.,non_debt,-
camel,2546,comment_7,Lets keep it at the simple/bean language. It may be overkill to add it to .bean / .to and @Body / @Header etc.,non_debt,-
camel,2546,comment_8,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2559,summary,HttpComponent only checks the registry for an no longer works.,non_debt,-
camel,2559,description,Basically you can't currently do this: HttpComponent only looks in the registry for an,non_debt,-
camel,2559,comment_0,Sending Adding Transmitting file data .. Committed revision 924849.,non_debt,-
camel,2559,comment_1,This commit caused problems. The test now fails,non_debt,-
camel,2559,comment_2,The problem is really HttpComponent allows endpoints to override the component configured options which is wrong. In fact endpoints should - use endpoint configured options over component configured - not mess with the component configured,non_debt,-
camel,2559,comment_3,"I think I got it covered now, will do more testing. But I have reworked camel-http and camel-jetty a little big to - use endpoint configured options over component configured - not mess with the component configured - using a single shared as that is whats meant - properly registering http/https with correct port number on SchemeRegistry on",test_debt,low_coverage
camel,2559,comment_4,The patch which fixes the issues listed above,non_debt,-
camel,2559,comment_5,trunk: 925759. Committed patch. 100% test with entire Camel.,non_debt,-
camel,2559,comment_6,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2617,summary,CamelContext - Properties - Add a so we have type safe options,non_debt,-
camel,2617,description,Currently CamelContext have a {{Properties}} for misc options. We should introduce a so we have type safe options in Java DSL and Spring XML. Then we also have one place to *advert* and document the options we currently support.,design_debt,non-optimal_design
camel,2617,comment_0,"@Claus, We should not let use to change the within the Camel route, as the CamelContext properties can be see across the exchanges, if there are more than one thread which call the route processors, and they are changing the CamelContext properties at the same time, it will cause some trouble. Current Camel provides a more safe way to configure the Java DSL XML DSL",non_debt,-
camel,2617,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2621,summary,File consumer - Polling from network share on Windows may regard files as not a file,non_debt,-
camel,2621,description,May return {{false}} on Windows if consuming from a network share etc. So we should just regard anything that is *not* a directory as a file.,non_debt,-
camel,2621,comment_0,trunk: 931466,non_debt,-
camel,2621,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2633,summary,camel-http - Endpoint options should not change the http component configured options,non_debt,-
camel,2633,description,"If a http endpoint is using {{httpBindingRef}} option to use a special binding, then it would change the binding on the http component, which means than any new http endpoints created thereafter will use what binding that aforementioned endpoint was using.",non_debt,-
camel,2633,comment_0,trunk: 933235.,non_debt,-
camel,2633,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2650,summary,@Produce and @Consume injected on prototype beans needs a mechanism for automatic stopping when no longer in use,non_debt,-
camel,2650,description,"This only applies for prototype scoped POJOs which uses @Produce or @Consume to inject a Camel Producer/Consumer. As the prototype scoped POJO is short lived and there could potentially be created many of those POJOs. Then @Produce / @Consume will inject new instances of Producer / Consumer as well. And since there is no standard way of knowing when the POJO is no longer in need, which is where we can to stop the Producer/Consumer. For singleton scoped this is not a problem as CamelContext will keep track on the created Producer/Consumer in its internal _servicesToClose_. Which is then stopped when CamelContext stops. For prototype we need a different strategy such as - proxy it to use pooled producers/consumers which CamelContext manage the lifecycle - use a shared ProducerTemplate / ConsumerTemplate instead which CamelContext manages the lifecycle - other - maybe some thread local tricks",design_debt,non-optimal_design
camel,2650,comment_0,"Using @Produce or other Camel injected services will not consider singleton or prototype scope beans. For prototype scoped a WARN is logged about the need to manually stop the service after usage."" trunk: 934731.",non_debt,-
camel,2670,summary,Add a new example : loadbalancing with camel-mina,non_debt,-
camel,2670,description,"Loadbalancing with MINA Example This example show how you can easily use the camel-mina component to design a solution allowing to distribute message workload on several servers. Those servers are simple TCP/IP servers created by the Apache MINA framework and running in separate Java Virtual Machine. The loadbalancer pattern of Camel which is used top of them allows to send in a Round Robin model mode the messages created from a camel-bean component respectively to each server running on localhost:9999 and localhost:9998. MINA has been configured to send over the wire objects serialized and this is what is showed also in this example. The advantage of this apporach is that you don't need to use CORBA or Java RMI for the communication between the different jvm. The example has been configured to use InOut EIP pattern. The demo starts when every one minute, a Report object is created from the camel loadbalancer server. This object is send by the camel loadbalancer to a MINA server and object is serialized. One of the two MINA servers (localhost:9999 and localhost:9998) receives the object and enrich it by setting the field reply of the Report object. The reply is send back by the MINA server to the camel loadbalancer who will display in its log the content of the Report object.",non_debt,-
camel,2670,comment_0,code committed,non_debt,-
camel,2670,comment_1,"@ Charles I just checked out the example, I think you can put all the camel routes configuration files into a single module and use Profile to start the services one by one.",architecture_debt,violation_of_modularity
camel,2670,comment_2,Yeah that was my thought as well. For example camel-spring-jms does this afair. I think that it would be easier as there is only one component.,non_debt,-
camel,2670,comment_3,OK. I will merge the projects to only have one.,non_debt,-
camel,2670,comment_4,Merge three maven projects into one. Add profiles to run the test,non_debt,-
camel,2682,summary,Splitter - Should not use AggregateLatest by default but let the original message be the output if no AggregationStrategy configured,non_debt,-
camel,2682,description,This allows end users to continue routing the original message. Instead of now the just get the last splitted message which is confusing.,design_debt,non-optimal_design
camel,2682,comment_0,trunk: 939597.,non_debt,-
camel,2682,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2690,summary,- Only allow explicit configured to be promoted to TypeConverter,non_debt,-
camel,2690,description,"To avoid general purpose fallback converters such as ToString etc. in some situations be promoted to a real type converter which later can cause it to be used, where as it shouldn't. On the you can now use the option {{canPromote}} to indicate this. It is default {{false}}.",non_debt,-
camel,2690,comment_0,trunk: 939945.,non_debt,-
camel,2690,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2709,summary,Upgrade Activemq to 5.3.2,non_debt,-
camel,2709,description,"As ActiveMq 5.3.2 fixed lots of OSGi related issues, we need to update camel pom and apache-camel feature to use it.",non_debt,-
camel,2709,comment_0,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2722,summary,ManagedRoute should be unregister when the RouteDefinition is removed,non_debt,-
camel,2722,description,Here is the mailing thread which discusses about it.,non_debt,-
camel,2722,comment_0,Now the ManagedRoute will be unregistered when CamelContext shutdown the route. And we don't unregister the ManagedRoute when route is stopped to let ManagedRoute start itself from JMX console.,non_debt,-
camel,2722,comment_1,"Willem, good idea to use the shutdown to unregister the mbeans.",non_debt,-
camel,2722,comment_2,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2742,summary,camel-jms - Sending to WebSphereMQ must use specific setBooleanProperty methods to set JMS properties,non_debt,-
camel,2742,description,This code in JMSBinding Should detect the value type and use the and so on. Otherwise IBM thrown an exception. See nabble,non_debt,-
camel,2742,comment_0,trunk: 947018.,non_debt,-
camel,2742,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2746,summary,Please upgrade $jackson-version from 1.4.3 to 1.4.4,non_debt,-
camel,2746,description,1.4.3 has a nasty bug in treemap that won't parse booleans properly. :),non_debt,-
camel,2746,comment_0,trunk: 947250.,non_debt,-
camel,2746,comment_1,trunk: 947288. Upgrade to 1.5.2 which is the latest stable release.,non_debt,-
camel,2746,comment_2,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2756,summary,Introduce into,non_debt,-
camel,2756,description,We can set different to turn the subject of camel message into the spring Authentication. The old will can't make sure the customer converter will be loaded and override the default one.,non_debt,-
camel,2756,comment_0,Willem I think the xxxConverter name is misleading as you would think its a Camel TypeConverter. I wonder if there is a better naming for this? or the likes.,code_debt,low_quality_code
camel,2756,comment_1,"@Claus, Good catching, I'm heading to this refactoring now.",non_debt,-
camel,2756,comment_2,Committed the refacting patch into revision 948689.,non_debt,-
camel,2756,comment_3,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2758,summary,OnCompletion - Should use pipes and filters to ensure IN is OUT from last step,non_debt,-
camel,2758,description,OnCompletion will route the Exchange directly as is which means if you have set an OUT the first step in the onCompletion route may not use this OUT but the IN instead. And also add option {{useOriginalBody}} so you can do work based on the original input instead.,non_debt,-
camel,2758,comment_0,trunk: 948683.,non_debt,-
camel,2758,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2808,summary,Upgrade to Quartz 1.8.1,non_debt,-
camel,2808,description,Quartz 1.8.1 has been released. We may still need an OSGi wrapper for it as well for the features.,non_debt,-
camel,2808,comment_0,Updated the parent pom and feature pom.,non_debt,-
camel,2808,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2814,summary,Upgrade to groovy 1.7.3,non_debt,-
camel,2814,description,None,non_debt,-
camel,2814,comment_0,trunk: 954713.,non_debt,-
camel,2814,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2848,summary,ProducerTemplate - Ensure all methods throw,non_debt,-
camel,2848,description,To make it consistent so its always a which is thrown with the cause exception wrapped.,non_debt,-
camel,2848,comment_0,trunk: 957535.,non_debt,-
camel,2848,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2850,summary,Jetty component to support non blocking routing engine,non_debt,-
camel,2850,description,None,non_debt,-
camel,2850,comment_0,trunk: 957588. Now using Jetty continuations and the async jetty http client so both consumer and producer is as async as they can be.,non_debt,-
camel,2850,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2868,summary,try catch finally DSL to support non blocking routing engine,non_debt,-
camel,2868,description,None,non_debt,-
camel,2868,comment_0,trunk: 958616.,non_debt,-
camel,2868,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2879,summary,file component - Support using delete=true and moveFailed option at the same time,non_debt,-
camel,2879,description,"The file strategies was not designed to be as flexible as we may want them today, when introducing more and more options to dictate what should happens with the files when success or failure. As we got - noop - delete - move - moveFailed etc. you may combine then in ways in which we current doesn't support. You should be able to say that if OK then just delete the file, but if ERROR then move it to this error folder instead.",design_debt,non-optimal_design
camel,2879,comment_0,"If the user specify the delete=true and moveFailed=error, the failed File will be moved into error directory.",non_debt,-
camel,2879,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2892,summary,remove ugly warnings running tests since upgrading to jetty 7,code_debt,dead_code
camel,2892,description,lets zap these... [ main] log WARN,non_debt,-
camel,2892,comment_0,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2901,summary,Upgrade to HawtDB 1.1,non_debt,-
camel,2901,description,HawtDB 1.1 has been released. Change log at: We should upgrade to pick up the listed bug fixes:,architecture_debt,using_obsolete_technology
camel,2901,comment_0,upgraded trunk.,non_debt,-
camel,2901,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2926,summary,Injecting Properties into the PropertiesComponent with Spring,non_debt,-
camel,2926,description,The component is quite nice - however a good improvement would be allowing properties to be injected through Spring. Currently there are two ways to do this using Spring. 1) Declare a bean with an id of properties: This would be improved if we could simply say: This would allow properties to be declared in Spring using <util:properties/ 2) Inline within the camelContext defined in Spring: Similarly it would be nice to say: I noticed CAMEL-2791 is out there to access JVM system properties - it would be good to not have to load as a system property to access them in Camel.,non_debt,-
camel,2926,comment_0,"As a side note, this would be awesome for those of us who are using Camel inside an OSGi container and getting properties from the OSGi CM service!",non_debt,-
camel,2926,comment_1,Okay anything for OSGi. I am adding a {{ref}} scheme so you can do Where {{someId}} is a id of a to lookup in the {{Registry}}. Which in OSGi will check the OSGi service registry.,non_debt,-
camel,2926,comment_2,trunk: 962581.,non_debt,-
camel,2926,comment_3,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2940,summary,package is imported in camel-spring component - generates error on OSGI platform,non_debt,-
camel,2940,comment_0,code committed,non_debt,-
camel,2940,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2963,summary,dead letter channel - Should enforce the MEP to be InOnly,non_debt,-
camel,2963,description,"When a message is moved into DLQ the MEP should be enforced to be InOnly as we should not expect a reply. For example if using a JMS queue as DLQ. To avoid JmsProducer will expect a reply and wait for it, which never comes and then timeout after 20 sec.",non_debt,-
camel,2963,comment_0,trunk: 965414.,non_debt,-
camel,2963,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2972,summary,onException not working correctly when using routeContext,non_debt,-
camel,2972,description,"When using a routeContext, the last (in the xml-file) defined route-scoped <onException",non_debt,-
camel,2972,comment_0,Sample demonstrating the issue.,non_debt,-
camel,2972,comment_1,here is the bug fix,non_debt,-
camel,2972,comment_2,"Silvio thanks for the patch. However the fix requires a bit more work as the is the starting route id, and you could have the Exchange routed in multiple routes and one of them have a route scoped onException which should trigger. I got it fixed but it revealed an issue when you perform RecipientList EIP pattern in parallel mode. So the bonus is that we fix an issue in there as well.",non_debt,-
camel,2972,comment_3,trunk: 979762.,non_debt,-
camel,2972,comment_4,Closing all resolved tickets from 2010 or older,non_debt,-
camel,2976,summary,we should have a way to save updated routes to an XML directory somewhere so if folks update routes at runtime they can save them on disk & move them into source control,non_debt,-
camel,2976,description,"we should be able to add some kind of listener to camel-web so that as routes are changed at runtime, we can save the updated routes as an XML file somewhere so that folks can permanently store changed routes back in their source code if they want. e.g. we could add some kind of CamelContextAware listener which on change of a route an XML file from a known directory - using maybe ${routeId}.xml as the filename?",non_debt,-
camel,2976,comment_0,"That would be nice. As would being able to use the IDE to create/edit the route. The graphical part of the IDE is of limited use, but the property sheets are very useful. Julian",non_debt,-
camel,2976,comment_1,You can use the LifecycleStrategy and you have listeners when routes is added/remove. Then you can just marshal the route to XML using the JAXB APIs.,non_debt,-
camel,2976,comment_2,"This is hard to do on a generic way. There is hot deploy containers that can do this like jetty/tomcat for web apps, and karaf/smx for osgi apps etc.",non_debt,-
camel,2979,summary,FtpComponent: If login fails and disconnect=true another connection is opened.,non_debt,-
camel,2979,description,In a route such as below a second connection to the ftp server is opened if the login fails. <route <from / <to /</route Further description:,non_debt,-
camel,2979,comment_0,Ah yeah Camel will re-connect if the first connection fails. I am fixing this so it will not do this if as you have configured. So when you configure then Camel should never try to re-connect.,non_debt,-
camel,2979,comment_1,"trunk: 966699, 966700.",non_debt,-
camel,2979,comment_2,Andreas if you want to test this you need to update both camel-core and camel-ftp to 2.5-SNAPSHOT versions. Apache should build and upload SNAPSHOTs nighly on their maven server. You can see details here Otherwise I can attach the .jars to this ticket.,non_debt,-
camel,2979,comment_3,"Hi I've tested this and it works. When logging in with incorrect user/pass it only polls once and skips, which is nice. However could it be possible for the component to throw an exception as I would like to catch this exception and make sure that no further polls are done to the ftp (ie shut down the route/application).",design_debt,non-optimal_design
camel,2979,comment_4,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3013,summary,HttpProducer should support to set the http protocol version from message header,non_debt,-
camel,3013,description,Also need to merge this change into camel-http4.,non_debt,-
camel,3013,comment_0,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3027,summary,Use ROME 1.0-osgi,non_debt,-
camel,3027,description,See this forum There is apparently a ROME version which supports osgi. We should use it by default.,non_debt,-
camel,3027,comment_0,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3039,summary,Filter EIP - Should mark a property whether the message matched or not,non_debt,-
camel,3039,description,This allows end users to more easily know if the Exchange was filtered or not. There is now a property on the Exchange which contains a {{boolean}} if it was filtered or not. true = the Exchange was routed in the filter block false = the Exchange was *not* routed in the filter block,non_debt,-
camel,3039,comment_0,trunk: 983988.,non_debt,-
camel,3039,comment_1,See nabble,non_debt,-
camel,3039,comment_2,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3041,summary,Added OSGi JMX test for the auto assigned CamelContextId,non_debt,-
camel,3041,description,None,non_debt,-
camel,3041,comment_0,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3048,summary,Archetype for creating component should have better sample component,design_debt,non-optimal_design
camel,3048,description,The current sample is based on Direct component which is a bad sample as its a very special component. We should do a HelloWorldComponent - the producer will just print the message body to system out - the consumer is scheduled and triggers every 5th second with a dummy exchange,design_debt,non-optimal_design
camel,3048,comment_0,The pom.xml. Can you use the {{camel-test}} component instead of the test jar from camel-core. The test kit is in camel-test and this is what end users should use.,non_debt,-
camel,3048,comment_1,"Good point. I just updated the test to use camel-test instead of camel-core. Of course, it was a copy and paste error on my part ;)",non_debt,-
camel,3048,comment_2,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3050,summary,RouteBuilderRef should work out of the box with Spring 3 and dependency injection,non_debt,-
camel,3050,description,"Spring 3 changed internally how dependency resolution works. Its now worse as we have to play tricks in the Camel namespace handler to tell Camel the various pieces you can ref, should depend on Camel. Otherwise the dependency resolution in Spring is not working properly. This used to work like a charm in Spring 2.5.6.",design_debt,non-optimal_design
camel,3050,comment_0,trunk: 984775.,non_debt,-
camel,3050,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3054,summary,Provide a headerAs function analagous to bodyAs,non_debt,-
camel,3054,description,"To allow a header to be coaxed to a certain class, a {{headerAs}} ""function"" would be useful in the Simple language. It would provide the same function as the current {{bodyAs}}. One possible use case for this is: which would fail if {{foo}} is a string. Casting it to an integer with {{headerAs}} would solve this, as would #CAMEL-3052.",non_debt,-
camel,3054,comment_0,trunk: 985206.,non_debt,-
camel,3054,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3057,summary,CXFRS consumer should pass the protocol headers into camel message header,non_debt,-
camel,3057,description,Here is a mail thread which discusses about it.,non_debt,-
camel,3057,comment_0,Applied the patch into camel trunk.,non_debt,-
camel,3057,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3072,summary,proposed patch for camel-atom to allow disabling throttling in splitEntries mode,non_debt,-
camel,3072,description,"I would like to enable splitEntries in camel-atom so that each feed item is processed as a distinct message. However, I'd like to process each message as soon as it is available rather than spacing each entry out by consumer-delay. This patch creates a new flag, throttleEntries. Yields existing behavior when enabled (default). When disabled, all entries found on the Atom feed are processed with no delay.",non_debt,-
camel,3072,comment_0,trunk: 988435. Thanks for the patch Lorrin.,non_debt,-
camel,3072,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3077,summary,Cache Component needs to check for null values during GET operations,design_debt,non-optimal_design
camel,3077,description,"EhCache often has a bit of lag time when invalidating expired cache elements, first setting the Element value to null and then removing the key. If you are hitting a cache rapidly one often will run across a key that is present in the cache that still has a null element entry. The logic for successfully key retrieval just needs to be slightly tweaked to check for null values.",design_debt,non-optimal_design
camel,3077,comment_0,"If it helps, steps to reproduce: 1) Set timeToLiveSeconds to something like 30 seconds 2) Add an entry in a route 3) Wait 30-ish seconds 4) Attempt to make several GETs around the expiry time. One will likely result in an NPE as the entry is being removed by the EhCache monitor",non_debt,-
camel,3077,comment_1,trunk: 989026. Thanks for the patch.,non_debt,-
camel,3077,comment_2,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3084,summary,camel-jms - requestTimeout with 0 or negative value should mean no timeout,non_debt,-
camel,3084,description,This only affects the refactored camel-jms component.,non_debt,-
camel,3084,comment_0,trunk: 989637.,non_debt,-
camel,3084,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3100,summary,${file:length} should return 0 instead of null if the file length is 0,code_debt,low_quality_code
camel,3100,comment_0,Applied patch into camel trunk.,non_debt,-
camel,3100,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3112,summary,Add encoding option on file component,non_debt,-
camel,3112,description,To make it easier to read/write files using that given encoding. Currently you have to set that as a property on the Exchange using setProperty in the DSL. But when using @Produce or @Consume you only got the endpoint uri. So having this in the component makes it easier.,design_debt,non-optimal_design
camel,3112,comment_0,Introduced charset option into the so you can specify the encoding of file with this option.,non_debt,-
camel,3112,comment_1,Updated the wiki page for it.,non_debt,-
camel,3112,comment_2,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3118,summary,camel-spring causes wrong of dependent beans,non_debt,-
camel,3118,description,"Attached is a patch with a test that demonstrates the problem. The test uses a custom RouteBuilder and another bean that both implement When the beans' methods are called, these beans add their names to a shared list. When the method is called then ""configured"" is added to the shared list. These beans are wired as follows: Note the {{depends-on}} attribute on the bean: it should ensure that {{sampleBean1}} is being initialized before and the {{camelContext}}. Actual behaviour, however, is that the beans are initialized in the following order: # # {{camelContext}} # {{sampleBean1}} which is definitely wrong. The shared list contains the entries # {{test1b}} # {{configured}} # {{test1a}} This differs from the expected order # {{test1a}} # {{test1b}} # {{configured}} which cannot be observed. After some debugging, it seems the problem is related to the method. It does a lookup of the {{camelContext}} (i.e. *before* the application context finished initialization of dependent beans. The problem is that this lookup already triggers a method call. Even worse, this behaviour depends on the declaration order of the beans in the application context XML file. When the {{camelContext}} bean is moved to the top, the bean initialization are done in the correct order. To demonstrate that this is not a Spring-related problem, the attached test also contains another bean that plays the role of the {{camelContext}} but does nothing else than calling {{configure()}} on the injected route builder within In this case, the bean initialization occur in the expected, correct order. I didn't find a solution to this problem so far and need to dig in further (hope to find some time next week for that). If any of the committers (who are more familiar with camel-spring than I am) have already an idea how to solve that, I appreciate any hints.",non_debt,-
camel,3118,comment_0,"This issue is related to recent change of CAMEL-3050, I'm working on the patch for it now.",non_debt,-
camel,3118,comment_1,Applied patch into the trunk.,non_debt,-
camel,3118,comment_2,Thanks for fixing!,non_debt,-
camel,3118,comment_3,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3125,summary,"When the fetching of a feed causes, polling continues endlessly, flooding the logs and creating unwanted network load.",design_debt,non-optimal_design
camel,3125,description,"The problem is with the default implementation of When an error occurs rollback() is called, and this method just logs the error and returns false, which means the polling will not be retried for that execution of the run() method. This means it will be retried after the delay. I created an implementation that suspends the Consumer, which seems like acceptable default behavior. It also allows for extension and adding some hooks for custom stuff. In order for that to be useful it should be made easier to set your own implementation.",design_debt,non-optimal_design
camel,3125,comment_0,This is my class that solves the problem,non_debt,-
camel,3125,comment_1,"So you impl will invoke onError the first 2 times, and then the 3rd time it invokes onSuspend. Then when its in the future it will invoke onError. The misses is never reset. Also this is a stateful implementation which mean you should use a new instance on each different consumer.",non_debt,-
camel,3125,comment_2,"trunk: 998065. There is now a which is stateful and supports multiple consumers, so you can use it for multiple routes. It will reset if a consumer is being suspended, so when you manually start the consumer its been reset. There is a limit option you can set, its by default 3.",non_debt,-
camel,3125,comment_3,"I don't know much about the way Camel handles errors, but shouldn't this somehow tie into the error handling system? I suppose some kind of should be able to plug into the event of failing rss feed urls regards, Ernst",non_debt,-
camel,3125,comment_4,"Read chapter 4 in the Camel in Action book which tells you background about error handling and where it _lives_ / _applies_. The rule of thumb is the Camel error handling works with messages, eg when an Exchange has been created and handed from the component to Camel. Before that its per component specific how they deal with their errors. That is component specific.",non_debt,-
camel,3125,comment_5,And there is a ticket to consider some sort of feature in Camel to turn that component specific error into an Exchange and have that handled by the Camel error handler: CAMEL-1260,non_debt,-
camel,3125,comment_6,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3139,summary,Use as the default uuid generator as its faster than the JDK UUID generator,design_debt,non-optimal_design
camel,3139,description,The Java UUID is too slow. Switching back to AMQ based will improve performance. We should just have to switch to use the JDK UUID for the camel-gae component as it cannot use the AMQ based.,design_debt,non-optimal_design
camel,3139,comment_0,"Some numbers on my laptop from a single threaded test looping 500k 2010-09-21 08:33:14,070 [main ] INFO - First id: 2010-09-21 08:33:16,782 [main ] INFO - Last id: 2010-09-21 08:33:16,787 [main ] INFO - Took 2.713 seconds 2010-09-21 08:42:41,132 [main ] INFO - First id: 2010-09-21 08:42:41,303 [main ] INFO - Last id: 2010-09-21 08:42:41,309 [main ] INFO - Took 0.172 seconds 2010-09-21 08:37:31,807 [main ] INFO - First id: 1 2010-09-21 08:37:31,905 [main ] INFO - Last id: 500002 2010-09-21 08:37:31,910 [main ] INFO - Took 0.099 seconds",non_debt,-
camel,3139,comment_1,trunk: 999283.,non_debt,-
camel,3139,comment_2,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3157,summary,"Update Exchange javadoc to better explain properties, in, out, mep etc.",documentation_debt,low_quality_documentation
camel,3157,description,The Exchange java doc should be updated to help end users more about the API of the Exchange,documentation_debt,low_quality_documentation
camel,3157,comment_0,See current source code for update Feedback welcome,non_debt,-
camel,3157,comment_1,"Good work! If I may add something, then maybe a comment that the getOut/getIn methods does not map to the InOut/InOnly patterns.",documentation_debt,low_quality_documentation
camel,3157,comment_2,Thanks Tarjei I have added more notes to Exchange java doc.,documentation_debt,outdated_documentation
camel,3157,comment_3,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3160,summary,Add alternative start token for simple language to avoid clash with Spring PropertyPlaceholder,non_debt,-
camel,3160,description,"When using Spring property placeholders it will scan for ${ } tokens, which is the same set the Camel Simple language uses. This makes it harder to configure file endpoints where the file name is a simple expression because Spring interferes. So by introducing the $simple{ as alternative start token, we can avoid the clash.",non_debt,-
camel,3160,comment_0,trunk: 1001371.,non_debt,-
camel,3160,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3168,summary,Add REST based example using restlet or cxfrs as component,non_debt,-
camel,3168,description,We need a REST based example to show how to do that. It should accept XML/JSON as input.,documentation_debt,low_quality_documentation
camel,3168,comment_0,Isn't there already an example here:,non_debt,-
camel,3168,comment_1,Chris Love said on twitter he is working on a REST example for Camel,non_debt,-
camel,3168,comment_2,I've created a simple REST based CRUD application with Restlet and JDCB components here I think it might be a good match for this issue and CAMEL-3096 combined. WDYT?,non_debt,-
camel,3168,comment_3,Yeah that is a good start.,non_debt,-
camel,3168,comment_4,There is a few rest-dsl examples,non_debt,-
camel,3203,summary,Quartz routes are not started if quartz component is referenced after context was started,non_debt,-
camel,3203,description,"Quartz routes are not active if added after camel context was already started. Here is an elaborate description of the problem and a sample project that reproduces it: Sorry, no patch this time because I may not know enough about the internals of QuartzComponent, hesitate to offer anything but a boolean flag somewhere.",non_debt,-
camel,3203,comment_0,Settings as minor as there is a workaround,non_debt,-
camel,3203,comment_1,trunk: 1005489. Thanks for reporting and having a sample application to demonstrate the issue.,non_debt,-
camel,3203,comment_2,"Thank you Claus, no problem. I have another issue on Quartz coming. I will describe it in the next ticket.",non_debt,-
camel,3203,comment_3,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3217,summary,AMQP Osgi Test.,non_debt,-
camel,3217,description,Trivial AMQP test for PAX Exam.,non_debt,-
camel,3217,comment_0,Johan you are a committer now. Can you port your unit test to Apache?,non_debt,-
camel,3217,comment_1,Lets close old ticket that are not in demand.,non_debt,-
camel,3287,summary,remove http feature from Camel features as its duplicated in Karaf,code_debt,duplicated_code
camel,3287,description,In some cases it makes not possible to install camel-cxf out of the box etc.,non_debt,-
camel,3287,comment_0,trunk: 1028202.,non_debt,-
camel,3287,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3289,summary,Camel fails to start if no DNS resolution for hostname is available,non_debt,-
camel,3289,description,See AMQ-2965 We should apply this patch to Camel as well as we are using an id generator based on AMQ.,non_debt,-
camel,3289,comment_0,trunk: 1028229.,non_debt,-
camel,3289,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3298,summary,should close the connection when it is stopped.,non_debt,-
camel,3298,description,Here is a the mail thread[1] which is discussing about it.,non_debt,-
camel,3298,comment_0,Applied the patch into revision 1031458.,non_debt,-
camel,3298,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3315,summary,Upgrade to Jasypt 1.7,non_debt,-
camel,3315,description,Jasypt 1.7 has trimmed down on the 3rd party dependencies which means we can get rid of some libraries,build_debt,over-declared_dependencies
camel,3315,comment_0,We may have to have the SMX team create a new OSGi bundle for Jasypt 1.7,non_debt,-
camel,3315,comment_1,They have 1.7,non_debt,-
camel,3315,comment_2,Tracy I think the SMX bundle is *wrong* as Jasypt 1.7 got rid of commons-lang and commons-codec (the latter has bugs). So we need an updated SMX bundle which *do not* important/use any of those commons stuff.,architecture_debt,using_obsolete_technology
camel,3315,comment_3,I raised SMX4-736 about that and I'm fixing it right now.,non_debt,-
camel,3315,comment_4,I fixed it in ServiceMix Jasypt 1.7 bundle and deployed 1.7_2-SNAPSHOT artifact. I plan a ServiceMix bundles release end of this week.,non_debt,-
camel,3315,comment_5,I would like to upgrade to Jasypt 1.7 as we can lose the JDK1.5 stuff and other dependencies which is no longer needed.,architecture_debt,using_obsolete_technology
camel,3315,comment_6,trunk: 1074408.,non_debt,-
camel,3336,summary,Quartz component has minor issues with old versions of Quartz,non_debt,-
camel,3336,description,"Hello I have run into two issues using camel-quartz with old versions of Quartz (Quartz 1.6 and 1.7): 1. With Quartz 1.6, at shutdown, I get a message: 2. Starting of a delayed timer fails. I think warning the user would be a better idea. Patch is attached (it basically catches the NoSuchMethodError exceptions and acts accordingly)",non_debt,-
camel,3336,comment_0,Attached patch generated against the CAMEL Quartz trunk rev. 1031971,non_debt,-
camel,3336,comment_1,There is no guarantee that we support older versions of quartz. The component is build with 1.8.x.,non_debt,-
camel,3336,comment_2,trunk: 1034337.,non_debt,-
camel,3336,comment_3,Thanks for the patch.,non_debt,-
camel,3336,comment_4,"Hi Claus Thank you for having applied the patch. I know CAMEL is not supposed to support all versions of Quartz, but the patch being simple and useful for some use cases I did provide it. Cheers",non_debt,-
camel,3336,comment_5,Yeah the patch is great. Just that it should not have been categories as a bug :),non_debt,-
camel,3336,comment_6,You're probably right... :) Thank you,non_debt,-
camel,3349,summary,Race condition found in CxfRsEndpoint while getting the endpoint binding under load and performing sync and async invocation,code_debt,multi-thread_correctness
camel,3349,description,"The CxfRsEndpoint's getBinding method is not thread safe. At a customer site, I ran into an issue at startup if 2 threads raced to perform sync and async invocation, the code for getBinding (given below) would react in the following way. - Thread 1 would proceed to create a binding object - Thread 2 would mean while still find the binding to be null and proceed to create a new binding - Meanwhile thread one would have its binding and set the Atomic boolean for binding initialized and proceed to set the HeaderStrategy. - Thread 2 meanwhile would overwrite the original binding object and find that Atomic boolean already set and would have no way to associate a object since the flag is up. - In the absence of a copying of ProtocolHeaders etc will throw exceptions on every following request/invocation. public CxfRsBinding getBinding() { if (binding == null) { binding = new if { LOG.debug(""Create default CXF Binding "" + binding); } } if && binding instanceof { } return binding; }",requirement_debt,non-functional_requirements_not_fully_satisfied
camel,3349,comment_0,"Made the getBinding() method synchronized to overcome this issue. The penalty for this is very minimal since, the need for creating a binding is only on the first set of invocations. The binding is then held until the endpoint is in operation.",code_debt,multi-thread_correctness
camel,3349,comment_1,Applied patch to subversion trunk as revision r1037070,non_debt,-
camel,3349,comment_2,Nice catch. I wonder if the initialization of the binding can be done in doStart. This is much better as it would avoid the synchronized block on the getter. Which I assume is invoked lazy at runtime on processing a new Exchange. Generally initialization should be done in doStart because starting it is single threaded and we don't care _so much_ about performance at startup.,code_debt,multi-thread_correctness
camel,3349,comment_3,"Hi Claus, Yes, I completely agree. I was hesitating to do this since I have not studied this camel-cxf component code closely and was worried about side-effects and multiple rounds of testing since it is a heavily used component. BTW, I found this at a customer site and I have given them an identical patch for camel version 2.2. I will make this change in the coming weeks. Cheers, Ashwin...",non_debt,-
camel,3349,comment_4,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3351,summary,camel-irc component silently fails on nick collision,non_debt,-
camel,3351,description,When the camel-irc component connects to an irc server and there's a nick collision it silently fails. Also note there is no camel-irc component in Jira.,non_debt,-
camel,3351,comment_0,This will cause an NPE when the route is actually called but the root cause is hidden.,non_debt,-
camel,3351,comment_1,All irc events are just logged (I missed it in the logs). In this case the collision is logged as well as the server initiated disconnect. However the component code is unaware of the state. In general there are a lot of useful things that could be done with the various IRC events. Specifically though the code needs to be aware of disconnects.,design_debt,non-optimal_design
camel,3351,comment_2,I got carried away and made several changes and fixed a minor bug. Here are my notes: IrcComponent: - Removed IrcConfiguration member variable. Didn't make sense. Removed constructor with IrcConfiguration as the param. - ircLogger moved to method. IrcProducer: - Changed doStart to call instead of doing it in the start method. Ditto for IrcConsumer removing dupe code. - Check to see if we're still connected before sending a message in process. If disconnected throw a - Removed unused imports - Add a listener so we can get error messages - Change listener type from to IRCEventAdapter and added getter/setter for easier testing - doStop didn't remove the listener fixed. Added. This would've caused an NPE if a user was stopping individual routes. IrcConsumer: - Changed doStart to call instead of doing it in the start method. - Removed unused imports - Added check in onKick to see if we got kicked and rejoin if so - Change listener type from to IRCEventAdapter and added getter/setter for easier testing IrcEndpoint: - Extracted method getExchange. Same 2 lines of code in 9 methods. Slightly cleaner this way. - Added handleIrcError to handle any IRC errors that the producer or consumer hit - Added handleNickInUse to handle nick in use errors. On endpoint startup this would cause a failed connection. For the Consumer this would just mean we'd never consume anything. For the Producer sends would throw an NPE (now there's a check for a valid connection and is thrown instead) - Added method joinChannels - Added method joinChannel IrcConfiguration - Add autoRejoin setting - Changed key storage to a Dictionary. Added several new tests using mockito,code_debt,duplicated_code
camel,3351,comment_3,Applied patch with thanks to Tracy.,non_debt,-
camel,3351,comment_4,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3366,summary,Let FtpProducer support stepwise when uploading a file using a path,non_debt,-
camel,3366,description,"Some FTP server may have trouble uploading a file using a path, and requires to use a stepwise manner to CD to the target directory before uploading the file.",non_debt,-
camel,3366,comment_0,trunk: 1039026.,non_debt,-
camel,3366,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3376,summary,Use ServiceMix castor bundle,non_debt,-
camel,3376,description,"To avoid to go on the Springsource repo, I've created a ServiceMix Castor OSGi bundle. We could use it in the Camel features descriptor.",non_debt,-
camel,3376,comment_0,trunk: 1049059.,non_debt,-
camel,3376,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3430,summary,has issues with interception http endpoints which has multiple parameters,non_debt,-
camel,3430,description,See nabble The issue is when any endpoints have parameters which may be re-ordered when the endpoint is normalized.,non_debt,-
camel,3430,comment_0,trunk: 1049455.,non_debt,-
camel,3430,comment_1,This issue is fixed by CAME-3434 as well. As the first fix wasn't sufficient.,non_debt,-
camel,3430,comment_2,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3440,summary,Verify release notes links in the wiki,non_debt,-
camel,3440,description,"After the jira migration from activemq to the main ASF jira, the project id changed from 11020 to 12311211. As a result all links to Release Notes are broken and need to be updated.",documentation_debt,outdated_documentation
camel,3440,comment_0,All release note pages from 1.0.0 to 2.5.0 updated to use the new jira projectId and corresponding new version ids.,non_debt,-
camel,3451,summary,Rely on default values for attributes in CamelContext in Spring XML to avoid emitting default values when outputting the route as XML,non_debt,-
camel,3451,description,"If you let a Camel route output in XML then you will get hose attributes on CamelContext As we can let Camel know it should use a default value if its not pre configured, then we can change those default settings to null, to avoid JAXB outputting them. We do this for many of the other attributes. So this is just to get the remainder fixed.",non_debt,-
camel,3451,comment_0,trunk: 1051484.,non_debt,-
camel,3451,comment_1,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3461,summary,Put the CamelCxfMessage into Camel Message Header,non_debt,-
camel,3461,description,"There some important information such as ServletRequest can be access from the CXF Message, we need to let DefaultCXFBinding copy the CXFMessage into camel message to access these information from the Camel route.",non_debt,-
camel,3461,comment_0,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3461,comment_1,Could this possible also be added to,non_debt,-
camel,3462,summary,JmsProducer in InOnly MEP should use replyTo if configured on endpoint/header,non_debt,-
camel,3462,description,This allows you to with a InOnly MEP exchange to send the message to a JMS queue with a JMSReplyTo. Then you may have another route which can pickup and process those messages coming back to the JMSReplyTo queue. See nabble,non_debt,-
camel,3462,comment_0,trunk: 1052759.,non_debt,-
camel,3462,comment_1,trunk: 1061298. Fixed an issue that should always be configured to allow JMSReplyTo to be send in InOnly mode.,non_debt,-
camel,3469,summary,Add a wiki page which describes the procedure for updating the Camel dependencies,non_debt,-
camel,3469,description,"Upgrading to newer versions of the dependencies is indeed normally a simple procedure. If we add a wiki page with the procedure (which pom should be updated and checked, do we need an OSGI bundle from the ServiceMix guys, run the full test suite, ...), it could be one of the ""low hanging fruits"" for new to work on Camel.",documentation_debt,low_quality_documentation
camel,3469,comment_0,I will document this procedure by working on,documentation_debt,low_quality_documentation
camel,3469,comment_1,Added the wiki page [Upgrade dependency Would be nice if an English native speaker and one of the core committer could review this page.,non_debt,-
camel,3469,comment_2,Closing all resolved tickets from 2010 or older,non_debt,-
camel,3478,summary,Camel-XMPP Consumer doesn't pass XMPP headers,non_debt,-
camel,3478,description,"XMPP Consumer should pass XMPP headers as message headers, just like XMPP Producer accepts message headers as XMPP headers.",non_debt,-
camel,3478,comment_0,Thank you Claus! Much appreciated. -- Sent from my mobile device Hendy Irawan www.HendyIrawan.com,non_debt,-
camel,3478,comment_1,"Added XmppConstants and the following items to the message headers: Message type, Subject, Thread ID, From, Packet ID and To.",non_debt,-
camel,3478,comment_2,I'm going to rename the constants. New patch in a bit.,non_debt,-
camel,3478,comment_3,Fixed the Constant values. All set.,non_debt,-
camel,3478,comment_4,trunk: 1056660. Thanks Tracy for the patch.,non_debt,-
camel,3490,summary,camel-servlet should register on the OSGi HTTP service if deployed in OSGi,non_debt,-
camel,3490,description,None,non_debt,-
camel,3490,comment_0,See,non_debt,-
camel,3490,comment_1,"camel-sevlet can't know the aliens, servlet name and http context before it registers the servlet into the OSGi http service. It should be done in the customer bundle like the Web.xml does. In CAMEL-3485 you can bind the CamelServlet with the Camel-Servlet component easily, if the customer bundle activator export the Servlet as a CamelServletService like this.",non_debt,-
camel,3490,comment_2,"Hi Willem, I also had the problem of registering the camel servlet in osgi. I think I have found a quite nice solution. The source is attached. Perhaps this could solve this issue. The ServletRegisterer could be put in the camel servlet code. The beans.xml should be put in the META-INF/spring directory of the user. What do you think? Christian",non_debt,-
camel,3490,comment_3,"Hi Christian, I think the ServletRegisterer can help the user set the aliens, servlet name and http context. +1 for putting this file into the camel-servlet component. But I think the user should write the beans.xml himself to load the camel context, and setup the servlet. Willem",non_debt,-
camel,3490,comment_4,"I have committed the What about the I guess it could still be needed to share a servlet among several bundles. Btw. I thought about a completely different solution to this for OSGI. We could create a Servlet for each endpoint, register it when the endpoint comes up and deregister it when it goes down. At least in OSGI I think this would be much simpler. I dont know though if the same can be d done for servlets ouside osgi.",non_debt,-
camel,3490,comment_5,"Christian I think you must put the osgi code in a sub package, eg This ensures the classloader in non osgi environments will not barf about not able to find OSGI JARs. We do the same in camel-core, see the Activator class in the core, as its in a .osgi sub package.",non_debt,-
camel,3490,comment_6,Done. I am curious though why it is necessary. Is it because of the package scan for components? In normal class loading I thought each class would be loaded on its own.,non_debt,-
camel,3490,comment_7,What's the status of this ticket?,non_debt,-
camel,3490,comment_8,"I think we can resolve this issue as user can use the ServletRegiester to deploy the servlet into the OSGi as they want, and the bean.xml shows how to load the camel context at the same time.",non_debt,-
camel,3490,comment_9,Yes .. I first thought we could do even better by registering one servlet per endpoint. Now I found that this is only possible for osgi at the moment. I dont think it would be a goo idea to have osgi and non osgi so different. So I think what we now have with the is a good as we can get it before we can use servlet 3.0 so lets close this.,non_debt,-
camel,3518,summary,Properties component - Add SPI to provide a hook for looking up a property on demand,non_debt,-
camel,3518,description,"See nabble We need a simple API for end users to plugin custom impls which can lookup property values on demand. For example from JNDI, database, zookepper or whatever. The current API is based on returning a Properties instance. That is a bit to coarse grained.",non_debt,-
camel,3518,comment_0,We can do this now using custom functions etc.,non_debt,-
camel,3524,summary,camel-jetty - Add option to set jetty continuation timeout,non_debt,-
camel,3524,description,"By default Jetty uses 30 sec timeout for continuations. We should allow end users to configure a value of choice, in case their system may take longer time to process an exchange.",design_debt,non-optimal_design
camel,3524,comment_0,trunk: 1057238. Wiki updated,non_debt,-
camel,3529,summary,camel-atom - Issue with seda consumer,non_debt,-
camel,3529,description,"This test fails There is an issue with camel-atom starting 2 consumers. I have to dig into this, next morning.",non_debt,-
camel,3529,comment_0,trunk: 1058013.,non_debt,-
camel,3541,summary,"OnException - In XML DSL should validate that one exception class has been specified, otherwise it should fail",non_debt,-
camel,3541,description,If you have That should validate that an exception class has specified. As it should be. The Java DSL also requires at least 1 exception class defined.,non_debt,-
camel,3541,comment_0,trunk: 1058879.,non_debt,-
camel,3576,summary,camel-jms - Provided a task executor by default,non_debt,-
camel,3576,description,When using camel-jms the consumers dont provide a default task executor. That means spring just creates a thread manually and dont reuse the thread. We should provided a task executor from camel using the This allows us to use human readable thread names which can provide details about the consumer. The thread pool is also managed and even a 3rd party provider can manage thread creation. This only works when using Spring 3 as the task executor API is now aligned with the executor service api from the JDK. For Spring 2.x users we should not do this. We will then have to detect the spring version in use.,design_debt,non-optimal_design
camel,3576,comment_0,trunk: 1067658.,non_debt,-
camel,3576,comment_1,"trunk: 1067682. Don't use thread pool for single threaded reply manager as Spring DMLC is a bit pants as it will keep using new tasks every second when idle, and that just confuses people, as task count will grow very large.",design_debt,non-optimal_design
camel,3597,summary,- Should shutdown broker last to cleanly shutdown,non_debt,-
camel,3597,description,"The example uses an embedded broker which should be configured to shutdown after Camel, so the shutdown is appropriate.",non_debt,-
camel,3597,comment_0,trunk: 1064478.,non_debt,-
camel,3637,summary,Mistake in camel-eventAdmin feature,non_debt,-
camel,3637,description,"The Camel Karaf feature camel-eventAdmin is not correct. The features is described as follow: <feature version=""2.6.0"" <feature version=""2.6.0"" <bundle</feature but the camel-eventAdmin artifact correct name is camel-eventadmin: This typo mistake provides: Downloading: [INFO] Unable to find resource in repository central [INFO] [ERROR] BUILD FAILURE [INFO] [INFO] Can't resolve bundle [INFO] I'm gonna submit a patch to fix that.",documentation_debt,low_quality_documentation
camel,3637,comment_0,trunk: 1067890. Thanks for patch.,non_debt,-
camel,3660,summary,camel-hl7 should support no startByte,non_debt,-
camel,3660,description,"In Japan the HL7 message require no start block, the start byte currently is mandatory in the camel-hl7 component.",non_debt,-
camel,3660,comment_0,Do you have a link to some spec or description about this?,non_debt,-
camel,3660,comment_1,I'll also try to clarify this with some HL7 experts next week and see how this can be addressed.,non_debt,-
camel,3660,comment_2,Sorry for response late. I did not find a formal specification of such a Japan lower layer protocol. And I am not very sure whether the absence of the <SB> character (<0x0B>) means that this is not compliant with the HL7 defined v2 MLLP.,non_debt,-
camel,3660,comment_3,"I also checked that with the specs and some HL7 expert. Having no start byte is not covered by MLLP (and related best practices and IHE transactions). On the other hand, if support for missing start bytes is useful to camel-hl7 users, it makes sense to support it.",non_debt,-
camel,3675,description,The Camel Properties web page states that property placeholder are supported on the <jmxAgent The method needs to call passing in the value to get property placeholders to work with this attribute.,non_debt,-
camel,3675,comment_0,trunk: 1071531.,non_debt,-
camel,3675,comment_1,Thanks for reporting.,non_debt,-
camel,3677,summary,"When splitting inside another split, the custom aggregationStrategy is not used.",non_debt,-
camel,3677,description,"When splitting inside another split, the custom aggregationStrategy is not used. For example in the route: (where the does nothing more than to concat the bodies with a space inbetween.) The expected results would be: and But that is not what happens. The actual results are two times the same: The reason is, that the strategy is not used. In the class in the method {{protected AggregationStrategy exchange)}}, the first step is to find an aggregationStrategy in the Exchange. This is set to and because it is not null, this aggregation strategy will be used, not the one declared for the splitter.  A workaround would be to remove the AggregationStrategy of the Exchange, before it is aggregated, by using a processor with the following process method: After integrating this in my route, I got the desired results.",design_debt,non-optimal_design
camel,3677,comment_0,Can you try with Camel 2.6.0 - We have fixed issus with splitter in that release.,non_debt,-
camel,3677,comment_1,"Yes, sorry, I didn't use 2.6 yet, because I had different issues, and I didn't find the bug reported, so I didn't think it would have been fixed yet. But: yes! You fixed it in 2.6. My routes are working correct without my workaround.",non_debt,-
camel,3677,comment_2,Workaround no longer needed.,non_debt,-
camel,3690,summary,Endpoints may be shutdown twice as they are tracked in two lists in CamelContext,non_debt,-
camel,3690,description,Endpoint is a Service which means they are listed in both a endpoint and service list. They should only be listed in the endpoint list. This avoids issues with endpoints may be shutdown twice when Camel shutdown. See nabble,non_debt,-
camel,3690,comment_0,trunk: 1072897.,non_debt,-
camel,3709,summary,interceptFrom and from(Endpoint) don't work together,non_debt,-
camel,3709,description,"When using together with from(Endpoint), the below Exception occurs during the routes building process. Looking at reveals, that the FromDefintion just created has no URI. That causes the comparison to all the interceptFroms' URIs to fail. As far as I can tell, the way to fix this would be to add in the constructor endpoint)}}. Below the stack trace, there is a unit test that demonstrates the issue. Until it if fixed, it can be easily circumvented by adding the commented-out line, and then change to",test_debt,low_coverage
camel,3709,comment_0,You found the easter egg. There is a todo in the source code to support endpoint via ref's // TODO: Support lookup endpoint by ref (requires a bit more work),requirement_debt,requirement_partially_implemented
camel,3709,comment_1,trunk: 1074045,non_debt,-
camel,3709,comment_2,trunk: 1074058. Now works when from is using ref instead of url as well.,non_debt,-
camel,3737,summary,"in XML allows 2 configurations to be specified, <batchConfig> and <streamConfig> which is not ideal.",non_debt,-
camel,3737,description,Right now its possible in XML to specify both kinds of configuration; batchConfig and streamConfig. This isn't ideal and the definition is a bit floored. It should probably use the same JAXB stuff as MarshalDefinition so that it can take one element of a list of possible value types:,non_debt,-
camel,3737,comment_0,Looks like the needs to be the same kind of thing so it can be properly configured in XML,non_debt,-
camel,3737,comment_1,is fine in trunk :) its 2.6 that its not usable,non_debt,-
camel,3737,comment_2,trunk: 1075873.,non_debt,-
camel,3764,summary,Use antrun task for schemagen in camel-web,non_debt,-
camel,3764,description,to replace the use of the hacked and make it consistent across all components.,design_debt,non-optimal_design
camel,3764,comment_0,Done in r1079075.,non_debt,-
camel,3769,summary,Mail component issue with starttls option,non_debt,-
camel,3769,description,"The problem occurs when I read from a pop3 endpoint and send to an smtp endpoint in the same camel context with the and options. Required Java options for starttls are set: When I only configure one of either route, everything works fine. When I configure both, I get the following ; } This is because getDefaultInstance creates a Session object the first time it is called. Then it caches that Session and returns it for all subsequent calls. It also ignores the new and different properties for the second route. See also",non_debt,-
camel,3769,comment_0,I suggest to change this section of the code to always use getInstance instead of getDefaultInstance.,code_debt,low_quality_code
camel,3769,comment_1,"At least in my tests, the debugMode=true did not work either. I think this is beacuse creates a Session and sets the debug option for this session object. In the next few line the JavaMailProperties are set which actually resets the Session object in JavaMailSenderImpl answer to null.",non_debt,-
camel,3769,comment_2,"Changes are highlighted with kex word ""BUG FIX""",non_debt,-
camel,3769,comment_3,trunk: 1079708.,non_debt,-
camel,3775,summary,Properties component - Add option to set prefix and postfix tokens,non_debt,-
camel,3775,description,We should allow end users to set custom prefix and postfix tokens for the Camel properties component.,non_debt,-
camel,3775,comment_0,This is more important now since Groovy script has its GString that clash with simple ${ } syntax.,non_debt,-
camel,3775,comment_1,Its the simple language that has the GString clash with Groovy,non_debt,-
camel,3777,summary,Put and in it's own component to allow users to reuse it in its own OSGI integration tests,design_debt,non-optimal_design
camel,3777,description,Put and in it's own component (camel-osgi-test) to allow users to reuse it in its own OSGI integration tests It's the same as with CamelTestSupport and,design_debt,non-optimal_design
camel,3777,comment_0,"We should possible have a that uses pax exam as its base line. Willem have upgraded to pax exam 2, so it may be easier to do now.",design_debt,non-optimal_design
camel,3777,comment_1,There is some ticket about an osgi test component. However as there is pax-exam this is less needed as ppl can use plain pax-exam,design_debt,non-optimal_design
camel,3816,summary,in DefaultHttpBinding,non_debt,-
camel,3816,description,See nabble,non_debt,-
camel,3816,comment_0,"The end user is using curl to do a HTTP POST which seems to not send any HTTP headers at all, which often is unlikely and thus not seen this issue before.",non_debt,-
camel,3816,comment_1,Remember to fix this in camel-http4 as well.,non_debt,-
camel,3816,comment_2,"I just did some test on the issue, and found out it was caused by the request message was not right. As the Http Request was sending with which means the message body should be some string like but with the ""curl -v -d file.xml the message body became ""file.xml"", and DefaultHttpBinding throw the exception like that. And it has nothing to do with the Null Http header. I just did a quick fix for in Camel trunk, please check out latest Camel 2.8-SNAPSHOT to verify the fix.",non_debt,-
camel,3864,summary,camel-cache the ehcache.xml file is not up to date- have invalid parameters name,non_debt,-
camel,3864,description,The file ehcache.xml which is the default configuration file for ehcache initialized by camel-cache component contains deprecated description about jms replication.,documentation_debt,outdated_documentation
camel,3864,comment_0,I will prepare the patch and post it soon. Please note that example files at ehcache homepage are deprecated too :). How do i know that? I have tested it in real life with newest versions and have seen it's source code ;),non_debt,-
camel,3864,comment_1,Fixed parameters names. Note that i have found many posts across internet posted by people asking why it doesn't work as they have followed docs/examples etc. Simply it doesn't work because parameter have been renamed. Putting wrong parameter names causes NPE on put method to InitialContext which tells nothing about the real problem making it difficult to solve. This update will save your time as studying ehcache code (wondering what's wrong) is no more needed.,non_debt,-
camel,3864,comment_2,Thanks for the patch.,non_debt,-
camel,3875,summary,camel-cache (camel-itest-osgi)- Simple CacheManagerFactory spring ref. + ehcache config from xml test in OSGi env.,non_debt,-
camel,3875,description,Testing is cacheManagerFactory is really referenced and is ehcache configured with the right xml file. This test was mad to bug fix: HAVE A FUN! :),non_debt,-
camel,3875,comment_0,A patch to camel-itest-osgi project,non_debt,-
camel,3875,comment_1,Recreated again a i forgotten to attach one file to the patch- it happens :),non_debt,-
camel,3875,comment_2,"works fine, ready to test'n'go :)",non_debt,-
camel,3875,comment_3,Thanks for the test.,non_debt,-
camel,3888,summary,Tighten up @deprecated in camel-core to add more details what alternatives to use,non_debt,-
camel,3888,description,We should ensure all deprecated classes/methods in camel-core is documented what alternatives to use. Also if possible give a hint when it could be removed.,documentation_debt,low_quality_documentation
camel,3888,comment_0,"This is definitely not resolved. There are a lot of tests that are using which is marked deprecated. So far, I haven't found any documentation about an alternative to use. THere are a bunch (22) of tests that use it and I haven't been able to figure out any type of alternative for it. Thus, IMO, if there isn't an alternative and is important enough to be required by a bunch of tests, I have to wonder why it's deprecated. But no details on that either. Similar situation for Deprecated, no alternative. For TryDefinition, I could see a ""rethrow()"" or similar method added to define that behavior (which would match the Java users expectation), but that certainly isn't there right now.",documentation_debt,low_quality_documentation
camel,3888,comment_1,"Dan as said before just because a method is deprecated do *not* mean we should not unit test it. And hence why you would see unit test that uses the deprecated methods. The handled(true|false) on try .. catch, error handler, was a mistake and there are *no* alternative on those. Instead you should use for example onException (exception clause). Or in case of a doCatch you can rethrow the exception if you want that.",code_debt,low_quality_code
camel,3888,comment_2,"@Claus, if you read more carefully, the reason for reopening was that we need to document what to use instead of the deprecated feature. Has this been done? How would a user know what to do? Why did you mark the issue as resolved?",documentation_debt,outdated_documentation
camel,3888,comment_3,"@Claus, this issue was initially assigned to you. If you don't plan to add the necessary documentation, unassign or assign to somebody else. Just don't mark it resolved, because it is not.",non_debt,-
camel,3888,comment_4,Yes a suggestion for alternative has been added in the @deprecated javadoc.,non_debt,-
camel,3888,comment_5,"Dan/Hadrian you are welcome to look at the added documentation to @deprecated javadoc. The easiest is probably to see this commit rev: I spotted a minor mistake and corrected it in this: I checked all the i could find in (eg in main, and *not* test). You are of course welcome to improve the documentation if you find anything needed to be added.",documentation_debt,low_quality_documentation
camel,3888,comment_6,I believe most of the users are looking at the wiki/manual we need to document there. I'll take care of it.,documentation_debt,outdated_documentation
camel,3888,comment_7,Information about @deprecation in the API is standard documented in the java doc. And this is where users would expect the information and go look. We have never had any special wiki page or the likes where @deprecated API is being further detailed. IMHO this ticket can be resolved.,documentation_debt,outdated_documentation
camel,3888,comment_8,deprecated API is standard documented in the javadoc as any other project,non_debt,-
camel,3967,summary,Inject custom package scan class resolver asap to ensure loading of classpath works,non_debt,-
camel,3967,description,See nabble The issue is when using JBoss and Camel 2.7,non_debt,-
camel,3975,summary,Extend the JDBC and JPA based idempotent repository with a timestamp column,non_debt,-
camel,3975,description,See for details.,non_debt,-
camel,3975,comment_0,Committed r1103388,non_debt,-
camel,3975,comment_1,Updated the pages: - - -,non_debt,-
camel,3990,summary,Upgrade FreeMarker to 2.3.17,non_debt,-
camel,3990,description,"A new version has been released, and even includes a security fix, among other changes, as noted at There is no impact to the Java code.",non_debt,-
camel,3990,comment_0,We need an osgi bundle for freemarker to be released first before we can upgrade in Camel,non_debt,-
camel,4007,summary,Attachment Names not Encoded Correctly,non_debt,-
camel,4007,description,"If attachment file names contain unicode characters, they do not appear correctly encoded in the message.",code_debt,low_quality_code
camel,4007,comment_0,A patch is welcome.,non_debt,-
camel,4007,comment_1,Can you post an example of a filename having unicode characters?,non_debt,-
camel,4007,comment_2,"It could be that Spring or Javamail messes with the filename when there are unicode characters, surrounding it with quotes. I copied the passing unit for mail attachments and changed it to include an umlaut and now the test fails. Take a look at the output, extra quotes for such a filename... Expected :image/jpeg; name=logo2.jpeg Actual :image/jpeg; name=""logo2.jpeg"" The failing test is attached.",non_debt,-
camel,4007,comment_3,It works fine in camel. You have to use \u00DC when writing tests with non ascii inputs. \u00DC is unicode for the Umlaut char.,non_debt,-
camel,4037,summary,"Aggregator - Recovery when only having completion timeout condition, should re-establish timeout map upon restart",non_debt,-
camel,4037,description,"When using the persistent aggregation repository and you only use completion timeout, then upon restart, the currently partail completed exchanges does not re-activate their timeout completions in the timeout checker. So they will never complete. We need to store the timeout value for each exchange in the repository, and upon recovery, re-establish the timeout map, so they can timeout. See forum",non_debt,-
camel,4046,summary,Commit 1131092 broke the maven 3 builds,non_debt,-
camel,4046,description,"Commit 1131092 added: which is wrong as the ""components"" artifact is in "".."". The relativePath should just be ""..""",non_debt,-
camel,4046,comment_0,"Actually, leave the new relativePath, change the artifactId to camel-parent.",non_debt,-
camel,4046,comment_1,"Fixed by correcting the artifactId. Should have been 'camel-parent', not 'components'. Thanks for reporting it and providing a solution.",non_debt,-
camel,4132,summary,Update camel-atom to non-incubator version of abdera,non_debt,-
camel,4132,description,The camel-atom component is using an ancient incubator version of abdera which will make it hard to work with camel-cxf. Updating to 1.1.2 which is what CXF uses would help.,architecture_debt,using_obsolete_technology
camel,4132,comment_0,Applied patch with thanks to Dan. I also updated the org.apache.abdera package import version range and added the java mail bundle dependency to the camel-atom feature.,non_debt,-
camel,4139,summary,Unify blueprint and spring namespace parsing in camel-cxf,non_debt,-
camel,4139,description,"Currently, spring parses into a CxfEndpointBean whereas blueprint parses into a CxfEndpoint directly. This mismatch then requires extra casting in the CxfComponent. Also, it means there are features supported by Spring that are not supported yet by blueprint (like configuring interceptors). I'm attaching a patch that removes the spring specific CxfEndpointBean stuff and allows both spring and blueprint to parse directly into the CxfEndpoint (well, subclasses of) so the two approaches can be maintained together. It also reduces the usage of ""Strings"" for properties in CxfEndpoint and uses QNames where appropriate (added a Converter for that) and Class<?",code_debt,complex_code
camel,4139,comment_0,New version of patch that uses the bundle to load the class.,non_debt,-
camel,4139,comment_1,Just committed the patch of removing the and,non_debt,-
camel,4144,summary,Update Karaf validate profile to use Karaf 2.2.2-SNAPSHOT,non_debt,-
camel,4144,description,"As Karaf 2.2.2-SNAPSHOT validate plugin has better support to the Karaf feature validation, we should update the validate profile to use it. Now if you want to validate the change of apache-camel karaf feature, your just need to use this command in the features directory. BTW, it should be safe when we release CAMEL 2.8.0.",non_debt,-
camel,4144,comment_0,We should not release Camel 2.8 with any SNAPSHOT deps. So in case karaf 2.2.2 is not release beforehand we need to revert this patch.,non_debt,-
camel,4144,comment_1,Agree Claus. Let me check on my side if we can release Karaf 2.2.2.,non_debt,-
camel,4144,comment_2,"Jamie is cutting Karaf 2.2.2 currently, so there is a good chance it may be released in time before we start cutting Camel 2.8.",non_debt,-
camel,4144,comment_3,Willem already did and check the behavior on Camel trunk. I also updated camel-2.7.x to use Karaf 2.2.2 with the new behavior.,non_debt,-
camel,4144,comment_4,"We have upgraded to karaf 2.2.2 GA, so I guess this ticket can be resolved?",non_debt,-
camel,4144,comment_5,"Yes, you're right Claus. I updated camel-2.7.x yesterday evening to use Karaf 2.2.2 also.",non_debt,-
camel,4202,summary,camel-jms - Using request/reply over persistent queues should be faster,code_debt,slow_algorithm
camel,4202,description,"See nabble When using persistent replyTo queues for request/reply over JMS, then the ReplyManager should be faster to pickup replies. The default spring-jms timeout is 1 sec and it impacts the performance. Likewise the receiveTimeout should not be set on the reply managers as that does not apply here.",code_debt,slow_algorithm
camel,4202,comment_0,"Okay I think there is a tradeoff if we lower the spring receiveTimeout to lower than 1 sec. For example if we use 1 millis, then it will overload and send pull packets to the AMQ broker to receive messages. The reason why temporary queues is faster is because they dont have any JMSMessageListener as the persistent does. So with temporary queues it can pull every 1sec and pickup messages as fast as possible, as the received message is always for the reply manager.",code_debt,slow_algorithm
camel,4202,comment_1,A workaround is for the end user to use to eg pull messages 4 times per sec and thus be 4x faster.,design_debt,non-optimal_design
camel,4202,comment_2,"If there is an use case for using persistent queues that are *exclusive* for the Camel consumer, then we can avoid using JMSMessageSelector and thus be as fast as temporary queues.",code_debt,slow_algorithm
camel,4202,comment_3,"The reason why the persistent reply to example at the nabble discussion, is because Camel will have to use a Dummy value for JMSMessageSelector when there is no expected replies to receive. And it thus takes 1 sec. for it to timeout, before it can update the JMSMessageSelector with the CorrelationsIDs to receive. We may try to suspend/resume the message listener container, but we could potential end up with some synchronized issue where we would suspend the listener, where as in the mean time on another thread a new message is being send. And thus we may miss resume the listener. Therefore we end up not pulling the message from the AMQ broker. We should add some documentation / FAQ as the drawbacks of using persistent queues.",documentation_debt,low_quality_documentation
camel,4202,comment_4,"A new option is to be introduced: replyToType and its an enum with the following options: Temporary, Shared, Exclusive So if you set the then Camel assumes the reply queue is exclusive for Camel and will not use any JMS message selectors, as it pickup all the messages arriving on that queue. This means its as fast as temporary queues. In fact this will also help highlight the fact the current fixed replyTo queues are consider Shared by default. I will add better documentation at the jms page to help highlight this.",documentation_debt,low_quality_documentation
camel,4202,comment_5,"The is exclusive per CamelContext. So if you run in a clustered environment, then each node should use an unique replyTo destination name. See discussion on nabble on that link from the top.",non_debt,-
camel,4202,comment_6,Patch with code,non_debt,-
camel,4202,comment_7,Updated wiki page with documentation,non_debt,-
camel,4226,summary,"add Expressions support on to(), bean() and beanRef() to allow for dynamic endpints (similar to recipientList)",non_debt,-
camel,4226,description,"just a thought, but any reason we couldn't overload these APIs to also support Expressions? Is there another way to do this? bean(MyBean.class, beanRef(""myBean"", The goal being to make it (slighlty) easier/more flexible than using recipientList for the same...",non_debt,-
camel,4226,comment_0,"Frankly sometimes its best to post on @dev before opening a ticket. Anyway end users should use the EIPs supplied. And the recipient list is the dynamic EIP. There is a FAQ as well We should also be careful to not overload the DSL with new options etc. as it just confuses end users as they get a huge list in the IDE when accessing code completion. Likewise we should keep the DSL in sync between Java, XML and Scala (later is a bit more hard). But the Java and XML is in sync due the JAXB models. And you cannot with JAXB define that an expression should be *optional* so you can keep doing <to uri=""xxx""/ This is not possible and you get a validation exception as it would expect an expression, so you would have to do <to <constant</to Which is ugly/verbose, and breaks every Camel applications that are using XML DSL. I dont think its a good idea to add this only to Java DSL, as we have end users who migrate between Java and XML. Likewise its best they are 1:1.",design_debt,non-optimal_design
camel,4226,comment_1,"Ben you may be able to experiment with ToDefinition to add *optional* support for an But its just not easy with JAXB. If a workaround can be found, we could consider adding it in a future Camel release. It must be optional to avoid breaking a ton of existing apps / tooling etc.",non_debt,-
camel,4226,comment_2,"thanks Claus, I figured there were good reasons this wasn't done already, but its been requested by several people...so I had to ask (I'll do so on the dev list next time). I'll close this out for now, maybe we can revisit it down the road...",non_debt,-
camel,4226,comment_3,See discussion at,non_debt,-
camel,4230,summary,BeanProcessor - Improved exception message if failed to invoke method,code_debt,low_quality_code
camel,4230,description,"See nabble If for some reason the method cannot be invoked, you may get a caused exception We should catch this and provide a wrapped exception with a more descriptive error message, about the bean and method attempted to invoke.",code_debt,low_quality_code
camel,4230,comment_0,"I think it can be done as in the case protected Object invoke(Method mth, Object pojo, Object[] arguments, Exchange exchange) throws { try { return mth.invoke(pojo, arguments); } e) { throw new occurred invoking method: "" + mth + "" using arguments: "" + exchange, e); } catch e) { throw new occurred invoking method: "" + mth + "" using arguments: "" + exchange, e); } }",non_debt,-
camel,4230,comment_1,Thanks for the patch.,non_debt,-
camel,4240,summary,Unable to use XA JMS trabsaction on WebLogic,non_debt,-
camel,4240,description,"There is problem with camel-jms component deployed on WebLogic 10.3.X and XA connection factory. As spring developers suggests, if we use XA transaction on WebLogic we must set SessionTransacted to false and specify transactionManager. See comment #9 by Juergen Hoeller. But camel-jms setup transactionManager for MessageListener only when ""transacted"" property is true. So, it is not possible to use XA transactions with WebLogic JMS.",non_debt,-
camel,4240,comment_0,Andrey do you care to work on a patch and test it to see if works on WebLogic?,non_debt,-
camel,4240,comment_1,"Hi Claus, Yes, of course. I can try to create patch and check it on WebLogic 10.3.3. I can do it next weekend.",non_debt,-
camel,4240,comment_2,Andrey any update on this?,non_debt,-
camel,4240,comment_3,"Hi Claus, I attach two versions of patch for this issue. First - - contains significant changes in transaction settings behavior. Main point is to have identical meaning of and 'sessionTransacted' properties between JMS component and Spring 1. If 'transacted'==false && tm==null && then do nothing with transaction in 2. If 'transacted'==false && tm==null && then create and setup own tm (imho it is not good idea). 3. If 'transacted'==false && tm!=null, then setup only tm in 4. If 'transacted'==true && tm==null && then setup only sessionTransacted in 5. If 'transacted'==true && tm==null && then set sessionTransacted, create and setup own tm. 6. If 'transacted'==true && tm!=null, then set sessionTransacted and tm in Case 3 corresponds to typical J2EE configuration with XA tm. But I would like to do more tests with WebLogic and ActiveMQ. === Second patch version - ""light"" - It only allow setup tm without 'transacted' property. It is almost identical to current implementation and safer.",non_debt,-
camel,4240,comment_4,Thanks. I applied the light patch.,non_debt,-
camel,4272,summary,camel-jdbc should provide a option not set the autoCommit flag,non_debt,-
camel,4272,description,"When the jdbc connection is work as XA resource, the connection autoCommit flag doesn't support to be reset. So we should provide an option in the camel-jdbc endpont not to set the autoCommit flag on the connection.",non_debt,-
camel,4272,comment_0,Committed the patch into trunk. Updated the wiki page with the new added option resetAutoCommit.,non_debt,-
camel,4273,summary,Favor static member classes over nonstatic ones.,code_debt,low_quality_code
camel,4273,description,See,non_debt,-
camel,4273,comment_0,"Babka, thanks for the patch.",non_debt,-
camel,4273,comment_1,You welcome :-) BTW my first name ist Babak not Babka,non_debt,-
camel,4317,summary,Clean up the camel context file of the example after upgrading to CXF 2.4.x,code_debt,low_quality_code
camel,4317,description,"After upgrading to CXF 2.4.x, we don't need to import the resources file like any more, so it's time to clean up these imports.",code_debt,dead_code
camel,4317,comment_0,Applied patch into trunk with revision 2.8.x,non_debt,-
camel,4331,summary,"Avoid the redundant direct dependency on log4j by the components (of the scope 'test'), as it's transitively given for free through the slf4j-log4j12 dependency with the RIGHT / COMPLIANT version",build_debt,over-declared_dependencies
camel,4331,description,See the discussion here,non_debt,-
camel,4331,comment_0,I didn't remove the log4j dependencies by the pom's of camel-web & as there it's of the scope 'compile'. I assume these 2 declared dependencies were on purpose!,non_debt,-
camel,4331,comment_1,Thanks for the patch.,non_debt,-
camel,4357,summary,Move to,non_debt,-
camel,4357,description,I am currently looking into the dependencies betwen packages in camel-core. The packages org.apache.camel and form the camel api. So I am trying to make them not depend on other packages from camel-core. One problem there is the starter class Main. It needs access to impl packages as it needs to start camel. So it should not live in org.apache.camel. I propose to move it to To not break anything right now I will create a deprecated class Main in org.apache.camel that extends the moved Main. We can remove the deprecated version in camel 3.0,architecture_debt,violation_of_modularity
camel,4357,comment_0,There is also a Main class in camel-spring. It was actually started there as the first Main class we offered in Camel.,non_debt,-
camel,4357,comment_1,The one in camel-spring is not problematic as it is in a non API package,non_debt,-
camel,4357,comment_2,"There is a CS error as the license header is missing in one file. When adding a new package in camel-core, a package.html file should be included, which briefly summarizes the package. See some of the other for examples to copy. And in the camel-core/pom.xml file there is some javadoc grouping of packages. I guess the new main package is to be added as well.",code_debt,low_quality_code
camel,4357,comment_3,Ok. Will add that tomorrow,non_debt,-
camel,4398,summary,Add profile to camel-core pom.xml to have tools.jar added as dependency which is needed for CI servers,non_debt,-
camel,4398,description,"See nabble Jenkins will by default insert dependency if missing in published pom.xml, which have hardcoded link path to jenkins server. Instead we need to add a profile which is based on java.home path. See the links from the nabble talk above.",non_debt,-
camel,4398,comment_0,"Sorry the issue is the tools JAR is added as dep on the SNAPSHOTs, but not on the releases etc. So its a bit weird. Track down to be possible in parent/pom.xml with some antrun tasks that does some osgi versioning regex replacement. And since this dependency is added to camel-core it gets inheirted by all the other artifcats. But only for SNAPSHOTs, and not releases. So I guess the release profile makes something different. The tools JAR is thus not needed at all as a dependency to use Camel, so what we want is to get rid of tools JAR in the camel-core/pom.xml file.",design_debt,non-optimal_design
camel,4398,comment_1,"We should upgrade the bundle plugin as it does not need the special versioning stuff that the antrun plugin does, which drags in the tools JAR",non_debt,-
camel,4398,comment_2,"I have a better fix for this.... The tools jar is required for the schemagen stuff, not the OSGi stuff. In anycase, a better fix is coming.",non_debt,-
camel,4417,summary,Move base classes used by components from impl to support,non_debt,-
camel,4417,description,"Several classes in impl are used or extended by components. We should avoid this. The base classes should be moved to support. Examples are DefaultComponent, DefaultEndpoint, DefaultProducer. Another case is the The typeconverter is well placed in impl but the class also has a public static convert method that is used from many components. So this functionality should be moved to processor so it is available to components.",design_debt,non-optimal_design
camel,4417,comment_0,Since this code change will break pretty much any component developed outside of the current source tree this should only be considered for 3.x,non_debt,-
camel,4417,comment_1,I intend to create a stub class that extends the moved class in the old location. So I think this should be compatible. I would like to have such changes in 2.9.x as people then will have some time to adapt and they can already see where we are moving to,non_debt,-
camel,4417,comment_2,"Patch moving all support classes from impl to support. I moved the following classes and placed compatibility classes in their impl location: DefaultComponent, DefaultConsumer, DefaultEndpoint, DefaultExchange, DefaultMessage, DefaultProducer, DefaultUnitOfWork, ExpressionSupport, ProcessorEndpoint, ProducerCache, I moved the following classes without compat stubs as they were not needed outside camel-core: DefaultRouteNode, ExpressionAdapter, MDCUnitOfWork, MessageSupport, SimpleUuidGenerator I moved from processor to util as it is needed from support. The only problematic class I moved was DefaultConsumer as it needed So the above including the inner class had to move to util. The problem here was that the Bridge had to extend DelegateProcessor which of course is in processor. As util should not depend on processor I had to introduce an interface DelegateProcessor in camel that could be used to abstract from DelegateProcessor and This is a good thing anyway and I will open a jira to do this first. I also had to move PipelineHelper and to util as they were used from support classes. This is a fairly large patch. So I am not sure if it is good for 2.9. On the other hand if we wait with this till 3.0 we are either really incompatible or we can not remove the deprecated classes",design_debt,non-optimal_design
camel,4417,comment_3,"@Christian, all the changes you make in 2.9 should be backwards compatible. So if you make any changes, please make sure leave existing classes in place (even as extensions of refactored classes) and change as few tests as possible, ideally none. That ensures two things: one that we didn't break anything and existing code still works, second that users have a migration path that could take at any time. We can remove the old classes later in 3.0. Changes that break backward compatibility I'd leave for later.",code_debt,low_quality_code
camel,4417,comment_4,"API should be kept stable, moving to 3.0.",non_debt,-
camel,4417,comment_5,This change is probably too destructive even for 3.0,design_debt,non-optimal_design
camel,4430,summary,Using the ClientFactoryBean instead of ProxyFactoryBean to create the Client for the CxfProducer,non_debt,-
camel,4430,description,"As the proxy lifecycle cleanup work are done in CXF 2.4.3, we are facing some test failed in camel-cxf. After digging the code, I found the proxy instance which is created by the CxfProxyFactoryBean will be GC and the CXF client which is used in CxfProducer will be affect. The endpoint which is set on the conduit will gone, and CxfProducer will complain it with a NPE exception. We can use the to create client instead of CxfProxyFactoryBean to avoid the GC and NPE exception. I checked the difference between using CxfProxyFactoryBean and they are same in most case. We just need to take care of handler setting part which is used in JAXWS frontend.",design_debt,non-optimal_design
camel,4430,comment_0,Applied patch into trunk and Camel 2.8.x branch.,non_debt,-
camel,4461,summary,Add OSGi integration for camel-hazelcast,non_debt,-
camel,4461,description,None,non_debt,-
camel,4461,comment_0,Attaching some osgi integration tests for camel-hazelcat.,non_debt,-
camel,4461,comment_1,There is some exceptions thrown by hazlecast trying to load its configuration file,non_debt,-
camel,4461,comment_2,Patch applied to trunk,non_debt,-
camel,4461,comment_3,"The first error is a warning that no custom hazelcast config is provided and that hazelcast fallbacks to the default settings. I think the second has to do with shutting down the hazelcast bundle. The test still passes, doesn't it?",non_debt,-
camel,4461,comment_4,Yes the test still passes. But wonder if any end users who use camel-hazelcast sees the same errors when shutting down?,non_debt,-
camel,4470,summary,Expose load statistics in Camel Karaf command,non_debt,-
camel,4470,description,The new load statistics on CamelContextMBean and RouteMBean should be outputted in the Camel Karaf command.,non_debt,-
camel,4470,comment_0,Patch to enable load statistics,non_debt,-
camel,4470,comment_1,Thanks Scott for the patch. Its been applied to 2.10 and 2.9 branches.,non_debt,-
camel,4495,summary,Camel Spring-DSL Validator component does not like BOMs,non_debt,-
camel,4495,description,"Camel Spring-DSL Validator component does not like XML files with BOMs (e.g., EF BB BF -- UTF-8). Note: It also appears that the validator route adds BOMs to XML files.",non_debt,-
camel,4495,comment_0,I now programmatically strip off the BOMs... so this is no longer an issue to me.,non_debt,-
camel,4499,summary,align jasypt bundle version with karaf,non_debt,-
camel,4499,description,We're using version 1.7_2 of and the version of Karaf we're using is on 1.7_3. These should be the same.,non_debt,-
camel,4543,summary,Camel Blueprint support is limited/hardcoded to Aries,non_debt,-
camel,4543,description,"Camel Blueprint support is limited/hardcoded to Aries. This makes it impossible to use in JBoss 7 or with another blueprint implementation like Gemini. The following classes use various things from Aries. * * * * * Now obviously the last three are related to the custom namespace handler for Aries. It would be good if these were moved into their own module, something like or the like. That people can choose to include if they would like to use the custom blueprint namespace handler in Aries. Otherwise the camel-blueprint module should be implementation agnostic and work on all blueprint containers. Not just Aries.",architecture_debt,violation_of_modularity
camel,4543,comment_0,First Patch to remove Aries dependency from,non_debt,-
camel,4543,comment_1,"Patch that creates a new module called and moves all aries specific stuff into that. camel-blueprint no longer depends on any Aries classes. Tested camel-blueprint on JBoss 7.0.2 and works wonderfully. Haven't been able to test the new jar yet, as I haven't setup ServiceMix or the like..",test_debt,lack_of_tests
camel,4543,comment_2,"Bump, any progress on this? Currently we are having to repack the camel-blueprint jar to strip out the Aries specific classes/imports.",non_debt,-
camel,4543,comment_3,Just upgraded to the latest Camel release 2.10.3 and found that Aries specific class usage is more prolific than before. The following classes reference Aries. * * * * Also noticed that it looks like the contents of camel-core-osgi have been included in the camel-blueprint jar.,non_debt,-
camel,4543,comment_4,Can this issue be promoted to an earlier release because it seriously limits the use of camel in osgi projects?,non_debt,-
camel,4543,comment_5,I don't see much activity in Gemini Blueprint There were some commits to master branch recently however - let's wait few months...,non_debt,-
camel,4546,summary,"[WebConsole Archetype] Unable to run with ""mvn tomcat:run"" or in tomcat or jetty server",non_debt,-
camel,4546,description,"Even with a fix archetype (see [issue the only way to get the console running is to launch ""mvn jetty:run"". To reproduce, generate a project with the archetype ""mvn archetype:generate When the project is launched with ""mvn tomcat:run"", it does not display any error And localhost:8080 show a white page I tried with tomcat-maven-plugin 2.0-SNAPSHOT and ""mvn tomcat6:run"" or ""mvn tomcat7:run"", with no success. If you package the application with ""mvn package"" and drop the war in a tomcat server (7.0.22 for me) or a jetty server (7.5.3v20111011), the application start with no error, but localhost:8080 display a 404 page. Out of the box, I think the project should be able to run with tomcat and jetty at least (with or without maven plugin).",non_debt,-
camel,4546,comment_0,"This is not really a Camel problem. The mvn jetty:run works fine with the archetype. Its a bitch to get mvn tomcat7:run to work, as it does not seem to support overlays. The fix is on Apache Tomcat side, not Camel.",non_debt,-
camel,4561,summary,CxfComponent should create a new CxfEndpoint instance if the instance is lookup from configuration registry,non_debt,-
camel,4561,description,"CAMEL-4503 shows the bug, when we have uri parameters which can override the configuration of cxfEndpoint, the instance of cxfEndpoint from the configuration file could be changed with the uri parameters if the CxfComponnet doesn't return a new instance of the cxfEndpoint.",non_debt,-
camel,4561,comment_0,"When you do a .clone() then it copies the List as a reference. So you could potentially make changes in one instance, that reflects the parent instance. So when you clone then you need to do a deep-clone of the List etc. So its fully independent. See for example",non_debt,-
camel,4561,comment_1,"Haven't looked at the config reg stuff, but could the namespace parser set the bean as prototype? Then spring would create a new one each lookup. Does that work with Camel?",non_debt,-
camel,4561,comment_2,"Hi Dan Setting the bean scope to be prototype works great, now the tests are passed. BTW, we can do the same setting in the blueprint parser.",non_debt,-
camel,4561,comment_3,"Willem, cool. Glad that works. It really does make more sense. the cxfEndpoint stuff in Camel is really more of a ""configuration of an endpoint"", not really an endpoint itself (like it is in CXF). Thus, setting to prototype does make more sense. I like the fix. :-) Dan",non_debt,-
camel,4561,comment_4,Applied the patch into trunk and 2.8.x branch.,non_debt,-
camel,4593,summary,Upgrade abdera version to 1.1.3,non_debt,-
camel,4593,description,"There are bunch fixes of ABDERA-281, ABDERA-290 can help camel-atom work smoothly with OSGi platform. We need to upgrade the the abdera to 1.1.3 once it is out.",architecture_debt,using_obsolete_technology
camel,4593,comment_0,We have updated,non_debt,-
camel,4602,summary,file/ftp consumer - The filter option should be able to work on directories as well,non_debt,-
camel,4602,description,"See nabble By invoking the filter for directories, it allows end users to skip entire directories, which would make the polling faster. And no need to walk down unwanted directories. The logic need to apply for both file/ftp consumers, as well having unit tests to ensure it works.",test_debt,lack_of_tests
camel,4602,comment_0,"Hi, I attached possible solution with unit test.",non_debt,-
camel,4602,comment_1,"Thanks Michael for the patch. I went ahead and added a isDirectory method on GenericFile, this avoids any API breakings in the filter interface.",non_debt,-
camel,4602,comment_2,"Your solution is much better. I have a lot to learn about non-business projects. Claus, thanks for the very valuable lesson.",non_debt,-
camel,4607,summary,Have camel:log support using SLF4J Markers,non_debt,-
camel,4607,description,"This issue is in regards to the following post: In short, we would like to be able to make use of Markers, which are part of the SLF4J api. Markers allow you to add an ""ID"" to a log statement. They can then be used as filtering outside of log levels. The end result will be to be able to write something like the following: <camel:log message=""log message"" marker=""myMarker"" level=""DEBUG"" />",non_debt,-
camel,4607,comment_0,Possible patch attached,non_debt,-
camel,4607,comment_1,Taariq thanks for the patch. Do you mind looking into adding support for marker on the log component as well? eg as an option just like the level option we have on this component.,non_debt,-
camel,4607,comment_2,"I have committed the patch to trunk, which add support for marker to the Log EIP",non_debt,-
camel,4607,comment_3,"Sure Claus, LogComponent change is attached. You can drop the extra tests again no problem since the code is well covered.",non_debt,-
camel,4607,comment_4,Thanks for the 2nd patch. Added to log component as well.,non_debt,-
camel,4608,summary,Upgrade jackson to 1.9.2,non_debt,-
camel,4608,description,Jackson 1.9.2 has been released. We are using 1.8.6 currently.,non_debt,-
camel,4657,summary,"camel-jms - Request/Reply - Leak in ActiveMQSessionPool causing it to eat up memory, when using fixed replyTo queue names",design_debt,non-optimal_design
camel,4657,description,"See nabble This bug is in ActiveMQ, but creating a ticket to get it resolved as the leak is apparent when using Spring DMLC with CACHE_SESSION, which Camel by default does when doing request/reply over JMS with fixed replyTo queues. Then the consumer is not cached, and therefore created on each poll, but the ActiveMQSessionPool keeps growing in its internal list of created consumers, as the session is cached. Most likely a patch is needed to fix this in the AMQ side",design_debt,non-optimal_design
camel,4657,comment_0,This was a bug in ActiveMQ which has been fixed in the upcoming AMQ 5.6 release.,non_debt,-
camel,4676,summary,use fine-grained granularity feature for,non_debt,-
camel,4676,description,"split into camel-script-groovy camel-script-jruby so that we can control granularity much better, and version upgrades will be more manageable. In most cases, if the user does scripting, he'll probably standardise on only one particular language for all his routes.",design_debt,non-optimal_design
camel,4676,comment_0,also specify mvn repo url for google scriptengines stuff which isn't in maven central repo so that needn't change Karaf/Servicemix before install camel-script-. commit fix for trunk for 2.8.x branch,non_debt,-
camel,4733,summary,Dumping route to XML created by Java DSL using an expression with Transform EIP may not output the actual used expression,non_debt,-
camel,4733,description,Given this route Will be dumped as XML as: The <transform> is wrong as it should contain the expression definition properly.,non_debt,-
camel,4733,comment_0,Another example,non_debt,-
camel,4733,comment_1,Frist part fixed when using expression clause builder such as:,non_debt,-
camel,4736,summary,Add OSGi integration for camel-xstream,non_debt,-
camel,4736,description,"There's been a mail in the mailing list about camel-xstream not being blueprint friendly. Even though in the current trunk, it does work its a nice chance to add an integration test for it.",test_debt,lack_of_tests
camel,4736,comment_0,Committed on trunk,non_debt,-
camel,4741,summary,Camel Hazelcast Queue Producer should use add operation if none is defined.,non_debt,-
camel,4741,description,"This is a trivial issue, yet it is sometimes a bit annoying. People that are familiar with jms/activemq components expect that the default behavior of the a queue producer is to add the message to the queue, without having to specify a special operation. Hazelcast Queue Producer instead will through an exception if no operation is defined. It would be good if it was consistent with the rest of the queue producers.",design_debt,non-optimal_design
camel,4741,comment_0,Committed r1210367 to trunk.,non_debt,-
camel,4774,summary,Updated to camel-jclouds to use jclouds-karaf 1.2.1_3,non_debt,-
camel,4774,description,"There is an issue when installing the camel-jclouds feature, while the obr resolver is installed. A workaround for this has been added to the jclouds-karaf feature 1.2.1_3. We should upgrade to it.",non_debt,-
camel,4774,comment_0,Committed r1213695 to trunk.,non_debt,-
camel,4793,summary,Improve Camel aws-sdb component,non_debt,-
camel,4793,description,I would like to improve the aws-sdb component in the following places: - Using the [Amazon operation instead of CamelAwsSdbXXX -- Add OSGI integration tests I would like to resolve this issue for Camel 2.9.0 because renaming of the operation names are not backwards compatible. I will resolve this issue today.,design_debt,non-optimal_design
camel,4793,comment_0,I need additional two hours or so tomorrow to resolve this issue...,non_debt,-
camel,4793,comment_1,"Hi Christian, if you need any help with this let me know. BTW is there any agreed naming convention for components URI options and message headers? Looking at the existing components seems like URI options can have component specific names whereas the headers use format? But in this case, if you want to override an option from the URI using the message header, you have to specify a different name, which may be lead to confusion. Any thoughts?",design_debt,non-optimal_design
camel,4793,comment_2,Will make sure the documentation is updated in the next few hours,documentation_debt,outdated_documentation
camel,4793,comment_3,"There is still no documentation for sdb component, I will add updated docs today",documentation_debt,outdated_documentation
camel,4793,comment_4,"Sounds good Bilgin! Let me know if you need any help with it. And sorry for the bigger refactoring. I want to make sure we can support all features Amazon simpledb offers without the need for an API change which we cannot do in a micro release, e,g, Camel 2.9.1.",non_debt,-
camel,4815,summary,feature doesn't work,non_debt,-
camel,4815,description,"camel-atom feature defines a dependency to abdera-core, which require woodstox dependency: Error executing command: Could not start bundle in feature(s) camel-atom-2.9.0, cxf-abdera-2.5.1: Unresolved constraint in bundle [62]: Unable to resolve 62.0: missing requirement [62.0] package;",non_debt,-
camel,4815,comment_0,Fixed on trunk: revision 1222765.,non_debt,-
camel,4815,comment_1,Fixed on camel-2.8.x: revision 1222769.,non_debt,-
camel,4815,comment_2,I tested it with Karaf 2.2.4 and Camel 2.9 on MacOS and it works well (with the actual code in trunk): cmueller$ ./karaf clean cmueller$ ./karaf clean features:install camel-atom,non_debt,-
camel,4815,comment_3,It also works with the latest 2.8.4-SNAPSHOT version on Karaf 2.2.4 and MacOS: cmueller$ ./karaf clean cmueller$ ./karaf clean features:install camel-rss,non_debt,-
camel,4815,comment_4,"It works also with the brand new Karaf 2.2.5: cmueller$ clean __ __ ____ / //_/____ __________ _/ __/ / ,< / __ `/ ___/ __ `/ /_ / /| |/ /_/ / / / /_/ / __/ /_/ |_|\__,_/_/ \__,_/_/ Apache Karaf (2.2.5) Hit '<taband '[cmd] --help' for help on a specific command. Hit '<ctrl-d cmueller$ clean __ __ ____ / //_/____ __________ _/ __/ / ,< / __ `/ ___/ __ `/ /_ / /| |/ /_/ / / / /_/ / __/ /_/ |_|\__,_/_/ \__,_/_/ Apache Karaf (2.2.5) Hit '<taband '[cmd] --help' for help on a specific command. Hit '<ctrl-d features:install camel-rss",non_debt,-
camel,4826,summary,"Tech edits of JavaDoc and other code comments -- lets -> (let's, allow, {})",non_debt,-
camel,4826,description,"Partial conversion of ""lets do..."" to ""let's do..."", ""allows for doing"", or ""do..."" as appropriate, and associated other nitpicks found. ""Let's"" retained more where the code is part of an example or tutorial or if it's a matter-of-opinion on how to implement.",non_debt,-
camel,4826,comment_0,Thanks for the patch.,non_debt,-
camel,4854,summary,BAM - database constraint violation when restaring application,non_debt,-
camel,4854,description,"Take BAM example from (or any other one, it doesn't matter). Here is the one I'm using: ActivityBuilder request = ActivityBuilder response = response First run of the application and everything works. Restart application and try to activate one of BAM rules, there will be an exception that database constraint has been violated. Every time application is started, Camel tries to do following sql insert insert into (name, id) values (?, ?, ?) but name columne must be unique in . Workaround for this is to purge BAM tables every time you want to restart the application, but it's not a solution.",non_debt,-
camel,4854,comment_0,"I tried to solve on my own, and the problem occurs because of invalid XML configuration. The BAM example that is in binary distribution <bean id=""activities"" <property name=""jpaTemplate"" ref=""jpaTemplate""/ <property </bean should be changed to <bean id=""activities"" <constructor-arg <constructor-arg </bean Second version calls valid constructor for ProcessBuilder which creates ProcessName. The first one leaves process name as null.",non_debt,-
camel,4854,comment_1,I fixed the ProcessBuilder in camel-bam to create the processor name if not explicit configured.,non_debt,-
camel,4871,summary,"Request with Content-Type= throws ""Cannot read request parameters due Invalid parameter, expected to be a pair but was "" when body is empty.",non_debt,-
camel,4871,description,"ISSUE -- When CAMEL Servlet component receiving request with Content-Type= as below it throws Cannot read request parameters due Invalid parameter, expected to be a pair but was "" when body is empty. Request  <html </head <body <form method=""post"" <div <input type=""hidden"" name=""ID"" value=""fim page"" / <input type=""hidden"" name=""RelayState"" / <input type=""hidden"" name=""SAMLResponse"" / </div </form <span id=""user_msg"" <script </body</html RESOLUTION - Added defensive check, body is not null and not blank, to avoid Invalid parameter exception. I've updated class for that. // Push POST form params into the headers to retain compatibility // with DefaultHttpBinding String body = //Added defensive check, body is not null and not blank, to avoid Invalid parameter exception. ( My changes) if (body != null && !body.equals("""")) { for (String param : body.split(""&"")) { String[] pair = param.split(""="", 2); if (pair.length == 2) { String name = charset); String value = charset); if != null && value, { name, value); } } else { throw new ""Invalid parameter, expected to be a pair but was "" + param); } } }",non_debt,-
camel,4871,comment_0,We need new release to resolve this issue.,non_debt,-
camel,4871,comment_1,"Amit, can you re-attach the file attachments and mark [x] in grant license to Apache. Otherwise we cannot use your work.",non_debt,-
camel,4871,comment_2,"I fixed this using not empty check, on the camel-http and camel-http4 components.",non_debt,-
camel,4871,comment_3,Please find the re-attached file with mark[x] in grant license to Apache. Thanks for you help!!!,non_debt,-
camel,4905,summary,Trivial typos in tests,documentation_debt,low_quality_documentation
camel,4905,description,None,non_debt,-
camel,4905,comment_0,Thanks for the patch.,non_debt,-
camel,4928,summary,Async API support by the Timer component,non_debt,-
camel,4928,description,It would be great to have timer component support asynchronous API. Such a feature can be useful when timer component generates events which must be processed by multiple threads. Current implementation of the timer component makes a blocking call so the usage of thread pools hardly possible to process multiple timer event simultaneously.,design_debt,non-optimal_design
camel,4928,comment_0,New _async_ parameter has been added in the included patch. So by default the behavior is exactly the same as in the previous version of timer and if one want to use asynchronous API _async=true_ parameter should be added in the endpoint uri. The patch also includes _doSuspend_ and _doResume_ methods in the TimerConsumer to be able to stop the route faster and to prevent the timer running during stopping the route when using async. api.,non_debt,-
camel,4928,comment_1,"I think the consumers should use the Async API by default. People can disable async in their routes by setting synchronous=false, on the endpoint uri's if they want that disabled. Or we could introduce as a general option, for people to control this. The JMS consumer already have the asyncConsumer option.",non_debt,-
camel,4928,comment_2,"I noticed that *synchronous=false* is used primarily by producers and *asyncConsumer* parameter exists only in jms consumer, so I suppose that *synchronous* parameter can be used to determine whether both producers or consumers should be synchronous. (Maybe jms consumer should consider using of *synchronous* parameter instead of *asyncConsumer* to prevent introduction of new parameters for the same thing in other components) So, the patch was replaced to involve usage of *synchronous* parameter.",non_debt,-
camel,4928,comment_3,Thanks for the patch.,non_debt,-
camel,4953,summary,Add interface to allow processors in routes to be notified that a shutdown is in progress,non_debt,-
camel,4953,description,"This will allow EIPs and processors to be aware that a shutdown is in progress, so they can do any custom work they may need to do before hand. For example stateful EIPs such as the aggregator, may want to force completion of its current aggregated messages.",non_debt,-
camel,4953,comment_0,There is a slight API change in ShutdownAware,non_debt,-
camel,4958,summary,camel-jms - JmsConsumer make it less verbose logging in case of an exception or TX rollback,code_debt,low_quality_code
camel,4958,description,"When using a JMS route, and an exception occurs, then you get a WARN logging from Spring JMS that there is no error handler configured. And you get the stacktrace etc. However Camel itself will also by default log the stacktrace, so we have it logged twice. We should make this less verbose. And allow end users to more easily customize the logging level and/or whether stracktraces should be included.",code_debt,low_quality_code
camel,4958,comment_0,"Is now less verbose, and Spring no longer complains about no error handler configured. Added options to tweak logging levels and verbosity.",code_debt,low_quality_code
camel,4959,summary,Incorrect caching type converter misses for NaN,non_debt,-
camel,4959,description,"When converting Double or Float with value NaN, returns ""null"". But Exchange, Object) interpret this ""null"" as ""suitable conversion not found"" and cache misses. This lead to completely forgetting of conversion for given types. For example, when conversing Double to Long, all works until Double is NaN. After that, conversion for ""Double-to-Long"" marked as misses. And camel stop do any conversion for Double-to-Long until restart. Possible solution is to modify ObjectConverter`s methods to return ""Void.TYPE"" instead of ""null"" for NaN.",non_debt,-
camel,4959,comment_0,That is correct. Andrey can you provide a patch with unit test that test this fix?,test_debt,low_coverage
camel,4959,comment_1,"Hi, Claus. Yes, I can try to make patch with unit test for ObjectConverter. But suggested solution - return Void.TYPE from toXXXX() methods of ObjectConverter - is not correct :) ObjectConverter is converter, so that methods must declare return type correctly - Short, Long, Integer. It is not possible to return Void from toXXXX()... May be it is correct to return 0 for NaN? At least JDK do it: == Double d = Double.NaN; Float f = Float.NaN; + "", "" + f.shortValue()); == will print ""0, 0""",non_debt,-
camel,4959,comment_2,"Yeah lets return 0 for the primitive types. Patches is welcome, with unit tests.",test_debt,low_coverage
camel,4959,comment_3,"I did ""Refactor to Method"" for the case if the to be converted value is NaN:",non_debt,-
camel,4959,comment_4,"IMHO the changes introduced by of this ticket causes regression failure. To avoid null values being misinterpreted as cache misses we do now convert e.g. Float.NaN = which was not the case before. Why not just simply *not* mark the conversion as miss *if* the conversion result is (Float.NaN / Double.NaN == And I simply don't get why the misses cache is a ConcurrentMap: To my understanding would be just fine! Why do we need a Map to memorize the cache misses? Where we then do things like On it which is not really intuitive while reading the code, is this maybe because ideally we want to do lookup by the misses cache in O(1) instead of O(N)? See also:",design_debt,non-optimal_design
camel,4959,comment_5,"Yeah I have corrected the coverters to deal with NaN. And yes we need to lookup fast in type converter registry as its used a lot in Camel, and it should be as fast as possible, as it can become a bottleneck.",code_debt,slow_algorithm
camel,4959,comment_6,"Yeah dealing with NaN's directly in itself was what I was looking for, Thanks!",non_debt,-
camel,4993,summary,Add session information from cometd to camel message headers,non_debt,-
camel,4993,description,"The cometd component has the ability to authenticate a connection with a custom SecurityPolicy.[1] This information can then be added to the cometd session for further calls. Unfortunately, session attributes aren't passed to the camel message for processing by endpoints and processors further down the line. 1.",non_debt,-
camel,4993,comment_0,"Patch for session attributes in Cometd to be added at camel headers. This will enable support for downstream authorization of messages passed through the Cometd security policy. This feature is disabled by default with a uri parameter to enable it. Additionally only long, string, int, and double are supported as other message mediums may not support more. Currently, I just log an info message when an incompatible type is a session attribute. You may want to change this to be something else.",non_debt,-
camel,4993,comment_1,"Applied the patch into trunk with thanks to Joshua, I also fixed some Checkstyle errors.",code_debt,low_quality_code
camel,4993,comment_2,"I noticed that I had made an error and not brought over a change from my branch into the patch. The initial version didn't pass the property to the binding. I have added a fix as well as more unit tests to cover this. Additionally, I have added support for booleans as values for the session headers.",non_debt,-
camel,4993,comment_3,I found an issue with the previous patch. Fixes and more tests attached in second patch.,test_debt,lack_of_tests
camel,4993,comment_4,"@Joshua, You can create another issue for it, incase we release camel 2.10.0. I will go through your new patch when I get some time today.",non_debt,-
camel,4993,comment_5,Done -,non_debt,-
camel,4995,summary,Wrong type returned in DSL for such as,non_debt,-
camel,4995,description,"In ProcessorDefinition camel returns the (as opposed to its parent) while constructing the AST. The problem can only be noticed when such nodes are used within something like a ChoiceDefinition, in which case the parent (choice) is never returned, so there cannot be another when after something like dynamicRouter is used in a 'when'. A fix is relatively easy for the Expression flavor of the DSL and I will commit a fix shortly, but the ExpressionClause flavor is a bit trickier. I am looking into a solution for that that does not require an api change.",non_debt,-
camel,5008,summary,Stream handling inconsistent.,design_debt,non-optimal_design
camel,5008,description,"When working with streams, stream caching must be activated in order to use log:set trace, otherwise the streams will be consumed, as stated here (""#Using Streaming Bodies""). When the stream caching now gets activated, the streams will be reseted after each step (as far as I Understand). This makes it impossible to work with InputStreams in a pipe manner (e.g. Read the first char, then in the next step work with the next chars), as the stream is after this every time in the beginning. I would except that the stream caching provides a mechanism for the ""user"" to be able to read it more than once. Also its the right procedure to reset the streams after they are traced with the tracing mechanism, BUT the should be reseted to the state they were before and not to the very first beginning. I didn't dig into the code that deep but it seems that exactly this happens from user perspective. So to summarize there are several problems: - Working in stream in camel is impossible when log:set debug trace get enabled. (Thus enable Stream caching) - When Stream caching is enabled it becomes impossible to work with ""stream pointers"" as camel reset the streams to the very beginning. I illustrated the problem in the attached jUnit test.",design_debt,non-optimal_design
camel,5008,comment_0,Works if streaming gets out commented in test. Sorry if the test is a bit incomplete.,test_debt,low_coverage
camel,5008,comment_1,Currently I'm not sure but it seems to be more like 3 Bugs - log:set trace destroys streams when stream caching deactivated - stream:caching shouldn't do a reset or it should be a possibility to disable this - when tracing on stream caching there should be a mark before the trace and then a reset (however this might destroy a user mark),non_debt,-
camel,5008,comment_2,"This is *not* a Camel bug/issue, if your logger is reading the stream, and causing the stream to not be re-readable. In the parts where Camel is logging a message content, it will by default skip message contents that is stream based. See the methods.",non_debt,-
camel,5008,comment_3,And in your case just do not use the streamCaching from Camel. Then your message body is left untouched by Camel.,non_debt,-
camel,5008,comment_4,"Sorry for the confusing Bug report. The log:set of what I talked about is from the the Talend ESB (Karaf), so ignore that.",non_debt,-
camel,5012,summary,Starting and stopping Camel should be less verbose,code_debt,low_quality_code
camel,5012,description,"When starting and shutting down Camel, it reports a bit stuff at INFO level. We should make it less verbose. For example the type converter logs 3-4 lines, we should just log 1 line instead.",code_debt,low_quality_code
camel,5012,comment_0,Now Camel is less verbose by default.,code_debt,low_quality_code
camel,5045,summary,Memory leak when adding/removing a lot of routes with JMX enabled,design_debt,non-optimal_design
camel,5045,description,"If you add and remove a lot of routes to CamelContext and have JMX enabled, then the will accumulate a map of provisional JMX performance counters for the route mbeans. The map should be cleared after usage, as the map is no longer needed. Memory will accumulate as the map has reference to old objects which cannot be GC.",design_debt,non-optimal_design
camel,5045,comment_0,"Also CAMEL-4500 introduced a leak as well, in terms of ManagedTracer being kept around in a separate Map. We need to remove not needed tracer from that map as well.",design_debt,non-optimal_design
camel,5045,comment_1,I created CAMEL-5046 to track the leak from CAMEL-4500 with ManagedTracer as it only affects 2.9 onwards.,design_debt,non-optimal_design
camel,5064,summary,Route using programmatically defined CxfEndpoint doesn't appear as OSGi service,non_debt,-
camel,5064,description,"If we create a route with a CXF endpoint programmatically: CxfEndpoint endpoint = new CxfEndpoint(); and register the route using the route is started and appears in the MBeans, but, in Karaf, routes:list doesn't display the route (as OSGi service is not there).",non_debt,-
camel,5064,comment_0,"Hi JB, How does the CxfEndpoint be referenced in your route ? Willem",non_debt,-
camel,5064,comment_1,"Hi Willem, I create a RouteDefinition and put directly in the from: RouteDefintion route = new RouteDefintion(); And I register the route definition in the CamelContext.",non_debt,-
camel,5064,comment_2,"After a complete cleanup of the data folder, I don't have the issue anymore.",non_debt,-
camel,5068,summary,Allow to configure cache option on in XML DSL,non_debt,-
camel,5068,description,See nabble It would be nice to expose the cache option in the so people can enable/disable cache.,non_debt,-
camel,5068,comment_0,"Without your help to leave such trivial task to dummies like me, how could I ever survive in this challenging community? Thanks :-) Though a question: I've got already a fix for this ticket in place on my workspace however I did touch only: - camel-core-xml (modified - camel-spring (Only added one additional unit-test) Are you really sure that: - camel-spring - camel-bluprint Should be affected by this ticket? Or do I (again) miss something?",non_debt,-
camel,5068,comment_1,"Yeah the implementation is in those other places, as its re-used by spring/blueprint. But add unit tests in camel-spring and blueprint to verify that it works in the XML DSL.",non_debt,-
camel,5068,comment_2,Did put a Tip on the Wiki regarding this as well.,non_debt,-
camel,5085,summary,Failover EIP - Should use defensive copy of exchange before failover to avoid side effects,non_debt,-
camel,5085,description,"When using the failover load balancer, it works directly on the given exchange, and in case of a failover, it clears the exception state etc. However this does not work too well as if you use a processor directly then you can mutate the message before failover, which mean when the exchange is failed over, then its a 100% copy of the input message, but the previously mutated message. We should do like the other EIPs by doing a defensive copy of the exchange.",non_debt,-
camel,5184,summary,Faster testing with seda endpoints by shutting down seda endpoints faster,test_debt,expensive_tests
camel,5184,description,"When shutting down seda endpoints they take a lille while to shutdown properly. However during testing we dont need to do that, so we could shortcut this and shutdown faster. For example the camel-test kit could tweak that. As well in camel-core. I suspect we can cut down minutes of testing times.",test_debt,expensive_tests
camel,5184,comment_0,Testing seda component in Camel core Before: [INFO] Total time: 2:34.154s Improvement 1: [INFO] Total time: 1:54.353s Improvement 2: [INFO] Total time: 53.398s,non_debt,-
camel,5184,comment_1,Added pollTimeout option to seda endpoint. The testing in camel-core will set that to a lower value to cut down 100 sec during testing.,non_debt,-
camel,5184,comment_2,"Backported to 2.9 branch, as it cuts more than 5 minutes in total testing camel-core.",non_debt,-
camel,5192,summary,spring.schema is not up to date,non_debt,-
camel,5192,description,"The URI of the Spring schema has been updated in Camel 2.9.1 and 2.8.4: We can see that starting from 2.9.1 and 2.8.4, we suffix the schema with -spring or -blueprint. However, the spring.schema resource file has not been updated with this suffix.",non_debt,-
camel,5192,comment_0,Looks like a bug .. I think we should also update the spring.schema file to reflect this change,non_debt,-
camel,5206,summary,Different servlets interfere with each other,non_debt,-
camel,5206,description,See If using two wars with one servlet in each with the same name then servlet endpoints may end up on the wrong servlet. I think this should not happen with pure war deploys. Trying to check if camel is in a shared lib folder. In any case this behaviour is not what people expect naively so we should try to fix that so that each war has its own combinaation of servlets and servlet endpoints that do not interfere with others.,non_debt,-
camel,5206,comment_0,We now detect the duplicate servlet name and fail starting the servlet. Then people will detect this quicker and can remedy by using unique servlet names. This applies only when they use a shared lib to share Camel JARs in the container.,non_debt,-
camel,5218,summary,bombing out,non_debt,-
camel,5218,description,"I am setting up an own hazelcast instance on HazelcastComponent, it is created by Spring, the problem is when is called, a is throw, because createOwnInstance is null.",non_debt,-
camel,5218,comment_0,"Hello Henrique, thanks for reporting! I have it fixed for Camel 2.9.3 and Camel 2.10.0. Camel 2.8.x doesn't have this problem. Best, Christian",non_debt,-
camel,5222,summary,The file consumer should use the charset encoding when reading the file if configured,non_debt,-
camel,5222,description,"See CAMEL-5215. This applies to the consumer as well. We should tighten this up, to ensure the charset is always used if configured. Currently the charset option could be shadowed if you did a convertBodyTo and specified another charset etc. Also we should tighten up to use the charset configuration on the file endpoints. And added DEBUG logging which charset is being used for reading/write the files.",non_debt,-
camel,5238,summary,camel-avro - Fails testing on Windows,non_debt,-
camel,5238,description,Seen this on windows,non_debt,-
camel,5238,comment_0,Okay its an issue with the avro producer not handling the response properly.,non_debt,-
camel,5342,summary,Shaded conflicts with existing jar,non_debt,-
camel,5342,description,"Package is included/shaded inside the camel-core jar. It is not very nice if is already on the path. It is a deal breaker, if their versions are different. For example cassandra-1.1.1 requires which is missing from the version included in camel. It would be nice if was included as a normal dependency. Comment in the pom.xml says ""Shade the googlecode stuff for OSGi"". Well, if that is strictly required, maybe it could be better included in camel-core-osgi package. In any case, if it must be shaded at all, it would be safer to use relocation property of the maven-shade-plugin. In this case, camel could stay with the version it wants, without conflicting with explicit dependencies.",build_debt,build_others
camel,5342,comment_0,"CLHM v1.3 includes an OSGi manifest, if that helps.",non_debt,-
camel,5342,comment_1,Updated the shade plugin configure for it.,non_debt,-
camel,5380,summary,should provide a method for adding a customized TinyBundle,non_debt,-
camel,5380,description,Right now the generated TinyBunlde in the only generates a TinyBundle containing the blueprint.xml file. It might be mandatory to change this to contain also the needed classes. I'm preparing a patch for this.,non_debt,-
camel,5380,comment_0,Patch for solving this.,non_debt,-
camel,5380,comment_1,Thanks for the patch.,non_debt,-
camel,5383,summary,[JDBC component] Add ResultMetaData as a header value,non_debt,-
camel,5383,description,JDBC Component could (optionally?) add list of column names (retrieved from resultMetaData) to the message headers. Claus suggested something like : List<String,non_debt,-
camel,5383,comment_0,start working on it. try to see if i can contribute to camel,non_debt,-
camel,5383,comment_1,"G'day Henryk, I'm a new contributor. Is there a way to take ownership of an issue/enhancement and drive it? Thanks -Dev",non_debt,-
camel,5383,comment_2,Create a patch and attach it here. Read first: - -,non_debt,-
camel,5383,comment_3,"G'day Chris, Henryk, Can you please review the unit test to see if this is exactly what you are looking for? Thanks -Dev",non_debt,-
camel,5383,comment_4,"Hi Chris, I'm having trouble with my IDE and cant create a patch. Can you please import the attached classes into your workspace and give this code a try? Instead of returning a List of columnNames, I thought it would be a good idea to return a Set of columnNames. Please let me know your thoughts. Cheers",non_debt,-
camel,5383,comment_5,"Hi Chris, did you get a chance to review the attached classes? Thanks -Dev",non_debt,-
camel,5383,comment_6,Thanks for the patch.,non_debt,-
camel,5414,summary,SqsEndpoint can't retrieve existing queue url with visibility timeout different than default,non_debt,-
camel,5414,description,"This would happen in 2 scenarios: 1. Queue already exists with vsibility timeout different than 30 seconds (say use AWS Console to create a queue and set a different visibility timeout). Using this queue as an endpoint and problem will occur 2. Queue DOES NOT already exist and is created by SqsEndpoint (createQueue). If configured, it will still work...the first time. But restarting the endpoint (or the whole camel app) and the problem will occur.",non_debt,-
camel,5414,comment_0,This is the test which I think follows Amazon behavior and shows that getting the queue url with the current SqsEndpoint implementation is broken. I'd be more than happy to provide the patch if someone could just check the test and give me the go :) Maybe I missed something and it's not broken...,non_debt,-
camel,5414,comment_1,"I can confirm, this is an issue. Thanks for reporting! Would you like to work on a patch? I think I could provide a fix by tomorrow...",non_debt,-
camel,5414,comment_2,"I committed a patch which fixed this behavior. The SQS endpoint will now list the existing queues and use the url of the existing queue *AND ITS ATTRIBUTES!* If the queues doesn't exist, we will create a new one with the provided attributes. Is this also your expected behavior? Or should we also call and update all of them if the queue already exists?",non_debt,-
camel,5414,comment_3,"Thanks a lot Christian. I would expect the endpoint to be setup as specified in the URI options, so I'd vote for setting the queue attributes when the queue already exists. What do you think?",non_debt,-
camel,5414,comment_4,"Yes, makes sense for me too. I will work on it in the next days.",non_debt,-
camel,5414,comment_5,I updated the behavior as discussed.,non_debt,-
camel,5442,summary,Bindy OneToMany and Fix protocol repeating,non_debt,-
camel,5442,description,"Hi, I'm trying to build a data feed using the FIX protocol. So far I've been able to process the data request but I'm having problems with the market data response. I'm using Bindy and created a model using the @OneToMany annotation, so as to model a data repeating group @OneToMany(mappedTo = private However when the model is marshaled into a Fix message, only the first item in list in the repeating group gets processed. A similar issue has been addressed here Is this resolved already and which version is this resolved in?. Thanks",non_debt,-
camel,5442,comment_0,Any update on this ?,non_debt,-
camel,5442,comment_1,There has been a number of fixes and whatnot in camel-bindy. Try again with latest code to see if it fixes your problem.,non_debt,-
camel,5527,summary,Maven archetype - Generates wrong plugin for surefire plugin,non_debt,-
camel,5527,description,The blueprint archetype creates wrong pom.xml. The groupId for the surefire plugin is wrong.,non_debt,-
camel,5527,comment_0,This does not affect Camel 2.9.,non_debt,-
camel,5527,comment_1,I removed the surefire plugin as its no longer needed,code_debt,dead_code
camel,5543,summary,Karaf Command 'camel:route-list' throws,non_debt,-
camel,5543,description,"Sometimes I get an NPE when listing routes in Karaf: 2012-08-28 12:20:01,734 | INFO | l Console Thread | Console | 36 - - 2.2.8 | Exception caught while executing command",non_debt,-
camel,5543,comment_0,Thanks for reporting.,non_debt,-
camel,5543,comment_1,"This exception can happen when multiple CamelContext have the same ID. The CamelController class makes the assumption that CamelContext IDs are unique keys. This is not true in an OSGi container, where several CamelContexts can be deployed in distinct bundles, which do not know each other.",non_debt,-
camel,5575,summary,Add new HttpEndpoint Option on Http4 component,non_debt,-
camel,5575,description,"Our third-party application accepts Date as Http header, but filters the date. To support out third-party application I want new feature to add Custom http filter on a Http4 Endpoint. I made the following changes on HttpComponent createEndpoint method to support custom http filter as opetion qq on Http4 Endpoint. Please find source code in the attached HttpComponent.java file. = parameters, if == null) { = parameters, } if != null) { } else { }",non_debt,-
camel,5575,comment_0,You can setup you customer by setting it directly on the HttpComponent like this.,non_debt,-
camel,5575,comment_1,As the user could write a customer without extending the I'd like to change the option name to The patch will be applied to camel-http and camel-http4 components.,non_debt,-
camel,5575,comment_2,Applied the patch into trunk and also update the wiki page for it.,non_debt,-
camel,5575,comment_3,Would you please add this feature to 2.9.5 and Camel 2.10.3 releases so we don't have to wait for 2.11.0.,non_debt,-
camel,5575,comment_4,Camel 2.9.5 and 2.10.3 are already in VOTE. I'm afraid it's to late to back port this fix to these versions.,non_debt,-
camel,5575,comment_5,"Christian, The changes to add this feature is minor and risk free. Actually this feature help us to resolve the defect. We have been waiting for last two months for Camel 2.11.0 release, but Camel 2.11.0 is not going to release pretty soon. We really appreciate your help! Thanks, Amit Patel On Dec 3, 2012, at 5:51 PM, Christian Mller (JIRA) [ ] Christian Mller commented on CAMEL-5575: Camel 2.9.5 and 2.10.3 are already in VOTE. I'm afraid it's to late to back port this fix to these versions. Add new HttpEndpoint Option on Http4 component Key: CAMEL-5575 URL: Project: Camel Issue Type: New Feature Components: camel-http Reporter: Amit Patel Assignee: Willem Jiang Fix For: 2.11.0 Attachments: HttpComponent.java Our third-party application accepts Date as Http header, but filters the date. To support out third-party application I want new feature to add Custom http filter on a Http4 Endpoint. I made the following changes on HttpComponent createEndpoint method to support custom http filter as opetion qq on Http4 Endpoint. Please find source code in the attached HttpComponent.java file. = parameters, if == null) { = parameters, } if != null) { } else { } -- This message is automatically generated by JIRA. If you think it was sent incorrectly, please contact your JIRA administrators For more information on JIRA, see:",non_debt,-
camel,5575,comment_6,I have back ported this features to Camel 2.9.6 and 2.10.4.,non_debt,-
camel,5586,summary,Fail constantly on CI server and local machine,non_debt,-
camel,5586,comment_0,The latest JDK7 build right before was O.K.:,non_debt,-
camel,5586,comment_1,I intend to upgrade to the newest v0.19.9 version.,non_debt,-
camel,5586,comment_2,I polished the camel-elasticsearch codebase a bit:,code_debt,low_quality_code
camel,5586,comment_3,O.K. it's fixed now. The tests also run now much faster than before (through overriding the On my box now running all the tests take about 3 seconds compared to around 30 seconds before!,test_debt,expensive_tests
camel,5586,comment_4,I'll still keep the ticket open because of SMX4-1233.,non_debt,-
camel,5586,comment_5,Many thanks for fixing this issue!,non_debt,-
camel,5586,comment_6,Thanks it works now,non_debt,-
camel,5587,summary,ExceptionHandler - Add method that has consumer parameter,non_debt,-
camel,5587,description,"See nabble If people want to share a custom ExceptionHandler among a number of route consumers, then when an exception is being handled, they wont know which consumer failed. We could add a method with the consumer parameter. But that would break the old API. But that may be okay as using a custom ExceptionHandler is not commonly used.",non_debt,-
camel,5587,comment_0,We could just add a (Object source) as parameter where {{this}} will be passed in as parameter. Then the implementation can check its type and do type cast to get the Consumer etc.,non_debt,-
camel,5587,comment_1,About 20 or so Camel components would need to migrate to this newer API.,non_debt,-
camel,5587,comment_2,Posted on @dev to discuss this impact,non_debt,-
camel,5587,comment_3,"Kinda old, but still instructive:",non_debt,-
camel,5587,comment_4,"Lets move this to 3.x as there is some API changes needed. And we may still decide to keep it as is, if there isn't a need for this anyway?",non_debt,-
camel,5689,summary,Netty - Should use ordered thread pool,non_debt,-
camel,5689,description,"We should favor using and to ensure ordering of events in a channel. See nabble We can add an option so people can turn it off, like we did for mina2.",non_debt,-
camel,5689,comment_0,"2.9 branch is using a much older netty release, so lets just keep it as is.",non_debt,-
camel,5723,summary,camel-jaxb: partClass and partNamespace dynamically set by header,non_debt,-
camel,5723,description,"The Camel JAXB Data Format allows to specify a partClass and partNamespace on the data format configuration. If you have many cases of partial marshalling or unmarshalling, you're forced to configure as many data formats as part classes you'll ever need to handle. Aside from being inconvenient, it makes route initialisation pretty inefficient because a JAXBContext is created per data format, performing a full scan and reflection of the package each time. Slows down route startup considerably. Enhance the Camel JAXB Data Format so that it's capable of doing partial unmarshalling at runtime based on message headers.",non_debt,-
camel,5723,comment_0,"You can use the data format component and use a ""dynamic to"". But yeah maybe also nicer to improve jaxb dataformat to support a header to control the behavior.",non_debt,-
camel,5723,comment_3,The fix version for this (and probably other issues fixed between the two release candidates for 2.19) should be 2.19 (see:,non_debt,-
camel,5763,summary,Enrich Camel message with JMSMessageID for InOnly messages sent to JMS destination,non_debt,-
camel,5763,description,"This allows end users to access the actual JMSMessageID the JMS vendor have assigned to the sent message. For example to log it, so the message can be traced etc. See nabble",non_debt,-
camel,5763,comment_0,We should add option so people can turn this on|off.,non_debt,-
camel,5763,comment_1,There is a new option you can turn on to have the JMSMessageID set on the Camel Message header. Read the JMS docs for more details.,non_debt,-
camel,5763,comment_2,"Notice that this option can be set on component level, so you dont have to enable it per endpoint. This is also doable for many of the other options.",non_debt,-
camel,5764,summary,Simple language - Add CamelContext as OGNL to simple language,non_debt,-
camel,5764,description,This allows invoking operations on the camelContext easier from the simple language.,non_debt,-
camel,5768,summary,Upgrade to Spring 3.1.x by default and add Spring 3.0 integration tests.,non_debt,-
camel,5768,description,"We should have Spring 3.1 as default out of the box for people using camel-spring etc. We should then add a spring 3.0 integration tests for backwards sanity tests. For OSGi users, then the Karaf / SMX container provides Spring features that Camel uses. So that can either be Spring 3.0.x or 3.1.x based depending on what ppl have configured the container as.",non_debt,-
camel,5768,comment_0,There is now spring30 and spring31 versions in parent/pom.xml. To flip version is easy as just point spring to spring31.,non_debt,-
camel,5768,comment_1,"And as we have been running spring 3.1 CI tests before Camel 2.10 got releases then we have throughly test coverage, and a switch should be possible.",non_debt,-
camel,5768,comment_2,"current we still use the spring 3.0.x by default, I think we need to update the default profile for it.",non_debt,-
camel,5768,comment_3,"Spring 3.1 is now the default spring version. Added itest testing with spring 3.0.x. Disabled the CI project for spring 3.1, as its now default.",non_debt,-
camel,5782,summary,"regression : invalid created, works on 2.10.1",non_debt,-
camel,5782,description,"In if I don't have any configuration, the created contains a null atttribute collection and AWS emit an error. In 2.10.1, no problem. Workaround in 2.10.2 : force the create to contain a valid attribute collection by defining a configuration in camel. For exemple: - if I add an argument to my - Exception : {{ Caused by: Failed to create route because of Failed to resolve endpoint: due to: The request must contain the parameter Attribute.Name. ... 58 common frames omitted Caused by: Failed to resolve endpoint: due to: The request must contain the parameter Attribute.Name. ... 66 common frames omitted Caused by: The request must contain the parameter Attribute.Name. ... 74 common frames omitted }}",non_debt,-
camel,5782,comment_0,Hello Julien! Thanks for reporting and my apologize for the inconvenience. It's now fixed for Camel 2.10.3 and 2.11.0. Feel free to test the SNAPSHOT versions.,non_debt,-
camel,5782,comment_1,Great! Thx! I confirm that you have resolved my problem with 2.10.3 and 2.11.0 SNAPSHOT versions.,non_debt,-
camel,5805,summary,OSGi support of camel-xmlrpc,non_debt,-
camel,5805,description,We need provide camel-xmlrpc featureand xmlrpc-client bundle for user to use.,non_debt,-
camel,5805,comment_0,Applied patch into trunk.,non_debt,-
camel,5814,summary,support to configure the XmlRpcClient,non_debt,-
camel,5814,description,XmlRpcClient should be able to configured with URI options or some pluggable interface.,non_debt,-
camel,5814,comment_0,Applied the patch into trunk.,non_debt,-
camel,5852,summary,Regression in JDBC Aggregation Repository,non_debt,-
camel,5852,comment_0,"Did you upgrade while the datastore still had data from the old version? As the code is changed its advised to drain the data store first, before upgrading. I dont think this is something we can do anything about.",non_debt,-
camel,5852,comment_1,You need to empty your store before upgrading.,non_debt,-
camel,5878,summary,Support/Upgrade to Spring 3.2,non_debt,-
camel,5878,description,See for details,non_debt,-
camel,5878,comment_0,Requires Karaf 2.3.2,non_debt,-
camel,5950,summary,Cache producer is not thread safe,requirement_debt,non-functional_requirements_not_fully_satisfied
camel,5950,comment_0,"Applied the patch into trunk, camel-2.10.x and camel-2.9.x branches.",non_debt,-
camel,5950,comment_1,That was super fast. Thank you very much!,non_debt,-
camel,5961,summary,camel-dropwizard component,non_debt,-
camel,5961,description,"It would great to have a camel-dropwizard component. The details on dropwizard can be found here: * * Github: Since Dropwizard works off a standard JVM (w/o the need of an app-server) and same holds good for Camel, a camel-dropwizard integration would make AppServer reduntant for serving RESTful webservices with Enterprise Integration (Dropwizard internally uses a highly tuned variant of Jetty for HTTP). Even a camel with Metrics could be a good idea to collect JVM metrics about different routes. Any thoughts or comments?",non_debt,-
camel,5961,comment_0,Yeah at first glance this could be a good idea. Users listening on this. Remember to use the JIRA voting system. And of course fell free to comment. Or work on such a component. We love contributions.,non_debt,-
camel,5961,comment_1,"Not sure how Dropwizard would integrate with Camel (since Dropwizard is based on Guice, and Camel's Guice Support is, as we say in Brazil, ""mais feio que bater na me""), but here's what I've made using Spring instead, to measure Exchange Events of interest Note it depends on a kludge to tag those exchanges. In my routes, I define the CamelMetricSuffix Exchange Property (as shown in the gist) (Besides that, we're using metrics-spring - If there's enough interest to turn this into a Camel Module, just let me know. I do have a CLA ready. Thanks",non_debt,-
camel,5961,comment_2,"I like the way you use Metric to monitor the Exchange states, we can create a new component for it. If you want to work with Dropwizard, you can take a look at the or these component just route the REST request to the Camel router and Camel doesn't care if you uses Spring or Guice to do the DI work.",non_debt,-
camel,5961,comment_3,"Interesting. I used to think that Dropwizard required Guice, but instead, it doesn't depend on any DI Container at all. I'll think about it",non_debt,-
camel,5983,summary,Avoid the negative-tests to behave as false-positive,non_debt,-
camel,5983,description,"We've got bunch of (negative) tests on the current codebase expecting a thrown {{XYZException}} however they don't realize if the expected exception is *not* thrown, the typical pattern for this is: Which correctly should be:",test_debt,expensive_tests
camel,5983,comment_0,Fixed the tests *not* being false-postive on the trunk as well as the 2.10.x brunch:,non_debt,-
camel,5983,comment_1,Indeed there are 4 false-positive tests on the trunk codebase which I'm attaching to this ticket as *partial* fix.,non_debt,-
camel,5983,comment_2,The previous two commits include two {{camel-mongodb}} tests being fixed as well however I could not verify if they are really false-positive or not as a *real* run of them would require a MongoDb installation. Any committer having such a installation is welcome to verify these tests *truly*.,non_debt,-
camel,5983,comment_3,Actually there were even much more tests being affected than I initially thought:,non_debt,-
camel,5983,comment_4,Updated the attached *partial* patch as well which now includes more negative tests being *effectively* false-positive.,non_debt,-
camel,5983,comment_5,O.K. already fixed bunch of the tests being attached to this ticket (see Following the list of the ones which still need to be fixed: Hope to find a free time slot by the next days to fix them as well.,non_debt,-
camel,5983,comment_6,All the remaining tests being mentioned by the previous post have been now fixed as well.,non_debt,-
camel,6028,summary,"camel-hdfs - Support CamelFileName to write to a new file, when not using split strategy",non_debt,-
camel,6028,description,"When using camel-hdfs to write to files, without using split strategy. Then the file is always the same which is the file configured on the endpoint. We should add support for the CamelFileName header so you can give the file a new name, relative to the endpoint configuration. Then it works more like the file producer would do.",non_debt,-
camel,6028,comment_0,added support for this in commit:,non_debt,-
camel,6028,comment_1,"upon review, this probably could use Expression support as well...adding that next",non_debt,-
camel,6028,comment_2,added Expression support in Commit:,non_debt,-
camel,6038,summary,Possible NPE is OSGi bundle has no symbolic name,non_debt,-
camel,6038,description,See,non_debt,-
camel,6043,summary,Improve the BeanInfo performance,code_debt,slow_algorithm
camel,6043,description,See for details.,non_debt,-
camel,6043,comment_0,"I just went thought the patch, current it just suppose to take a map which works a cache from the camel context. But user need to setup the map first before we use it. Just as Claus has mentioned in the mail we could use LRUSoftCache to this kind of cache. BTW, LRUSoftCache is thread safe. Now the generic cache facility which could be provide by Camel just come across my mind. The Cache can be managed by JMX, and it can meet out needs of cache in different camel component. Maybe we could use it later in BeanInfo.",non_debt,-
camel,6043,comment_1,Applied the patch with thanks to Antoine.,non_debt,-
camel,6043,comment_2,"I further improved on this and let the cache be enabled by default, and controlled by the bean component.",non_debt,-
camel,6043,comment_3,"This further improved performance for bean and simple language when using OGNL like expressions, that is evaluated at runtime. Now the cache ensures we do not introspect the same class types over and over again.",code_debt,slow_algorithm
camel,6057,summary,Camel Spring registry does not look into parent contexts,non_debt,-
camel,6057,description,trunk has next method: @Override public <T return } but this method does not look into application context parent contexts. should be used instead.,non_debt,-
camel,6057,comment_0,When do you use parent context in Spring? Do you have use-cases using that?,non_debt,-
camel,6057,comment_1,"The typical usecase is to have the servicelayer and infrastructure beans in a shared parent context, and having the web/view stuff (e.g. controllers, viewresolvers) as child contexts of this (in a",non_debt,-
camel,6057,comment_2,"When you have multiple Spring DispatcherServlets within a single webapp and each of these servlets have their own app contexts, you might want to share beans between them. So you add a another context with these shared beans, which is the parent of each of the servlet contexts.",non_debt,-
camel,6057,comment_3,"As for me, I am using static parent context in tests to provide test beans that are created in test code (e.g. mocks). I've found this bug, when camel-jpa could not find my test entity factory. Fortunately, I could add explicit jpa component bean with property filled.",non_debt,-
camel,6057,comment_4,Fix is in pull request,non_debt,-
camel,6057,comment_5,Thanks for the patch.,non_debt,-
camel,6098,summary,Add script to the DSL to invoke a expression language and allow noop changes on the Exchange,non_debt,-
camel,6098,description,See nabble We should have a script in the DSL The script is then invoked and allows noop to avoid changing the message body. Currently you can only do ... and that may change the message body with the return value from groovy. With <script,non_debt,-
camel,6098,comment_0,"We should allow to load the script from classpath, so you can do",non_debt,-
camel,6104,summary,Authentication does not work for Weblogic JMS,non_debt,-
camel,6104,description,"Dear collegues, I'm trying to configure JMS integration between my application (in Weblogic) and Tibco EMS. As an example I used next tutorial: Application context is configured as suggested: <bean id=""weblogic"" <property </bean <jee:jndi-lookup But connection cannot be established due to authentication issues. Weblogic logs show (attached): Caused By: invalid name or password Tibco console shows: 2013-02-26 22:08:25 connect failed: not authorized to connect I'm sure that login&password are specified correctly. When I turn off authentication (on the Tibco side), or try to connect directly (without Weblogic), everything works fine. So I assumed that issue is between Camel and Weblogic and even found some examples of similar cases: Tibco EMS connectivity parameters are configured in WL console according to the attached instruction. Appreciate your help",non_debt,-
camel,6104,comment_0,Adding the attachments,non_debt,-
camel,6104,comment_1,"Hi Igor We prefer people using the mailing list / forum / or 3rd party sites like Stackoverflow for questions and help on using Camel, as we say here: There are many more people active on those channels than this JIRA tracker.",non_debt,-
camel,6104,comment_2,And try setting and see if you get another kind of error later.,non_debt,-
camel,6104,comment_3,And what Camel version is that stacktrace from?,non_debt,-
camel,6104,comment_4,"Hi Claus Thanks for your reply! I've tried to remove this parameter at all - nothing changes, still not able to connect the Tibco. The only difference is that in testing mode (according to the Tibco logging) there are only 2 connection attempts (1 per each of the 2 nodes), while without it Camel tries to connect all the time application is running. This one is from 2.9.2. Initially I've tried 2.10.3, but it contains issues with XSD validation (I think the root cause is: so I've decided to use 2.9.2. Anyway I had the same error in 2.10.3 as well. I can do so, if it's still required. Should I?",non_debt,-
camel,6104,comment_5,"There is a new Camel 2.10.4 out. Also the latest 2.9.x release is 2.9.5. But they may have the same issue for you. Yes using the mailing list / forum, is where more users is around and can help. And there is some users who also use Tibco like you.",non_debt,-
camel,6111,summary,- May parse blueprint xml file concurrently and start 2 Camels,non_debt,-
camel,6111,description,Argh Aries Blueprint may parse the same blueprint xml file concurrently. Assume its the influence of pojosr that may trigger this. As running Camel on Karaf has not seen started the same XML file twice. See nabble,non_debt,-
camel,6111,comment_0,This issue is only on 2.11,non_debt,-
camel,6128,summary,JAXB fallbackConveter unmarshal falls with XStreamReader,non_debt,-
camel,6128,description,"When using Get the request object from the CxfPayload Message, we will get this kind of error. Caused by: - with linked exception: Namespace URIs and local names to the unmarshaller needs to be interned.] ... 48 more Caused by: Namespace URIs and local names to the unmarshaller needs to be interned. ... 52 more",non_debt,-
camel,6128,comment_0,Applied patch into trunk and 2.10.x-fixes branch.,non_debt,-
camel,6178,summary,Camel Flatpack should respect the ignoreExtraColumns and allowShortLines options for delimited files,non_debt,-
camel,6178,description,None,non_debt,-
camel,6178,comment_0,I'll put together a patch here in the next couple weeks but I can't assign this to myself in the system.,non_debt,-
camel,6178,comment_1,"I think you are not the developer of apache camel, so the issue cannot be assigned to you. We will release the camel 2.11.0 soon, if you want this patch to be a part of camel 2.11.0, you may need to hurry :)",non_debt,-
camel,6178,comment_2,"Willem, ya I realize why I couldn't assign it, I just wanted to let people know I was going to fix it. I won't have time to get it into 2.11.0 which is why I marked it as 2.11.1. If someone else can get it done by 2.11.0 that would be great. I'm also going to test against 2.10.x branch so we can backport the fix.",non_debt,-
camel,6178,comment_3,Patch to allow short/long lines in delimited files - tested patch against both 2.11 and 2.10.5 branches,non_debt,-
camel,6178,comment_4,"Chris, did you already filled and sent an ICLA to Apache? In this case, I could get you the karma to assign tickets to yourself...",non_debt,-
camel,6178,comment_5,I added this to the Camel 2.11 release notes and updated the WIKI pages with the correct version which with this option is available.,documentation_debt,low_quality_documentation
camel,6178,comment_6,Thanks Chris for the patch!,non_debt,-
camel,6178,comment_7,"Christian, yes my ICLA is on file with Apache since I'm a committer on another project. I shouldn't have to resend it for Camel right?",non_debt,-
camel,6178,comment_8,Not necessary. I added you to the camel-contributors group. You should now be able to assign tickets to yourself (after a new log in). Keep contributing! We are watching you...,non_debt,-
camel,6188,summary,"CXF cosumer should set Exchange's charset name, if content type provides one",non_debt,-
camel,6188,description,"The CXF consumer copies the content-type http header to the camel exchange. This header may indicate the character set used in the request (for instance and if so this should be made available in the normal place for Camel (i.e. a property in the exchange called This may (of course) be done in each route by a separate processor, but it simplifies life if this is done by default. seems the logical place) Sample processor that performs this job.",design_debt,non-optimal_design
camel,6188,comment_0,Applied the patch into trunk.,non_debt,-
camel,6233,summary,does not support nested Spring properties,non_debt,-
camel,6233,description,"does not support recursive properties (as Spring property placeholder does). Given the following Spring properties file : If I use the expression in the from clause of my RouteBuilder, I get an exception :",non_debt,-
camel,6233,comment_0,Should be supported from 2.11 see I wonder if the support can be extended to use the Spring properties notation $ with the instead of {{}},non_debt,-
camel,6233,comment_1,"Indeed, support of Spring properties notation would be nice.",non_debt,-
camel,6233,comment_2,"Any idea when Camel 2.11 will be available ? 2.11 release was planned for march 2013, and there is only 4 open issues remaining, so can we expect the release to happen soon ? I don't complain about this delay, but this issue is blocking one of my projects. So I just want to know if I should wait a few days or if I should find a workaround.",non_debt,-
camel,6233,comment_3,You can follow the 2.11 release talk at the Camel @dev mailing list,non_debt,-
camel,6273,summary,camel-example-etl failed,non_debt,-
camel,6273,comment_0,"I cannot reproduce the issue. May be I executed ""mvn install -Phibernate"" followed by ""mvn camel:run"". In this case the Hibernate dependencies are not in the class path. Using ""mvn camel:run -Phibernate"" works. Also the openjpa and eclipselink profiles",non_debt,-
camel,6291,description,"Currently <routeContextWhen ProcessorDefinition is asked to makeProcessor, it calls that replaces any placeholder with current routeContext data. This makes it impossible to futher use such ProcessorDefinition with any other routeContext with different placeholder resolution data.",non_debt,-
camel,6291,comment_0,"says: ""Reusable routes The routes defined in <routeContext/So, this is a bug in documented feature. If you think, it's an enhancement, the page should be corrected.",documentation_debt,low_quality_documentation
camel,6291,comment_1,Thanks for reporting.,non_debt,-
camel,6296,summary,Support parameters on Camel-Http (HttpClient3),non_debt,-
camel,6296,description,"parameters are not supported for HttpClient3 on Camel-Http component at least for version of camel 2.9.4. Here is a patch that provides support for parameters support. There is no test case provided, but a patch.",test_debt,lack_of_tests
camel,6296,comment_0,The patch needs some review.,non_debt,-
camel,6296,comment_1,Applied the patch into trunk.,non_debt,-
camel,6320,summary,Provide support for ICAL data format,non_debt,-
camel,6320,description,ical is popular data format used for scheduling and handling calendar events. It may be easily integrated into camel thanks to ical4j library.,non_debt,-
camel,6320,comment_0,I thought you'd attach the patch to this jira. I assume though that you meant us to pick up the version from github per your,non_debt,-
camel,6320,comment_1,I think your test file data.ics is missing. Could you please add it? Adding a bit of logging would help too. Thanks for your contribution.,code_debt,low_quality_code
camel,6320,comment_2,"Lukasz, thanks for the contribution. I did commit it to the trunk for now. Will create a placeholder for doc in the wiki, we'd appreciate some documentation. Since it's a new component and 2.12 will not be out anytime soon, I will port it back to the maintenance branch and then close this issue.",documentation_debt,outdated_documentation
camel,6320,comment_3,Can we get this new component documented?,documentation_debt,outdated_documentation
camel,6320,comment_4,"I added the documentation for the iCal data format. It would be good, if you could have a look at it.",non_debt,-
camel,6332,summary,and/or operator support in twitter keywords search,non_debt,-
camel,6332,description,Extend the twitter component to support and/or operator in keywords search. See,non_debt,-
camel,6332,comment_0,See this page for advanced searches,non_debt,-
camel,6349,summary,camel-restlet - Should set response correctly on Exchange depending on has out or not,non_debt,-
camel,6349,description,See nabble,non_debt,-
camel,6383,summary,Disable tracer mbean/wrapped processor if not explicit enabled on camel context,non_debt,-
camel,6383,description,We should not include tracer mbean/wrapped processor out of the box. As that just adds longer stacktraces and unnessasary processor wrappings. If people want to use the tracer they should explicit enable it on CamelContext by setting tracing=true.,non_debt,-
camel,6384,summary,Add debugger mbean for tooling,non_debt,-
camel,6384,description,This allows tooling such as karaf commands / others to do Camel EIP debugging. We have for SPI. What we need is a JMX layer to make it friendly for tooling.,non_debt,-
camel,6384,comment_0,Added docs at,non_debt,-
camel,6400,summary,"CamelContext fails to start using placeholder in ""simple"" in conjunction with",non_debt,-
camel,6400,description,"Recently after upgrading from 2.10.3 to 2.11.0 I have encountered the following bug. The problematic part was looking like this: Camel was failing to start with the following exception I have created a small test to reproduce the bug, and thanks to git bisect I found the commit that introduced this bug. It is commit ea4c0ab5 ""CAMEL-6233: does not support nested Spring properties"". The problem is that it is trying to resolve the whole simple with the property resolver but after resolving the spring property it treats as a spring placeholder and fails to find it. There is a workaround. If is replaced by then Camel starts just fine. The test to reproduce is attached.",non_debt,-
camel,6400,comment_0,test to reproduce CAMEL-6400,non_debt,-
camel,6400,comment_1,"You should use $simple{ } to avoid spring clashing with this, or set the option on the to ignore unknown placeholderes. Its a caveat when using this",non_debt,-
camel,6400,comment_2,I added a note to the docs about this clash.,non_debt,-
camel,6400,comment_3,Not sure what you mean by $simple{}. Can you please correct the above example? But besides this still think there is a bug. Because when first time it replaces all properties and returns the parsed one with nested spring placeholder it should not pass the whole to resolve the nested ones but need to pass only the part.,non_debt,-
camel,6400,comment_4,You should do,non_debt,-
camel,6403,summary,Add support for UnitOfWork per request,non_debt,-
camel,6403,description,"When using a {{camel-jetty}} endpoint, the {{UnitOfWork}} is not being managed by the servlet handling the request but by the Camel route that's being invoked. This means that some resources have already been removed/cleaned up when the servlet is writing the response, e.g. files for cached streams have already removed before the servlet gets a chance to read from them. It would be nice to have an option available to configure the servlet itself to handle the unit of work and mark it {{done}} after the HTTP response has been written. That way, the unit of work can be matched up with the actual HTTP request.",design_debt,non-optimal_design
camel,6403,comment_0,"If a consumer wants to handle the uow itself, such as this use, case then it should do When it creates the Exchange And then when its done processing and all its needed work it must do These methods is part of DefaultConsumer. This has been implemented for - camel-http - camel-jetty - camel-netty - camel-netty-http - camel-cxf",non_debt,-
camel,6403,comment_1,"Ideally the consumer should write its response in the async callback done method, then there UoW is not done yet. Though some of these http related components is a bit different and needed this support. Well spotted Gert.",requirement_debt,requirement_partially_implemented
camel,6446,summary,Support JAXB annotations in Jackson Data Format,non_debt,-
camel,6446,description,Provide the capability to marshall/unmarshall JAXB annotated objects as JSON,non_debt,-
camel,6446,comment_0,Thanks for the patch. Feel free to help with adding some docs to:,documentation_debt,outdated_documentation
camel,6550,summary,FTP proxy component,non_debt,-
camel,6550,description,"I couldn't find a way to work with proxy using FTP camel component, someone know how to deal with it ? Thanks!",non_debt,-
camel,6550,comment_0,See this page how to get help and how to ask questions etc,non_debt,-
camel,6563,summary,camel-netty - unable to consume on UDP multicast addresses,non_debt,-
camel,6563,description,"When using a route to listen to UDP multicast address , no messages seem to get consumed. No exceptions are observed. Multicast address is defined as addresses in the range of 224.0.0.0 through 239.255.255.255 Input was simple string (e.g. ""Test String"") Example Route: <route <from Found an old topic in the user discussion forum that seems related. Did not find any unit tests in the Camel source code exercising this behavior.",test_debt,lack_of_tests
camel,6563,comment_0,"To listen for UDP multicast addresses, I think the code may have to do some different logic when binding to the IP address. I found this link on the web which may have code that is relevant The camel-netty code currently does this for UDP non-mulicast address (e.g. localhost). This is in the source file. Channel channel = I think for UDP multicast address (e.g. 225.1.1.1), it needs to join a group on a particular network interface. It might looks something like this: DatagramChannel channel = String networkInterface = == null ? LOOPBACK_INTERFACE : NetworkInterface = I think the implication here is that the user might have to specify an interface name as an option of the camel-netty component when using a UDP multicast address.",non_debt,-
camel,6563,comment_1,Pull request: Pull request developer discussion topic:,non_debt,-
camel,6563,comment_2,Thanks for the patch.,non_debt,-
camel,6578,summary,Upgrade CXF to 2.7.6 / 2.6.9,non_debt,-
camel,6578,description,None,non_debt,-
camel,6578,comment_0,"for camel 2.12.x, 2.11.x cxf-2.7.5 -jettison-1.3.3 - for camel 2.10.x cxf-2.6.8 -jettison-1.3.3 ->jettison-1.3.4",non_debt,-
camel,6578,comment_1,Don't forget to update the Camel 2.12.0 release page at,documentation_debt,outdated_documentation
camel,6578,comment_2,okay. have updated the 2.12 release page.,non_debt,-
camel,6579,summary,Cannot build Camel source code with with OpenJDK Java 8,non_debt,-
camel,6579,description,I've tried to make a build with Java 8. It's just an early test depend on this: [ERROR] Failed to execute goal (generate-sources) on project camel-spring: An Ant BuildException has occured: taskdef A class needed by class cannot be found: [ERROR] using the classloader [ERROR] Failed to execute goal (generate-sources) on project camel-spring: An Ant BuildException has occured: taskdef A class needed by class cannot be found: using the classloader Method) Caused by: An Ant BuildException has occured: taskdef A class needed by class cannot be found: using the classloader ... 19 more Caused by: taskdef A class needed by class cannot be found: using the classloader Method) ... 21 more Caused by: at Method) ... 34 more Caused by: ... 37 more,non_debt,-
camel,6579,comment_0,depends on,non_debt,-
camel,6579,comment_1,Assume it builds on java 8,non_debt,-
camel,6580,summary,camel-jms - Dont allow CACHE_NONE for for temporary queues,non_debt,-
camel,6580,description,See nabble,non_debt,-
camel,6580,comment_0,Updated doc as well with a note about CACHE_NONE not allowed for temp queues,non_debt,-
camel,6593,summary,Predicates from java dsl are not dumped to xml correctly,non_debt,-
camel,6593,description,"Predicates defined in the java dsl are not dumped to xml when using jmx. Eg, this java dsl route: Will be dumped as this: The This seems similar to CAMEL-4733.",non_debt,-
camel,6593,comment_0,Also seems to affect setting properties. Eg: Gets dumped with an empty tag as well.,non_debt,-
camel,6593,comment_1,"This is expected as these builders cannot be represented in plain text, such as simple etc can do. Though in the future we may be able to map these to a simple text expression we can use to dump eg",non_debt,-
camel,6593,comment_2,We now dump the predicate in the model though its just a toString representation of the predicate that the ValuerBuilder has built. But at least you can see some information now.,non_debt,-
camel,6610,summary,Always got when customized id of wireTap component,non_debt,-
camel,6610,description,"when I'm tring to execute below route: I always got {color:red} Exception in thread ""main"" Index: -1 {color} I tried on 2.11.1, 2.11.2-SNAPSHOT, both of them have the same problem (not sure 2.12-SNAPSHOT).",non_debt,-
camel,6610,comment_0,"Applied the patch into master, camel-2.11.x and camel-2.10.x.",non_debt,-
camel,6610,comment_1,"Great, Thanks.",non_debt,-
camel,6618,summary,update camel-lucene to use version 4.4,non_debt,-
camel,6618,description,"should consider upgrading to newer version of Lucene 4.4...not sure what other dependencies/timing would be involved in this upgrade, but the camel-solr and camel-elasticsearch components could also use upgrades to start using Lucene 4.x APIs...",non_debt,-
camel,6618,comment_0,CAMEL-5922 wants to update to Lucene 4.0,non_debt,-
camel,6618,comment_1,duplicate of CAMEL-5922,non_debt,-
camel,6620,summary,No mention of authMethod in http4 documentation,documentation_debt,outdated_documentation
camel,6620,description,"I got an exception saying that an authMethod value is required. So, I went to the docs and there is no mention of authMethod or the acceptable values. I had to search the code find the AuthMethod enum to know what value is acceptable and to find the authMethod parameter name.",documentation_debt,outdated_documentation
camel,6620,comment_0,"I just checked the code AuthMethod is just used in camel-http component, not the camel-http4 component. So it is not strange that there is no mention of AuthMethod in the http4 document. I also checked the camel-http wiki page, there is entry of the options about the AuthMethod.",documentation_debt,outdated_documentation
camel,6623,summary,Support attaching to SQS queues where the user doesn't have permission to list the queues (manually build URL),non_debt,-
camel,6623,description,Currently the code assume that the user either has permission to list all the queues on their account or run a query request to get the queue url. In situations where security is more controlled you might have permission to read/write to/from a queue but not have permission to query SQS for the URL. In these cases there needs to be a way to manually build the URL from the pieces in the configuration.,non_debt,-
camel,6623,comment_0,This patch will manually build the queueURL if both the region and accountID are provided. This allows for attaching to queues that don't allow listing and querying.,non_debt,-
camel,6623,comment_1,Applied the patch into master and camel-2.12.x branches with thanks to Chris. I also updated the wiki page for it.,non_debt,-
camel,6628,summary,ProducerTemplate - Allow to turn off event notifier,non_debt,-
camel,6628,description,There can be situations where you do not want the producer template to emit events for exchange sending + sent. See nabble,non_debt,-
camel,6628,comment_0,There is now API on ProducerTemplate to turn this on|off (default is on),non_debt,-
camel,6635,summary,PollingConsumer from a scheduled consumer such as file/ftp can use a regular thread pool instead of being scheduled,non_debt,-
camel,6635,description,"Due the recent SPI which allows to plugin a different scheduler we can improved this and use a non scheduled thread pool, which avoids the suspend/resume and run for at least one poll ""hack"" we have today in the codebase. Instead we can use a regular thread pool as the scheduler, and then submit the task on demand when receive() is called on the PollingConsumer API.",design_debt,non-optimal_design
camel,6635,comment_0,Darn the polling consumer logic wasn't too well equipped for this kind. Had to revert to use the old way to allow picking up late files that arrives later when the poll starts.,non_debt,-
camel,6641,summary,SJMS component throws class cast error when used with IBM Webshpere MQ,non_debt,-
camel,6641,description,When using the IBM MQ JMS classes and pub/sub over a TOPIC the method throws a cast class exception. The session is being cast to a TopicSession which for MQ it is not. Using pure jms this cast is not required as the session class is aware of the destination type.,non_debt,-
camel,6641,comment_0,Created a patch that fixes it on my desktop.,non_debt,-
camel,6641,comment_1,Thanks Nigel for the patch! It's applied into the master and the camel-2.11.x maintenance branch.,non_debt,-
camel,6650,summary,AggregationStrategy - Allow to use a pojo with no Camel API dependencies,non_debt,-
camel,6650,description,"See also CAMEL-3671 When using the AggregationStrategy with the various EIPs, then its tied to the Camel API. We should allow people to use plain pojos, and have Camel call a method on the pojo. This would require a convention of the parameters of the method; so Camel knows what to pass as values.",non_debt,-
camel,6650,comment_0,See details at the aggregate eip page,non_debt,-
camel,6658,summary,- is broken on master,non_debt,-
camel,6658,description,mvn clean install -rf fails testing this example.,non_debt,-
camel,6658,comment_0,It was due a mistake in CAMEL-6650 in multicast EIP.,non_debt,-
camel,6661,summary,xslt: include href is empty,non_debt,-
camel,6661,description,"The use of the xlst function, document(''), gives an exception like: include href is empty"" when transforming, using the component camel-xslt. It seems that camels XsltUriResolver only is capable to handle files either from the classpath or the file system. With the xlst function, document(''), the xslt processor should be able to handle that the result ""is the source XML of the XSLT document itself"". Below is an example. A XML document: <?xml version=""1.0"" A XSL document: <?xml version=""1.0"" version=""1.1"" <date:months <date:month length=""31"" abbr=""Jan"" <date:month length=""28"" abbr=""Feb"" </date:months <xsl:template match=""root"" <xsl:variable name=""month-node"" <xsl:element name=""MyDate"" <xsl:value-of </xsl:element After transformation the following should be produced: <?xml version=""1.0"" encoding=""UTF-8""?",non_debt,-
camel,6661,comment_0,Three lines need to be added to XsltUriResolver; The attached unit test shows that this will solve this issue. Place the xsl file in:,non_debt,-
camel,6661,comment_1,"Applied the patch into master, camel-2.12.x and camel-2.11.x branches with thanks to Rene.",non_debt,-
camel,6716,summary,fails to create with interface containing multiple methods without parameters,non_debt,-
camel,6716,description,"I you attempt to create a with interface containing multiple methods without parameters, you get It's due to following code: For the second method without parameters, the null value is already in the inTypeNameToQName, so it tries to evaluate this: which fails with NPE. The question here is, why method without parameters still needs an array of types containing one element with those null values.",non_debt,-
camel,6716,comment_0,Patches is welcome,non_debt,-
camel,6716,comment_4,"Sorry for this two pull request. I've close the first one, and submitted a new one. Hope it should be helpful.",non_debt,-
camel,6716,comment_5,"Applied the patch into camel master, camel-2.13.x and camel-2.12.x branches with thanks to Andrea",non_debt,-
camel,6735,summary,Tracer causes Premature end of file in split,non_debt,-
camel,6735,description,See for details. This issues doesn't occur in 2.12.0 onwards.,non_debt,-
camel,6735,comment_0,attached a polished test from Preben which shows the issue.,test_debt,lack_of_tests
camel,6735,comment_1,This works in 2.12.x onwards. Hunting this down on 2.11.x is low priority. End users is encourage to upgrade if they really need this.,defect_debt,uncorrected_known_defects
camel,6745,summary,sftp consumer - Option ignoreFileNotFound should be used for ignoring file permission errors as well,non_debt,-
camel,6745,description,See nabble,non_debt,-
camel,6745,comment_0,The option is renamed to,non_debt,-
camel,6774,summary,Karaf and servlet 2.5/3.0 due recent jetty upgrade causes karaf problems!,non_debt,-
camel,6774,description,Running tests in fails with features that uses servlet 2.5 api with Karaf 2.3.3. I suspect its the recent jetty 8.x and servlet 3 upgrade that is causing problems.,non_debt,-
camel,6774,comment_0,These tests fails Failed tests: Cannot get component with name: websocket,non_debt,-
camel,6774,comment_1,With  recent fixes we are down to 2 erros Cannot get component with name: websocket,non_debt,-
camel,6774,comment_2,"Yeah I've got those two remaining tests fixed as well on my workspace. Running a full build right now to make sure everything is fine. Also [this CXF test being excluded inside the POM of works again, so why I will re-enable it again.",non_debt,-
camel,6774,comment_3,O.K. I managed to fix those Karaf tests. Just realized that there're other failing tests by the module with or without of the fix for this ticket. Are we already aware of this? Is there any ticket around for this? These are the ones failing for me but I didn't look into them:,non_debt,-
camel,6793,summary,"Camel 2.12 shows unexpected warning ""Cannot determine current route from Exchange""",non_debt,-
camel,6793,description,"Since upgrading from 2.10.6 to 2.12.1 Camel logs the following warning: It looks like the ""splitter"" is causing the problem. If the ""<split Here's the example route for reproduction: Is it a bug or do I have to configure something new? 2.10.6 and previous versions did not show this kind of behavior (meaning: did not log this warning)!",non_debt,-
camel,6793,comment_0,Thanks for reporting. I have reproduced the issue and have a pending fix.,non_debt,-
camel,6793,comment_1,I just wanted to confirm that I am having the same issue,non_debt,-
camel,6809,summary,header used in preference to uri exchange name,non_debt,-
camel,6809,description,"method uses the existing header in preference to the exchange name that is part of the endpoint uri. This means that when consuming off one exchange and producing onto another, the needs to be explicitally removed, otherwise the messages will not be sent to the exchange name defined in the destinition uri. The exchange name is a required part of the endpoint uri. so would it be better to ignore the header completely in the RabbitMQProducer? At least then the messages would go where you were expecting them to. Happy to submit a pull request on Github if that helps.",non_debt,-
camel,6809,comment_0,"Yeah, we love contributions. And of course you can send a pull request via GitHub.",non_debt,-
camel,6809,comment_1,Pull request submitted.,non_debt,-
camel,6809,comment_2,"In Camel the message header can override the setting of the endpoint, if you want to build a proxy, I think we could add an option bridgeEndpoint in camel-rabbitmq endpoint to let the producer ignore the header setting as we do in camel-http.",non_debt,-
camel,6809,comment_3,Added bridgeEndpoint option to ignore the header. Applied the patch into trunk and camel-2.12.x branches.,non_debt,-
camel,6819,summary,Use exceptionHandler on Xmpp Consumer,non_debt,-
camel,6819,description,"Hi! What do you think about adding a call to the exceptionHandler in the doStart() method of XmppConsumer class ? It would looks like : try { connection = } catch (XMPPException e) { if { throw new not connect to XMPP server."", e); } else { final String = return; } } This allowed an exceptionHandler to be notified of failure connection. The doesn't call exceptionHandler because the exception is thrown while starting the consumer.",non_debt,-
camel,6819,comment_0,"Yeah good idea, you are welcome to work on a patch.",non_debt,-
camel,6819,comment_4,"The original pull request come from a wrong branch, so I've close it and I've submitted this new one. Sorry. Now is ok. Bye",non_debt,-
camel,6819,comment_5,Applied the patch with thanks to Andrea and I also added a check to avoid the NPE of exceptionHandler.,non_debt,-
camel,6826,summary,Use Mock Objects Instead of Live HazelcastInstances to Speed Up Testing,test_debt,expensive_tests
camel,6826,description,"The unit tests for the camel-hazelcast component use real HazelcastInstance objects, which is very slow. We should use mock objects instead to speed up testing. Testing the integration can be done with a few, select tests, but the majority of the logic can be tested with mocks.",test_debt,expensive_tests
camel,6826,comment_0,"I have been able to successfully speed up the producer tests. However, the consumer tests are a bit tricky. They use a callback API to do their work. I don't know if it's worth mocking all of that out or not, but I'll play with it. I'm committing what I have, though.",test_debt,expensive_tests
camel,6826,comment_1,"I have implemented mocks for the consumer tests. The spring-based tests still need to be mocked out as well as the seda tests. It is much faster now, though.",test_debt,expensive_tests
camel,6826,comment_2,Spring tests have been updated as well as the error messages tests.,non_debt,-
camel,6826,comment_3,"Is there more work needed, or can this ticket be resolved?",non_debt,-
camel,6826,comment_4,"This is still in-progress, unless you just want to start a new one for taking care of the seda tests. I believe that's all that's left.",non_debt,-
camel,6826,comment_5,Its fine and fast now,non_debt,-
camel,6826,comment_6,So you are the sole person who can make this determination?,non_debt,-
camel,6826,comment_7,"You mean it's fast enough after the changes made? Sorry didn't realize this was the one with work already done. If we like how it works now, I'm cool with closing this, but it should be marked as ""fixed"" since something was actually done.",non_debt,-
camel,6849,summary,Update to jruby 1.7.5,non_debt,-
camel,6849,description,None,non_debt,-
camel,6849,comment_0,This possible breaks features. Doing a validate causes,non_debt,-
camel,6849,comment_1,"Yeah, there was a problem alright. JRuby is now split into 2 OSGi bundles: jruby-core and jruby-stdlib. The old jruby artifact can still be used for Maven builds since it now just has transitive dependencies to core and stdlib.",non_debt,-
camel,6880,summary,camel-http4 - Should support defining multiple components with different names,non_debt,-
camel,6880,description,"See nabble If you define multiple http4 components and assign their names as: ""http4-foo"", and ""http4-bar"" etc, then the address uri is not resolved correctly.",non_debt,-
camel,6896,summary,Add descriptions in,non_debt,-
camel,6896,description,"I've been writing a URI Builder for NetBeans using the Property Sheet api. Everything works great with the API that was added in 2.12, but it would be really nice to be able to grab parameter documentation through this api as well. For instance, with the property sheet, I can set a Short Description of what each endpoint does, and the user would be able to go from there.",documentation_debt,outdated_documentation
camel,6896,comment_0,"Tracked by CAMEL-7999, where we add component documentation at both design and runtime.",non_debt,-
camel,6933,summary,Support Xerces global features configuration,non_debt,-
camel,6933,description,We need to support to configure the feature of Xerces from out side of camel route such as system property to disable some feature.,non_debt,-
camel,6933,comment_0,"Applied the patch into master, camel-2.12.x and camel-2.11.x branches.",non_debt,-
camel,6933,comment_1,"Now you can set the Xerces feature by setting System properties like this + "":"" + ""false"");",non_debt,-
camel,6954,summary,camel-mina2 - UDP protocol should use worker pool as well,non_debt,-
camel,6954,description,"We need to enable the worker pool for UDP as well, otherwise we have only one thread process the message out of the box.",non_debt,-
camel,6954,comment_0,"Only a problem with mina2, mina has worker pools already for TCP and UDP.",non_debt,-
camel,6962,summary,Allow binding name to be specified in JibxDataFormat,non_debt,-
camel,6962,description,The camel-jibx data format class does not allow the user to specify the JiBX binding name. This feature is necessary when dealing with input bindings that are different than output bindings for the same Java class.,non_debt,-
camel,6962,comment_0,Couldn't find a way to grant ASF license for the patch via JIRA -- but I grant it.,non_debt,-
camel,6962,comment_1,"Hi Peter, Don't worry about it, the default setting for submitted patch is granted with ASF license.",non_debt,-
camel,6962,comment_2,"Applied the patch into master, camel-2.12.x and camel-2.11.x branches with thanks to Peter.",non_debt,-
camel,6962,comment_3,"Just pushed a commit that added the new option to the model in the camel-core, this allows users of the XML DSL to configure it as well. That is a rule of thumb on the data format. That their options should also be exposed in the model class in camel-core.",non_debt,-
camel,6964,summary,Camel FileComponent: Done file will not be removed if moveFailed option is configured and an error occurs,non_debt,-
camel,6964,description,"Only the ""real"" file is moved to the directory specified with the moveFailed-option. The done file still exists in the source folder and will not be deleted.",non_debt,-
camel,6964,comment_0,"First time git, hopefully correct. Patch is for ""master"" version",non_debt,-
camel,6964,comment_1,Thanks for reporting and the patch. Notice that the done file should only be deleted on rollback if moveFailed is in use.,non_debt,-
camel,6973,summary,Add a simple expression for null,non_debt,-
camel,6973,description,"Adding a simple expression that can be used to set the body to null, as in <setBody background",non_debt,-
camel,6973,comment_0,I think this is already implemented: passes. public void testNull() throws Exception { Object.class)); },non_debt,-
camel,6973,comment_1,"Jan, actually, the test code that you referred to was added with this patch (see the source list) and that was not there before. so, you are right that it's already implemented but it was not implemented before. regards, aki",requirement_debt,requirement_partially_implemented
camel,6973,comment_2,"Ok, but why not close this issue?",non_debt,-
camel,6973,comment_3,"because 2.12.x was being cut for a 2.12.2 release because of that a few things were not done (integrating into 2.12.x, updating the documentation).",documentation_debt,outdated_documentation
camel,6980,summary,Camel Karaf commands - Should sort output and also use wider tables,non_debt,-
camel,6980,description,"So we have nicely sorted output, and also wider tables so longer camel and route names can be listed.",non_debt,-
camel,6980,comment_0,The list commands now calculate max width and show the data nicely. Also the list endpoints masks passwords.,non_debt,-
camel,6981,summary,Route model should include attribute if id is custom assigned or auto generated,non_debt,-
camel,6981,description,When a route is dumped as XML you may not know if the id is explicit assigned or was auto generated. That information would be handy to know. The information is already available but just not included in the XML dump.,non_debt,-
camel,6981,comment_0,"Just to make sure that my understanding is correct, can you confirm that this new information will automatically be available in the class as that JAXB class is generated out of the XSD?",non_debt,-
camel,6981,comment_1,"The attribute customId=""true"" is included if a custom id was assigned. If auto assigned value then the attribute is not present.",non_debt,-
camel,6981,comment_2,"Ah yeah you have this information already in java code if you have the model classes. See: This method is on all the models in Camel routes. Its just the XML representation that didn't have this information, so when you looked at the XML you wouldn't be able to know if the id was auto generated or custom assigned.",non_debt,-
camel,7015,summary,Allow timeout on the first Exchange for aggregator,non_debt,-
camel,7015,description,"It would be nice to have a possibility of a timeout starting from the first Exchange for messages having the same when using Aggregation. This scenario is currently not supported because ""Its a bit tougher to implement as it would require an improvement to TimeoutMap to support that as well."" The full scenario is described in this thread:",design_debt,non-optimal_design
camel,7015,comment_0,I've added a pull request for this : But my commit messages have a wrong jira number... Sorry about that.,non_debt,-
camel,7015,comment_1,What is the idea with the Will it not confuse end users that there is 2 timeouts now?,design_debt,non-optimal_design
camel,7015,comment_2,"Willem hold on any merges, as at first glance I think its confusing with that max timeout option.",design_debt,non-optimal_design
camel,7015,comment_3,"At first I though of adding an option telling whether or not the timeout is for the first message received in this correlation group or for the last message. But then I though, why not provide both timeouts ? You might want to 5 seconds after the last exchange received but not more than 60 seconds after the first message. That way, if you have an exchange every 4 seconds, you'll aggregate after one minute an not any longer. Thus I named this parameter because I didn't wanted to rename the parameter. I'm not saying that it's the right way, or maybe it's not properly named. Don't hesitate to ask if you want more insights about this patch.",design_debt,non-optimal_design
camel,7016,summary,JMX - Update route from xml on route mbean should update current route only,non_debt,-
camel,7016,description,"If you do not have id of the route in the XML then Camel thinks its a new route to be added. We should ensure we handle that, and only update current route as that is the intend of this operation. If you want to add new routes use mbean operation on camelcontext instead.",non_debt,-
camel,7071,summary,clazz) returns root cause instead of thrown exception,non_debt,-
camel,7071,description,If I catch SQLException and wrap it in new then returns but returns SQLException. This impacts ExchangeException annotation which calls Reason for that is in constructor of which for some reason calls,non_debt,-
camel,7071,comment_0,is used to make sure we can get the first cause of exception in the catch() DSL and ErrorHandler. Here are the Javadoc of the,non_debt,-
camel,7071,comment_1,This is strange way to solve that. Should not we (application developers) use exception wrapping? Please note this method is used By,design_debt,non-optimal_design
camel,7071,comment_2,"As an application developer, you can just use to access the exception.",non_debt,-
camel,7071,comment_3,"As I mentioned in the initial description, call to is used in interpreter of \@ExchangeException annotation. I guess this annotation should be equivalent of",non_debt,-
camel,7106,summary,PGP decryption sometimes can't find the secret key,non_debt,-
camel,7106,description,"Camel Crypto fails to decrypt PGP files encrypted by GnuPG and other programs, when the secret key used to encrypt the symmetric key is not the first on the keyring (using Bouncy Castle's definition of a keyring). The error message is ""Provided input is encrypted with unknown pair of keys."" Since the encryption key in files from other PGP programs normally is a subkey and is not the first one in the keyring (which is typically the master key), this means that decryption fails in many cases. The fix is a one-liner to to use a BC method that searches for the secret key rather than assume it's the first one on the secret keyring. I will attach a patch.",non_debt,-
camel,7106,comment_0,"This issue is resolved in CAMEL-7052 few days ago, please try to use the latest camel-2.13-SNAPSHOT to verify the fix.",non_debt,-
camel,7127,summary,CXF component doesn't work with Spring 4,non_debt,-
camel,7127,description,CXF 2.x uses deprecated class removed in Spring 4. Because of that camel-cxf doesn't work with Spring 4. This issue has been discussed on this (2) forum thread already. (1) (2),architecture_debt,using_obsolete_technology
camel,7127,comment_0,"As CXF 3.0 remove the dependency of spring-jms, this issue was addressed by upgrade CXF version to 3.0.0.",non_debt,-
camel,7133,summary,camel-1 Error running main from:,non_debt,-
camel,7133,description,"I try to run the camel Main standalone and get the I saw CAMEL-1348 which looks the same. The fix in CAMEL-1348 was to add a jms dependency. I tried that to no avail. I provided the small appContext and pom, in the Environment section, to help test. Here is the stacktrace: [ERROR] [ERROR] Error occurred while running main from: [ERROR] at Method) Caused by: Error creating bean with name 'camel-1': Initialization of bean failed; nested exception i",non_debt,-
camel,7133,comment_0,I decided to try to whittle things down. So I removed everything in the appContext except one pojo bean and an empty camelContext and I still get the error. Caused by: Error creating bean with name 'camel-1': Initialization of bean failed; nested exception Method) So I'm stumped.,non_debt,-
camel,7133,comment_1,"It may relate to the POJO bean, can you try to remove it this time?",non_debt,-
camel,7133,comment_2,"I commented the pojo out. It didn't help. The stack trace says: Error creating bean with name 'camel-1'. I did clean it up a bit: <?xml version=""1.0"" <cxf:rsServer id=""rsServer"" </cxf:rsServer <!-- Camel Configuration -- <camel:camelContext id=""camel-1"" <camel:dataFormats <!-- here we define a Json data format with the id hrmJsonInput and that it should use the TestPojo as the class type when doing unmarshal. The unmarshalTypeName is optional, if not provided Camel will use a Map as the type -- <camel:json id=""Json"" library=""Jackson"" / </camel:dataFormats <camel:route id=""route1"" <camel:from <!-- <camel:unmarshal ref=""Json""/ <camel:to / </camel:route <!-- loads resource annotations -- <bean <property value=""false""/ </bean <!-- <bean id=""transformer"" </bean</beans>",non_debt,-
camel,7133,comment_3,"Can you remove the <cxf:rsServer id=""rsServer"" I don't think you can put the url options into rsServer address attribut.",code_debt,low_quality_code
camel,7133,comment_4,Please use the mailing list / user forum for getting help with Camel and this kind of question. The JIRA tracker is not the right place for these situation. See details here how to get help,non_debt,-
camel,7139,summary,Problem with MvelExpression class visibility between camel bundles,non_debt,-
camel,7139,description,I am having problems with with class visibility between osgi bundles when using camel 2.12 and 2.13-SNAPSHOT. I have created a showcase to reproduce the problem based on camel example projects. I have tested the created solution in camel 2.10 and problem does not exist. I have created a sample application which is able to reproduce the problem. Please pick it up from my forked repo: The problem occurs when - - project containing only domain classes required by different osgi projects ex.( ) - - example project which has dependency on Now has following camel context. When I deploy both feature to karaf and place a message on seda://myMvelTest queue I get following exception in karaf.,non_debt,-
camel,7139,comment_0,I am happy to create a pull request with showcase example.,non_debt,-
camel,7139,comment_1,I reproduced the problem with itest please see the attach. To Fix the problem a possible solution is remove TCCL to MVEL until is not resolved Please see the attach for the patch;I hope it's ok for you. If it's ok for you please clean it a bit Any feedback is welcome.,code_debt,low_quality_code
camel,7139,comment_2,Yeah you are welcome to cleanup the patch.,code_debt,low_quality_code
camel,7139,comment_3,osgi-itest for mvel,non_debt,-
camel,7139,comment_4,Thanks for the patch.,non_debt,-
camel,7150,summary,Provides options to setup the parameter on the WebSocketServlet,non_debt,-
camel,7150,description,We should provides the setting of WebSocketServlet in the camel-websocket uri.,non_debt,-
camel,7150,comment_0,"Applied the patch into camel master, camel-2.12.x and camel-2.11.x branches. I also updated the wiki page for the new added options.",non_debt,-
camel,7176,summary,camel-vertx - Should support async request/reply,non_debt,-
camel,7176,description,See nabble,non_debt,-
camel,7177,summary,camel-vertx - Should allow to use clustered and non clustered vertx,non_debt,-
camel,7177,description,"We should default vertx in non clustered embedded mode. And then only if end user configure the component with a host port, then its clustered using that.",non_debt,-
camel,7177,comment_0,The default mode is non clustered now. You can configure host / port to setup clustered vertx as its VertxFactory API denotes.,non_debt,-
camel,7201,summary,PGPDataFormat: allow caching of PGP keys via key access interface,non_debt,-
camel,7201,description,"In the current PGPDataFormat implementation, you provide the public and secret keyring as file or byte array and the keyrings are parsed into object representation during each call. This is fine if you want dynamically exchange the keyrings. However, this has a performance impact. In the provided patch I added the possibility that the PGP keys can be cached so that the performance can be improved. I did this by adding the two interfaces PGPPublicKeyAccess and PGPSecretKeyAccess. So user can now implement his own key access or use the provided default implementations and",design_debt,non-optimal_design
camel,7201,comment_0,Applied the patch into camel master branch with thanks to Franz. I also renamed the implementation classes as camel usually does.,non_debt,-
camel,7201,comment_1,Applied the patch into camel master branch with thanks to Franz. I also updated the wiki page for the change.,non_debt,-
camel,7208,summary,ManagedCamelContext - should support decoding xml,non_debt,-
camel,7208,description,"The updated route(s) in xml format may have been encoded, which means we should support decoding the xml prior to loading it as a model. For example {{ }} placeholders may be encoded into %7B%7B and %7D%7D which would cause the routes not being able to be updated correctly. Adding a boolean parameter to the operation so you can control if you want to decode or not.",non_debt,-
camel,7210,summary,does not work if component name has dash in name,non_debt,-
camel,7210,description,For example if component is direct-vm we cannot load the component docs. Also the ftp components has special location.,non_debt,-
camel,7215,summary,Stop and un-schedule jobs on removal of route,non_debt,-
camel,7215,description,When using jobs are not deleted when a route is removed. I got the following exception when removing a route and re-adding it to a running context. The should remove the scheduled jobs on route removal. This is what does.,non_debt,-
camel,7215,comment_1,Pull request to unschedule jobs:,non_debt,-
camel,7230,summary,SJMS does not respect QoS settings (ttl/persistence) for sending to queues,non_debt,-
camel,7230,description,Reproduced here:,non_debt,-
camel,7230,comment_0,Fixed with this commit:,non_debt,-
camel,7244,summary,PGPDataFormat: verification with subkey restricted by User ID does not work,non_debt,-
camel,7244,description,"If you verify a PGP signature with a subkey which should have a certain User ID, then you run into an error. Reason: The implementation has not taken into account that the subkey has no User ID on its own. One must consider the User ID of its master/primary key.",non_debt,-
camel,7244,comment_0,Applied the patch into camel master branch with thanks to Franz.,non_debt,-
camel,7300,summary,HL7 converter should not perform validation,non_debt,-
camel,7300,description,"Currently the TypeConverter in the camel-hl7 component uses a default PipeParser. This parser performs HL7 message validation (using the default validation profile) even if validation on the endpoint is disabled. IMHO the TypeConverter shouldn't perform any validation at all, it's not the responsible of a TypeConverter. It should just convert types. Validation is already the responsibility of the consumer and enabled by default.",non_debt,-
camel,7300,comment_0,Pushed to master. Not this patch makes use of a deprecated API call. In time we should move the entire camel-hl7 component to the new HAPI API: CAMEL-7301,code_debt,low_quality_code
camel,7300,comment_1,We need to merge the patch into camel-2.13.x branch once the camel-2.13.0 is officially released.,non_debt,-
camel,7300,comment_2,Backported patch to camel-2.13.x branch and camel-2.12.x branch.,non_debt,-
camel,7304,summary,does not work where uri needs to be normalized,non_debt,-
camel,7304,description,"is not intercepted because uri passed to is not normalized. As a result createProcessor() method fails to match uri, getUri()) and is not created.",non_debt,-
camel,7304,comment_0,"You can use wildcard, and do",non_debt,-
camel,7304,comment_1,"Thanks, Claus. It was quicker for me to normalize URI passed to Andre",non_debt,-
camel,7306,summary,Camel:Kafka NPE when trying to consume messages from kafka server,non_debt,-
camel,7306,description,"I've installed and configured zookeeper and kafka servers using apache quick start guide on ubuntu running in vmware. Launch producer for already created topic ""page_visits"" and try to consume messages and nothing happens. Trying to debug KafkaEndpoint i can see that server doesn't send me any key so here: new String(mm.key())); i got NPE and it silently fails.",non_debt,-
camel,7306,comment_0,Here is my sample project to reproduce this issue,non_debt,-
camel,7313,summary,camel-sql - Add support for fetching generated primary keys when using INSERT,non_debt,-
camel,7313,description,We should add support for fetching primary keys that may be used when doing an INSERT. The JDBC has API to fetch that. See also nabble,non_debt,-
camel,7313,comment_4,"Applied the patch into camel master, camel-2.13.x and camel-2.12.x branches with thanks to Grzegorz.",non_debt,-
camel,7313,comment_6,Resolved. Documentation updated.,non_debt,-
camel,7319,summary,Dead not working JUnittest,code_debt,dead_code
camel,7319,description,"The test in is ""dead"" since checkin git-svn-id: if you enable xalan on testing classpath.",code_debt,dead_code
camel,7319,comment_0,"Applied the patch into camel master, camel-2.13.x and camel-2.12.x branches.",non_debt,-
camel,7342,summary,Implement flag for the SMPP consumer as it works for the producer.,non_debt,-
camel,7342,description,"If a SMPP consumer route is configured but the the initial connection attempt fails (eg if the SMSC is not accessible), then the route fails Connection refused) and the camel context shuts everything down. For SMPP producers there is a flag which allows the route to be created and go into a reconnection attempt cycle until the SMSC is available. Please implement this flag for the consumer. See discussion from last year here:",non_debt,-
camel,7342,comment_0,"I just went through the discussion of the mailing list, I agree with Christian Mueller, we should not implement the flag for the SMPP consumer. For the SMPP consumer, it need to be connect to the server before the consumer is started, otherwise camel don't have chance to start the consumer again. The work around could be set the route autoStart option to be false if you don't want the SMPP consumer connect to the SMSC when the camel route is loaded, then you can start the camel by calling the route start when the time is ready.",non_debt,-
camel,7342,comment_1,"Having looked at this issue afresh I wonder if my description of implementing the flag is a cause of confusion and reluctance to add this feature. On re-reading, I realise that this description of what is required is misleading - of course you always want the consumer to attempt to connect upon start of the route, however if the bind to the SMSC fails for whatever reason, I need it to enter into a reconnection attempt cycle until the SMSC is available rather than stopping the route (and any others in the same camel context). Anyway, this is a requirement for me, and so I have implemented the functionality change for myself as a branch from 2.15.1. It is configured using a new option If this is set to true, then a reconnect thread is started instead of trying once only to create the session. The major change here is the removal of the wait for the thread completion in the reconnect method (to allow doStart to complete), and movement of the reconnectLock mutex.",documentation_debt,low_quality_documentation
camel,7362,summary,Add URI option support of maxTotalConnections and connectionsPerRoute,non_debt,-
camel,7362,description,We could support to set these parameters through the http4 endpoint uri if we don't specify the connection manager in the http component.,non_debt,-
camel,7362,comment_0,Applied the patch into camel master and updated the wiki page.,non_debt,-
camel,7367,summary,groovy.lang.Script,non_debt,-
camel,7367,description,When trying to upgrade from 2.11.4 to either 2.12.3 or 2.13.0 a problem with using Groovy pops up. The symptom is: groovy.lang.Script Method) Method) ... This runs inside a Karaf 2.3.4 with features <feature <feature loaded. groovy.lang.Script comes from the groovy-all/2.2.2 bundle. The bundle is loaded by the camel-groovy feature.,non_debt,-
camel,7367,comment_0,Can I have a look of you camel route? Can you try to install camel-script and camel-groovy at the same time?,non_debt,-
camel,7367,comment_1,Are you use a producer template to send a message to a Camel route? And are you using blueprint or spring or something else?,non_debt,-
camel,7367,comment_2,FYI: I work with Martin Sorry for the late response. I made a small project that shows the problem. It works when the route is published this way: <bundleIt does not work when I try to import this bundle: <bundle Have a look at the ReadMe.txt for further details.,non_debt,-
camel,7367,comment_3,"Hi Michael, I just had a chance to go through your test cases. I found it's common OSGi bundle issue, not the camel issue. when you install the camel route (blueprint.xml) from pax-exam will create a bundle with these information: You can see the bundle header is *, which means this bundle can import all the package. If I add the below lines into maven-bundle-plugin section of I cannot get any any more.",non_debt,-
camel,7367,comment_4,It's not an issue of camel.,non_debt,-
camel,7412,summary,[docs] The docs state camel-jdbc does not support transactions.,documentation_debt,low_quality_documentation
camel,7412,description,The docs at state This statement however seems to be wrong. I have a demo at that uses the camel-jdbc component in an XA transaction scenario without errors. Can someone please confirm the docs is wrong and I can correct it in the docs? This statement was introduced in,documentation_debt,low_quality_documentation
camel,7412,comment_0,"I am going to remove the statement for now. Please let me know if this raises any concerns. Again, have tested the camel-jdbc component (2.12.0) with XA transactions myself and am confident that it works correctly.",non_debt,-
camel,7412,comment_1,Fixed in revision 31 of,non_debt,-
camel,7413,summary,should allow to use a custom login URL,non_debt,-
camel,7413,description,"Right now, the is only able to use the default Salesforce login URL defined in the class. When using test platform, it would be great to be able to provide a ""custom"" login URL.",non_debt,-
camel,7413,comment_0,"I fixed this by making all of the parameters configurable, and updating the documentation. Here are the new docs. ## Usage ## The plugin configuration has the following properties. * - Salesforce url to use to generate the dtos. Defaults to but is useful for development environments. * - Maximum time to hold the connection open to Salesforce when generating the dtos. Defaults to 60 seconds, which is usually fine unless you have a big Salesforce schema or slow connection. * - Salesforce client Id for Remote API access * - Salesforce client secret for Remote API access * - Salesforce account user name * - Salesforce account password (including secret token) * - Salesforce Rest API version, defaults to 27.0 NOTE: Currently no version higher than 27.0 will work. * - Directory where to place generated DTOs, defaults to * - List of SObject types to include * - List of SObject types to exclude * - Java RegEx for SObject types to include * - Java RegEx for SObject types to exclude * - Java package name for generated DTOs, defaults to For obvious security reasons it is recommended that the clientId, clientSecret, userName and password fields be not set in the pom.xml. The plugin should be configured for the rest of the properties, and can be executed using the following command: mvn The generated DTOs use Jackson and XStream annotations. All Salesforce field types are supported. Date and time fields are mapped to Joda DateTime, and picklist fields are mapped to generated Java Enumerations.",documentation_debt,outdated_documentation
camel,7413,comment_1,The is working in version 2.20.2. Should this be resolved?,non_debt,-
camel,7413,comment_2,This was implemented in 2.19.,non_debt,-
camel,7431,summary,Impossible to use an existing javax.mail.Session with,non_debt,-
camel,7431,description,"When using with an existing javax.mail.Session instance (i.e. retrieved from JNDI) it is not possible to leave host and port unconfigured. My JavaMailSender is configured as follows: @Bean public JavaMailSender mailSender() { JavaMailSender jms = new return jms; } mailSession is a preconfigured Session instance retrieved from JNDI registry. I do not know about it's configuration. When calling the send() method, it tries to connect using the connect(host, port, username, password) method passing the (unconfigured) host and port which defaults to localhost:0 and then fails. In case the session is supplied, it should call connect() instead.",non_debt,-
camel,7431,comment_0,"StackTrace of the Exception : Could not connect to SMTP host: localhost, port: 0; nested exception is: connect: Address is invalid on local machine, or port is not valid on remote machine Method) Source) at Method) Caused by: connect: Address is invalid on local machine, or port is not valid on remote machine at Method) ... 78 more",non_debt,-
camel,7431,comment_1,"I just checked the code of java mail, we just need to set the default port of to -1, then we can get the same result if the host, username and password parameter are not set.",non_debt,-
camel,7431,comment_2,"Applied the patch into camel master, camel-2.13.x and camel-2.12.x branches.",non_debt,-
camel,7456,summary,Camel PropertiesComponent ignores custom parser in Blueprint,non_debt,-
camel,7456,description,"I have implemented a custom PropertiesParser which allows me to use system property placeholders in propertyPrefix and propertySuffix. In my use case the propertyPrefix is defined as where container.stage is a jvm option defined at container creation. The value is one of dev, test and prod. This works fine in Java DSL world (SCR bundle), but custom parser is ignored in Blueprint. Here is sample of my blueprint xml: The reason it did not work was because by default, it uses blueprint property resolver to bridge PropertiesComponent to blueprint in order to support looking up property placeholders from the Blueprint Property Placeholder Service. Then it always creates a object and set it to The customer Property Parser I created was only set into the object as a delegate Property Parser. Therefore, it was always the method parseUri() from the object got invoked. The same method from your custom parser was ignored. For more detail, please take a look at function. The only workaround is to add the attribute to <camelContext Because once I set it to false, I will no longer be able to lookup from blueprint property service.",non_debt,-
camel,7456,comment_0,"Frankly this has not been the intention to support custom properties parser in OSGi. So what is the use case for having a custom parser? And having both blueprint and your custom what should happen as there are 2 parsers? So if a property is to be parsed should it use - use blueprint first, and parse using custom afterwards - use custom first, and parse using blueprint afterwards - or something else?",non_debt,-
camel,7456,comment_1,"If I have a custom parser, then I'd expect my custom parser to be used first and then blueprint or default parser afterwards. There are a lot cases that people might want to use combination of them. For instance, in this case, I configured ""propertyPrefix"" property to use property placeholder $\{container.stage} here: My custom parser basically just tries to look up system property for the value of the property placeholder ""container.stage"" to decide what environment I am in. Say ""dev"", ""prod"" or ""test"". Depends on the value returned by my custom parser, I can either go to blueprint or just default file property to find relevant value. And I still want to use blueprint in this case. I understand that it was not the intention to support custom parser with blueprint. But I think it is reasonable use case I presented here and more people are going to use blueprint in future so I believe the custom parser with blueprint issue should be addressed in my opinion.",non_debt,-
camel,7456,comment_2,"Custom parser is set as the delegate (see for example which decrypts properties which were first resolved by BlueprintParser) and is called after main parser. If your use case is *only* about making resolvable, then maybe better&simpler would be to do it while configuring than by changing parser-delegate order.",non_debt,-
camel,7456,comment_3,uses provided (configured on parser as delegate (see example with The provided use case is rather about making resolvable against system/env properties and this is how it was resolved.,non_debt,-
camel,7456,comment_4,Reopening to update the docs,non_debt,-
camel,7456,comment_5,Documentation updated,non_debt,-
camel,7461,summary,Idempotent inconsistensies / idempotent consumer should allow for messageId of any type,non_debt,-
camel,7461,description,"See for a background. Basically there is inconsistency between a idempotent consumer and the repository. The repository is capable of holding any type, while the consumer is non-parameterized and uses String as it's message type. It would be very handy to have the messageid as a domain type for any application, and thus the consumer should allow for a parameterized type T. This will also probably mean that should allow for any persistent type. If doing this camel would be generic on the type, and allow for supporting application domain types w/o customizations.",design_debt,non-optimal_design
camel,7461,comment_0,"Its a bit tricky as users would need to configure the type and make sure it matches the implementation of the repo etc. And also potential an API change. As String is a generic type the impl can just support that type too, if it uses integers etc.",non_debt,-
camel,7479,summary,Test fails in non-English environments,non_debt,-
camel,7479,description,"Some Java exception messages are language specific and JUnit tests evaluating theses messages fail in a non-English environment. Such assertions are done in and * The assertion ""Invalid content was found"" fails for the German message ""Ungltiger Content wurde beginnend mit Element ""age"" gefunden"" * The assertion ""The content of element 'person' is not complete"" fails for the German message ""Content des Elements ""person"" ist nicht vollstndig""",non_debt,-
camel,7479,comment_1,"Fix successfully tested by running unit tests with languages en, de, sv, zh and using the following Maven configuration:",non_debt,-
camel,7479,comment_3,Thanks for the PR. It has been pushed to the branches.,non_debt,-
camel,7480,summary,camel:run plugin - Add jolokia agent,non_debt,-
camel,7480,description,This allows to introspect the running camel app easier using jolokia.,non_debt,-
camel,7480,comment_0,"This is a bit tougher as we would need to start a JVM with the javaagent of jolokia So the camel:run plugin would need to start the process itself, and stop it, just as mvn exec:java does.",non_debt,-
camel,7530,summary,Expose the component options for Camel CXFRS,non_debt,-
camel,7530,description,Expose the component options for Camel CXFRS as has already been done for camel-core components and selected others such as jms / ftp / twitter etc.,non_debt,-
camel,7542,summary,Expose component options for Camel JCR,non_debt,-
camel,7542,description,"Expose component options for Camel JCR using @UriEndpoint and @UriParam annotations. This has already been implemented for other components such as camel-core, camel-jpa and others (e.g. see CAMEL-7507).",non_debt,-
camel,7542,comment_0,Fixed with commit,non_debt,-
camel,7559,summary,use bindy as a stand-alone library outside of camel,non_debt,-
camel,7559,description,Bindy is a great library for csv serialization but users can't always use camel. Ideally one could use bindy without camel or with it.,non_debt,-
camel,7559,comment_0,Not sure if there is the bandwith to maintain bindy outside Apache etc.,non_debt,-
camel,7559,comment_1,Would it really be that complex to decouple it from camel in such a way that it is not required to use camel? If not I suppose it could be forked.,non_debt,-
camel,7575,summary,BeanInvokeProcess should copy the attchment from the resultExchange,non_debt,-
camel,7575,description,"As the user may add the attachments in the bean method call, we should support to copy the attachments back to the exchange when call the bean method.",non_debt,-
camel,7575,comment_0,"For BeanExpression, we are not supposed to modify the exchange or the message in the evaluation processor, camel just want to know the result of the evaluation.",non_debt,-
camel,7587,summary,MessageHistory stores passwords in plain text,non_debt,-
camel,7587,description,"The MessageHistory feature currently keeps passwords in plain text in case they are part of the URI. does some sanitizing, but only for the from node - other nodes/processors are currently not sanitized. In order to prevent handling sensitive information in the message history in general, I would suggest to sanitize the URI already when storing a MessageHistory item.",design_debt,non-optimal_design
camel,7587,comment_0,"Applied the patch into master, camel-2.13.x and camel-2.12.x branches.",non_debt,-
camel,7603,summary,camel-cxfrs need to store the security context information into the message header,non_debt,-
camel,7603,description,We need to store the security context information when generate the camel exchange from CXF request message as camel-cxf does.,non_debt,-
camel,7603,comment_0,"Applied the patch into camel master, camel-2.13.x and camel-2.12.x.",non_debt,-
camel,7624,summary,camel-jackson - Add option to set inclusion to skip null fields from pojos,non_debt,-
camel,7624,description,"It takes a bit to configure jackson to skip null fields from pojos. You can annote your pojos but then they have jackson annotations on their classpath. But we need an option on json dataformat in camel, so you can configure that option. Some details at",non_debt,-
camel,7624,comment_0,There is an include option that can be configured now to NOT_NULL,non_debt,-
camel,7644,summary,Scala camel DSL creates numerous DefaultCamelContext instances,non_debt,-
camel,7644,description,"Since the camel DSL is invoked prior to being invoked there is no camel context set on the delegate java RouteBuilder which causes it to create a new context when the first dsl method is invoked. With the implementation of CAMEL-7327 introduced in 2.13.1 which stores created camel contexts in a set in this causes instances of DefaultCamelContext to be leaked, they are never removed from the static set. This is especially aparrent during unit testing. The following test shows that an additional context is registered for the scala route builder as opposed to java. Verification of the leak can be requires profiler and capturing of heap after termination of the test case (in ParentRunner.java).",code_debt,low_quality_code
camel,7644,comment_0,"I just dug the code and found out all the unit tests which extends the CamelTestSupport has the same problem which lets the RouteBuilder creates a default camel context when the first dsl method is used. To resolve this, we need to pass the camel context instance to the RouteBuilder to avoid create the default camel context there. For adding the created camel context into the static set, I think maybe we can do it when the camel context is start the CAMEL-7327.",non_debt,-
camel,7644,comment_1,What is the status of this ticket?,non_debt,-
camel,7644,comment_2,"Okay so the problem is that camel-scala RouteBuilder runs the route configure in its constructor, it should use a configure method just like java does.",non_debt,-
camel,7644,comment_3,"We are adding a ScalaRouteBuilder ppl should use that requires to pass in a CamelContext, then its a fairly easy migration effort.",non_debt,-
camel,7644,comment_4,See this unit test how to use the new ScalaRouteBuilder and with lazy modifier when using ScalaTestSupport,non_debt,-
camel,7681,summary,Add Bulk Index mode to Elasticsearch component,non_debt,-
camel,7681,description,"Hi, It would be nice if the ES component could support the bulk mode to index several documents in one row in a faster and more performant way. I propose a new operation named *BULK_INDEX*. The incoming body would be a *List* of any type that is already accepted (XContentBuilder, Map, byte[], String). Or simply a Collection may be, if a List is too narrow-minded. The result could be a *List of the id* of the successfully indexed documents. Have you any tips on the way you usely treat this kind of operation",non_debt,-
camel,7681,comment_1,"Applied the patch with thanks to Sbastien, I also updated the camel-elasticsearch wiki page for the new added option.",non_debt,-
camel,7681,comment_2,I was going to ask you the best way to update the documentation but you were faster. Thanks !,documentation_debt,low_quality_documentation
camel,7690,summary,Rest DSL - Create blueprint based example,non_debt,-
camel,7690,description,We have a WAR example for Apache Tomcat etc. We should have a osgi example as well.,documentation_debt,outdated_documentation
camel,7690,comment_0,A plain JAR example. As a WAB is not simple to create.,documentation_debt,low_quality_documentation
camel,7693,summary,PropertyPlaceholder fails when string contains three consecutive curly brackets,non_debt,-
camel,7693,description,"I've attempted to update my Camel project from version 2.13.1 to 2.13.2, however my unit tests now fail due to the way in which the PropertiesComponent now treats three curly brackets that are in a JSON string I am building using property values. E.g.: I have this in my route definition :- Note: I can work around the issue if I separate the curly consecutive brackets with spaces. Test case to replicate",non_debt,-
camel,7693,comment_0,"This issue is introduced by CAMEL-7429 which support to concatenation properties. As the patch is based on looking SuffixIndex first to decide if there is property need to be parsed, the consecutive curly brackets introduces the ambiguity. It makes sense that camel throw the exception like this:",non_debt,-
camel,7693,comment_1,It's not a bug from my point of view. You can use the ${properties.xxx} to avoid error just like this:,non_debt,-
camel,7715,summary,SjmsConsumer and SjmsProducer do not remove thread pool when stop,code_debt,multi-thread_correctness
camel,7715,description,"SjmsConsumer and SjmsProducer always register a new ThreadPool on Camel context every time a new instance is created for an endpoint. If consumer or producer is stopped or removed or even component is removed, thread pool still exists.",code_debt,low_quality_code
camel,7715,comment_1,"Applied the patch into master, camel-2.13.x and camel-2.12.x branch with thanks to Cristiano.",non_debt,-
camel,7721,summary,Support to setup the SaxParserFactory from the exchange property,non_debt,-
camel,7721,description,It could be handy if we can setup the SAXParserFactory from the exchange property.,non_debt,-
camel,7721,comment_0,Now you can set the SaxParserFactory by setting exchange property with key XmlConverter can use it if possible.,non_debt,-
camel,7757,summary,camel-restlet 2.13.1 throwing EOFException on reading ZipInputStream,non_debt,-
camel,7757,description,Please refer to :,non_debt,-
camel,7757,comment_0,Patch for the fix.,non_debt,-
camel,7757,comment_1,"Applied the patch into camel master, camel-2.13.x and camel-2.12.x branches with thanks to Sandeep.",non_debt,-
camel,7788,summary,Support for rfc 5424/5425/6587 in syslog component,non_debt,-
camel,7788,description,We got a PR for this,non_debt,-
camel,7805,summary,Camel Olingo2 doesn't set HTTP headers correctly,non_debt,-
camel,7805,description,"Olingo2 component doesn't set HttpHeaders property from configuration on its API proxy, which ignores any custom HTTP headers altogether.",non_debt,-
camel,7805,comment_0,Fix committed to master branch,non_debt,-
camel,7813,summary,Make camel-jms more robust for replyTo,design_debt,non-optimal_design
camel,7813,description,"When a JMS queue is used as a camel consumer for a route it may well be one of possibly many intermediate stops in a chain of processing. If the previous processing step itself used Camel to route the message, then both the JMS replyTo and the camel-header JMSReplyTo will both be populated with the same value. This will cause an infinite loop. Of course, this is in some sense a developer error, but it is a pain to constantly add code to clear the camel JMSReplyTo header if it equals the destination. This should probably be internal to the camel-jms component itself.",design_debt,non-optimal_design
camel,7813,comment_0,"Yeah lets see if we can in the consumer detect if its destination is the same as a JMSReplyTo and then remove that header, as it would cause it to send it back to itself. We could have a option to turn this off, if people are crazy and want to send it back to the same queue as a kinda while loop",non_debt,-
camel,7813,comment_1,"Hello, For some reasons I can see the issue is still in 2.16.1. When Camel sends a reply it keeps JMSRepyTo/ReplyTo on the JMS Message. Have to add though that I'm using the activemq component, version 5.12.1.",non_debt,-
camel,7907,summary,Add support for to jms component,non_debt,-
camel,7907,description,Camel actually does not support,non_debt,-
camel,7907,comment_0,Patch Suggestion,non_debt,-
camel,7907,comment_1,Thanks for the patch.,non_debt,-
camel,7909,summary,camel-netty-http consumer need to close the connection if the response connection header is close,non_debt,-
camel,7909,description,"Current Netty Http Consumer decided if it need to close connection by checking the request connection header. But if the response connection header is 'close', the netty http consumer need to close the connection at the same time.",non_debt,-
camel,7909,comment_0,"Applied the patch into camel master, camel-2.14.x and camel-2.13.x branches.",non_debt,-
camel,7922,summary,MQTT endpoint misses QoS > 0 messages due to startup timing issue,non_debt,-
camel,7922,description,"When the MQTT Endpoint is started the MQTT connection is immediately established, causing an immediate influx of persisted messages (put on the topic when the client was not available). Issue is that at this point, most likely no consumers are available yet to process these messages. *Receiving a PUBLISH message* Publish message are received without any consumers. Result : msg with QoS *No consumers registered yet* Only when this finishes will Camel be able to process the messages. These messages will never be picked up. Perhaps it's more the responsibility of the consumer / producer to start a connection when they get attached to the endpoint ?",non_debt,-
camel,7922,comment_0,"Applied the patch into camel master, camel-2.14.x and camel-2.13.x branches.",non_debt,-
camel,7923,summary,Camel-xmpp unit test is failing from time to time,non_debt,-
camel,7923,comment_0,"Hi Christian I ran the test in my box, the test isn't failed. Can you check your fire wall setting? I checked the [apache I didn't find the failed test of Regards, Willem",non_debt,-
camel,7923,comment_1,"It doesn't fail each time. It's only failing a few times. But by having too many ""fragile"" test, it's annoying to do a build because of the many many attempts you need.",test_debt,flaky_test
camel,7940,summary,Disable SSL security protocol by default,non_debt,-
camel,7940,description,Camel doesn't disable the SSL protocol by default.,non_debt,-
camel,7940,comment_0,"Applied the patch into camel master, camel-2.14.x and camel-2.13.x branches.",non_debt,-
camel,7940,comment_1,Switched to use TLS by default in the netty components as well:,non_debt,-
camel,7954,summary,Camel-box should create https connections using,non_debt,-
camel,7954,description,Currently camel-olingo2 uses SSLContext directly to create connections; it should use,design_debt,non-optimal_design
camel,7954,comment_0,"Component updated, need to update documentation",documentation_debt,outdated_documentation
camel,7956,summary,Camel-salesforce should create https connections using,non_debt,-
camel,7956,description,Currently camel-olingo2 uses SSLContext directly to create connections; it should use,design_debt,non-optimal_design
camel,7956,comment_0,"Component updated, need to update documentation",documentation_debt,outdated_documentation
camel,7970,summary,Container does not see the unregister event,non_debt,-
camel,7970,description,Clients of the Container SPI see contexts being added but not removed. In wildfly we expose every context as an msc service to allow for service dependencies on it. When a context gets unregistered we must also unregister the msc service. should probably be exposed in the same way as,non_debt,-
camel,7970,comment_0,Yeah unfortunately that breaks the API. You can use EventNotifer to register a callback to get notified when the context is stopping/stopped.,non_debt,-
camel,7970,comment_1,We can add a unmanage method to in Camel 3.0.,non_debt,-
camel,7970,comment_2,"Conceptually, integration code needs a way to hook into the CamelContext before start() is getting called",non_debt,-
camel,7970,comment_3,There is a destroy method on camel context tracker now,non_debt,-
camel,8029,summary,Fix licensing issue with camel-xmljson,non_debt,-
camel,8029,description,See,non_debt,-
camel,8029,comment_0,I suggest to add some defails as <description Then you can tell the user how to install that bundle needed.,documentation_debt,low_quality_documentation
camel,8029,comment_1,The fix through this ticket has broken the OSGi test See also my comment,non_debt,-
camel,8029,comment_2,For the time being I did {{@Ignore}} the test on the master branch.,non_debt,-
camel,8036,summary,JettyComponent should not setup the security handler more than once,non_debt,-
camel,8036,description,"As we create multiple consumer for the rest component, it could introduce an issue that camel could add security handler more than once if user setup the security handler on the rest endpoint.",non_debt,-
camel,8036,comment_0,"Applied the patch into camel master, camel-2.14.x and camel-2.13.x branches.",non_debt,-
camel,8051,summary,feature camel-core does not install in karaf 4,non_debt,-
camel,8051,description,feature:repo-add camel 2.15-SNAPSHOT feature:install -v camel-core Karaf then tries to install the karaf shell console from karaf 2.4.0 which of course fails. I first removed the reference to the karaf spring feature repo which transitively imported the karaf standard feature. After that change it could not resolve the package: The problem here was that we forgot status=provisional in the import.,non_debt,-
camel,8068,summary,should set ID of splited attachment,non_debt,-
camel,8068,description,The currently splits the attachments by each ID but do not provide the ID as an additional header. It would be usefull for the processing code to know the ID so they simply get them e.g. by instead of looking in the map for names etc. or iterate over it.,non_debt,-
camel,8068,comment_0,"Good idea, a PR is welcome",non_debt,-
camel,8068,comment_1,Attached a patch that set header with the attachment ID on the copy message.,non_debt,-
camel,8068,comment_2,Thanks for the patch. We do like when there is an unit test with code changes so we can verify this.,test_debt,lack_of_tests
camel,8068,comment_3,"Currently I'm not able to build the test, they complain about missing documentation... Also compilation fails on my pc becuase MailSorter uses Java 7 style and the compile level seems java 1.6 ... So I'm currently try to sort out how to build camel at all from the git checkout :-(",test_debt,lack_of_tests
camel,8071,summary,unable to handle JAR-Files located at WEB-INF/lib,non_debt,-
camel,8071,description,"We are creating a WebApplication which also provides camel endpoints. One of the endpoints triggers a bindy action to read records from a fixed-length-record input file. For this purpose we are using camel bindy. I this conjuction we discovered that there is an issue when trying to locate those classes which are annotated with since the are located in a JAR-File. The project structure looks like this: - project-ear -- project-war  If we locate the classes directly at camel is able to discover the annotated classes. After some debugging we have found out, that is trying to open an InputStream like this: which of of course does not exist. If you remove in those cases ""/my/package"" from the File the resolver works perfectly well since will be called correctly.",non_debt,-
camel,8071,comment_0,"You need a special camel-jboss resolver, see",non_debt,-
camel,8071,comment_1,"Since we are using 2.13.3 we've thought that it camel-jboss is obsolete. ""From Camel 2.8 onwards there is no longer the need for using this camel-jboss component as Camel is now capable of loading type converters without package scanning. This requires though that all 3rd party components with type converter must define the name of the converter classes as FQN in the file. See more details at Type Converter. One exception is if you are using Bindy component as it still needs this JBoss resolver.""",non_debt,-
camel,8071,comment_2,Yes when using bindy its needed. Though there is a wildfly-camel project so maybe in the future JBoss EAP can ship this resolver out of the box?,non_debt,-
camel,8071,comment_3,"On your system is there a file path for this And in that lib directory there is the JAR files (but they are not expanded) ? If so the trick would be to know this from the url above which is just a string. And then add some logic that tries to list the dir for .jar files and then use a jar url loader to scan inside those JARs. All extra work needed to do so, which with some hard-work may be possible to add in camel-core. You can welcome to experiment with this as you got an EAP app server on your system and use bindy.",non_debt,-
camel,8091,summary,does not consider,non_debt,-
camel,8091,description,"The does not consider the context property that is supposed to limit the size of the logged payload. It is possible to set a maxChars on the but that has a different semantics (limits the length of the formatted exchange, not of the message payload) and is complicated to set in some cases (e.g. in the case of the default error handler) The attached extension also honors the context property when formatting the exchange.",design_debt,non-optimal_design
camel,8091,comment_0,Thanks for the PR,non_debt,-
camel,8101,summary,Add runCommand to MongoDB Camel component operations list,non_debt,-
camel,8101,description,Add runCommand to MongoDB Camel component operations list Javadoc of MongoDB driver is there I should update wiki right after the PR is merge.,documentation_debt,outdated_documentation
camel,8101,comment_1,Applied the patch into camel master branch with thanks to Pierre.,non_debt,-
camel,8101,comment_3,We need to update the documentation with the new command,documentation_debt,outdated_documentation
camel,8101,comment_4,Yes Totaly agree. I also want to add a aggregate example. I just sign the comiter agreement yesterday. Will update wiki soon. Best regards,documentation_debt,outdated_documentation
camel,8101,comment_5,Hi Pierre Did you have any chance to update the documentation?,documentation_debt,outdated_documentation
camel,8101,comment_6,Just did it. Thanks to remind me doing it. Could you review it as it is the first time i edit the wiki ?,non_debt,-
camel,8120,summary,always strips delimiters,non_debt,-
camel,8120,description,"The constructor of includes a boolean parameter, which is always set to true by the method (both directly and within which delegates to This parameter needs to be set to false when the end-of-frame delimiter is part of the message (e.g., the closing tag of an XML document), and therefore must not be discarded. Suggested fix: - Add a new constructor to that includes the parameter, which is then passed through to the delegate - Add a new method signature for that includes the parameter - refactor existing method to delegate to the new method",non_debt,-
camel,8120,comment_0,"If my current work load lets up before someone grabs this, I'll submit a patch.",non_debt,-
camel,8120,comment_1,Applied the patch into camel master branch.,non_debt,-
camel,8174,summary,Added to track the long processing exchange,non_debt,-
camel,8174,description,"It could be helpful is we can track the long processing exchange, user can log or send alarm when getting these exchange by extending the",non_debt,-
camel,8174,comment_0,"Could you hold on this, as there is some thoughts for future Camel release with monitoring of long running exchanges, and as well being able to gain insight to current running exchanges, and do a per exchange dump of their message history, so you can view per exchange where it currently is, and its message tracing. So please wait till after new year and lets discuss here or on the mailing list.",non_debt,-
camel,8174,comment_1,"Hi Claus, I know you did some related work recently, this issue is try to do some work without changing the camel-core API which means we can back port the patch to the patch branch. I already committed the a patch for it, it just added a new class and some tests, but I can leave this JIRA open before we discuss it after new year.",non_debt,-
camel,8174,comment_2,"Yeah sure, lets work on a better solution for Camel 2.15 onwards.",non_debt,-
camel,8174,comment_3,"Willem, we have a better solution out of the box with the default inflight repoistory. It has a browse API now, that returns details about how long time the exchange has been inflight. And its also available from JMX so end users can use that to track long processing exchanges. And for Karaf we have a Camel command also. So I would like to remove the as its no longer needed, and also its code is not complete. Any thoughts?",design_debt,non-optimal_design
camel,8174,comment_4,"Hi Claus, It's good to see that we have better API to track the long processing exchange, I will remove the from the repo shortly.",code_debt,dead_code
camel,8174,comment_5,"Okay just a note the file hasnt been removed yet, but you have marked this ticket as resolved",non_debt,-
camel,8210,summary,camel-script - A ruby test keeps failing,non_debt,-
camel,8210,description,The CI server report this failure Which happens locally as well. I guess the test runs for a too short time to process all 200 messages. I get about 70-80 processed messages before it fails. Also the test prints to system out which should use the logger instead.,non_debt,-
camel,8210,comment_0,"It's my fault, I should not force the script builder to use the compiled script which could cause NPE error. I just committed a quick fix for it.",non_debt,-
camel,8218,summary,REST DSL with RestletComponent doesn't support servelet container,non_debt,-
camel,8218,description,"""I'm trying out the REST DSL in 2.14.1 and I'm not able to get it to work with restlet within a servlet container."" Here is the [mail about it.",non_debt,-
camel,8218,comment_0,Applied the patch into camel master and camel-2.14.x branch.,non_debt,-
camel,8226,summary,Deprecated feature dataSourceRef not working correctly,non_debt,-
camel,8226,description,If several sql endpoints are defined using dataSourceRef attribute the latest one will not get dataSourceRef removed in createEndpoint causing validation exception.,non_debt,-
camel,8226,comment_1,"Well spotted, in fact the code is a bit wrong as the endpoint options should overrule the component configured, the correct code would be",non_debt,-
camel,8226,comment_2,Thanks for the PR,non_debt,-
camel,8253,summary,CXFRS Throws for Empty Headers,non_debt,-
camel,8253,description,"Getting an when empty HTTP request headers are passed to a camelrs web method calls (See stack trace below). The culprit appears to be related to the check on line 284 of class More specifically here: Internally the ArrayList class will perform a range check and if the index is greater than or equal to the size of the list the is thrown. So we probably want to add an additional check before this one, something like: == 0 before the check above, since we probably want to account for empty lists. Index: 0, Size: 0 at ~[na:1.7.0_25] at ~[na:1.7.0_25] ~[na:1.7.0_25]",non_debt,-
camel,8253,comment_0,"Applied the patch into camel master,camel-2.14.x and camel-2.13.x branches.",non_debt,-
camel,8254,summary,camel-elasticsearch - Test hangs,non_debt,-
camel,8254,description,For some reason this test hangs on my laptop each time I do a full Camel test. And maven just report some generic error.,non_debt,-
camel,8277,summary,camel-hbase - Provide a row prefix filter for the scan method,non_debt,-
camel,8277,description,It can be useful to perform some scan requests based on a HBase row prefix filter. A filter could be added to the existing set of available filters.,non_debt,-
camel,8277,comment_0,Here the patch that fix bugs :,non_debt,-
camel,8312,summary,XML External Entity (XXE) issue in XPath,non_debt,-
camel,8312,description,If the documentType of an XPath expression is set to a class for that no type converter exists and the data to which the expression is applied is of type WrappedFile or String the XPath will seem to work anyway. However this setup will create issues by using an InputSource created from the String or Generic file.,non_debt,-
camel,8312,comment_0,"One thing I overlooked: the javadoc of the method says: ... For example you can set it to InputSource to use SAX streams. By default Camel uses Document as the type. ... Using InputSource as documentType is a particularily bad idea. If the XPath implementation supports it (Saxon does, the JDK implementation doesn't), SAXSource can be a more memory efficient choice for the docuementType. We should probably also change the Javadoc here...",design_debt,non-optimal_design
camel,8312,comment_1,"There were some issues in camel-saxon tests, so I fixed the test in there, too...",non_debt,-
camel,8312,comment_2,"Hi Claus, are you sure that you want to delay this till 2.15.0? I think this issue is serious. Best regards Stephan",defect_debt,uncorrected_known_defects
camel,8321,summary,camel-box - Do not use dot in header keys,non_debt,-
camel,8321,description,This is standard Camel that header keys never has dots as that dont work well with various components such as jms and whatnot.,non_debt,-
camel,8321,comment_0,Indeed - hit that problem with custom headers driving velocity templates.,non_debt,-
camel,8321,comment_1,"I've attached a sed script that will make the change. I left the PROPERTY_PREFIX alone - if you want to change that too it's easy by hand: String PROPERTY_PREFIX = ""CamelBox.""; String = PROPERTY_PREFIX + String CHUNK_SIZE_PROPERTY = PROPERTY_PREFIX + ""chunkSize""; But the file: should be changed: folderId = + ""folderId"", ""0"", String.class); should now be: folderId = ""0"", String.class); on two occasions. The sed can be invoked by putting it in ~ and running a find from the camel-box folder as: find . -type f -a -exec sed -i -f ~/box.sed ""{}"" \; It builds after this change but will be incompatible with existing users code.",non_debt,-
camel,8321,comment_2,"any thoughts on this? If the prefix header is changed from CamelBox. to CamelBox (without dot). Would the message headers still be able to use as options, as said above? eg before After Notice that the first letter of the option is upper cased? Or we can make the case not matter? I wonder if this requires any change in the core api component or what?",non_debt,-
camel,8321,comment_3,"Why are Box headers being used/fed in other These headers are only supposed to be fed to the Box component, then should be cleaned up n the next step. I wasn't aware of the Camel 'no dots in header' convention. I'll have to take a look at the API framework to see if that's affected. At the very minimum code generation maven plugin is affected since it uses that convention for all generated components, and also would make the new components backward incompatible.",design_debt,non-optimal_design
camel,8321,comment_4,"I'll fix it in the framework and the code generation plugins so that it supports both legacy '.' prefixed headers as well as the Camel case 'no dots' format. New components will use the camel case format in test code. Existing component integration tests could be updated to not use the '.' prefix, but won't have to be updated right away if the legacy style is also supported.",non_debt,-
camel,8321,comment_5,"Good idea Dhiraj. If you have time to do this before Camel 2.15 that would be good, if not then please push this for Camel 2.16.",non_debt,-
camel,8321,comment_6,"Added support for camel case header names in API Component Framework. The framework defaults to '.' separated property names in generated test code and at runtime, but also looks for camel case property names. Also modified Box component to support camel case names for folderId and fileName. Changes pushed to master and 2.14.x branches.",non_debt,-
camel,8328,summary,camel-spring-boot - A fatjar unit test fails,non_debt,-
camel,8328,description,"See Henryk, when you have a little moment, then it would be great if you had a look at that failing test. Maybe its due spring-boot upgrade causing the test to fail.",non_debt,-
camel,8328,comment_0,Fixed in I managed to reproduce this on my machine - let's see if my commit fixes issue on CI.,non_debt,-
camel,8328,comment_1,Yay! Fixed (1). (1),non_debt,-
camel,8328,comment_2,I wonder if there is an osx issue as I get failures this morning with latest code And the test prints to console,non_debt,-
camel,8328,comment_3,I've changed the name of the test package as OSX seems to confuse test package name with class Can you give it a shot now?,code_debt,low_quality_code
camel,8328,comment_4,"Yes works now, thanks.",non_debt,-
camel,8333,summary,Upgrade async-http-client to 1.9.8,non_debt,-
camel,8333,description,None,non_debt,-
camel,8333,comment_0,commit fix for master for camel-2.14.x branch,non_debt,-
camel,8333,comment_1,Freeman do you mind taking a look as camel-ahc-ws component can no longer compile,non_debt,-
camel,8333,comment_2,"Just one question: Why are you using ahc 1.9.0, not 1.9.8?",non_debt,-
camel,8333,comment_3,"Hi Claus, Sure, I'm on it. The commit it is on the way. And sorry for it. Freeman",non_debt,-
camel,8333,comment_4,"Hi Stephan, Good question, as we upgrade, use the latest version should be good. Freeman",non_debt,-
camel,8337,summary,Upgrade httpcomponents client to 4.3.5,non_debt,-
camel,8337,description,"camel-aws, camel-box, camel-cxf, camel-hipchat, camel-http4, camel-restlet, camel-solr would be affected",non_debt,-
camel,8337,comment_0,commit fix for master for camel-2.14.x branch,non_debt,-
camel,8360,summary,doesn't use custom configured DataConverter,non_debt,-
camel,8360,description,": null, null, genericClient);}} should be null, genericClient);}} ?",non_debt,-
camel,8360,comment_0,A patch is welcome,non_debt,-
camel,8360,comment_1,I've submitted a pull request that completes the support for user-supplied DataConverter: This is my first patch to camel/camel-aws. Please let me know if there is anything else I need to do to get the patch accepted?,non_debt,-
camel,8360,comment_2,Thanks Steven for the PR,non_debt,-
camel,8370,summary,missing from documentation,documentation_debt,outdated_documentation
camel,8370,description,"I sent the following to the list earlier today: After digging around in the code, it looks like the HTTP status code is set via the message header line 308). However, there is no mention of this message header in either the camel-netty-http or camel-netty4-http documentation. It would be helpful to add this header to the list of applicable message headers, and also to include an example that demonstrates how to set the response status code and body: Finally, the existing ""Access to Netty types"" example should be modified to be clear that only the request can be accessed in this way.",documentation_debt,low_quality_documentation
camel,8370,comment_0,If you want to help with doc then see here,non_debt,-
camel,8370,comment_1,There is a custom response code sample at,non_debt,-
camel,8374,summary,Create Eclipse Kura component,non_debt,-
camel,8374,description,We should provide Eclipse Kura (1) component to easily start Camel routes from Kura container (M2M message gateway). In the first place I propose to create base activator that could be used as to easily add new OSGi bundles containing Camel routes. (1),non_debt,-
camel,8374,comment_0,Initial commit in revision - .,non_debt,-
camel,8374,comment_1,"Is there more work to this, or can this ticket be closed?",non_debt,-
camel,8374,comment_2,It can be closed. Initial version of the component is ready.,non_debt,-
camel,8397,summary,Support Salesforce Analytics,non_debt,-
camel,8397,description,Salesforce has recently introduced an [Analytics Camel Salesforce component should be updated to support this API.,non_debt,-
camel,8397,comment_0,"Fix merged into master, camel-2.15.x, camel-2.14.x branches",non_debt,-
camel,8440,summary,Components with windows test failures,non_debt,-
camel,8440,description,The following components have/may have unit test failures - jms - apns - aws - beanstalk - barcode -elasticsearch - gae - hdfs - hdfs2 - mina - mina2 - mqtt - quartz2 - restlet - univercity-parser - zookeeper,non_debt,-
camel,8440,comment_0,"There is likely an issue in jetty with stream caching and files with windows, causing the file to not be deleted. Failed tests: 303, 1, Errors: 0, Skipped: 9",non_debt,-
camel,8440,comment_1,There is a separate ticket about the univerciry,non_debt,-
camel,8441,summary,Add required 3rd party dependencies to the BOM,non_debt,-
camel,8441,description,For wildfly-camel we have to redefine a number of camel 3rd party dependencies like this These should probably come from the camel BOM,non_debt,-
camel,8441,comment_1,Applied the patch into camel master branch and fix some build error and warning message.,non_debt,-
camel,8465,summary,Add groups/getPosts endpoint to camel-linkedin,non_debt,-
camel,8465,description,"Add groups/getPosts endpint that covers following resource: Similar functionality as this resource provides people/getPosts mapped to , but it requires role to be set.",non_debt,-
camel,8465,comment_1,"Merged the patch into master, camel-2.15.x and camel-2.14.x branches with thanks to Tomas.",non_debt,-
camel,8495,summary,camel-ganglia: support case-insensitive configuration values,non_debt,-
camel,8495,description,The arguments for many of the camel-ganglia configuration options are case sensitive (must match the case used in the enum names). This should be relaxed somehow to support case-insensitive configuration strings.,non_debt,-
camel,8495,comment_0,"In Camel 2.15 - Type Converter to enum's is now case insensitive, so you can convert safely level=info to an enum with name Level.INFO etc.",non_debt,-
camel,8495,comment_1,"OK, as the camel-ganglia component was only released for 2.15.0 then I've changed the state of this bug to ""Not a Problem""",non_debt,-
camel,8496,summary,Create Camel Paho component,non_debt,-
camel,8496,description,"We already have Camel MQTT (1) component based on the Fuse MQTT client. However we should provide integration with Eclipse Paho as well, because it is the most popular MQTT library and many users would prefer to rely on it. (1)",non_debt,-
camel,8496,comment_0,"As the paho jar is in a 3rd party repo it may need to be addded into the features.xml as a direct url for it to download, as karaf does not come out of the box with that repo in its config.",non_debt,-
camel,8496,comment_1,I will do it.,non_debt,-
camel,8496,comment_2,Can you configure the brokerUrl on the component so you dont have to configure it on the endpoint? eg just like the jms component etc.,non_debt,-
camel,8496,comment_3,Not yet. Good point - I'll move more options to the component level.,non_debt,-
camel,8496,comment_4,Added some component level options,non_debt,-
camel,8497,summary,Add extra capabilities to the github component,non_debt,-
camel,8497,description,"To extend the use cases that the github component can be used for, we should add the following capabilities: - able to set the 'state' on a commit associated with a PR - this enables the ""All is well"" green tick to be added to the PR - retrieve list of files (summary details) associated with PR - get a specific file content",non_debt,-
camel,8497,comment_1,Github PR available:,non_debt,-
camel,8497,comment_2,Merged the patch into camel master and camel-2.15.x branches.,non_debt,-
camel,8629,summary,Update AWS SDK from version 1.9.17 to version 1.9.30,non_debt,-
camel,8629,description,"After the latest update from old version of SDK to 1.9.17, I've opened a ticket into Servicemix JIRA to ask for a bundle release of latest AWS SDK version, 1.9.30. The ticket has been resolved. We need to update the dependency from 1.9.17 to 1.9.30.",non_debt,-
camel,8629,comment_1,PR submitted: Andrea,non_debt,-
camel,8629,comment_2,"The OSGi versions is ""strange"" so they actually use underscore, so it should be",non_debt,-
camel,8629,comment_3,The last OSGi release is .22,non_debt,-
camel,8629,comment_4,Logged a ticket about aws bundle .30 needed,non_debt,-
camel,8629,comment_5,We will do the osgi upgrade later when new bundles is released,non_debt,-
camel,8629,comment_6,Sorry also for this.,non_debt,-
camel,8629,comment_7,In the last days I've opened a ticket for this on ServiceMix JIRA: and they resolved it yesterday.,non_debt,-
camel,8633,summary,Servlet & Multipart,non_debt,-
camel,8633,description,The servlet component cannot handle attachments from a multipart request. In our situation a REST multipart call is made and the attachments must be handled. I created a extension on the DefaultHttpBinding and added functionality for multipart parsing. May be this functionality can be added to apache camel.,non_debt,-
camel,8633,comment_0,There is a new option on the serlvet component you must enable:,non_debt,-
camel,8651,summary,Update Ehcache from version 2.9.1 to 2.10.0,non_debt,-
camel,8651,description,We need to update Ehcache from version 2.9.1 to version 2.10.0,non_debt,-
camel,8651,comment_1,PR submitted: Andrea,non_debt,-
camel,8688,summary,Removed StreamCache when doing a Wiretap,non_debt,-
camel,8688,description,"Hi, I noticed a bug where the body (StreamCache) was already removed before the exchange reached the end (in the Wiretap route). I added a test as an attachment. f you run the test you can clearly see the temp file deletion followed by the closed stream exception: Tried 1 to delete file: with result: true Cannot reset stream from file I encountered the same issue during a more complex route that does some splitting (zip file) and multicasting. This occurred on Camel 2.14.1 so it could be fixed by but I need to test this. Kind regards, Geert",non_debt,-
camel,8688,comment_0,Solution proposal by :,non_debt,-
camel,8688,comment_1,"Hi, I am working on a patch. Regards Franz",non_debt,-
camel,8688,comment_2,"Hi, Here is the patch. It consists of two files. One for camel-core and one for camel-netty-http. I had to change the clone() method in the interface StreamCache to StreamCache clone(Exchange exchange) because I needed to count the exchanges which are relevant for the cached file. Regards Franz Forsthofer SAP SE e-mail:",non_debt,-
camel,8688,comment_3,Thanks for the patch. There we are few other components where the copy needed to be adjusted. But its all sorted now.,non_debt,-
camel,8709,summary,Camel-Infinispan: Add Async operations to available set,non_debt,-
camel,8709,description,We have to add the async operation available on Infinispan. Methods like: - PutAsync - PutAllAsync - PutIfAbsentAsync - RemoveAsync - ReplaceAsync,non_debt,-
camel,8709,comment_0,Related commit:,non_debt,-
camel,8734,summary,camel-netty-http - Should ignore case of context-path,non_debt,-
camel,8734,description,"We should ignore case when matching context-path. We have some code that matches using .startsWith. But that does not match with different case, such as Customer vs customer. People dont really want to expose HTTP services where the context path uses different case. So if people type /customer or /Customer or /CUSTOMER then it should match the same. See nabble",non_debt,-
camel,8734,comment_0,"Hi, Please consider that URLs should be case sensitive according to W3. [quote]URLs in general are case-sensitive (with the exception of machine names). There may be URLs, or parts of URLs, where case doesn't matter, but identifying these may not be easy. Users should always consider that URLs are The header CamelHttpPath should provide me what the user actually entered.",code_debt,low_quality_code
camel,8734,comment_1,"I totaly agree with Hans comment. From my point of view this is a regression. For example for rest services, the path must be case sensitive. Otherwise this is a lost of information. This will break many rest web services (starting by my application).",non_debt,-
camel,8734,comment_2,The header is as the user typed. The check is only on the matching of whether the consumer should trigger or not which is case insensitive. If users would like case sensitive then log a new ticket. And use the vote system to see what the demand is.,non_debt,-
camel,8734,comment_3,"The problem is that the HTTP_PATH header is modified, and thus the case insensitiveness is not only for context match. See in netty4, the path is converted to lower case and put back to header, overwritting the original HTTP_PATH. : if != null) { // need to match by lower case as we want to ignore case on context-path path = .... } path);",non_debt,-
camel,8734,comment_4,Thanks the header is no longer lower cased but kept as is. Fixed on both master and 2.15.x branch.,non_debt,-
camel,8738,summary,Referring to constants using type,non_debt,-
camel,8738,description,"Hi, In documentation link following is written Camel 2.11: To refer to a type or field by its FQN name. To refer to a field you can append .FIELD_NAME. For example you can refer to the constant field from Exchange as: When I tried it it works. But for example I have class Constants which contain inner class InnerConstants which have a field. I cannot access it from following way. Exception Found: Cannot find class Following is the structure of my Constant class class Constants { public static String test =""Test1234""; public static class InnerConstants { public static String fieldName = ""fieldName"" } } My route was: <route <camel:from uri=""stream:in"" / <camel:setBody <log loggingLevel=""INFO"" message=""${body}"" /</route Let me know if you need any other clarification.",non_debt,-
camel,8738,comment_0,Your class should be a public class. eg class Constants { should be public class Constants {,non_debt,-
camel,8738,comment_1,"Ok my bad, missed the public identifier while typing. The class is public. I can access which is the field and not in inner class.",non_debt,-
camel,8738,comment_2,Its not for nested inner classes.,non_debt,-
camel,8738,comment_3,"I used inner classes for of my code. My Constant can contain inner class of DbConstants, date Formats etc. Otherwise I have make separate classes of all my constants. How could I achieve this from within the framework ? public class Constants { public static String system =""dev""; public static class DbConstants{ public static String fieldName = ""fieldName"" } public static class DataFormat{ public static String format= ""dd/mm/yyyy"" public static String format1 = ""dd/mm/yyyy HH"" } }",non_debt,-
camel,8738,comment_4,Not resolved.,non_debt,-
camel,8738,comment_5,You need to refer to the class using $ which is how the java classloader expects this.,non_debt,-
camel,8780,summary,Camel exec component have trouble to load arguments list from message header,non_debt,-
camel,8780,description,When trying to set the following header: We receive the following error:,non_debt,-
camel,8780,comment_0,"It turns out that DefaultExecBinding try to turn the String into a List, but other type converter just introduces an exception when doing it.",non_debt,-
camel,8780,comment_1,"Applied the patch into camel master, camel-2.14.x and camel-2.15.x branches.",non_debt,-
camel,8819,summary,Jetty Maven Plugin - 8.1.7 does not exist,non_debt,-
camel,8819,description,"The new jetty 8.1.7 release do not include the maven plugin. So we need to use the older release for that ;( 8.1.16.v20140903 [WARNING] The POM for is missing, no dependency information available [WARNING] Failed to retrieve plugin descriptor for Plugin or one of its dependencies could not be resolved: Failure to find in was cached in the local repository, resolution will not be reattempted until the update interval of central has elapsed or updates are forced",non_debt,-
camel,8844,summary,Treat absolute FTP directories as invalid and convert to relative,non_debt,-
camel,8844,description,"Currently there are issues when directory in (s)ftp(s) endpoint is absolute (or contains arbitrary number of leading slashes). Sometimes the path is normalized, sometimes don't and camel-ftp code can't find correct prefix in _absolute_ path when retrieving remote file. [Documentation I suggest normalizing the behavior and treating absolute FTP endpoint URI directories as relative. When URI contains absolute path, WARNing will be printed:",design_debt,non-optimal_design
camel,8844,comment_0,Resolved in version 2.16.,non_debt,-
camel,8868,summary,Camel-Hazelcast: Add remainingCapacity operation to queue,non_debt,-
camel,8868,description,When using Hazelcast queue it can be useful to support the remaining capacity method inside the queue producer.,non_debt,-
camel,8868,comment_0,Related commits:,non_debt,-
camel,8879,summary,Camel-sjms doesn't treat InOnly and InOut equally,non_debt,-
camel,8879,description,"The Camel-sjms component doesn't treat messages being sent to the InOnlyProducer equally to the ones sent to the InOutProducer. As far as i can tell, the InOnlyProducer handles messages the correct way, by splitting up the When used with WebSphere MQ, the InOutProducer causes an exception: JMSCC0083: An incorrect object of type was provided.",non_debt,-
camel,8879,comment_0,"I wrote a patch a few months ago for this issue, and we've been testing it internally. It seems to work.",non_debt,-
camel,8879,comment_1,Would you be able to attach an unit test also?,test_debt,lack_of_tests
camel,8879,comment_2,We are in fact deprecating this batch message. And have marked in the javadoc that its only available for in only.,non_debt,-
camel,8881,summary,Error when generating DTO objects,non_debt,-
camel,8881,description,"I was wondering if the current Camel-salesforce component is able to deal with the current Salesforce developer edition summer 15 (Api version 34) because I'm getting the below error when trying to generate the DTO objects... it seems that some object's field cause problems. --Stack -- [INFO] Salesforce login... [INFO] Login user at Salesforce url: [INFO] Login successful [INFO] Salesforce login successful [INFO] Getting Salesforce Objects... [WARNING] Generating Java classes for all 275 Objects, this may take a while... [INFO] Retrieving Object descriptions... [INFO] Logout successful [INFO] [INFO] BUILD FAILURE [INFO] [INFO] Total time: 5.888 s [INFO] Finished at: [INFO] Final Memory: 18M/183M [INFO] [ERROR] Failed to execute goal (default-cli) on project test-camel: Error getting SObject description for Unrecognized field ""encrypted"" (Class not marked as ignorable [ERROR] at [Source: line: 1, column: 593] (through reference chain: Failed to execute goal (default-cli) on project test-camel: Error getting SObject description for Unrecognized field ""encrypted"" (Class not marked as ignorable at [Source: line: 1, column: 593] (through reference chain: Method) Caused by: Error getting SObject description for Unrecognized field ""encrypted"" (Class not marked as ignorable at [Source: line: 1, column: 593] (through reference chain: ... 19 more Caused by: Unrecognized field ""encrypted"" (Class not marked as ignorable at [Source: line: 1, column: 593] (through reference chain: ... 21 more [ERROR] [ERROR] Re-run Maven using the -X switch to enable full debug logging. [ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles: [ERROR] [Help 1]",non_debt,-
camel,8900,summary,Javadoc parser in API Component Framework misses first method with void return type in JDK7,non_debt,-
camel,8900,description,"When using JDK 7 to generate Javadoc, the Javadoc parser based on fails to extract the first method if it has a void return type. For some reason the combination of the Javadoc generated by JDK7 and its Parser class gets the elements in the wrong order, causing the parser to go into the wrong state missing the first method. The next method element puts the parser back in the right state. The fix is to not let the parser come out of the METHOD_SUMMARY if it has not seen any methods at that point.",non_debt,-
camel,8900,comment_0,"Fix merged into master, camel-2.15.x and camel-2.14.x branches.",non_debt,-
camel,8933,summary,mail consumer (imap) polls continuously (not using the default polling interval),non_debt,-
camel,8933,description,The mail consumer (at least IMAP) would poll email continuously unless the consumer.delay is explicitly set. Below is the IMAP debug log. It should stop at A6 instead of continuing indefinitely. This does not happen with 2.14.x.,non_debt,-
camel,8933,comment_0,Thanks for reporting.,non_debt,-
camel,8940,summary,Message routing to a method in the bean is not working if generics is used.,non_debt,-
camel,8940,description,"Message routing is not working if generic method inteface is used. I have added the code snippet here. Bean component: public interface IHandler<T{ @Handler public void handle(T data); } public class implements IHandler<Employee{ public { super(); } @Override public void handle(Employee data) { } } Routing config: public RouteBuilder route() { return new RouteBuilder() { @Override public void configure() throws Exception { builder is called""); ThreadPoolBuilder poolBuilder = new ExecutorService customPool = } }; }",non_debt,-
camel,8940,comment_0,You should likely remove the @Handler annotation if that is the one from Camel.,non_debt,-
camel,8940,comment_1,"Even if I removed the @Handler anotation, it is not working",non_debt,-
camel,8940,comment_2,"Be more specific what is not working, what happens?",non_debt,-
camel,8940,comment_3,"I am expecting when employee object is sent to seda:test, it should route to handler method. I am getting below exception. 2015-07-07 20:02:28.372 ERROR 7175  [#4 - customPool] : Failed delivery for (MessageId: on ExchangeId: Exhausted after delivery attempt: 1 caught: No method invocation could be created, no matching method could be found on: Message History RouteId ProcessorId Processor Elapsed (ms) [route1 ] [route1 ] [vm://seda:test ] [ 149] [route1 ] [threads1 ] [threads ] [ 96] [route1 ] [bean1 ] ] [ 2] Exchange Exchange[ Id ExchangePattern InOnly Headers BodyType Body ] Stacktrace No method invocation could be created, no matching method could be found on: Note: If, I am not used generics, it is working perfectly.",non_debt,-
camel,8940,comment_4,"The issue found in 2.15.0. The same issue has been fixed in 2.15.1 I beleive . I have upgraded to 2.15.2 and the issue is not found. Please close this issue. Root Cause in 2.15.0 : BeanInfo.java for (Method source : methods) { for (Method target : methods) { // skip ourselves if target, true)) { continue; } // skip duplicates which may be assign compatible (favor keep first added method when duplicate) if target, false)) { } } } overrides.clear(); } Fix in 2.15.1 ============ for (Method source : methods) { // skip bridge methods in duplicate checks (as the bridge method is inserted by the compiler due to type erasure) if (source.isBridge()) { <==== fix this problem continue; } for (Method target : methods) { // skip ourselves if target, true)) { continue; } // skip duplicates which may be assign compatible (favor keep first added method when duplicate) if target, false)) { } } } overrides.clear(); }",non_debt,-
camel,8940,comment_5,Problem is fixed in 2.15.2,non_debt,-
camel,8952,summary,Expression or predicates - allow to refer to external script on file/classpath,non_debt,-
camel,8952,description,We should allow users to just type a file or classpath name as the value and Camel will load the resource from that. For example You may want to use a more powerful language like groovy.,non_debt,-
camel,8993,summary,camel-swagger component doesn't work properly in karaf,non_debt,-
camel,8993,description,"I'm using camel 2.15.2 with its REST DSL and camel-swagger in an OSGi environment (I tried both karaf 2.4.3 and karaf 4). I'm using the servlet component by exposing it as OSGi services : * for my camel route * for the swagger documentation. Then I use the swagger ui to see the documentation. I can see my rest endpoint and the documentation I put in ""description"" but it's not able to show me the model for the request and the response (defined in ""type"" and ""outType""). However if I use the same routes and the same servlet in a spring-boot environment, it works like a charm. So it seems to be a classpath problem. The whole discussion is available here : The project I use to reproduce the problem is here : (normally all needed information is in the README.md)",non_debt,-
camel,8993,comment_0,Doubt this is easy to work. First we are working on a camel-swagger migrated to just java (without scala). For osgi you may need to add dynamic imports and other osgi classloader stuff to make it swagger able to read your model classes.,non_debt,-
camel,8993,comment_1,"Agree, this isn't easy to find the root cause here. If I have time and if I find a workaround, I'll keep you inform in this ticket.",non_debt,-
camel,8993,comment_2,There is a new camel-swagger-java module. The old scala based is deprecated,non_debt,-
camel,9017,summary,Camel-Hazelcast: should check useRecovery before using persistedCache,non_debt,-
camel,9017,description,See Nabble:,non_debt,-
camel,9029,summary,JGroups managed routes can be started too early,non_debt,-
camel,9029,description,"doesn't filter non-view messages properly which can cause the wrong route to be started. This issue was not so easy to detect, as usually the channels used for cluster management doesn't send any other messages, than view ones.",non_debt,-
camel,9029,comment_0,Fixed and backported to 2.15.3.,non_debt,-
camel,9031,summary,Dependency missing in camel-kafka feature,non_debt,-
camel,9031,description,The following error occurs when deploying the camel-kafka feature,non_debt,-
camel,9039,summary,"Feature camel-core contains bundles, which should be made optional for micro-services deployment",non_debt,-
camel,9039,description,The camel-core feature loads the following bundles that are only useful in a console based karaf container: Loading these bundles should be made optional so that the camel-core feature (and all the component features that depend on it) can be easily used in a micro-services deployment of Karaf4.,non_debt,-
camel,9039,comment_0,"Yeah but we should make sure they are loaded by default when people do a 'camel' feature install. You may move these to some new 'camel-core' feature or something that just has the very basics, i guess even without camel-blueprint or camel-spring either.",non_debt,-
camel,9039,comment_1,"So Karaf supports loading bundles and config on the condition that some other feature is already installed. It is used in Karaf to conditionally load its own features, when the {{shell}} feature is loaded, the same thing we want to do for camel-core. I have committed the change to add the same conditional check around bundles and left {{camel-catalog}} as it is for tooling support. So if camel-core is loaded in a karaf container with {{shell}} feature, the command bundles will be loaded, otherwise they won't. This way existing karaf containers won't be affected, and we don't have to worry about all the component features that load {{camel-core}} feature.",non_debt,-
camel,9039,comment_2,Wrapped in a conditional for karaf {{shell}} feature. Commit pushed to master and camel-2.15.x branches.,non_debt,-
camel,9092,summary,MQTT consumer receives duplicate messages after broker restart,non_debt,-
camel,9092,description,"if clientId is specified, after ActiveMQ broker restart, camel-mqtt consumer starts to receive duplicate messages. Please see the testcase attached.",non_debt,-
camel,9092,comment_0,Maybe its a ActiveMQ problem?,non_debt,-
camel,9092,comment_1,"Possibly, but so far it can't be reproduced without camel. I tried a plain ActiveMQ testcase with almost same broker settings on ActiveMQ 5.12.0 : but it didn't hit any duplicates. I'm still looking for the root cause, but I'm kind of stuck :(",non_debt,-
camel,9092,comment_2,"Tweaked to use CallbackConnection for sender as well - still it succeeds, so no duplicate. Note that this testcase depends on mqtt-client connection recovery feature unlike camel-mqtt is doing it by itself.",non_debt,-
camel,9092,comment_4,"I haven't yet made completely clear why that duplicates happen though, the pull request I submitted did fix the issue. As is also invoked when connection recovery happens in mqtt-client internally, -invoking disconnect() due to this event triggers unnecessary connection re-initialization- not really, just doing connected = false without disconnect(), so it's even worse as the old connection is remained open. I also added setting tracer to MQTT when trace is enabled.",non_debt,-
camel,9092,comment_6,"The {{connected = true}} is OK in {{onConnected()}}. Where it needed to be removed was from because the MQTT client handles its own reconnection. Setting {{connected = false}} was tricking the endpoint into creating yet another connection while the original one was trying to reconnect. I think this led to more than 1 consumer showing up on the topic and hence the appearance of duplicates. Thanks for the report and the patch, . I merged it with the aforementioned change as well as making the {{uri}} param final in the constructor of MQTTEndpoint.",non_debt,-
camel,9114,summary,Upgrade Jgroups to version 3.6.7.Final,non_debt,-
camel,9114,description,"Jgroups 3.6.5.Final is out. We can upgrade. I'm trying to test the new bundle and to install the camel-jgroups feature in karaf with camel-itest-karaf test. The behaviour is weird: Looking at the manifest.mf of Jgroups 3.6.4.Final I see this: while in 3.6.5.Final we have and this is normal. I would have expected no problem with the camel-itest-karaf test, but it doesn't work. Also if you take a look at the config.properties of camel-itest-karaf, at the following path you'll see the following row",non_debt,-
camel,9114,comment_0,It should be fixed in the next release: 3.6.7.Final,non_debt,-
camel,9133,summary,fails intermittently,non_debt,-
camel,9133,description,"The fails intermettently, maybe once every 5 times for me, with the error below: T E S T S Running 1, 1, Errors: 0, Skipped: 0, 4.954 sec <<< FAILURE! - in 4.867 sec <<< FAILURE! Assertion error at index 0 on mock mock://result with predicate: Results : Failed tests: 1, 1, Errors: 0, Skipped: 0",non_debt,-
camel,9151,summary,Wrong statistics for subroutes,non_debt,-
camel,9151,description,Some statistics seem to be wrong for subroutes. method is called several times for a same exchange: first by the subroute and then by its parent route. The problem is that context.stop() is called several times so counters is greater than 1 for the subroute instead of one. A solution could be to reset the context into the exchange as soon as the stop method has been called:,non_debt,-
camel,9151,comment_0,Thanks for reporting. the timers should be per route.,non_debt,-
camel,9156,summary,camel-swagger-java - Add JMX and Camel commands,non_debt,-
camel,9156,description,So the api can be reachable from JMX and also from the Camel commands in Karaf etc.,non_debt,-
camel,9156,comment_0,There is a JMX operation now on the RestRegistry mbean.,non_debt,-
camel,9164,summary,errorHandlerRef causes NoSuchBeanException on uninstall,non_debt,-
camel,9164,description,Specifying an errorHandlerRef on a route causes following WARN on uninstall,non_debt,-
camel,9164,comment_0,Here is a reproducer -,non_debt,-
camel,9165,summary,Access to swagger api-docs gives 204,non_debt,-
camel,9165,description,This works on in 2.15.2 and the latest 2.16.x CrossRef:,non_debt,-
camel,9165,comment_0,"What version does work and not work, its a bit confusing what you tell. And are you using the camel-swagger scala based module? Then api-docs is setup using a servlet and you have to ensure you do that correctly. From Camel 2.16 onwards use the new camel-swagger-java module, the scala based is deprecated.",non_debt,-
camel,9165,comment_1,Migrating from 2.15.2 (which we use now) to 2.15.4 would break this functionality. Migrating from 2.15.2 to 2.16.0 would work fine. The issue only shows on the camel 2.15.x branch,non_debt,-
camel,9165,comment_2,"Can you research on your side what happens, check logs and whatnot.",non_debt,-
camel,9165,comment_3,the scala module is deprecated - use the java module in 2.16 onwards.,non_debt,-
camel,9169,summary,Introduce RestDsl,non_debt,-
camel,9169,description,Will help REST DSL jobs refer to individual multipart parts,non_debt,-
camel,9169,comment_0,Can you provide an example? The rest param type is for documenting the parameter?,non_debt,-
camel,9181,summary,"Simpler, less picky ScrHelper",design_debt,non-optimal_design
camel,9181,description,"Previous ScrHelper could break when there are certain XML libraries in the classpath (e.g. XOM or Saxon). Also, it doesn't work with component description files generated by the latest version 1.21.0 (format has changed). This is a simpler, less picky implementation using StAX.",design_debt,non-optimal_design
camel,9181,comment_4,Thanks for the PR,non_debt,-
camel,9200,summary,Context component conflates endpoints with the same local id from different CamelContexts,non_debt,-
camel,9200,description,"The context component experiences clashes between endpoints with the same local name, but from different contexts. If two contexts both contain a local endpoint ""direct:out"", any route involving both endpoints will fail to start due to a ""Multiple consumers for the same endpoint is not allowed: As an example, the following set of routes will fail due to this issue: See thread:",non_debt,-
camel,9200,comment_0,Patch against master See also:,non_debt,-
camel,9200,comment_1,The camel-context component is depreacted,non_debt,-
camel,9200,comment_2,Thanks for the patch. Sorry for the delay of merging.,non_debt,-
camel,9201,summary,Improved Camel CDI component,non_debt,-
camel,9201,description,"Since version 2.10.0 of Camel, the Camel CDI component hasn't been actively maintained and suffers from few design flaws that impede improvements and new features. Over the last couple of months, work has been done to refactor and improve the existing Camel CDI component. However, given that a redesign was required to make it more _CDI spirit_, the heavy work has been done in a separate project and the rational for an overhaul and the contribution list documented here: As people started using and contributing to it, it'd be better to have that improved version contributed back to the project as soon as possible to avoid diverging to much. That new version of the Camel CDI component provides the following: h4. New features * CDI events Camel endpoint * Camel events to CDI events bridge * Type converter beans * OSGi integration h4. Improvements * Better Camel context customisation and lifecycle * Better multiple Camel contexts support * {{@Uri}} and {{@Mock}} Endpoint Qualifiers Unification * Dependency on DeltaSpike has been removed (only remaining for Main support) h4. Compatibility * CDI 1.0, 1.1, 1.2 * Java SE: Weld 1.x, Weld 2.x, OpenWebBeans 1.2.x, 1.6.x * Java EE: WildFly 8.x, 9.x, WildFly Camel * OSGi: Karaf 4 h4. Test coverage * 100+ test cases * 90% test coverage * Profiles for (Weld, OpenWebBeans) x (CDI 1.0, CDI 1.2) h4. Non-backward compatibility h5. Compile time * The {{@ContextName}} qualifier does not have a default empty value anymore as it is irrelevant * The class has been removed, the standard can be used instead h5. Run time * DeltaSpike is not used anymore for the sourcing of the configuration properties. This new version is agnostic to any configuration sourcing mechanism and delegates that concern to the application so that it can declare a custom bean whose sourcing is tailored to its need. DeltaSpike can still be easily used by the application by declaring a bean configured with a / relying on DeltaSpike. See for an example. More details can be found in",non_debt,-
camel,9201,comment_1,Thanks for all this hard work. The code has been pushed to master branch. I noticed the do not shutdown graceful if pressing ctrl + c. I wonder if the hangup interceptor is not enabled? You can reproduce by running the example according to its readme file.,non_debt,-
camel,9201,comment_2,"I did just that after rebase and before pushing, that was working fine to me. I'm giving it a second try right away.",non_debt,-
camel,9201,comment_3,that works for me. Is there anything I could be missing?,non_debt,-
camel,9201,comment_4,Let me rebuild all the code and try again.,non_debt,-
camel,9201,comment_5,"Yes, I rebuilt {{camel-core}} to have it working.",non_debt,-
camel,9201,comment_6,Yep it shutdown graceful now.,non_debt,-
camel,9205,summary,REST endpoint with CORS sends invalid header value for,non_debt,-
camel,9205,description,"I tried enabling CORS for our camel REST endpoint and it does not really work. The header is sent, as it should be. Unfortunately the value of the header is ""*, *"" which is not accepted as correct by newer browser versions. So please change the default behavior to send only ""*"". I tried setting the header value manually: <restConfiguration component=""servlet"" bindingMode=""json"" port=""8080"" enableCORS=""true"" <corsHeaders But this results in a header value of ""*, localhost"" which is also not accepted. A workaround is to set <corsHeaders value=""""/",non_debt,-
camel,9205,comment_0,"Try with a newer release as the default is set to a * only. So not sure why its *, * in your example.",non_debt,-
camel,9205,comment_1,Are you use camel-swagger by any chance? It had some code that was using addHeader which could maybe cause the * to be added 2 times.,non_debt,-
camel,9205,comment_2,using setHeader all the places now,non_debt,-
camel,9226,summary,Metrics are double-counted for last route in multicast,non_debt,-
camel,9226,description,"When using the to collect metrics from routes, all metrics for the last route in a multicast record double the expected value.",non_debt,-
camel,9226,comment_0,"Attempted to recreate the problem in a unit test, but it passes:",non_debt,-
camel,9226,comment_1,I had messed up my dependencies and picked up version 2.15.3 of camel-metrics. I can confirm this was fixed in 2.16.0,non_debt,-
camel,9226,comment_2,Ah good to hear. You are welcome to submit your unit test as a PR / .patch file then we can add it to camel-metrics so we have it there too.,test_debt,lack_of_tests
camel,9226,comment_3,"Will do. Just checking it fails for 2.15.x, otherwise it probably doesn't add much!",non_debt,-
camel,9226,comment_4,New unit test fails for 2.15.3. Created pull request,non_debt,-
camel,9226,comment_7,Thanks for the test,non_debt,-
camel,9236,summary,Tokenize with regex grouping has a small bug,non_debt,-
camel,9236,description,"Looks like a copy/paste error is preventing the grouping functionality from being executed by the helper object. group should be passed to the delegate's tokenize method. /** * Evaluates a token expression on the message body * * @param token the token * @param regex whether the token is a regular expression or not * @param group to group by the given number * @return the builder to continue processing the DSL */ public T tokenize(String token, boolean regex, int group) { return regex); }",non_debt,-
camel,9236,comment_1,Thanks for the PR,non_debt,-
camel,9244,summary,camel-paho - Endpoint must implement publishing of retained messages option,non_debt,-
camel,9244,description,"Currently PahoEndpoint allows to set the QoS option. But the boolean retained is a basic MQTT protocol feature that should be implemented. Like in the Paho publish method: #publish(String topic, byte[] payload, int qos, boolean retained) Additionally it would be nice to have the MqttConnectOptions of setWill as also ""Last Will and Testament"" (LWT) is a basic protocol feature.",non_debt,-
camel,9244,comment_0,Would you like to handle this or should I do it?,non_debt,-
camel,9244,comment_2,The commit is almost ready,non_debt,-
camel,9248,summary,Exception is thrown when receiving a message where JMSDestination is null,non_debt,-
camel,9248,description,The change linked below causes camel-jms throws an exception when a message is received where JMSDestination is null (when used with WebSphere MQ). This works as expected in 2.15.3. The message looks like this (anonymized) JMSMessage class: jms_text JMSType: null JMSDeliveryMode: 2 JMSExpiration: 0 JMSPriority: 5 JMSMessageID: JMSTimestamp: 1445609217800 JMSCorrelationID: null JMSDestination: null JMSReplyTo: JMSRedelivered: false JMSXAppID: JMSXDeliveryCount: 1 JMSXUserID: MQXPLO IBM277 JMS_IBM_Encoding: 785 JMS_IBM_Format: MQSTR JMS_IBM_MsgType: 8 11 JMS_IBM_PutDate: 20151023 JMS_IBM_PutTime: 14065780 The following is a link to the troublesome patch included in 2.16.0:,non_debt,-
camel,9248,comment_0,Thanks for reporting,non_debt,-
camel,9258,summary,"camel-boon - Lists, Maps with camel-boon",non_debt,-
camel,9258,description,"Any thoughts on how to add support for list, maps of pojos. I did a simple patch that mimicked the behaviour of camel-jackson and added a useList method on BoonDataFormat to tell the component that a List was to be expected. ps. camel-boon is missing as a choosable component when creating issues. .ds",non_debt,-
camel,9258,comment_0,Sounds good a patch is welcome.,non_debt,-
camel,9258,comment_1,"Data formats is a bit special when you add a new option, you need to add the option in camel-core too, there is a xxxDataFormatModel class in there where you need to add it.",non_debt,-
camel,9258,comment_2,Thank you. I'm going to add it :-),non_debt,-
camel,9258,comment_3,Now it should be ok. Thanks for the hint! Good for the next time or for future dataformat :-),non_debt,-
camel,9312,summary,Rest DSL should support defaulting values for query strings,non_debt,-
camel,9312,description,Popular restful implementations such as plain JAX-RS or Spring MVC support defaulting values for query strings. Camel rest DSL should support this same behavior in order to make easy it to use.,non_debt,-
camel,9312,comment_0,Can you explain more what you mean?,non_debt,-
camel,9312,comment_1,If we take the following JAX-RS endpoint: We are able to default the query parameter in case the client side does not send it. It will be great if {{camel-rest-dsl}} allows this same behavior. Please let me know if I make myself clear.,non_debt,-
camel,9312,comment_2,"You can manually provide default values for everything in the route section, but that seems tedious. Edit: you can specify a defaultValue in but this value is not used.",non_debt,-
camel,9312,comment_3,You can now specify default values for query param in the rest-dsl which will be added as header if missing in the rest call,non_debt,-
camel,9338,summary,Upgrade,non_debt,-
camel,9338,description,"Should be updated to latest release as we are far behind on this one. It may no longer work in OSGi and that is fine, just remove the feature",architecture_debt,using_obsolete_technology
camel,9357,summary,KuraRouter should provide default routes configuration,non_debt,-
camel,9357,description,{{KuraRouter}} can be configured via XML and OSGi Configuration Admin. In such cases there is no need to implement {{configure()}} method.,non_debt,-
camel,9357,comment_0,Fixed in,non_debt,-
camel,9359,summary,logs NULL header values to WARN,non_debt,-
camel,9359,description,"As described on this post on the mailing list logs NULL header values to WARN (since Camel 2.16). Some standard components such as camel-activemq, however, write a lot of headers being mostly NULL such as JMSDestination, JMSCorrelationID, etc. Hence, I would suggest to change the log level from WARN to INFO in line",non_debt,-
camel,9367,summary,The Apache Camel Components Poster links broken,non_debt,-
camel,9367,description,The Apache Camel Components Poster links at the end of Components page [1] seems to be broken. [1],non_debt,-
camel,9403,summary,camel-examples should not be in BOM,architecture_debt,violation_of_modularity
camel,9403,description,"We should remove the camel-examples from the BOM. The examples are to be built from source, and its included in the .zip download.",non_debt,-
camel,9403,comment_0,"We also only had some of the examples in the bom, not all of them.",documentation_debt,outdated_documentation
camel,9412,summary,Correct documentation,documentation_debt,low_quality_documentation
camel,9412,description,fix some typo correct url,documentation_debt,low_quality_documentation
camel,9412,comment_1,For simpler things like this a new JIRA is not needed. A PR is just fine.,non_debt,-
camel,9438,summary,Unable to use camel-example-cxf,non_debt,-
camel,9438,description,Got this exception when run,non_debt,-
camel,9438,comment_1,got this correct output after changes,non_debt,-
camel,9486,summary,Camel component docs - Add support for username password configured in userinfo part of uri,non_debt,-
camel,9486,description,"Some components allow to configure username and/or password in userinfo part of uri, such as ftp / ssh components. This alternative syntax should be supported, so Camel catalog and the component docs is aware of this.",non_debt,-
camel,9499,summary,JMS - destination type for temp should use dash instead of colon,code_debt,low_quality_code
camel,9499,description,We use colon today which makes the uri ambigious and confusing. As colon is a separator for other options. So we should use dash instead temp:queue -temp:topic -,design_debt,non-optimal_design
camel,9499,comment_0,"This old syntax is the natural syntax in activemq itself. So we can leave it as is, and improve the catalog/tooling to do special handling of this option for AMQ.",non_debt,-
camel,9540,summary,- Should support ENV and properties,non_debt,-
camel,9540,description,So you can refer to ENV or {{ }} placeholders. What needs to be done is to let the properties placeholder execute the input string first. eg call See nabble,non_debt,-
camel,9540,comment_0,"Luca, here is a ticket to have some fun with.",non_debt,-
camel,9540,comment_1,"I did a quick test on beanio (2.17-SNAPSHOT, and the two syntax below are properly supported: Anything I've missed ?",non_debt,-
camel,9540,comment_2,Ah yeah we have likely since added support for property placeholders on all the <dataFormats It may still be good to do this as some components leverage that helper to load resources. But then again you often configure endpoints using uris and get that there fore free too.,non_debt,-
camel,9540,comment_3,Should we close this ? I can reply to the mailing list,non_debt,-
camel,9540,comment_4,A git grep indicate that a few components uses it to configure on component level which you configure using old school getter/setter. So it can benefit there. But most others are endpoints and data formats which already do property placeholder now out of the box.,non_debt,-
camel,9540,comment_5,Yeah its fine to close it - already implemented,non_debt,-
camel,9540,comment_6,Already implemented,non_debt,-
camel,9556,summary,Main - We should @deprecate enableHangupSupport,non_debt,-
camel,9556,description,"This is now default out of the box. But we should keep the old method name, so old code can still compile and run. eg ad and mark it as @deprecated.",non_debt,-
camel,9575,summary,Computing Content-Type,non_debt,-
camel,9575,description,"See Content-Type header of response should be ""application/xml"", not ""application/json"". It seems that if Content-Type header of *request* has been configured, it is used as a Content-Type header of *response*.",non_debt,-
camel,9575,comment_1,created PR.,non_debt,-
camel,9575,comment_2,Please discuss this more on the mailing list and provide more details about your use case,non_debt,-
camel,9580,summary,XSLT Component: Support custom URI resolver which depends on dynamic resource URI of the endpoint,non_debt,-
camel,9580,description,"The XSLT component allows already specifying custom URI resolvers via the parameter uriResolver. However, when your custom URI resolver depends on the dynamic URI of the endpoint then the current solution is not sufficient. Suppose you have a dynamically created XSLT endpoint, like where the resource URI of the XSLT document is also dynamic (in the example via the header XSLT_File). In this case you cannot hand-over the dynamic resource URI to the custom URI resolver. The contribution solves this problem by introducing a new parameter uriResolverFactory where you can specify a factory which implements the method URIResolver camelContext, String resourceUri) This method is called when the endpoint is created and gives you access to the endpoint resource URI. We also support that the resource URI factory can be set on the component so that you must not set the factory on each endpoint. Further advantage of the new approach is that you can now easily extend the default resource resolver which also depends on the resource URI of the endpoint.",non_debt,-
camel,9580,comment_0,I made a similar enhancement for the XSLT component as I have done for the Validator component. See,non_debt,-
camel,9580,comment_1,wiki updated,non_debt,-
camel,9594,summary,Use swagger.json instead of api-doc in examples,non_debt,-
camel,9594,description,We should use swagger.json which is the convention name used by swagger api-docs was the old for 1.x spec.,documentation_debt,low_quality_documentation
camel,9594,comment_0,We can now use /swagger.json or /swagger.yaml with camel-swagger-java so we can update the examples to use the url.,documentation_debt,low_quality_documentation
camel,9594,comment_1,Contributions is welcome,non_debt,-
camel,9616,summary,Support looking up MetricRegistry by type only in the Metrics component,non_debt,-
camel,9616,description,"For the moment, a custom {{MetricRegistry}} bean can be provided but must be named. It would be easier to relax that constraint in case only one {{MetricRegistry}} bean exist and do the lookup by type only.",design_debt,non-optimal_design
camel,9621,summary,camel-restlet - The producer should support message body as bytes or streams,non_debt,-
camel,9621,description,"It uses string as the message body. We should try to detect the current body type and use byte array / input stream etc that restlet has a representation that fits, and then use string as fallback.",non_debt,-
camel,9621,comment_0,This applies when you set a content type to json/text etc as it fallback to be form based which is string representation.,non_debt,-
camel,9643,summary,CXF SOAP consumer fails when running in Karaf,non_debt,-
camel,9643,description,"A simple Camel CXF SOAP web services consumer (server) works fine in standalone mode. But when deploying it in Karaf, it would fail if Camel 2.16.x is used. It would work fine if 2.15.x is used. streamCache is set to true. The following error was received. A zip file is attached containing a maven project (along with Karaf config info) that can reproduce this issue. This is critical because we cannot move to 2.16.x until this issue is resolved. Can't transform a Source of type Method)[:1.8.0_60]",non_debt,-
camel,9643,comment_0,This zip file contains the maven project that can reproduce the issue if deployed in Karaf 4.0.4. The README file in the zip contains the feature install commands to install the cxf and camel in Karaf.,non_debt,-
camel,9643,comment_1,"I know the Camel team is very busy working on many important features and issues. But I do hope this Jira issue will be fixed soon because there is no workaround that I know of. Without a fix, we are stuck in 2.15.x. :-) Thanks!",non_debt,-
camel,9643,comment_2,Have you tried with Camel 2.17.x ?,non_debt,-
camel,9643,comment_3,"Hi Claus, Thanks for reaching out and following up on this issue. With 2.17.0, I have not been able to install it successfully in Karaf 4.0.4. When I install my application bundle, I received an error: Exception: Configuration problem: Failed to import bean definitions from URL location Offending resource: URL nested exception is Configuration problem: Unable to locate Spring NamespaceHandler for XML schema namespace Offending resource: OSGi In the release notes, there was a mentioning about camel-spring not installed by default. But camel-spring is brought in by camel-cxf automatically. Here is what I installed in karaf 4.0.4: feature:repo-add cxf 3.1.6 feature:install cxf cxf-rt-security feature:repo-add camel 2.17.0 feature:install camel-core camel-cxf camel-groovy bundle:install -s Thank you for your help!",non_debt,-
camel,9643,comment_4,"Hi Claus, This issue seems to have been fixed by 2.16.3 (I will test it more thoroughly later). But I still cannot test it under 2.17.0 or 2.17.1 because I have to use Spring for DSL, transactions and jdbc templates but I have trouble to even load my bundle in Karaf with camel-spring (or spring-dm). camel-sql also cannot be loaded into Karaf (installing it would cause Karaf to hang). If the issue is indeed fully fixed in 2.16.3, then I can start to move my application off 2.15.x. I hope more tutorials are provided and/or updated for 2.17.x, especially in the area of deploying camel/cxf with spring in karaf. Thank you for your help!",documentation_debt,outdated_documentation
camel,9689,summary,Websocket Component Failing to Install in Karaf 4,non_debt,-
camel,9689,description,When installing the camel-websocket component in Karaf 4.0.4 the process hangs. When I try to install via Karaf's hot deploy I get: karaf@root()> ERROR: Bundle [94] Error starting Unable to resolve [94](R 94.0): missing requirement [94](R 94.0)] Unresolved requirements: [94](R 94.0)] Unable to resolve [94](R 94.0): missing requirement [94](R 94.0)] Unresolved requirements: [94](R 94.0)],non_debt,-
camel,9689,comment_0,Added StackOverflow URL:,non_debt,-
camel,9689,comment_1,You need to install servlet 2.6,non_debt,-
camel,9689,comment_2,And it uses jetty 8 and karaf 4 comes with jetty 9 - so you cannot run this easily in karaf 4. Camel 2.18 will drop support for old karaf versions and upgrade camel-websocket to be jetty 9 based.,non_debt,-
camel,9689,comment_3,Thank you Claus. Is the release date of Camel 2.18 known? I'll try using karaf 3.0.6 for now.,non_debt,-
camel,9689,comment_4,We did some recent improves in the karaf features file so this can install on karaf 4 now. You can help test the 2.17 release by building the source code and test it on your karaf 4,non_debt,-
camel,9689,comment_5,"This ERROR still happening on Camel 2.17.x (including the latest 2.17.7) when installing the camel-websocket feature on to Karaf 4.x Camel-websocket bundle ask for javax.servlet [2.6,3), but actually there is no any existing servlet-api match this version range, means no quick workaround for it. In case of there is a plan release a new 2.17.8 in the future, this commit should be backport into 2.17 branch.",non_debt,-
camel,9689,comment_6,No plan for 2.17.8,non_debt,-
camel,9690,summary,bean parameter binding should check parameter types when using simple expressions,non_debt,-
camel,9690,description,"Hello Im using camel 2.16.2 and Im finding the bean parameter binding doesnt seem to work very well on overloaded methods. See below for an example Here are the routes And here are the tests I dont understand why test2Param_string and test2Param_classB throw ambiguous call exceptions. Heres a sample stack trace. From looking at the code in BeanInfo, I *think* it just tries to match the type on the body and if it sees multiple possible methods then it throws the exception. I believe it should go further and try to match the type on the other parameters as well? To get around this issue temporarily, Ive had to write an adapter class that wraps around ClassA but its not an ideal solution.",design_debt,non-optimal_design
camel,9690,comment_0,"This requires to evaluate those simple expressions during finding methods to get their actual type in the parameters, so the right one can be picked. A side effect is that the simple language is evaluated both at discovering the methods, and then when calling the method.",non_debt,-
camel,9709,summary,Define jackson version in dependency management,non_debt,-
camel,9709,description,"Without proper dependency definition, clients may see a variety of versions",build_debt,build_others
camel,9709,comment_2,Thanks,non_debt,-
camel,9721,summary,Camel spring-batch can't be used in OSGI (karaf 4.0.4),non_debt,-
camel,9721,description,"Hi all the camel-spring-batch component does only depend on spring-batch (by pom.xml) but in the karaf features.xml it is said that it depends on camel-spring (which is not correct but in test). So this has a very downside, that drags the deprecated sprin-dm and impossible to run anythign on spring higher than 3.2. And the spring-batch version used by camel (2.16.2 needs spring-batch 3.0.4, that depends on spring 4, which is blocked by adding spring-camel), result, jar hell, and unable to read the XML Namespace errors. Good this is: removing the dependency of camel-spring and adding spring directly, solves the issue and can run spring-batch in any upper version correctly. thanks!",build_debt,build_others
camel,9721,comment_0,See,non_debt,-
camel,9721,comment_1,"Hi Claus I've read your email, it sounds like the new version would not present the issue, anyway, if camel-spring-batch depends only on org.springbatch, that should be the dependency and not camel-spring. I think the pom.xml and features.xml should be aligned. As this project has many, many dependencies, the shorter the dependency chain the better. Does it make sense?",build_debt,under-declared_dependencies
camel,9721,comment_2,Please look at the current code on master branch,non_debt,-
camel,9752,summary,Quartz2 Scheduled route too many workers,non_debt,-
camel,9752,description,"When any file:// or ftp:// consumer has a quartz2 schedule it can start throwing exceptions because too many worker-threads are busy at the same time Both workers can find the same files during filling the maxMessagesPerPoll, or during processing of those files. This happens when the route can not process all files before the next trigger happens. example (every minute): Attached you can find a stacktrace that would happen very often if worker1 processes and moves files that worker2 would also like to start processing. This does not happen when using scheduler=spring or when using delay=1m. The only way I have found to make sure a file or ftp component does not use multiple threads while consuming very large batches is to annotate the class with I am not familiar enough with the Camelcode to say what side effects it has and if this would prevent any quartz job in camel to now be single threaded, even if the user does not want it to be. But to me it looks like an oversight when moving from quartz to quartz2. A file or ftp consumer should be single threaded while retrieving.",code_debt,multi-thread_correctness
camel,9752,comment_0,Andrea can you apply the patch to those branches. Thanks Hans for the patch.,non_debt,-
camel,9752,comment_1,", yes :-)",non_debt,-
camel,9789,summary,should not start endpoint if Camel is starting up,non_debt,-
camel,9789,description,See nabble,non_debt,-
camel,9789,comment_0,"After upgrading to Camel 1.17.0 my Routes are crashing with stacktraces like the following (seems to be related to this issue, my own packages are masked with **): 14:32:31 | [main] CamelApp | ERROR | Camel App crashed, Reason: Error creating bean with name 'camelContext' defined in class Invocation of init method failed; nested exception is Cannot add component as its already previously added: properties Error creating bean with name 'camelContext' defined in class Invocation of init method failed; nested exception is Cannot add component as its already previously added: properties Related cause: Error creating bean with name 'camelContext': Requested bean is currently in creation: Is there an unresolvable circular reference? Caused by: Cannot add component as its already previously added: properties t ... 12 more Related cause: Error creating bean with name 'camelContext': Requested bean is currently in creation: Is there an unresolvable circular reference? Method)",non_debt,-
camel,9789,comment_1,"*UPDATE*: I tried out different things with the new Camel Version 2.17.0 and the new POM. This leads to a invalid JavaConfig Classes on my side. My config classes in Camel 2.16.0 looks like: This works fine with Camel 2.16.1 but in Camel 2.17.0 the app crashes with above error. Changing the configuration to this, works in both versions (mainly copied from",non_debt,-
camel,9789,comment_2,"Hubertus, Maybe you can put a sample project together as a .zip and attach this JIRA or put it on github somewhere.",non_debt,-
camel,9789,comment_3,Trying to improve this so endpoints are not pre-started if you use getEndpoint. Also other services that routes / components etc may register are being deferred being started until later.,non_debt,-
camel,9789,comment_4,Yeah for spring-boot I think not using is the approach. That module was written before spring-boot and is not as much in use as camel-spring-boot is.,non_debt,-
camel,9789,comment_5,"That's good to know, i was a bit miss lead by the documentation. I think a sample for ""good pratice"" with spring boot, java config and camel would help.... Much to do at the moment, but i will try to zip a basic sample project and upload it to this JIRA",documentation_debt,low_quality_documentation
camel,9789,comment_6,Hubertus so I have modified to be more like camel-spring-boot and add the routes after CamelContext has been fully created. So together with that fix and this ticket your issue should be fixed/improved a lot.,non_debt,-
camel,9812,summary,Camel leaves Kafka consumers running after shutdown,non_debt,-
camel,9812,description,"After shutting down a camel context, there are still threads running kafka consumers. In the logs after the shutdown I can see: So in theory the context is stopped, but I can see threads running with the polling of the sockets of kafka consumers (see attached immage). This deployed in an application server (wilfly in my case), causes a lot of issues, because apps get deployed and undeployed without stopping the JVM, but threads from previous deployments are left there. Please also bear in mind that kafka (9.0.1) throws warning messages due to the fact that un expected config items are thrown to the kafka consumer properties. Thanks!",code_debt,multi-thread_correctness
camel,9812,comment_0,"Thanks for reporting. Sound like the consumer need to call some stop/shutdown on kafka somewhere if its not already doing that, or missing something.",design_debt,non-optimal_design
camel,9812,comment_1,I will take a look :-),non_debt,-
camel,9812,comment_2,"It seems we weren't closing the consumer. Now it should be ok. I guess we should take a look to Consumer Group to make possible the use of multiple consumers on the same topics. Also we need to review the kafka properties, but I guess this should be in another JIRA",non_debt,-
camel,9812,comment_3,Do you want me to open the other ticket?,non_debt,-
camel,9812,comment_4,"Yes, please :-) This way we can trace better.",non_debt,-
camel,9812,comment_5,"done,",non_debt,-
camel,9812,comment_6,Thanks :-),non_debt,-
camel,9866,summary,@PropertyInject doesn't work with Spring-Boot,non_debt,-
camel,9866,description,When I upgraded Camel Spring-Boot project to use Camel 2.16.3 from Camel 2.16.2 the @PropertyInject annotations broke. The problem occurs when using the annotation in a Spring-Boot application like this Running the code above with command causes the following exception: Using placeholders like still works just as before. I tried to browse through changes between 2.16.2 and 2.16.3 but I didn't catch any obvious reason for this problem. I created a runnable demo of the problem here:,non_debt,-
camel,9866,comment_0,The problem was introduced in CAMEL-9431. It should be enough to not add the properties component in because it is added later later in,non_debt,-
camel,9866,comment_2,Thanks for the PR,non_debt,-
camel,9866,comment_3,"Thank you, Tomas!",non_debt,-
camel,10009,summary,Using <to> with id and ref fails,non_debt,-
camel,10009,description,If you use spring and then refer to an endpoint And then in a route has Then you get this spring error,non_debt,-
camel,10009,comment_0,If you use then it works,non_debt,-
camel,10022,summary,add a Spring Boot HealthIndicator to check that all camel contexts have started up and all the routes started OK,non_debt,-
camel,10022,description,see the section on writing custom health indicators: it'd be awesome if any spring boot camel application automatically got a health indicator to show if any camel contexts or routes fail to startup (or fail during their lifetime),non_debt,-
camel,10022,comment_0,There is a basic health indicator now.,non_debt,-
camel,10022,comment_1,We need the health check API in camel-core for fine grained status of routes etc.,non_debt,-
camel,10033,summary,camel-avro - Cannot install in karaf,non_debt,-
camel,10033,description,It hangs during installation in karaf,non_debt,-
camel,10033,comment_0,We were missing property in parent POM and camel-avro is using it in its Karaf feature definition.,non_debt,-
camel,10048,summary,Memory leak in RoutingSlip,code_debt,low_quality_code
camel,10048,description,"RoutingSlip has a cache of error handlers implemented as a ConcurrentHashMap. This map stores error handlers as values, and uses some synthetic objects as keys. For some kind of destinations provided in routing slip, map lookup operation does not work. Hence, new error handlers are always added to the map and existing error handlers never reused. Finally, the program runs out of memory. The synthetic keys are actually instances of class Such key is based on two objects: RouteContext and destination Processor. Neither RouteContext nor Processor do not require their implementations to provide equals() and hashCode() methods. Strictly speaking, caching implementation in RoutingSlip is incorrect, because it uses hash map in the discouraged way. However, for some cases it works. The problem occurs when routing slip contains a 'sync' destination, in other words - destination is a Processor that does not implement AsyncProcessor interface. RoutingSlip determines destination producer via and the latter uses method. This method creates new instance of Processor for every processor that is not an instance of AsyncProcessor. This is where the problem hides: new object has different hash code (defined by Object.hashCode()) and new object isn't equal to the object used as a key in the hash map (well, Object.equals()). Finally, new key for the hash map is calculated, lookup operation cannot find this key in the hash map, new key-value pair is put into the hash map.",design_debt,non-optimal_design
camel,10048,comment_0,Adding a unit test to reproduce,non_debt,-
camel,10048,comment_1,"How to fix? I think, the best way is to remove this dangerous caching at all. There might be a temptation to implement equals() and hashCode() methods in the helper class in a way to delegate both these calls to the processor wrapped by this class. However, the root cause of the problem is the incorrect usage of a hash map. Key must implement equals() and hashCode(). We cannot require all implementations of Processor and RouteContext to implement these methods - it would be an unmotivated bloating of their contracts with irrelevant functionality. Error handlers in RoutingSlip are short-living objects, they shouldn't get into Old Gen, so GC will clean them without significant performance overhead.",design_debt,non-optimal_design
camel,10048,comment_2,"Hello Claus, Is it ok to use Processor as a key in a hash map? Should every implementer keep this in mind and provide equals() and hashCode()? Javadoc in Processor interface tells nothing about it.",documentation_debt,low_quality_documentation
camel,10060,summary,Cannot scan package to find custom converters,non_debt,-
camel,10060,description,"When custom converters are included in a spring-boot nested jar, and the corresponding declares just the package containing the converters (not their full class names), spring-boot is not able to find them. Eg. whenever you include the camel-core *test-jar* in a spring-boot application, the application crashes with the attached exception. This happens only when the application is packaged and launched with `java -jar everything works if the application is not packaged. I experienced the same behavior with a sample module created ad hoc. The problem is related to package scanning, when names are used in the descriptor there are no problems.",non_debt,-
camel,10060,comment_0,The package scanning is deprecated and you should really use the FQN in the marker file to refer to your classes of type converters.,non_debt,-
camel,10060,comment_1,A custom implementation of (extending the default) is likely needed for spring boot to be able to scan inside spring boot fat JARs,non_debt,-
camel,10060,comment_2,"Yes, actually, I didn't want to use it, it's the camel-core test-jar, that is included transitively by several modules in test scope, that requires package scanning (making integration tests fail). I reported it also to understand if the same technique of package scanning is used elsewhere for some core feature.",non_debt,-
camel,10060,comment_4,Thanks for the PR,non_debt,-
camel,10144,summary,Salesforce keeps breaking backward compatibility by adding fields to older API versions,non_debt,-
camel,10144,description,"Salesforce adds fields to even after an API has been released. This needs to be handled in the component by ignoring unknown properties for that type. XStream doesn't support doing this for an single DTO, but it can be done for all types by using It will make it ignore all unknown properties for all inbuilt DTOs as wells user generated ones. But since the XML payload is less popular, hopefully this behavior won't be an issue.",non_debt,-
camel,10144,comment_0,"This is a generic problem with Salesforce, so the solution is to follow the REST principle of ignoring unknown fields altogether. This behaviour will be common to JSON and XML and will avoid future issues where running code breaks in production because Salesforce decided to add fields from new APIs in responses to older API versions.",non_debt,-
camel,10144,comment_1,Fixed on branches master and camel-2.17.x,non_debt,-
camel,10144,comment_2,"The fix doesn't seem to be working. As part if this fix the annotation = true) was added to the base class ""AbstractDTOBase"" Unfortunately this annotation has not been defined as ""@Inherited"" and thus is not inherited by the sub classes. Maybe this annotation should be added to ""sobject-pojo.vm"" so that it gets added to each of the generated classes",non_debt,-
camel,10144,comment_3,"Interesting point about the {{@Inherited}} annotation, but the effect seems to be that it's being inherited. Try removing the annotation from {{AbstractDTOBase}} and running the test it should fail without the annotation on the base class. This is on both _master_ and _camel-2.17-x_ branches even though they use different version of jackson.",non_debt,-
camel,10144,comment_4,"Never looked into RestResources. Let me check why that works. I suspect something to do with XStream VS Jackson processing Now what I did look into and tested, was user generated DTO . So, for example, if I create a DTO for say Account sObject and then somebody, later on, adds a new, say, a custom field, to Account, the deserialization fails. If the annotation is added to the generated class rather then the base class, deserialization passes.",non_debt,-
camel,10144,comment_5,"I ran the test after commenting out the field and it ran without any errors. If the annotation didn't work in the base class, the missing field would have caused the test to fail. BTW, the field does return a non-default {{true}} value when it's present in the class. Try your test by commenting out an existing field from the generated Account DTO. Not sure why that case would be different from when a new Salesforce field is added, but give it a shot.",non_debt,-
camel,10144,comment_6,You are right. Its working as expected. Not sure why though :( Please ignore,non_debt,-
camel,10153,summary,"[Camel-cxf] the spring version range in the Import-Package should be [3.2,4)",non_debt,-
camel,10153,description,"Camel 2.17.x upgraded spring version to 4.x in most of the components. but for camel-cxf component, it still has to use spring-dm and spring version 3.x, the spring version range in the Import-Package should keep [3.2,4), not [4.1,5). Now the ERROR will happen when install camel-cxf feature into karaf container (in case of both Spring 4.x and Spring 3.x are installed in the container) To fix it, make change to the pom.xml",architecture_debt,using_obsolete_technology
camel,10153,comment_0,Looks like in master the spring version for spring-dm is already extended and a separate spring-dm module is extracted. So I think it makes sense to limit the camel-cxf spring version to reflect the camel-spring limitations like you propose in the description. For later camel version we then seem to have a path to work with higher spring versions.,non_debt,-
camel,10273,summary,[Jetty] missing jmx object if custom thread pool is used,non_debt,-
camel,10273,description,"If a custom configuration for the ThreadPool is passed, the correspondent JMX object is not published. The issue here is that enableJmx(), that adds the mbeans listeners is called only after setThread method, so this explains how come the threadpool is not exposed on JMX. I've put a simple PR that just adds a check for {{enableJmx}} flag at component level, but I wonder if I should make it smarter to take in account possible settings only at Endpoint level or instead if we should remove it the flag per Endpoint altogether.",non_debt,-
camel,10273,comment_2,Thanks for the PR,non_debt,-
camel,10291,summary,Camel RabbitMQ invalid handling of message timestamp,non_debt,-
camel,10291,description,At the moment the RabbitMQ component is does not map the timestamp of a message appropriately. The outbound mapping (producer) expects the timestamp of the camel message is of type String whereas the String is just the long value representing the timestamp. However the timestamp is already a java.util.Date when the producer just forwards a message from a rabbitmq consumer as the timestamp is already a java.util.date as define in The provided pull request provides a compatible change. So it still keeps the old behaviour as fallback so that the long value is evaluated if the provided data is not a java.util.Date,non_debt,-
camel,10291,comment_2,Thanks for the PR :-),non_debt,-
camel,10362,summary,itest-spring-boot - Nagios and Jira fails locally,non_debt,-
camel,10362,description,CamelNagiosTest CamelJiraTest fails locally if you run them in,non_debt,-
camel,10362,comment_0,"Nicola, I wonder if you get these errors also?",non_debt,-
camel,10362,comment_1,I missed it. Checking now.,non_debt,-
camel,10362,comment_2,no problems with these tests now.,non_debt,-
camel,10376,summary,BeanInfo#introspect does not work correctly with bridge methods,non_debt,-
camel,10376,description,"Instead of selecting implementation method, {{bridge method}} is used. We faced an issue with conversion of parameter when {{bean}} implements generic interface. For example having {{bean}} implementation like this: would lead to {{beanInfo}} containing {{Method}} with signature in {{methodMap}} This is not correct as conversion of parameter is not possible in this case. I could find an issue with the same problem that was previously fixed (CAMEL-8940), but later on it was broken with CAMEL-9656 (commit 5639b78).",non_debt,-
camel,10376,comment_1,Thanks for the PR,non_debt,-
camel,10420,summary,camel-xxx-starter - Allow custom changes to the pom.xml files,non_debt,-
camel,10420,description,"Poms are auto-generated in the starters. We should remove the ""add dependency"" logic from the maven plugin and allow custom edits to the starter poms. Additional dependencies should be declared in the poms, as well as test dependencies.",non_debt,-
camel,10444,summary,Return header with key from update and remove operations,non_debt,-
camel,10444,description,Since release 2.18 MongoDB component does not return a header with key constant) after completing operations update and delete. This is a breaking change but can easily be patched in MongoDbProducer class. It's probably more a bug report rather than feature request depending on if the change was by design.,non_debt,-
camel,10444,comment_0,A github PR is welcome,non_debt,-
camel,10476,summary,configAdminFile not used to populate property placeholders in when run via camel-maven-plugin,non_debt,-
camel,10476,description,"Problem: When running with a Camel Blueprint project a configAdminFile is not used to populate propertyplacehoders in when exectued with So a user can't run camel locally in a similar way to running in Karaf with file based property placeholder values. Workaround: I think, but haven't tested yet, that you can work around this locally using the methods described here: and/or how this solution appears to use exec:java locally and loads the properties via To reproduce the problem: Create a new project using (You need to change the log4j config to make it run.) To reduce the time, I created a project that runs here: Instead of using a default in the blueprint XML for the I setup the POM to include the following: In Camel 2.15.2 or earlier, this file would be loaded when mvn camel:run was invoked and the properties would be available via the PID at run time. After the changes made in CAMEL-9313, it appears that the method is only called in when the createTestBundle pathway is taken in java.lang.String, boolean, java.lang.String, java.lang.String, java.lang.String, So it appears test using get this functionality (as shown by the tests) but things executed from camel:run do not. Here you can see in Camel 2.14 that call to is made after the bundelContext is created. In the master branch version, that call is no longer made from main after the context is returned. I made a change locally to add a similar call to in Camel 2.18: Here is the output of the log statement from the example before this change: Here is the output of the log statement from the example after this change: As you can see before the change, the ${greeting} property is not poplulated via propertyplacehoder. After the change it is replaced. Given all the discussion of timing related issues in CAMEL-9313, I'm hesitant to say this is a good enough solution or that it aligns with the intention of the changes made in that fix. Given that configAdminFileName and configAdminPid are passed into perhaps the call to should happen inside createBundleContext or one of it sub-methods. Overall, I ""think"" a user should be able to use the configAdminPid and configAdminFileName settings to load properties via camel:run rather than work aound it, but I could be persumptious there.",design_debt,non-optimal_design
camel,10476,comment_0,Thanks for detailed description. I'll have a look at the end of this week.,non_debt,-
camel,10476,comment_1,Let me know if I can provide any more information or if something isn't clear. I also was able to successfully test the workaround and I can run locally(in an IDE) using java:exec with the same properties file that the unit tests load by overriding and reading that config file. The only thing I am not confident in with the workaround is the difference between running in a DefaultCamelContext vs a in the IDE.,design_debt,non-optimal_design
camel,10476,comment_2,"The problem is field. Or rather the fact that it's not set in All JUnit tests (extending from by default use which returns {{true}}. When you simply do {{mvn clean test}}, neither {{target/classes}} nor contain file which is necessary to treat the location as ""bundle"" to be picked up in That's why in JUnit tests, explicit bundle is created using Only this method does the ""initial configadmin file"" trick (see for details). When running using {{mvn clean camel:run}} you'll simply get Gave up waiting for service because no ""bundle"" provides your blueprint XML descriptor. You need at least {{mvn clean package camel:run}}, so you have these ""bundles"" installed by felix-connect (a bundle with blueprint descriptor is #1): When running under JUnit, you have (a bundle with blueprint descriptor is #0): If both was {{true}} and we used {{mvn package}}, we'd have *two* bundles with blueprint descriptor. Manifest from (created by Manifest from (created by This is a reason of some problems we had when running JUnit tests *after* {{mvn ... package}}. So the solution should be to perform the ""initial configadmin file"" trick when running with {{mvn camel:run}} and without {{camel:run}} should pick up a bundle containing blueprint descriptor which was created by {{mvn package}} Also we should prevent using two bundles containing the same descriptors. I already have an idea.",non_debt,-
camel,10476,comment_3,"Because {{mvn camel:run}} is *not* a test, for such scenarios, should always equal to {{false}}. That's why I'll separate ""test bundle"" and a ""trick bundle"" that initializes configadmin before running blueprint.",non_debt,-
camel,10476,comment_4,"Ah, one more thing. creates TinyBundle without header. Thus explicitly uses: which eventually translates to And because {{mvn camel:run}} doesn't set setting {{descriptors}} or doesn't make sense with {{<useBlueprint>}} == {{true}}.",non_debt,-
camel,10476,comment_5,Fixed in: * * * * There were already and but all of them explicitly used: Now I've added which calls,non_debt,-
camel,10477,summary,[jruby] Upgrade to 1.7.26,non_debt,-
camel,10477,description,"Upgrade {{jruby}} to {{v1.7.26}} Note: this requires to explicitly set in test, while in used in multithread envs.",non_debt,-
camel,10477,comment_2,This upgrade frequently causes CrossRef:,non_debt,-
camel,10477,comment_5,jruby is deprecated so lets leave it as-is,non_debt,-
camel,10500,summary,Camel-Git: Add allowEmpty commits option,non_debt,-
camel,10500,description,"By default, JGit behaves differently than native git allowing empty commits. This first change is to add a new option to the GitEndpoint to be able to turn that option to false. I let it default to ""true"" in a first move to have backward compatibility but suggest we could default to ""false"" to match native git behaviour (git commit --allow-empty) This will raise an exception for empty commits, may be a silent mode to log warn would be a better option rather than doing error handling in camel.",non_debt,-
camel,10507,summary,Make TypeReference inline anonymous classes constant,non_debt,-
camel,10507,description,"Addressing the issue raised in the review, Jackson TypeReferences should be declared constant.",code_debt,low_quality_code
camel,10507,comment_1,We dont need a JIRA for internal refactorings like this.,non_debt,-
camel,10517,summary,Remove unnecessary SuppressWarnings,code_debt,low_quality_code
camel,10517,description,None,non_debt,-
camel,10517,comment_2,Thanks for the PR,non_debt,-
camel,10563,summary,camel-hazelcast: add an option to provide a custom configuration (custom Config object or configuration file location),non_debt,-
camel,10563,description,None,non_debt,-
camel,10563,comment_0,Need to add better handling for hz instance cleanup,code_debt,low_quality_code
camel,10575,summary,snakeyaml: add an option to filter classes the yaml parser can construct,non_debt,-
camel,10575,description,None,non_debt,-
camel,10575,comment_0,Commits related to this issue break wildfly-camel integration CrossRef:,non_debt,-
camel,10575,comment_1,"this may be expected as you now need to register the POJOs you want to de-serialize, test have been updated too. ping me if you need any help",non_debt,-
camel,10629,summary,Add labels to component level options,non_debt,-
camel,10629,description,We have fine grained details on each endpoint options. But the component options dont have @UriParam etc and are therefore often not labeled or have secure = true|false. We should add those using @Metadata annotation on the component level so we have that for the component docs and for tooling,non_debt,-
camel,10629,comment_0,Done for - camel-core - a..s components,non_debt,-
camel,10668,summary,Hystrix / ServiceCall - Allow to configure global configuration,non_debt,-
camel,10668,description,"It would be nice if we could allow to configure camel-hystrix and ServiceCall using spring boot auto configuration, eg from the / yml file. Then you can easily set a global configured timeout, and those other hystrix configurations. And with this we get tooling support which can show documentation at your finger tips. This should be global configuration, which you can override in your camel routes if you setup a local hystrix configuration there.",non_debt,-
camel,10668,comment_0,This is a bit tricker as its EIP configuration. But it would be good to do this for other things like error handler etc.,non_debt,-
camel,10668,comment_1,Should we do the same for ServiceCall ? I can work on both later on if needed,non_debt,-
camel,10668,comment_2,Yeah that would be good too,non_debt,-
camel,10668,comment_3,question: should any additional custom conf inherit from the default ? i.e. : So the conf on the route inherit from myconf which inherit from the deault one.,non_debt,-
camel,10668,comment_4,My initial thought would be yes. What do you think? Or how does it work today?,non_debt,-
camel,10668,comment_5,"I do not remember such a hierarchy, as far as I remember it stops on the first parent but make sense to have such inheritance possible",non_debt,-
camel,10678,summary,Transformer registry JMX,non_debt,-
camel,10678,description,We should use each individual fields instead of a string field eg use fields - from - to - name - model As individual fields. And use name as the index field if its intended to be unique. This allows users to better query/browse the registry from JMX as well to find which transformers there is. Also remember to adjust the code in the jolokia command.,design_debt,non-optimal_design
camel,10716,summary,Salesforce Composite API wrongly checks for minimum supported version,non_debt,-
camel,10716,description,"There is a check in the that checks if the configured version is at least 34.0, this check wrongly reports an error if the version is configured exactly at 34.0.",non_debt,-
camel,10758,summary,camel-ahc - Upgrade to newer version,non_debt,-
camel,10758,description,None,non_debt,-
camel,10758,comment_0,See nabble,non_debt,-
camel,10806,summary,Create camel-rxjava2 component,non_debt,-
camel,10806,description,A we have now it would be nice to have a RxJava 2 implementation of the API,non_debt,-
camel,10806,comment_4,Thx,non_debt,-
camel,10924,summary,PingCheck API : Support validation through JMX,non_debt,-
camel,10924,description,None,non_debt,-
camel,10924,comment_0,There is a RuntimeCamelCatalog now in camel-core that has validation API based on the components that are on the classpath.,non_debt,-
camel,10927,summary,Add Kafka topic-based,non_debt,-
camel,10927,description,Add implementation that uses a Kafka topic as a log.,non_debt,-
camel,10927,comment_1,Jakub I granted your user karma to self assign tickets. There is some more comments on the PR before its ready. But great work and good to see your contribute source code to Camel as well,non_debt,-
camel,10950,summary,Enable camel-docker configuration to accept a custom,non_debt,-
camel,10950,description,"One of the drawbacks with the docker-java library used in the camel-docker component is that it depends on Jersey for the JAX-RS client. This can be problematic if you already have another JAX-RS implementation on the classpath. Therefore, it'd be nice if you could specify the that you want to work with. By default it'd be but you could choose to use the if you wanted to avoid Jersey. Similarly, users could implement their own and have camel-docker load this when it comes to build the Docker client.",design_debt,non-optimal_design
camel,10950,comment_0,Are you working on a PR?,non_debt,-
camel,10950,comment_1,"Yep - just testing some mods now, so will hopefully have a PR ready soon. I'll assign this issue to myself.",non_debt,-
camel,10983,summary,Fail early and show meaningful log for invalid endpoint URI in Blueprint,non_debt,-
camel,10983,description,"An extra white space in blueprint xml (e.g. {{<to uri="" sql:select * from eng""/ while spring throws relevant exception",non_debt,-
camel,10983,comment_1,Thanks for the PR,non_debt,-
camel,10996,summary,camel-kafka - Upgrade to 0.10.2.0,non_debt,-
camel,10996,description,None,non_debt,-
camel,10996,comment_0,we need a SMX bundle of this new version also,non_debt,-
camel,10996,comment_1,Already done Need to wait for the end of the month.,non_debt,-
camel,11052,summary,throwing exception when using JAXB/JAX-WS generated code,non_debt,-
camel,11052,description,"When using WsImport, the structure of the generated Exception class for SOAP Faults have only a two arg constructor generated ie: However in the code is trying to get a reference to the single argument constructor, before it tries to find/use the two argument constructor, which would cause a to be thrown as no such constructor exits on the class. The solution is to move the variable inside the catch block. So we try to get/use the two arg constructor and if that fails then we try to get/use the single argument constructor, which is the real logic of that try/catch block anyway.",non_debt,-
camel,11052,comment_0,You are welcome to work on a github PR,non_debt,-
camel,11052,comment_1,Thanks . I've started working on a patch. Can this issue be assigned to me?,non_debt,-
camel,11052,comment_2,Kieran sounds good. You cannot assign ticket as that is for committers and users whom have contribured more patches. Its fine with a comment here that you work on this. Lets us know how it goes,non_debt,-
camel,11052,comment_4,Merged to master from,non_debt,-
camel,11104,summary,Performance issue in camel sftp,non_debt,-
camel,11104,description,"We have implemented a camel route where we are having camel sftp producer to transfer a files to remote SFTP location but on performance testing on client environment and on our local environment we have observed degradation in the time for transferring files to remote SFTP location. Please find the detailed analysis below. The we tried the various test in our local environment. In each test we put around 22 files on camel file consumer and each file took below time to write the file. PFB details When target directory having 20,000 files. Camel sftp producer took around 1 minute 43 second to a transfer file DEBUG 07:00:38 (Camel thread #6 - INFO 07:00:38 (Camel thread #6 - INFO 07:00:38 (Camel thread #6 - DEBUG 07:00:38 (Camel thread #6 - DEBUG 07:02:19 (Camel thread #6 - DEBUG 07:02:19 (Camel thread #6 - DEBUG 07:02:19 (Camel thread #6 - DEBUG 07:02:20 (Camel thread #6 - DEBUG 07:02:20 (Camel thread #6 - INFO 07:02:20 (Camel thread #6 - When target directory having 40,000 files. Camel sftp producer took around 3 minute 17 second to transfer file DEBUG 07:47:23 (Camel thread #6 - using exchange: INFO 07:47:23 (Camel thread #6 - from IOP FTP directory INFO 07:47:23 (Camel thread #6 - : to ICOMS FTP directory DEBUG 07:47:23 (Camel thread #6 - DEBUG 07:50:40 (Camel thread #6 - DEBUG 07:50:40 (Camel thread #6 - DEBUG 07:50:40 (Camel thread #6 - DEBUG 07:50:41 (Camel thread #6 - DEBUG 07:50:41 (Camel thread #6 - INFO 07:50:41 (Camel thread #6 - Similarly when we achieved the files from target directory. It took around 6 sec.It seems like there is a performance issue with camel sftp component. Does it list the files in target directory which is taking time. PFB the producer route which we set up",code_debt,slow_algorithm
camel,11104,comment_0,Use the user forum or user mailing list to ask for help,non_debt,-
camel,11104,comment_1,Already put on user mailing list,non_debt,-
camel,11104,comment_2,and don't open the same issue two times.,non_debt,-
camel,11171,summary,- RAW() and child endpoint issue,non_debt,-
camel,11171,description,"component has an issue with the usage of {{RAW()}} function in child endpoint configuration. will mishandle the the content of {{RAW()}} , when at some point the escaped ampersand symbol is unescaped, and a wrong set of parameters is used. The attached PR fixed the issues and adds a unit test to verify the behavior before and after the fix.",test_debt,low_coverage
camel,11196,summary,Camel connectors - Allow to configure in one place and let it figure out component vs endpoint level,non_debt,-
camel,11196,description,"A Camel connector can be configured on two levels - component - endpoint Just like a regular Camel component. But we should allow users to configure a connector in one place, and not worry about if its component or endpoint level. And then let camel-connector when it prepares the connector figure out all of this for you. This may require supporting loading configuration from external resource files such as .properties files etc. Or something else. But from end users we can make this easier.",design_debt,non-optimal_design
camel,11196,comment_0,"We should also make it easy to reuse an existing connector and have separate configuration, eg so the same user can use a twitter-connector and then configure it for user A and user B in the same application without configuration clashes. Today the component level would reuse the same component / configuration. So we may want to find a way to clone a component or whatever we can think of. A little bit of problem is the generated spring boot auto configuration which is hard-coded to one prefix.",design_debt,non-optimal_design
camel,11196,comment_1,"We may generate something like: Then the spring boot auto-configuration would: - register and configure the component with ``component-name`` component as usual - register and configure number of bean according to the configuration map of the same type of the ``component-name`` but registered with a different name so that they can be referenced by name as usual but with properties inherited from component-name (or maybe not) Of course the drawback is that the [my-component] won't get auto-completed y IDEs but this solution would give a good flexibility and you can get the name of the supported parameters, theirs default and the doc from the root configuration.",non_debt,-
camel,11196,comment_2,It may be useful to look again at the old CAMEL-10031.,non_debt,-
camel,11196,comment_3,Taken to do some experiments,non_debt,-
camel,11282,summary,Camel components should extend DefaultComponent,non_debt,-
camel,11282,description,We should extend the plain DefaultComponent (the is deprecated) ensure there is a plain default no-arg constructor so it makes using and configuring components easier. See eg SO If it was just a plain no-arg constructor then the bean style would have worked.,design_debt,non-optimal_design
camel,11299,summary,Camel Rest DSL Does Not Creating OPTIONS routes for defined routes,non_debt,-
camel,11299,description,"Camel does not generating the options answer for all routes defined using component servlet, ex: For /v1/customers the request OPTIONS return 200 OK but for /v1/customers/1 the request OPTIONS return 404 not found.",non_debt,-
camel,11299,comment_0,This is also a problem with camel-jetty,non_debt,-
camel,11403,summary,Karaf feature camel-linkedin fails,non_debt,-
camel,11403,description,Due to an upgrade of htmlunit and jetty versions we now get the following errors when installing the camel-linkedin feature: AND Other features install fine because they directly depend on the jetty feature.,non_debt,-
camel,11403,comment_2,Looks like I was mixed up in my dev branch where I was using the cxf-http-underdow instead of cxf-http-jetty... no issue :),non_debt,-
camel,11408,summary,Camel Servlet Web Page,non_debt,-
camel,11408,description,"On page the section ""Sample when using OSGi"" is missing the code snippets. Can these examples be put back in and, if possible, be verified? Thanks much!",documentation_debt,low_quality_documentation
camel,11408,comment_0,"The component docs are in adoc files with the source code - the wiki is dead so don't update there. Make sure to fix/update in adoc, and if you want you can do wiki too. But wiki only changes will be lost in the future when wiki is discarded completely",documentation_debt,outdated_documentation
camel,11417,summary,route-reset-stats completion issue,non_debt,-
camel,11417,description,It seem to be a small issue in tab completion of Karaf command The allowed argument for this command is context but currently it auto completes the routes.,non_debt,-
camel,11504,summary,Add link checker,non_debt,-
camel,11504,description,"We should be accessible and there should be no broken pages in the website, to do that we should incorporate",documentation_debt,low_quality_documentation
camel,11504,comment_0,Would be good to add it to the website build as a script in the and also so it can be run from the build to,non_debt,-
camel,11504,comment_1,Not sure what is the state of the art tool for checking links these days...,non_debt,-
camel,11504,comment_2,We could use: Thoughts ?,non_debt,-
camel,11504,comment_3,That could be a good start and an easy way to fix broken links we have currently. I was hoping that we might have a build-time tool/integration with a tool to check for links. I think that would be useful for PR jobs (when those get fixed).,test_debt,lack_of_tests
camel,11504,comment_4,I see that there is a maven plugin to check link and generate a report : I can try to use it to see if it make sense.,non_debt,-
camel,11504,comment_5,"please do, I think it'll require a Maven project, and we don't have one in {{camel-website}}, also I'm not sure how tied that plugin is to Maven site generation. I don't wan't to discourage you, anything that gives us a failure and a list of pages and links that are broken will work.",non_debt,-
camel,11504,comment_6,"Oh yes you're right, using a maven plugin is not a good idea, I will search another solution.",non_debt,-
camel,11504,comment_7,We can now run the link checker via {{yarn run check}}.,non_debt,-
camel,11524,summary,Camel File Consumer fails when doneFileName contains '$',non_debt,-
camel,11524,description,"I have running a blueprint file with a camel context that has the following: This causes Camel to throw the following exception when I receive files with names of the following form: Most likely the '$' in the name is the problem. Is there any way to escape it? Thanks, Saycat",non_debt,-
camel,11524,comment_0,"No there is not, your workaround is to not use $ in the file name, which also is a bad habit to do so. The source code needs to be patched where you need to quote the file name in the GenericFileEndpoint method via You are welcome to work on a github PR to fix this",code_debt,low_quality_code
camel,11524,comment_1,"Hi Claus Thanks for the response. The source of the '$' is Ola Hallengren's SQL maintenance script and that in turn is escaping a '\' in a database engine name. It seems like a better idea to fix the Camel code and make it more robust. I will look into providing a patch for this. Regards, Saycat",code_debt,low_quality_code
camel,11534,summary,Incorrect transferExchange option test in camel-jms component,non_debt,-
camel,11534,description,transferExchange set to 'true' or 'false' doesn't affect to the JUnit test,non_debt,-
camel,11655,summary,Camel-Nagios: Deprecate EncryptionMethod and use Encryption Enum,non_debt,-
camel,11655,description,None,non_debt,-
camel,11655,comment_0,"IMHO this's a breaking change for the upcoming {{2.20.0}} version as the query parameter has been simply removed and replaced with a new {{encryption}} query parameter, which _would_ break user's code making use of this option. AFAIK a minor release should not include any breaking changes, right? Shouldn't we better somehow mark as deprecated and encourage users to make use of the new {{encryption}} parameter instead? Other than that some feedback on your made code changes: What would be wrong to do: Instead of the following if / else if: As because: Also maybe mark the enum itself as deprecated so we don't forget to remove it in Camel 3.",design_debt,non-optimal_design
camel,11655,comment_1,"That code portion is needed because currently jsendnsca works only with Encryption NONE, XOR and Triple des, the others aren't working. I guess that we can accept a change like this one is just a minor change to align with the new library. So IMO it's not a problem for the end users.",requirement_debt,requirement_partially_implemented
camel,11655,comment_2,"Thanks for your feedback. Could you please point to the bug report or ticket you're talking about so the real rationale behind this jira ticket becomes clear to others. I guess it's about this enum: IMHO we should not restrict the API only to those three enum members NONE, XOR and Triple_Des just because the others don't work. If that's really the case it should be resolved on the jsendnsca side and we _should not_ restrict the API to only those 3 members just because the others don't work. IMHO Camel should be transparent about this. What if the Blowfish encryption gets fixed by the next jsendnsca release and we update to that version in Camel? Are we going to suddenly allow that Blowfish encryption as well and add another {{else if}} for that? And what do you think about marking enum as deprecated? Does it make sense?",non_debt,-
camel,11655,comment_3,"It's stated here in this thread: for Blowfish, yes, in case it will be supported we can add another else if there. It doesn't make sense to allow all the encryption if most of them don't work in nsca. Marking the as deprecated in my opinion it's not so important, it's just an option. If we start to deprecate all the options when we upgrade something we will end with a lot of dead code until Camel 3.0 (that actually doesn't have an ETA by the way). If this is something critical for you, you can revert commit and do what you think is best for this component. It was just an attempt to avoid the death of camel-nagios.",requirement_debt,requirement_partially_implemented
camel,11655,comment_4,"LOL it's not critical for me but for the community :-) I am not convineced by your answers and kindly ask  &  to share their thoughts about this ticket as well as it's corresponding code changes. To summurize: - The made changes are breaking for the upcoming 2.20.0 _minor_ release if a Camel based application already makes use of the query parameter today in production. - IMHO Camel should be transparent and not restrict an underlying library API just because it seems to be buggy or non-working. Bugs _should_ be resolved by the underlying library itself and not _artificially_ through Camel by hiding/restricting a given API. See my comments above regarding this point. - In general, shouldn't we mark a Camel API as deprecated and encourage users to make use of the right/new API. E.g. as it's done when some {{StringHelper}} new utiliy methods were extracted out of {{ObjectHelper}} and the corresponding {{ObjectHelper}} methods were marked as deprecated, see CAMEL-10389. This would make it much much easier both for Camel code base itself as well as user's code to avoid using the legacy API. I guess that's exactly what the {{@Deprecated}} annotation good for. I don't seem to understand what it would be wrong with such an approach, that's, marking query parameter as deprecaterd and encourage useres to make use of the new {{encryption}} query parameter and then support both of them. Supporting both would not break a pre-existing application on the production and then we could find and remove the deprecated API by Camel 3 much easier. WDYT?",design_debt,non-optimal_design
camel,11655,comment_5,I'll do it.,non_debt,-
camel,11655,comment_6,"Done. Next time I'll bring a lawyer with me, since you are _not convinced by my answers_ :-) By the way nagiosSettings doesn't have a setEncryptionMethod method anymore.",non_debt,-
camel,11655,comment_7,Thanks for fixing.,non_debt,-
camel,11734,summary,Upgrade grpc-java to 1.6.1,non_debt,-
camel,11734,description,It'd be nice if we could upgrade camel-grpc to use the latest grpc-java library as there are some improvements to how it does class loading:,architecture_debt,using_obsolete_technology
camel,11734,comment_0,do you want to take a look at this one?,non_debt,-
camel,11734,comment_1,", I'm working on CAMEL-11695 right now and will try to update the gRPC library in this work However, CAMEL-11695 will use TLS with OpenSSL library involving some native (linux, osx, windows supported) code and need to be check how it will work in OSGi environment. TLS with JDK also requires JRE based agent but not recommended for production use due to low performance.",non_debt,-
camel,11734,comment_2,great :-) Maybe we can document very well how everything works and then setup an example in OSGi (we currently have a spring-boot example and a kubernetes one).,documentation_debt,low_quality_documentation
camel,11734,comment_3,Fixed in CAMEL-11695,non_debt,-
camel,11745,summary,Deprecate Camel-Castor,non_debt,-
camel,11745,description,None,non_debt,-
camel,11745,comment_0,Already done here:,non_debt,-
camel,11840,summary,camel-itest-karaf - CamelLinkedinTest fails,non_debt,-
camel,11853,summary,camel-reactor : create karaf feature,non_debt,-
camel,11853,description,None,non_debt,-
camel,11853,comment_0,"This may require a little bit of work also on side, postponed to 2.21",non_debt,-
camel,11853,comment_1,What work do you see is needed on the side?,non_debt,-
camel,11853,comment_2,"The feature itself is easy but I do not remember how service discovery was done in so it may just need to be properly tested, if we have time before 2.20, we can try to make it :)",non_debt,-
camel,11853,comment_3,Okay I added the feature then some end user may try it out and shout out if it does not work ;),non_debt,-
camel,11853,comment_4,I'll resolve it then :),non_debt,-
camel,11868,summary,Migrate java transport client to the new high level rest client,non_debt,-
camel,11868,description,The current java transport client is due EOL in near future and it will be a good idea we switch the the new high level rest client instead. The new high level rest client is only released from version 5.6.0 and towards so an general upgrade of the dependencies is required for the ELK component. This also add the support for basic authentication without the need of x-pack. This is usefull when running ELK behind a reverse proxy.,architecture_debt,using_obsolete_technology
camel,11868,comment_0,"Nice idea, but would be good to keep both components and backward compatible.",non_debt,-
camel,11868,comment_1,"It might be a good idea, since not all features is supported in the high level rest client yet I already got it to work with the rest client, but then I would suggest we create a super class for and a and In the we decide which one we should instantiate based on the type client on the class path. This of course mean the client dependency is not included in the the component. Regarding the it's not possible to use the Hight level REST client since it only support ELK from 5.6.0 and forward. One think to be aware of with the REST API that it is slightly different, which mean various request and response object is not called the same with the REST client. So either we have to document this in the manual or we have to come up with a common request / response object for both clients",design_debt,non-optimal_design
camel,11868,comment_2,"Can we not create a new component that use this new rest client only. And then the two older components can stay for older users, and eventually we can start marking them as deprecated because the java client is EOL from Elastic and the rest client is the future/way to go.",design_debt,non-optimal_design
camel,11868,comment_3,"And by creating a new component you have more freedom and do not have to 100% compatible with those older components. End users may have to migrate/change something on their end, and that is expected / okay.",non_debt,-
camel,11868,comment_4,"sound like a plan, and you can assign me to it if you want. I already have working branch",non_debt,-
camel,11868,comment_6,Do we have the Karaf feature for this component?,non_debt,-
camel,11868,comment_7,"No there is no karaf feature. You are others is welcome to work on it, it seems there is a bunch of dependencies so the feature is likely a bit big.",non_debt,-
camel,11888,summary,cluster service : add a cluster service based on JGroups Raft,non_debt,-
camel,11888,description,See,non_debt,-
camel,11888,comment_0,do you think we should implement this one or use jgroups-raft,non_debt,-
camel,11888,comment_1,"Hi , an implementation based on JGroups views would suffer the same limitations as the one based on JGroups Locks (i.e. not enforcing only one master in case of split brain); an implementation based on jgroups-raft would be more interesting in that regards adding a new option for the users to cope with spilt-brains situations. I'd love to implement it fi you are looking for volunteers.",non_debt,-
camel,11888,comment_2,"title updated, please go ahead :)",non_debt,-
camel,11888,comment_3,"I'm leaving this marked for 2.23.0, let us know if you're able to finish",non_debt,-
camel,11888,comment_4,I ended up implementing the whole {{jgroups-raft}} component:,non_debt,-
camel,11995,summary,Support Composite API,non_debt,-
camel,11995,description,"Camel Salesforce component supports Composite Tree and Composite Batch APIs, there is a new [sub-API called simply for executing multiple REST calls (up to 25) in a single request.",non_debt,-
camel,11995,comment_0,Pull request by Vassilis Spiliopoulos to add Composite API support,non_debt,-
camel,11995,comment_6,"Camel Salesforce component now has support for the third Composite API variant, along with Batch and Tree.",non_debt,-
camel,12034,summary,"- Search Operation: If Map or String is used in Message Body, ""size"" and ""from"" parameters are always ignored",non_debt,-
camel,12034,description,"Hi, I am using component of Camel 2.20.1. I have found and issue. The description follows. *If you use Map or String in message body for SEARCH operation, ""size"" and ""from"" parameters are always ignored hence you always get just default 10 results.* For example - if your map contains query like this: (in terms of simplicity - following is String representation of the map): Issue I suspect is present in class: and its method _public static SearchRequest queryObject, Exchange exchange)_ Inner if condition basically extracts only query part from the map and *""size"" and ""from"" get lost from the query* Same issue is with usage of the String in the message body: Only workaround for this is to use *SearchRequest* object in a message body where you can explicitly set ""size"" and ""from"" on SearchSourceBuilder object. For example: I don't know what was the developer's intention for having such a condition which removes these parameters from the query. Thank you very much in advance if anybody can have a look and verify if this is a valid concern.",non_debt,-
camel,12034,comment_0,"Since we need to deprecate this component in favor of the rest one and rename it to elasticsearch-rest, do we really need to do this? I believe we need to update the rest stuff to version 6.1.x and deprecate elasticsearch5 before the 2.21.0 release.",non_debt,-
camel,12034,comment_1,Yeah lets do that,non_debt,-
camel,12034,comment_2,I'm raising a new Jira for this.,non_debt,-
camel,12034,comment_3,"Okay we have deprecated these components, but if there is a little bug it would still be nice to try to fix this. Anyone is welcome to help contrbute",non_debt,-
camel,12034,comment_4,"This was not really intended, but I am adding support for providing from and size in the map / json string",non_debt,-
camel,12042,summary,Rendering of code/snippets inside documentation is broken,documentation_debt,low_quality_documentation
camel,12042,description,"The rendering of code snippets on the camel homepage is broken. For example see I expect the following should be java syntax highlighting? {{ javaEndpoint endpoint = PollingConsumer consumer = Exchange exchange = Also further down the page I get: {{The example below illustrates I'm not sure if this is the right place to report this, but since the homepage is part of the github repository it seems to make sense using the projects bug tracker. I checked the Gitter history and had a look at the ML and nobody seems to mention this. I've also tried to search on JIRA but always end up with Issues for Zookeeper or other projects.",documentation_debt,low_quality_documentation
camel,12042,comment_0,"The old wiki system is deprecated, a new website and documentation is in the works. You can help with the new docs which are the adoc files in the source code you can find in the src/main/docs folder of the various Camel components.",documentation_debt,outdated_documentation
camel,12042,comment_1,The official docs are now on Github as mentioned in the home page of site. We are working on a new website.,non_debt,-
camel,12104,summary,Unintuitive default cxf timeout behavior,design_debt,non-optimal_design
camel,12104,description,"There is very strange behavior in Camel cxf and cxfrs timeouts which could lead to sensitive data being released. Below is a code sample which illustrates the unexpected behavior. I think any developer would expect the test API to return ""Valid Response"" or some kind exception, but in fact it returns ""SENSITIVE DATA"" due to the default continuationTimeout of 30 seconds. This issue seems to have been introduced by",design_debt,non-optimal_design
camel,12104,comment_0,Would you be able to build an unit test of this sample code so we can take that and add to the tests of camel-cxf and work on a fix.,test_debt,lack_of_tests
camel,12104,comment_1,Also as a workaround you can try with synchronous=true on the CXF endpoint,non_debt,-
camel,12104,comment_2,"Okay so the issue is in Apache CXF where it will onTimeout that is called from Jetty or Servlet3 async API then just call resume(), see screenshot",non_debt,-
camel,12104,comment_3,"The CXF Continuation API itself lacks the concept of timeout, so you cannot get that state to know there was a timeout and then fail accordingly.",non_debt,-
camel,12104,comment_4,or  I wonder if you guys may have any comments on this. I added an unit test to camel-cxf named:,non_debt,-
camel,12104,comment_5,I think we can extend cxf Continuation interface a bit to add an isTimeout method so that we know the timeout happen and can handle this situation accordingly outside CXF,design_debt,non-optimal_design
camel,12104,comment_6,Thanks Freeman,non_debt,-
camel,12166,summary,FTPConsumer - no disconnect if polling files fails,non_debt,-
camel,12166,description,"We encountered an issue downoading files from various different ftp servers. If listing the files to consume fails for whatever reason, the FTP client does not disconnect from the server, even if disconnect=true is specified in the endpoint URL. This leads to the problem, that upon the next poll, the connection ist still open and the component starts to navigate stepwise to the target folder which will fail, because we are not in the home folder of the FTP user anymore. This leads to an which will again cause the poll to fail without a disconnect. From now on, polling this endpoint will fail until restarting the containing route. Adding stepwise=false as an workaround did not work in our case because one of teh external FTP servers will return absolute paths when calling the list command, resulting in an error when trying to download the files.",non_debt,-
camel,12166,comment_0,Did you tried with newest version of Camel? 2.16.x and 2.17.x are no longer maintained and they won't have new releases.,non_debt,-
camel,12166,comment_1,"Yes try with latest releases, also we have improved with a force disconnect in Camel 2.21 onwards.",non_debt,-
camel,12166,comment_2,I checked the source code on git and could not find any changes regarding a disconnect in method poll() of class GenericFileConsumer before opening this issue. Updating Camel is not an option at the moment. We already tried to substitute ServiceMix 6.1.1 with the latest version of Karaf and Camel with no success.,non_debt,-
camel,12166,comment_3,"It end up calling some rollback code, that performs the disconnect. Its a bit more complicated as ftp extends file component. But as said you use an old version of Camel we dont support anymore.",code_debt,complex_code
camel,12166,comment_4,Thank you for the hint with the rollback. I am already using a custom poll strategy and could implement a fix for my usecase there.,non_debt,-
camel,12166,comment_5,"We ran into a similar issue today on an old AX400 system where we have to up- and download files from via FTP. While we hadn't issues before (used a camel-driven client application on their system since Camel 2.14.1 (and before) with regular version updates of Camel during this period, in the early morning suddenly the same behavior as mentioned here occurred and we roughly needed half a day to fix this issue even though the AX400 server maintainer assured that he didn't change anything in regards to general server and FTP server configurations. Note that the most recent versions deployed contained Camel 2.20.1 and for testing even 2.21.0-SNAPSHOT. Our issue was caused by Camel connecting in active mode to the server by default. On analyzing the Apache FTP client log we noticed that after successfully interacting with the server after the second time listing (or the previous PORT command) the mentioned exception occurred in our logs. INFO | jvm 1 | 2018/01/29 17:51:47 | CWD tmp INFO | jvm 1 | 2018/01/29 17:51:47 | 250 Befehl CWD erfolgreich. INFO | jvm 1 | 2018/01/29 17:51:47 | CWD some_subdir INFO | jvm 1 | 2018/01/29 17:51:47 | 250 Befehl CWD erfolgreich. INFO | jvm 1 | 2018/01/29 17:51:47 | SYST INFO | jvm 1 | 2018/01/29 17:51:47 | 215 UNIX-Typ: L8 Version: BSD-44 INFO | jvm 1 | 2018/01/29 17:51:47 | PORT 1,0,0,130,196,47 INFO | jvm 1 | 2018/01/29 17:51:47 | 200 PORT-Befehl erfolgreich. INFO | jvm 1 | 2018/01/29 17:51:47 | LIST INFO | jvm 1 | 2018/01/29 17:53:02 | 425 Keine Datenverbindung Connecting to the FTP server via FileZilla worked and after studying the produced logs from FileZilla we noticed that it communicates in passiveMode which we gave a try. After adding passiveMode=true we managed to read and write files to the server again. Why the errors occurred without any interactions on our or the admins behalf is a different storry.",non_debt,-
camel,12182,summary,Connection should be reset in the event of an acknowledgement timeout,non_debt,-
camel,12182,description,"When a MLLP Sender (Producer) times-out waiting for the MLLP Acknowledgment message, the TCP connection should be reset. Currently, it is left open which can lead to MLLP protocol issues.",non_debt,-
camel,12182,comment_0,Corrected this with,non_debt,-
camel,12216,summary,Intermittent time outs,non_debt,-
camel,12216,description,"Hi, We are using the sales force component and trying to write to external salesforce health cloud using the camel salesforce component. Most of the times, the data seem to flow. However abruptly I see the below errors. If I reprocess the failed one again, it would work. But looks like this keeps happening a few times. Your suggestions, thoughts would be helpful. 2018-01-30 21:06:29.399 INFO 31221  [[JPASalesForce]] : In PersonProcessor, Message received for patient MRN :: 5000307798 2018-01-30 21:06:29.401 INFO 31221  [[JPASalesForce]] salesforce-route : Message Processed in AccountProcessor! Posting RequestBody to Salesforce :: SAPLING SAPLING Zombie""} 2018-01-30 21:07:29.410 ERROR 31221  [92963-scheduler] : Failed delivery for (MessageId: on ExchangeId: Exhausted after delivery attempt: 1 caught: Unexpected error \{0:null} executing Message History RouteId ProcessorId Processor Elapsed (ms) [salesforce-route ] [salesforce-route ] ] [ 60064] [salesforce-route ] [doTry6 ] [doTry ] [ 60010] [salesforce-route ] [convertBodyTo1 ] ] [ 0] [salesforce-route ] [process21 ] ] [ 2] [salesforce-route ] [process22 ] ] [ 1] [salesforce-route ] [log35 ] [log ] [ 0] [salesforce-route ] [to8 ] [ 60007] Stacktrace Unexpected error \{0:null} executing [na:1.8.0_66] at [na:1.8.0_66] at [na:1.8.0_66] at [na:1.8.0_66] at [na:1.8.0_66] at [na:1.8.0_66] at [na:1.8.0_66] Caused by: Total timeout 60000 ms elapsed ... 8 common frames omitted 2018-01-30 21:07:29.411 WARN 31221  [[JPASalesForce]] : Execution of JMS message listener failed. Caused by: - Unexpected error \{0:null} executing Unexpected error \{0:null} executing [na:1.8.0_66] at [na:1.8.0_66] at [na:1.8.0_66] Caused by: Unexpected error \{0:null} executing ~[na:1.8.0_66] at ~[na:1.8.0_66] at ~[na:1.8.0_66] at ~[na:1.8.0_66] ... 3 common frames omitted Caused by: Total timeout 60000 ms elapsed ... 8 common frames omitted",non_debt,-
camel,12216,comment_0,Please start a thread in the users mailing list.,non_debt,-
camel,12216,comment_1,I have published this issue in the mailing list but have not heard any thing yet.,non_debt,-
camel,12216,comment_2,We are seeing issues with GET's and Patch as well : A sample stack : Unexpected error \{0:null} executing,non_debt,-
camel,12216,comment_3,can't figure out a fix any where in Jira or in the mailing list,non_debt,-
camel,12216,comment_4,"Please do not reopen issues. You have received reply on the users mailing list here: And please do not use JIRA to ask questions, we use JIRA to track issues and it is difficult enough to use it for that without the added noise of non-issue posts. There are plenty of ways to get prompt response from Q&A sites like or by enlisting the services of [companies that offer commercial We have a helpful guide on [how to get support as that you are encouraged to read.",non_debt,-
camel,12216,comment_5,Link to the exact user mailing list post:,non_debt,-
camel,12290,summary,CxfEndpoint with JDK9:,non_debt,-
camel,12290,description,When I tried to run tests on JDK9 I get:,non_debt,-
camel,12290,comment_0,Can you tell more about how you run your application,non_debt,-
camel,12290,comment_1,SpringBoot 1.5.10.RELEASE + camel-spring-boot + camel-cxf,non_debt,-
camel,12290,comment_2,"Hi , Do you still have this issue now? I assume you should use JDK11 now, and please let me know if you still have this issue with latest camel+cxf. And if you still have issue, please clarify the CXF version(you should use CXF 3.3.x which has JDK 11 support) you are using and a reproducer project is more helpful. Thanks! Freeman",non_debt,-
camel,12290,comment_3,I believe CXF version was related to camel-cxf (which was in version 2.20.1) Unfortunatelly I'm no longer in that project,non_debt,-
camel,12290,comment_4,"Thanks ! Since java9 isn't a LTS java version and CXF 3.3.x has JDK11 support, I'd like to close this issue for now. We can reopen if we do need it in the future. Freeman",non_debt,-
camel,12291,summary,"Blueprint error: ""name is already instanciated as null and cannot be removed""",non_debt,-
camel,12291,description,"The error ""name is already instanciated (sic) as null and cannot be removed"" is thrown when configuring a component in Blueprint using property placeholders. I noticed when trying to migrate my project to Camel 2.20.2 (from 2.17.4) Example: The bean is referenced in a Camel route: Throws exception: Removing the property placeholder fixes the issue: According to Guillaume Nodet, this seems to have been introduced by this pull request: Solution proposed by Guillaume Nodet is to change line 1056 of to I tested this change locally and it seems to work, however I don't know if this can cause other problems.",non_debt,-
camel,12291,comment_0,"The fix for 12251will be in 2.20.3, but it doesn't help",non_debt,-
camel,12291,comment_1,The fix for CAMEL-12251 should not help with original problem - it should just make it visible in logs ;),non_debt,-
camel,12291,comment_2,"Yeah, I was on a train and I messed up with the issue.",non_debt,-
camel,12291,comment_3,"Yeah the proposed solution by gnodet is good, lets use that.",non_debt,-
camel,12330,summary,camel-rabbitmq - Allow to configure connection settings on component level,non_debt,-
camel,12330,description,"The component has no options. But we should allow to configure connection settings etc so you can configure this once. And also maybe let it auto-detect rabbitmq connection factory, so spring-boot users can just configure it the spring-boot way.",non_debt,-
camel,12330,comment_0,First set of commits with work on this. More options needed to be added on component level as well. Added a new to demonstrate some of this.,non_debt,-
camel,12330,comment_1,"Got most of the options added, there is still some more, need to look into which ones that make sense to add",non_debt,-
camel,12399,summary,CxfRsProducer doesn't configure while using the Proxy API,non_debt,-
camel,12399,description,"The CxfRsProducer doesn't configure a on the client while using the Proxy API. When using the HTTP API this is working fine. So when i create an endpoint like this: In case i use the HTTP API the header ""foo"" is sent but not when i use the Proxy API.",non_debt,-
camel,12399,comment_0,I'm working on a PR,non_debt,-
camel,12399,comment_6,patch applied on behalf of Mike Schippers with thanks!,non_debt,-
camel,12414,summary,Empty/null response from netty4-http template producer,non_debt,-
camel,12414,description,"When using netty4-http producer to submit and retrieve data from a remote HTTP service, the response body is always null, even though the service is returning data. I have created a minimal sample project that shows this problem.",non_debt,-
camel,12414,comment_0,Cannot reproduce with latest code.,non_debt,-
camel,12414,comment_1,And with text content type,non_debt,-
camel,12414,comment_2,"Okay it works fine with an unit test that does not use pax-exam. Its some weird osgi stuff, it may not start/run properly via pax-exam inside the osgi/karaf container with the unit test.",code_debt,low_quality_code
camel,12443,summary,camel-salesforce - Ability to use 'key' directly instead of pointing resource for,non_debt,-
camel,12443,description,"Hello, Could you extend the way of using JWT OAUTH2.0 authorisation for Salesforce component? For the current moment it uses resource uri (path to file, classpath, URL), but we want to use key directly (from String variable): String { ... PrivateKey key = (PrivateKey) .... We would like to have possibility to set the key directly from String variable, so there will be new way how to configure JWT for SF: String keyValue=""This is some key content""; loginConfig = new <-- new option Thanks, Andrey",non_debt,-
camel,12443,comment_0,"Hi , thank you for raising this issue. Can you explain what content would be set in the {{keyValue}} in your example? I'm assuming PKCS#8 or SSLeay encoded RSA key? And why using {{KeyStore}} via doesn't work in your case?",non_debt,-
camel,12443,comment_1,"Hi Zoran, It can be PCCS#12. The reason of this request is that it is not always possible to keep certificate in the filestystem, classpath or some URL. It can be stored in db as a content. When we get this cert from db we want to pass it to config.",non_debt,-
camel,12443,comment_2,"{{KeyStore}} is Java default and preferred interface for fetching key material and there is a PKCS#12 keystore implementation shipped with JDK (now the default instead of JKS). Could you not provide that KeyStore implementation with the URL with returning an InputStream based on the content of the database? If we were to add support for specifying key as a String value, we would still need to provide the format and the password for that key, this would complicate the implementation without any real need as you could use the {{KeyStore}} API to accomplish the same.",non_debt,-
camel,12443,comment_3,We were able to override keystoreparameters and stream to use our file.,non_debt,-
camel,12624,summary,ActiveMQ Artemis AMQP integration issue with topic prefix hardcode,non_debt,-
camel,12624,description,"Currently we are running Camel AMQP component against Active MQ 5 (Amazon MQ) we want to move to an Artemis solution but this hasn't worked seamlessly. In CAMEL-9204 I believe a hardcoded topic prefix of ""topic://"" was introduced I think for a workaround of an Active MQ 5 bug. In Artemis this means Camel connects to a topic named instead of and therefore receives no events. The only possible workaround is to manually create the connection factory which means then the topic prefix is not set. My believe is that the hardcoded topic prefix is a bug and should be removed or an option to override at the AMQP component level should be introduced. Bug location: AMQPComponent.java line 59 Workaround:",design_debt,non-optimal_design
camel,12624,comment_0,"hardcoded topic prefix is not a bug and need for backward compatibility with ActiveMQ 5. Yes, it's possible to add property to the which will skip setTopicPrefix if needed.",non_debt,-
camel,12624,comment_6,Thanks for the quick action . I do believe newer versions of Active MQ no longer need the topic prefix but agree that maintaining backwards compatibility is on balance more important than a seamless experience with the Artemis replacement.,non_debt,-
camel,12625,summary,Create FHIR Camel component with example,non_debt,-
camel,12625,description,Create Camel component to integrate with endpoints. I'll submit a PR,non_debt,-
camel,12646,summary,camel-spring-boot - Auto configuration of complex types should be more tooling friendly,design_debt,non-optimal_design
camel,12646,description,"If you have complex types like and wants to allow to configure this via spring boot autoconfiguration in - then the generated spring boot classes with all the options will use getter/setter of types That seems correct, but the spring-boot tooling itself (that generates additional json file) will skip those as it only support primitives and string types. So we may need to fool, and generate the getter/setter as String type as you use it for configuring it as a bean reference by id anyway, eg = #myDataSource We can add in the javadoc that the type is",design_debt,non-optimal_design
camel,12646,comment_0,This is the generated source code snippet,non_debt,-
camel,12646,comment_1,And these are the options in the file Notice how the dataSource option is missing.,non_debt,-
camel,12646,comment_2,"The screenshot shows now with the data-source included, and so in the generated file by spring-boot",non_debt,-
camel,12713,summary,relative paths can remove scheme from xslt URI,non_debt,-
camel,12713,description,"When using relative paths in an XSLT referenced from another one, the used URI scheme may be ignored. e.g. in case of an XSLT specified by URI that includes another one: <xsl:include In this case, the URI of child.xsl is resolved as just ""child.xsl"" instead of omitting the URI scheme. The cause is XsltUriResolver, in which is used to calculate the relative URI, but this call ignores the colon separator and considers as a single path part. I'm creating a pull request which passes the URI without scheme to",non_debt,-
camel,12713,comment_4,Thanks for reporting and the PR,non_debt,-
camel,12724,summary,Simple SFTP-to-File integration with charset options fails,non_debt,-
camel,12724,description,"Simple SFTP-to-File integrations with {{charset}} conversion like: fails to output a file in correctly. Depending on the combinations, it sometimes converts the charset wrongly and sometimes it simply doesn't output a file to the target dir. The root cause is that {{SftpOperations}} puts instead of {{byte[]}} or {{InputStream}} to the exchange file body when retrieving a file: which then results in no converter from {{OutputStream}} to {{java.io.Reader}} being found in downstream, and thus the File producer handles a {{RemoteFile}} awkwardly when outputting a file.",non_debt,-
camel,12744,summary,Restlet when used as client doesn't use the configured SSL properties,non_debt,-
camel,12744,description,We're missing configuration options needed for the Restlet client to use the configured SSL properties.,non_debt,-
camel,12744,comment_0,"Is this implemented, if so this ticket can ber resolved",non_debt,-
camel,12930,summary,Ability to execute DML statements in Google Bigquery component,non_debt,-
camel,12930,description,Now the component hasn't ability to execute standard SQL statements. It will be usefull if one could to send theese staments like the sql component do. It possible by executing Google API: bigquery.jobs.query,non_debt,-
camel,12930,comment_0,Added new component: google-bigquery-sql (see patch),non_debt,-
camel,12930,comment_1,"I'll take a look at your patch ASAP. Usually the preferred way for this kind of stuff is a PR on github to be able to review on the platform, but no problem. Thanks for the contribution.",non_debt,-
camel,12930,comment_2,I'd created the PR:,non_debt,-
camel,12930,comment_3,Merged to master.,non_debt,-
camel,12958,summary,Wrong camel context bound in service registry of jbpm/Kie Server,non_debt,-
camel,12958,description,Wrong camel context is set in the ServiceRegistry of jBPM when creating kie container specific camel context.,non_debt,-
camel,13031,summary,camel-core - test failure:,non_debt,-
camel,13031,description,"[INFO] Running [ERROR] 7, 0, Errors: 6, Skipped: 0, 123.022 s <<< FAILURE! - in [ERROR] 20.529 s <<< ERROR! Assertion condition defined as a lambda expression in expected:<1 at Caused by: expected:<1 at [ERROR] 20.058 s <<< ERROR! Assertion condition defined as a lambda expression in expected:<1 at Caused by: expected:<1 at [ERROR] 20.078 s <<< ERROR! Assertion condition defined as a lambda expression in expected:<1 at Caused by: expected:<1 at [ERROR] 20.067 s <<< ERROR! Assertion condition defined as a lambda expression in expected:<1 at Caused by: expected:<1 at [ERROR] 20.095 s <<< ERROR! Assertion condition defined as a lambda expression in expected:<1 at Caused by: expected:<1 at [ERROR] 20.026 s <<< ERROR! Assertion condition defined as a lambda expression in expected:<1 at Caused by: expected:<1 at [INFO] [INFO] Results: [INFO] [ERROR] Errors: [ERROR] [ERROR] Run 1:  ConditionTimeout Assertion ... [ERROR] Run 2:  ConditionTimeout Assertion ... [ERROR] Run 3:  ConditionTimeout Assertion ... [INFO] [ERROR] [ERROR] Run 1:  ConditionTimeout Assert... [ERROR] Run 2:  ConditionTimeout Assert... [ERROR] Run 3:  ConditionTimeout Assert... [INFO] [INFO] [ERROR] 3, 0, Errors: 2, Skipped: 0",non_debt,-
camel,13031,comment_0,What OS and JDK are you using?,non_debt,-
camel,13031,comment_1,"Windows + CentOS $ java -version java version ""1.8.0_172"" Java(TM) SE Runtime Environment (build 1.8.0_172-b11) Java HotSpot(TM) Client VM (build 25.172-b11, mixed mode)",non_debt,-
camel,13031,comment_2,Which OS does this test fail every time?,non_debt,-
camel,13031,comment_3,on both Windows + CentOS VM,non_debt,-
camel,13031,comment_4,Does anyone else get this problem on your platforms?,non_debt,-
camel,13031,comment_5,"I got the same errors with master (Camel 3.x branch). Initially I thought it's a JDK issue so I tested both Oracle JDK and Open JDK but both failed for me. I'm also on CentOS 7 so is it a platform-specific issue, something like it runs only on Mac well?",non_debt,-
camel,13031,comment_6,It may be that the file system is slow to trigger events of file changes as it uses that new file watcher api from the jdk. Try to use a higher timeout than the 20 sec in the tests to see if they run faster.,non_debt,-
camel,13111,summary,Add missing spring boot auto configuration doc markers in the adoc files,documentation_debt,low_quality_documentation
camel,13111,description,"I noticed that we dont have in some component docs, the spring boot START END markers for the SB auto configuration docs we see WARNs when building SB starter JARs",documentation_debt,low_quality_documentation
camel,13111,comment_0,This is a good starter for contributing.,non_debt,-
camel,13111,comment_1,I'd be happy to take a look at this one.,non_debt,-
camel,13111,comment_2,Linking the PR.,non_debt,-
camel,13111,comment_3,PR Link,non_debt,-
camel,13111,comment_4,Is there something else to do on this?,non_debt,-
camel,13111,comment_5,": No sir. I think I had a misspelling on the PR, so the bot didn't pick anything up. This ticket is done. Sorry for the confusion.",documentation_debt,low_quality_documentation
camel,13111,comment_6,Thanks a lot for this contribution! :-),non_debt,-
camel,13154,summary,running error,non_debt,-
camel,13154,description,"When running the spring-boot-master, as we use the by default, I get the port is ready bound error when I start another instance. As the example doesn't use the web server anymore, the fix could we just remove the",non_debt,-
camel,13154,comment_0,Applied the patch into master and camel-2.23.x branches.,non_debt,-
camel,13154,comment_1,Can you please backport on camel-2.x branch too? It's the main maintenance branch for 2.x now,non_debt,-
camel,13154,comment_2,"Sure,  I just backported it to camel-2.x branch.",non_debt,-
camel,13203,summary,Check failing test,non_debt,-
camel,13203,description,Actually it is ignored.,non_debt,-
camel,13203,comment_0,could you please explain in bit detail what needs to be done,non_debt,-
camel,13203,comment_1,"MDC doesn't seem to work on camel 3, the test is related to this.",non_debt,-
camel,13214,summary,camel-mail: Add option to component level,non_debt,-
camel,13214,description,"Currently it's not possible to change/customize the for all camel-mail endpoints because it has to be configured on endpoint level. In the cxf component it's possible to configure it on component level too, so it's possible to provide a global Example: It would be an improvement to add the option also to component level.",non_debt,-
camel,13214,comment_0,"It should not be moved, but be there both places. Then endpoint override component level, this is how its done in other components.",design_debt,non-optimal_design
camel,13214,comment_1,I've edited the issue.,non_debt,-
camel,13214,comment_2,Pull request created.,non_debt,-
camel,13406,summary,"camel-pulsar - ""|"" in endpoint syntax breaks components adoc table",non_debt,-
camel,13406,description,Camel Pulsar endpoint syntax defined in: breaks component list table in as {{|}} is the column separator in asciidoc.,non_debt,-
camel,13406,comment_0,Very fast. Thanks :-),non_debt,-
camel,13406,comment_1,I'm regenerating the components readme now.,non_debt,-
camel,13533,summary,camel-salesforce: integration tests fail because salesforce answer is not valid xml,non_debt,-
camel,13533,description,"Some salesforce API response are not valid xml because xml element name contains space whereas they should not. For instance, the was failing because the response contained: <Ant Migration Tool ... </Ant Migration Tool",non_debt,-
camel,13533,comment_0,Xml related tests are now ignored for composite batch api,test_debt,low_coverage
camel,13637,summary,be able to enable access log for camel-undertow consumer endpoint,non_debt,-
camel,13637,description,add a option/flag so we can configure camel-undertow consumer endpoint to enable access log per each request,non_debt,-
camel,13637,comment_0,"an access log entry for one request looks like ""2019-06-12 10:07:54,919 [XNIO-18 task-1 ] INFO accesslog - 127.0.0.1 - - -0400] ""POST /foo HTTP/1.1"" 200 4""",non_debt,-
camel,13637,comment_1,"It would be awesome, if AccessLogReceiver could be pluggable bean in registry, so the access log entry can be postprocessed (and maybe redirected to another route and postprocessed eg with camel-grok).",non_debt,-
camel,13637,comment_2,"Hi , Yup, the AccessLogReceiver is pluggable. Cheers Freeman",non_debt,-
camel,13681,summary,camel-main - Allow ENV variables to configure any option,non_debt,-
camel,13681,description,Any of the options you can configure via such as: camel.main.name And so on should be configurable via ENV variables which will override any existing configuration. This is good practice in containers and also how SB can do etc.,design_debt,non-optimal_design
camel,13681,comment_0,We also need to add support for case insensitive keys all the way - so you can lookup setter methods when they are referred from ENV where its all upper cased,non_debt,-
camel,13682,summary,Wrong bean injection from OSGI,non_debt,-
camel,13682,description,"We issued strange behaviour with our OSGI container setup using services - references and injecting services into out beans using @BeanInject *bundle1* *bundle2* *- From this point, I would expect that OSGI serves only one specific implementation of interface by specifying component-name attribute but it does not. As a workaround we managed to inject the reverence using setter or specifing name in @BeanInject annotation - I do not know if this is normal behaviour. Please, check this. Camel version 2.16.1",non_debt,-
camel,13682,comment_0,Camel 2.16 is EOL and not supported. Please test with latest version first and report back.,non_debt,-
camel,13682,comment_1,"This is now @BeanInject work and was designed, to lookup beans of a given type from the registry, its not really taking those osgi specifics into account with that you setup in osgi blueprint with the component-name filtering.",non_debt,-
camel,13687,summary,NotifyBuilder not working as expected,non_debt,-
camel,13687,description,"am trying to test an error handling route. The does not work as expected (it always returns false). Created a main route and a test route to test the main route. I used Spring Boot - all other tests work fine, so there is no problem with the setup I guess. I have attached the sample project. Please uncomment the @Ignore method to test the method. Please read the README.md attached in the project",non_debt,-
camel,13687,comment_0,This is working as designed as the fromRoute is from the original route as its stated in the javadoc. To do what you want we need a new fromCurrentRoute that I am adding.,non_debt,-
camel,13700,summary,createRowModel failed to set cell's valueType,non_debt,-
camel,13700,description,"When function has been called, following code should check if `valueType` parameter has been set, if so, then set cell's valueType. However, the code set rowModel's rowType instead of cellModel's valueType, and, IMHO, this is not correct. My endpoint is configurated as following: Thanks!",non_debt,-
camel,13700,comment_0,Ah yeah that seems more correct. You are welcome to provide a PR,non_debt,-
camel,13742,summary,Extend amel-cmis component with new operations,non_debt,-
camel,13742,description,Camel-cmis supports only read and create operation. I would like to extend it with the following operations: - Delete folder - Delete document - Move folder - Move document - Rename folder - Rename document - Copy folder - Copy document - CheckIn - CheckOut - CancelCheckOut,non_debt,-
camel,13742,comment_0,PR:,non_debt,-
camel,13866,summary,Disruptor consumers are not being invoked when using and,non_debt,-
camel,13866,description,"I'm having a problem with disruptor consumers not being called (receiving messages) when switching from camel 2.17.1 to 2.24.1+ in a spring app. The code works fine with 2.17. To recreate issue clone and: mvn test (fails) edit pom.xml so camel dep is 2.17.1 mvn test (ok) alternatively keep camel 2.24 dep. and edit TestDisruptor.java and uncomment {{context.start()}} line and do mvn test (ok). It seems like calling on a context that is not yet started stopped working in 2.18 and onwards. Playing with autoStartup=""false"" and doing does not help which is odd as doing a manual context start when autoStartup=""true"" does work. With 3.0.0-M4 it does not work either but I do get a CamelContext is stopped when trying to create the consumer.",non_debt,-
camel,13866,comment_0,Workaround is to use a startup listener.,non_debt,-
camel,13866,comment_1,You cannot use ConsumerTemplate and ProducerTemplate on stopped CamelContext. It is expected and was implemented in CAMEL-13555.,non_debt,-
hadoop,85,summary,a single client stuck in a loop blocks all clients on same machine,non_debt,-
hadoop,85,description,"I was running a set of clients, running cp from one dfs to another dfs using fuse, one file per cp process. One client got stuck in a loop because of a bad block in one of the files (separate bug filed). The other clients, running in parallel in the background, all stopped making progress on their respective files.",non_debt,-
hadoop,85,comment_0,Fixing HADOOP-83 resolves this issue.,non_debt,-
hadoop,85,comment_1,Fixing HADOOP-83 resolves this issue.,non_debt,-
hadoop,93,summary,allow minimum split size configurable,non_debt,-
hadoop,93,description,"The current default split size is the size of a block (32M) and a SequenceFile sets it to be We currently have a Map/Reduce application working on crawled docuements. Its input data consists of 356 sequence files, each of which is of a size around 30G. A jobtracker takes forever to launch the job because it needs to generate 356*30G/2K map tasks! The proposed solution is to let the minimum split size configurable so that the programmer can control the number of tasks to generate.",non_debt,-
hadoop,93,comment_0,"With such big input files the default logic should split things into dfs block-sized splits. Smaller splits should only be used if this would result in fewer than mapred.map.tasks splits. What value do you have for mapred.map.tasks in your mapred-default.xml? Let's make sure that is working before we add a new min.split.size feature. I don't oppose the feature, but it should be generating 356*30G/32M splits, not 356*30G/2K splits as you claim. That's still a lot of splits. If it is too many then we should add the feature you're adding. Note that, as a workaround, it is also easy to implement this w/o patching by defining an InputFormat that subclasses InputFormatBase and specifies a different minSplitSize. But making that a long is a good idea. So, in summary, can you please confirm that the actual number of splits that you object to is 356*30G/32M splits, not 356*30G/2K? Thanks.",design_debt,non-optimal_design
hadoop,93,comment_1,"From what I've seen, it is always 32M fragments, but that is still 300k input splits/maps, which is a lot. We'd like to be able to drop that by an order of magnitude. (I think in this case that the input splitter never finished, so we don't know.)",design_debt,non-optimal_design
hadoop,93,comment_2,"Doug, you are right. The number of splits we got was 356*30G/32M, but still too many.",design_debt,non-optimal_design
hadoop,93,comment_3,Updated patch,non_debt,-
hadoop,93,comment_4,"Okay, I have applied this. For the record, patches are easier to apply if they are made from the root of the project. Also, new config properties should generally be added to hadoop-default.xml. Finally, the cast added in was not required.",non_debt,-
hadoop,95,summary,dfs validation,non_debt,-
hadoop,95,description,"Dfs needs a validation operation similiar to fsck, so that we get to know the files that are corrupted and which data blocks are missing. Dfs namenode also needs to log more specific information such as which block is replication or is deleted. So when something goes wrong, we have a clue what has happened.",design_debt,non-optimal_design
hadoop,95,comment_0,"I posted a similar complaint in HADOOP-35, including a patch to make it possible to report on individual file health from the dfs -ls command. I agree that something needs to be added, if only to facilitate debugging of the various block-loss problems I've been seeing.",design_debt,non-optimal_design
hadoop,95,comment_1,"we're thinking to have a name-server call that: 1. checks internal consistency 2. for each block, verifies that it actually exists on some data node 3. (optional, perhaps later) ask each data node to actually read its blocks and check their validity",non_debt,-
hadoop,95,comment_2,I vote for a dedicated thread on each name node using 1% of disk IO to validate blocks continuously. Low cost and uncovering bad blocks is high value. This would also let you collect disk stats reliability to determine if the disks were bad.,non_debt,-
hadoop,95,comment_3,"This map-reduce test reads all blocks of all files, and detects which of them are missing or corrupted. See HADOOP-101 discussion.",non_debt,-
hadoop,95,comment_4,"Posting here a similar performance test, which is related to HADOOP-72 Just because these two tests have too much in common. TestDFSIO measures performance of the cluster for reads and writes.",code_debt,duplicated_code
hadoop,95,comment_5,Is this issue still pertinent? We have an fsck utility and TestDFSIO has been committed via a separate JIRA issue.,non_debt,-
hadoop,95,comment_6,"I think Eric's comment is still important. But, probably a new issue to bring up. Ongoing are still problems - the ""fsck"" utility just covers the top-level problem.",non_debt,-
hadoop,95,comment_7,This was fixed by HADOOP-101,non_debt,-
hadoop,98,summary,The JobTracker's count of the number of running maps and reduces is wrong,non_debt,-
hadoop,98,description,"When a heatbeat comes in from a task tracker, the job tracker just adds the number of currently running maps and reduces. The jobs from the previous heartbear are never subtracted. This causes the scheduling to misjudge the ""loading"" levels of the task trackers.",non_debt,-
hadoop,98,comment_0,"This patch consolidate all of the places where status was put in to or out of the taskTrackers status map, so that the counts of maps and reduces are maintained consistently. I also added the cluster status to the webapp so that you can get a quick view of how busy the cluster is.",design_debt,non-optimal_design
hadoop,98,comment_1,"This looks great. I just committed it. Thanks, Owen.",non_debt,-
hadoop,188,summary,more unprotected RPC calls in JobClient.runJob allow loss of job due to timeout,non_debt,-
hadoop,188,description,"I fixed one of the RPC calls in JobClient.runJob, but I missed a couple of others.",non_debt,-
hadoop,188,comment_0,"This patch puts the try block around the entire body of the loop, so that if any IOExceptions are thrown in the runJob loop, the user gets a log message and a few retries before their job is killed.",non_debt,-
hadoop,188,comment_1,"I just committed this. Thanks, Owen.",non_debt,-
hadoop,225,summary,tasks are left over when a job fails,non_debt,-
hadoop,225,description,"when jobs are stopped or otherwise fail, tasks are often left around. the job tracker shows that there are map or reduce (mostly reduce) tasks running, when no job is running. these accumulate over time. eventually there are so many of those, that the job tracker can't launch new tasks, requiring a restart of the MR cluster.",non_debt,-
hadoop,225,comment_0,Has anyone seen this recently or can we close it?,non_debt,-
hadoop,225,comment_1,Not sure: - I haven't been monitoring this as much lately. - JT crashes too frequently to get a decent statistic Let's check in a few days Yoram,non_debt,-
hadoop,225,comment_2,We haven't seen this in a long time.,non_debt,-
hadoop,273,summary,Add a interactive shell for admistrative access to the DFS,non_debt,-
hadoop,273,description,Implement a shell that allows the user to work in the dfs like on the local fs. cd /user/ cd test put text.txt someText.txt cat someText.txt rm someText.txt,non_debt,-
hadoop,273,comment_0,Patch implements the interactive shell.,non_debt,-
hadoop,273,comment_1,The new and corrected version,non_debt,-
hadoop,273,comment_2,Added interactive DFS Shell,non_debt,-
hadoop,273,comment_3,i dont understand the motivation behind this since all of this can be done in a bash script. Marco can you explain what would be use case for this ?,non_debt,-
hadoop,273,comment_4,I don't care anymore. Please close this request. Thanks.,non_debt,-
hadoop,273,comment_5,I don't care anymore.,non_debt,-
hadoop,273,comment_6,"I still care! :) Alex Loddengaard and I wrote a simple command shell that uses jline (a readline implementation) to create a simple shell that includes some handy features like tab completion for HDFS. This supports access to the various FsShell commands in an interactive fashion. There's a definite need for something like this -- executing several {{hadoop fs ...}} commands in a bash script or in a regular interactive shell is very time-consuming due to the overhead of starting Java for each such command. Putting all the commands into a single JVM instance is a major win. When using Hadoop from outside of Java, programs that wish to interact with the DFS may need to run several commands through the {{hadoop fs ...}} interface, which is very slow. This shell: * Supports interactive use of HDFS (with a reasonable notion of a ""current directory"", etc) * Supports executing scripts of several commands External programs could write a script of several DFS operations and batch them up for execution in a single Java process. This changes some of the methods (e.g., {{ls()}}) from FsShell to be public instead of package-public so that their code can be reused here. We tested CmdShell by running the various commands locally, as well as testing short scripts of the commands included together. Some basic functionality unit tests are also included in this patch. If a committer could please re-open this issue and take a look at the attached code, I'd appreciate it.",non_debt,-
hadoop,273,comment_7,"Apparently there's no ""reopen"" command any more. Moved to HADOOP-6541.",non_debt,-
hadoop,285,summary,Data nodes cannot re-join the cluster once connection is lost,non_debt,-
hadoop,285,description,"A data node looses connection to a name node and then tries to offerService() again. HADOOP-270 changes force it to start dataXceiveServer, which is already started and in this case throws which goes on in a loop, and never reaches the heartbeat section. So the data node never re-joins the cluster, while from the out side it looks it's still running. This is another reason why we see missing data, and don't see failed data nodes.",non_debt,-
hadoop,285,comment_0,"This patch starts the data receiver thread in ""run"" instead of ""offerservice"". So it will not be restarted after a connect is lost.",non_debt,-
hadoop,285,comment_1,"I just committed this. Thanks, Owen!",non_debt,-
hadoop,289,summary,Datanodes need to catch and,non_debt,-
hadoop,289,description,- Datanode needs to catch when registering otherwise it goes down the same way as when the namenode is not available (HADOOP-282). - need to be caught for all non-registering requests. The data node should be shutdown in this case. Otherwise it will loop infinitely and consume namenode resources.,code_debt,low_quality_code
hadoop,289,comment_0,"This patch fixes the two problems described. I placed all registration logic inside the seems more logical to me. There is also a simple null value checkup included for FSNamesystem, didn't want to create a separate issue for that.",non_debt,-
hadoop,289,comment_1,"This patch causes unit tests to fail for me. For example, TestLocalDFS fails with: 2006-06-08 12:56:54,423 INFO ipc.Client - Client connection to 127.0.0.1:65312: starting 2006-06-08 12:56:54,432 INFO ipc.Server - Server handler 0 on 65312 call error: Unexpected version of data node reported: 0. Expecting = -2. Unexpected version of data node reported: 0. Expecting = -2. Method) 2006-06-08 12:56:55,370 INFO conf.Configuration - parsing 2006-06-08 12:56:55,390 INFO conf.Configuration - parsing 2006-06-08 12:56:55,395 WARN fs.FSNamesystem - Replication requested of 1 is larger than cluster size (0). Using cluster size. 2006-06-08 12:56:55,395 WARN dfs.StateChange - DIR* failed to create file on client hadoop because target-length is 0, below MIN_REPLICATION (1) 2006-06-08 12:56:55,396 INFO ipc.Server - Server handler 1 on 65312 call error: failed to create file on client hadoop because target-length is 0, below MIN_REPLICATION (1) failed to create file on client hadoop because target-length is 0, below MIN_REPLICATION (1) Method)",non_debt,-
hadoop,289,comment_2,Resubmitted the patch under Not failing this time. Sorry.,non_debt,-
hadoop,289,comment_3,Please replace the getLocalizedMessage and implicit toString with calls to which includes both the message and the call stack. The call stack helps a lot in finding and debugging the problem.,code_debt,low_quality_code
hadoop,289,comment_4,"I just committed this. Thanks, Konstantin.",non_debt,-
hadoop,304,summary,message correction,non_debt,-
hadoop,304,description,None,non_debt,-
hadoop,304,comment_0,should report the expected node name rather than its storage id.,non_debt,-
hadoop,304,comment_1,"I just committed this. Thanks, Konstantin.",non_debt,-
hadoop,305,summary,tasktracker waits for 10 seconds for asking for a task.,non_debt,-
hadoop,305,description,the tasktracker should ask for a job as soon as a job running on it is finished.,non_debt,-
hadoop,305,comment_0,"This patch lets the tasktracker ask for a new task as soon as a task is finished, else it just waits for 10 seconds to get a new task. This is part of Ben's patch submitted to HADOOP-249. This patch also combines the patch to HADOOP-283. Thanks Ben for your help in integrating the patch.",non_debt,-
hadoop,305,comment_1,"I just committed this. Thanks, Mahadev.",non_debt,-
hadoop,376,summary,Datanode does not scan for an open http port,non_debt,-
hadoop,376,description,The DataNode does not scan for an open http port. Only the singleton servers are allowed to have required port assignments.,non_debt,-
hadoop,376,comment_0,This patch enables the port scan.,non_debt,-
hadoop,376,comment_1,"I just committed this. Thanks, Owen.",non_debt,-
hadoop,400,summary,the job tracker re-runs failed tasks on the same node,non_debt,-
hadoop,400,description,"The job tracker tries not to run tasks that have previously failed on a node on that node again, but it doesn't strictly prevent it. I propose to change the rule so that when pollForNewTask is called by a TaskTracker, the JobTracker will only assign it a task that has failed on that TaskTracker, if and only if it has already failed on the entire cluster. Thus, for ""normal"" clusters with more than 4 TaskTrackers, you will be guaranteed that it will run on 4 different TaskTrackers. For small clusters, it will run on every TaskTracker in the cluster at least once. Does that sound reasonable to everyone?",design_debt,non-optimal_design
hadoop,400,comment_0,"I do see one possible hole. If a machine loses its TaskTracker, it gets a new one. Can we arrange for the new TaskTracker to inherit the task failures from its predecessor? That would be a bit hard ... but for this to work at all the tasks have to know what TaskTrackers they've flunked on. All the TaskTracker has to know is who its predecessors are to refuse tasks that have flunked on its TaskTracker site [usually, on its machine]. -dk",non_debt,-
hadoop,400,comment_1,This patch does: 1. It limits each TaskTracker to running this will prevent the problem that we saw where the last 2 reduces scheduled were put on the same node rather than different empty ones 2. It refactors obtainNewMapTask and obtainNewReduceTask to call a common utility function. It also replaces the two parallel loops with one. 3. Only allowed tasks that have failed on this task tracker to run if we have exhausted the cluster.,non_debt,-
hadoop,400,comment_2,"I just committed this. Thanks, Owen!",non_debt,-
hadoop,453,summary,bug in Text.setCapacity( int len ),non_debt,-
hadoop,453,description,There seems to be a bug in o.a.h.i.Text class in methd setCapacity( int len ) attached patch demonstrates the issue with unit test and fixes the problem.,non_debt,-
hadoop,453,comment_0,"I just committed this. Thanks, Sami!",non_debt,-
hadoop,481,summary,Hadoop mapred metrics should include per job input/output statistics rather than per-task statistics,non_debt,-
hadoop,481,description,"Currently hadoop reports metrics such as input bytes, input records, etc on per-task basis. Accurate aggregation of these metrics is required at the job-level and reporting should be done on a per-job basis.",non_debt,-
hadoop,481,comment_0,"Patch attached. It changes the (progress method has additional parameter) and TaskTrackerStatus has an added field), therefore their versions have been bumped up.",non_debt,-
hadoop,481,comment_1,"Patch attached. It changes the (progress method has additional parameter) and TaskTrackerStatus has an added field), therefore their versions have been bumped up.",non_debt,-
hadoop,481,comment_2,Instead of passing a long[] you should pass a struct that implements Writable. Probably TaskMetrics would be a good name for this.,code_debt,low_quality_code
hadoop,481,comment_3,Allright.I will resubmit the patch in a day or two.,non_debt,-
hadoop,481,comment_4,Here is the updated patch.,non_debt,-
hadoop,481,comment_5,patch submitted.,non_debt,-
hadoop,481,comment_6,"Shouldn't we use our existing metrics API for stuff like this? As with HADOOP-492, it seems like the TaskTracker and JobTracker should implement the MetricsContext API, providing a MetricsRecord factory. These can be used by the MapReduce kernel code for the metrics desired here, and in supplied to user code the uses in HADOOP-492. We might even need to write a multiplexing MetricsContext, that can send metrics to both the JobTracker and to, e.g., Ganglia. But we should not be adding new metrics APIs when we already have one. If the current metrics API is somehow inappropriate, let's fix that instead of create another.",non_debt,-
hadoop,481,comment_7,"FWIW, HADOOP-954 has changed all task related statistics to be per user (the user that submitted the job). Currently, these counters are aggregated per user: map_input_bytes map_input_records map_output_bytes map_output_records shuffle_input_bytes",non_debt,-
hadoop,481,comment_8,This also is already done.,non_debt,-
hadoop,507,summary,Runtime exception in when trying to startup namenode/datanode,non_debt,-
hadoop,507,description,"Here's the logs: arun@neo $ cat 2006-09-05 22:18:39,756 INFO conf.Configuration - parsing 2006-09-05 22:18:39,804 INFO conf.Configuration - parsing 2006-09-05 22:18:39,918 INFO util.Credential 22:18:39,935 INFO http.HttpServer - Version Jetty/5.1.4 2006-09-05 22:18:40,366 INFO util.Container - Started 2006-09-05 22:18:40,478 INFO util.Container - Started 2006-09-05 22:18:40,478 INFO util.Container - Started 2006-09-05 22:18:40,479 INFO util.Container - Started 2006-09-05 22:18:40,485 INFO http.SocketListener - Started SocketListener on 0.0.0.0:50070 2006-09-05 22:18:40,487 INFO util.Container - Started Exception in thread ""main"" Class can not access a member of class with modifiers ""public"" Caused by: Class can not access a member of class with modifiers ""public"" ... 8 more Steps to reproduce: 1. Start namenode/datanode 2. Run hdfs_test program (part of libhdfs) 3. Stop namenode/datanode 4. goto step 1",non_debt,-
hadoop,507,comment_0,This is yet another instance of the class permission problems. This patch changes the non-factory case to call which has the class initialization code.,non_debt,-
hadoop,507,comment_1,"Works great for me Owen, thanks!",non_debt,-
hadoop,507,comment_2,"I just committed this. Thanks, Owen!",non_debt,-
hadoop,508,summary,random seeks using FSDataInputStream can become invalid such that reads return invalid data,non_debt,-
hadoop,508,description,"Some of my applications using Hadoop DFS receive wrong data after certain random seeks. After some investigation I believe (without looking at source code of that it basically boils down to the fact that the method read(byte[] b, int off, int len), when called with an external buffer larger than the internal buffer, reads into the external buffer directly without using the internal buffer anymore, but without invalidating the internal buffer by setting the variable 'count' to 0 such that a subsequent seek to an offset which is closer to the 'position' of the Positioncache than the internal buffersize will put the current position into the internal buffer containing outdated data from somewhere else.",design_debt,non-optimal_design
hadoop,508,comment_0,Fixed.,non_debt,-
hadoop,508,comment_1,Fixed.,non_debt,-
hadoop,508,comment_2,"I just committed this. Thanks, Milind!",non_debt,-
hadoop,511,summary,mapred.reduce.tasks not used,code_debt,dead_code
hadoop,511,description,After some testing with nutch I found problem with ArithmeticException in partition function because numReduceTasks came in 0. After seting mapred.reduce.tasks in hadoop-site.xml to same value as default everything works. This bug disappear in SVN version of hadoop I this known issue. Please check also NUTCH-361 for detail explanation.,non_debt,-
hadoop,511,comment_0,"This was mostly misconfiguration by having the site file override the job's wishes. Note that now reduces = 0 is not an error, but rather specifying that you don't need a sort.",non_debt,-
hadoop,524,summary,Contrib documentation does not appear in Javadoc,documentation_debt,low_quality_documentation
hadoop,524,description,Documentation for contrib modules like streaming and the small jobs benchmark does not appear in the Javadoc.,documentation_debt,low_quality_documentation
hadoop,524,comment_0,Duplicate of HADOOP-371,non_debt,-
hadoop,537,summary,clean-libhdfs target of build.xml does not work on windows,non_debt,-
hadoop,537,description,"It produces the following: BUILD FAILED Execute failed: CreateProcess: make clean error=2 Besides, I would propose to have clean-* target for every compile-* target in build.xml. Some people probably don't build libhdfs or contrib, so why should they clean it.",code_debt,low_quality_code
hadoop,537,comment_0,"We shouldn't need a clean-X target for every compile-X target, since compile-X targets should place things in the build directory and the build directory should be removed by the single clean target. We should also try not to clutter the top-level build.xml. If a separate clean-X target is required for productive development in some subtree, then it should be placed in a separate build.xml or Makefile in that source subtree. It can then remove appropriate items from the top-level build directory. But, ideally, the top-level 'clean' target should be able to remove all generated items by simply removing the top-level build directory.",code_debt,low_quality_code
hadoop,537,comment_1,"This is an orthogonal approach but I like it too. In this case we should not have any clean-X targets at all. Some more details on this particular issue. The real problem is that ""compile-libhdfs"" creates temporary files in the source directory. Sure they need to be cleaned, but they should not be created there in the first place. I think this have already been discussed, don't remember where.",code_debt,low_quality_code
hadoop,537,comment_2,Here's a patch which ensures that no temporary files are created in the src/c++/libhdfs; including .o/.so etc. They are all put in build/libhdfs. In fact I've ensured that even the doxygen generated docs go to build/libhdfs/docs and can be packaged aptly. Konstantin - can you please confirm that it works for you? thanks. I've kept the 'clean-libhdfs' target around in build.xml so that devs working on libhdfs can use it without needing to invoke the top-level 'clean' which nukes the 'build' directory.,code_debt,low_quality_code
hadoop,537,comment_3,"I just committed this. Thanks, Arun!",non_debt,-
hadoop,543,summary,Error to open job files,non_debt,-
hadoop,543,description,I was running a faily large job on Hadoop release 0.6.2. The job failed because a lot of map tasks failed with following exceptions: Cannot open filename Source) not found Caused by: not found at ... 9 more Cannot open filename Source),non_debt,-
hadoop,543,comment_0,I believe all of the missing job.xml bugs were fixed by HADOOP-639.,non_debt,-
hadoop,551,summary,reduce the number of lines printed to the console during execution,design_debt,non-optimal_design
hadoop,551,description,"In the patch for HADOOP-423, I made the web ui and the job client cli format the percentage done consistently as 0.00 - 100.00%. This has the side effect of making the job client print a lot more lines to the user's console (up to 20k!).",design_debt,non-optimal_design
hadoop,551,comment_0,"This patch adds a parameter to the formatPercent method that specifies the desired number of digits after the decimal. The cli then passes ""0"" and the web ui passes ""2"". What do people think?",non_debt,-
hadoop,551,comment_1,"After thinking about it, I realized that modifying a static was a bad plan, so I just made a new format object in formatPercent.",non_debt,-
hadoop,551,comment_2,"I just committed this. Thanks, Owen!",non_debt,-
hadoop,561,summary,one replica of a file should be written locally if possible,non_debt,-
hadoop,561,description,"one replica of a file should be written locally if possible. That's currently not the case. Copying a 1GB file using hadoop dfs -cp running on one of the cluster nodes, all the blocks were written to remote nodes, as seen by fsck -files -blocks -locations on the newly created file. as long as there is sufficient space locally, a local copy has significant performance benefits.",code_debt,slow_algorithm
hadoop,561,comment_0,This fixesd two bugs in chooseTarget() routine that was causing the local machine to not get picked as a replica,non_debt,-
hadoop,561,comment_1,TThis fixed two bugs in chooseTarget(0 that was causing the local machine to not get oicked as a replica. There was a comparision between UTF8 and String.,non_debt,-
hadoop,561,comment_2,"I just committed this. Thanks, Dhruba!",non_debt,-
hadoop,564,summary,we should use hdfs:// in all API URIs,non_debt,-
hadoop,564,description,"Minor nit, but it seems that we should choose a protocol name that is likely not to conflict with other distributed FS projects. HDFS seems less likely to. Right now this will be trivial to change. Just wanted to socialize this before doing the search and replace. PS right now the dfs: usage is not consistent and has crept into a couple of new features the y! team is working on (caching of files and distcp).",code_debt,low_quality_code
hadoop,564,comment_0,Changed a bunch of dfs: to hdfs:,non_debt,-
hadoop,564,comment_1,"I just committed this. Thanks, Wendy!",non_debt,-
hadoop,595,summary,Browsing content of filesystem over HTTP not works,non_debt,-
hadoop,595,description,"When I click ""Browse the filesystem"" in my browser I get following: An error occurred while loading Could not connect to host (port 65535). In HADOOP 0.6.2 it works properly.",non_debt,-
hadoop,595,comment_0,"it works fine for me, using the default config: namenode uses info port 50070, datanode uses 50075",non_debt,-
hadoop,595,comment_1,"My steps: 1. Unpack hadoop-0.7.0.tar.gz into /root/hadoop-0.7.0 2. Set export JAVA_HOME=/usr in hadoop-env.sh 3. Change hadoop-site.xml: <?xml type=""text/xsl"" <!-- Put site-specific property overrides in this file. -- <configuration <property <name <value <description literal string ""local"" or a host:port for <property <name <value <description at. If ""local"", then jobs are run in-process as a single map and reduce task. <property <name <value <description The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time. </configuration 4. cd 5. ./start-dfs.sh -6. Connect -7. Click ""Browse the filesystem"" - An error occurred while loading Could not connect to host (port 65535).",non_debt,-
hadoop,595,comment_2,Has anyone seen this recently?,non_debt,-
hadoop,595,comment_3,This is a really old bug (0.7.0). Webui is expected to work and there are no known bugs about simple uses. I am closing this.,non_debt,-
hadoop,651,summary,fsck does not handle arguments -blocks and -locations correctly,non_debt,-
hadoop,651,description,"fsck does not pass arguments correctly to the servlet. As a result, only the last argument (except for the path) is passed to the servlet. Patch is forthcoming.",non_debt,-
hadoop,651,comment_0,This fixes the fsck arg-processing bug.,non_debt,-
hadoop,651,comment_1,Patch attached.,non_debt,-
hadoop,651,comment_2,"I just committed this. Thanks, Milind!",non_debt,-
hadoop,659,summary,Boost the priority of re-replicating blocks that are far from their replication target,non_debt,-
hadoop,659,description,"I see two types of replications that should be accelerated compared to all others. 1. Blocks that have only one remaining copy (but are required to have higher replication). 2. Blocks that have less than 1/3 of their replicas in place. The latter occurs when map/reduce sets replication of certain files to 10, and we want it happen fast to achieve better performance on the tasks. So I think we should distinguish two major groups of under-replicated blocks: first-priority (having only 1 copy or less than 1/3 of required replicas), and the rest. The name-node places first-priority blocks into the beginning of the neededReplication list, and the rest are placed at the end. That way the first-priority blocks will be replicated first and then the others.",design_debt,non-optimal_design
hadoop,659,comment_0,why not maintain a total order?,non_debt,-
hadoop,659,comment_1,"If you mean the total order by the number of copies remaining the problem is with the second case. We have 3 healthy copies of the block and we need 10. The total order does not boost priority of these blocks. If you mean the total order by the number of copies missing then we do not specifically focus on blocks having 1 remaining copy. The main disadvantage of any total order would be maintenance costs. Having 2 groups means that I should place nodes either in the beginning of the list or in the end O(1). Having total order will require log n operations for the access, and you cannot use hash since the number of collisions is expected to be large.",non_debt,-
hadoop,659,comment_2,"The purpose of this issue is to avoid losing data. So I will priotize blocks that have only one replica. I'd like to maintain two treesets, one of which contains all the blocks with one replica, while the other contains the rest blocks. Both treesets map blockid to a block. So both add and remove operations are O(logn). I feel that maintaing a total order is not neccessary and expensive.",non_debt,-
hadoop,659,comment_3,"Name-node needs some form of the global map of blocks in order to be able to remove blocks from neededReplications e.g. during block reports. We should also use a parallel list view of blocks that lists them in the priority order. Since we plan to have only 2 priority levels, we can add first-priority items to the beginning of the list and low-priority to the end. That way when we iterate the list we will first choose higher priority blocks automatically. Add and remove to/from the list are both O(1), so the performance remains the same as we have now. Iterating through the list is linear, which is better than what we have now O(n log n). I agree the main purpose of the issue is to replicate blocks that are about to become extinct. But if we can improve map/reduce performance by treating replication of config and jar files similarly, why not do that?",code_debt,slow_algorithm
hadoop,659,comment_4,"I am OK to priotize blocks that have less than 1/3 of their replicas in place. Then I would introduce 3 priority levels. The blocks that have only one replica have the highest priority, followed by blocks having less than 1/3 replicas, and then followed by the rest of the blocks. The data structures that suggested by Konstantin has the performance advantage. But I went through some code design and felt that the data structures and manipulations were quite complicated. It's not a simple solution. I am convinced that it is the right way to go. Besides, adding blocks to the begining or the end of the priority list is not an extensible solution when we have more than 2 priority levels.",design_debt,non-optimal_design
hadoop,659,comment_5,This patch priotizes blocks with only one replica over blocks with less than one third of its replication factor over other under replicated blocks for replication.,non_debt,-
hadoop,659,comment_7,"I just committed this. Thanks, Hairong!",non_debt,-
hadoop,659,comment_8,This patch fixes the unsupported remove method problem.,non_debt,-
hadoop,659,comment_9,"I committed this fix. Thanks, Hairong!",non_debt,-
hadoop,680,summary,checksum errors reading map output,non_debt,-
hadoop,680,description,we get many exceptions complaining about crc errors reading map output by reducers: Checksum error: at 978944,non_debt,-
hadoop,680,comment_0,The crc code is now inlined in the map outputs.,non_debt,-
hadoop,715,summary,build.xml sets up wrong 'hadoop.log.dir' property for 'ant test',non_debt,-
hadoop,715,description,"build.xml - line nos. 329-331 <sysproperty <sysproperty key=""test.src.dir"" <sysproperty value="".""/wrongly overrides 'hadoop.log.dir' causing it to be setup incorrectly.",non_debt,-
hadoop,715,comment_0,Here is a simple fix... I've also renamed the 'hadoop.log.dir' variable in ant to 'test.log.dir' since it is used only in the 'test' target for consistency and to ensure people don't get confused.,code_debt,low_quality_code
hadoop,715,comment_1,1,non_debt,-
hadoop,715,comment_2,"I just committed this. Thanks, Arun!",non_debt,-
hadoop,729,summary,has non-standard sh code,non_debt,-
hadoop,729,description,"uses the shell check ""-e"" which fails on Solaris; this caused a nightly build to fail.",non_debt,-
hadoop,729,comment_0,"Fixed to use ""-d"" instead of ""-e"".",non_debt,-
hadoop,729,comment_1,Can you please verify this Doug? I don't have access to a Solaris box.,non_debt,-
hadoop,729,comment_2,"I verified that the nightly build succeeds with this and committed it. Thanks, Arun!",non_debt,-
hadoop,758,summary,FileNotFound on DFS block file,non_debt,-
hadoop,758,description,While run the sort benchmark a reduce failed with: (No such file or directory) at Method),non_debt,-
hadoop,758,comment_0,"Looking around the code around the stack trace, it is hard to see how this could have happened. We should update the bug if we notice this again. I checked a large sorter benchmark run there does not seem to be anything close to this exception.",non_debt,-
hadoop,758,comment_1,One possible culprit suggested in this case is lack of space under tmp directory. I will try with a very small tmp directory.,non_debt,-
hadoop,758,comment_2,Running 'dfs -copyFromLoccal' with a very small tmp directory correctly fails with 'No space left on the device'. Should run this in a map reduce job.,non_debt,-
hadoop,758,comment_3,"The exception in the bug is that last exception that that occurred. It masks the first exception that would be a better indicator of the problem. ReduceTask.java (around line 313)Looks like try { /* run reducer */ } finally { /* close some streams */ } The above trace and the one in HADOOP-757 both are in finally {} and mask the exception in try {}. I will submit a patch that prints the exception thrown in try {} if finally block throws one. While trying reproduce the above trace I managed to produce ""Bad File Descriptor"" exception in HADOOP-757. In summary, it looks like these failures are possible with low tmp spaces but we don't log the exceptions that were triggered initially.",code_debt,low_quality_code
hadoop,758,comment_4,Attached patch removes finally block. This results in first exception thrown from ReduceTask.,non_debt,-
hadoop,758,comment_5,"Rather than ignore all those exceptions, wouldn't it be better to at least log them? Also, I'm not sure we need to proceed with all cleanups if any fail. And we shouldn't replicate the cleanup code. The problem is that exceptions in cleanups are masking the exception thrown in the body. Wouldn't something like the following work? IOException ioe = null; try { ... body ... } catch (IOException e) { ioe = e; throw e; } finally { try { ... cleanups... } catch (IOException e) { if (ioe != null) LOG.warn(e) else throw e; } if (ioe != null) throw ioe; }",design_debt,non-optimal_design
hadoop,758,comment_7,"... This is exactly what I had in my devel code. The method ('double code path') in the patch was Owen's preferred approach. If we don't need to do all the cleanups (may be because this will actually close the Java process and open a new one for the new task), then we don't need finally at all.",code_debt,low_quality_code
hadoop,758,comment_8,"I think we should make a good-faith effort to cleanup: user-errors can be common, and this also runs under LocalRunner, not always as a separate process. If there are errors in the cleanups we should log these as warnings, since they should not occur. Which raises the related question: why did the cleanup fail? Closing an open file shouldn't throw an exception. That looks like a bug in DFSClient, no?",design_debt,non-optimal_design
hadoop,758,comment_9,"Yes. FSDataOutputStream does not store its failure condition. so initial flush() fails and subsequent close() also result in flush() calls, which fail again. Do we want to fix that? I am not sure if this will require trapping exceptions in multiple places. will look into. Since there are multiple java filters in the stream, close() actually results in many calls. Java FilterOutputStream ignores exceptions from flush() during close.",non_debt,-
hadoop,758,comment_10,"Neither of these should fail, should they?",non_debt,-
hadoop,758,comment_11,"First time flush/write can fail, common reason is lack of space. After that what should the subsequent flush() do? Should they produce the exception again (current behavior) or should the silently ignore the call()?",non_debt,-
hadoop,758,comment_12,"If the second flush() is under close() then its exception should be ignored, I think.",non_debt,-
hadoop,758,comment_13,Sure. I will make sure that is simple.,non_debt,-
hadoop,758,comment_14,I meant to say : Sure. I will make sure ignores or logs the exception but does not throw it. That is simple.,non_debt,-
hadoop,758,comment_15,hmm... does a lot apart from flush(). I need to understand it better to make the changes.,non_debt,-
hadoop,758,comment_16,Owen brought up another important point. We want close() to throw exceptions since many times the last flush might occur only in close() and we should know about any failure.,non_debt,-
hadoop,758,comment_17,"Based on the above discussion, shall we retain the patch that is submitted? It ignores exception in clean up only when regular code path has already thrown one. HADOOP-757 has another patch the fixes 'BadFileDescriptor' issue in DFSClient. It also resets before opening a new block file instead of after. This removes repeated attempts by DFSClient to write.",non_debt,-
hadoop,758,comment_18,"The phased file system should not be committed when there is an exception. Other than that change, this looks reasonable.",non_debt,-
hadoop,758,comment_19,Thanks Owen. attached 858_2.patch moves out of clean up.,non_debt,-
hadoop,758,comment_20,1,non_debt,-
hadoop,758,comment_21,"I just committed this. Thanks, Raghu!",non_debt,-
hadoop,829,summary,Separate the datanode contents that is written to the fsimage vs the contents used in over-the-wire communication,non_debt,-
hadoop,829,description,"In the existing implementation, the Namenode has a data structure called DatanodeDescriptor. It is used to serialize contents of the Datanode while writing it to the fsimage. The same serialization method is used to send a Datanode object to the Datanode and/or the Client. This introduces the shortcoming that one cannot introduce non-persistent fields in the DatanodeDescriptor. One solution is to separate out the following two functionality into two separate classes: 1. The fields from a Datanode that are part of the ClientProtocol and/or DatanodeProcotol. 2. The fields from a Datanode that are stored in the fsImage.",design_debt,non-optimal_design
hadoop,829,comment_0,This patch clarifies the following rules: 1. Information that is part of the ClientProtocol or DatanodeProtocol is part of DatanodeInfo. 2. Information that is local to the Namenode and need not be persisted across namenode restarts is in DatanodeDescriptor. 3. Information that is persisted across Namenode restarts is in DatanodeImage.,non_debt,-
hadoop,829,comment_1,This change separates out the three usage of Datanode information: 1. Information that is part of the ClientProtocol or DatanodeProtocol is part of DatanodeInfo. 2. Information that is local to the Namenode and need not be persisted across namenode restarts is in DatanodeDescriptor. 3. Information that is persisted across Namenode restarts is in DatanodeImage. The initial idea of the refactoring was to enable new transient fields in the DatanodeDescriptor or DatanodeInfo that does not affect the fsimage. We have achieved it with this patch.,non_debt,-
hadoop,829,comment_4,the previous path file was bad because it did not have full path names.,non_debt,-
hadoop,829,comment_6,"I just committed this. Thanks, Dhruba!",non_debt,-
hadoop,836,summary,unit tests fail on windows (/C:/cygwin/... is invalid),non_debt,-
hadoop,836,description,"Under windows, I get the following exception from some of the unit tests: Invalid file name: Method)",non_debt,-
hadoop,836,comment_0,"Paths without schemes were being used in MapTask.java. So the local split file was being written into dfs by mistake. This patch fixes that problem, makes the dfs client reject illegal names immediately, and adds a new static method to get the local file system.",non_debt,-
hadoop,836,comment_2,"I just committed this. I made two minor changes. I used instead of new URI(""file:///"") to name the LocalFileSystem, and I replaced calls to with Thanks, Owen!",non_debt,-
hadoop,848,summary,randomwriter generates too many errors of type:,non_debt,-
hadoop,848,description,"randomwriter with speculative execution turned 'on' fails with too many errors: 2006-12-25 06:50:38,880 INFO Error from failed to create file for on client XXX because pendingCreates is non-null. 2006-12-25 06:50:48,203 INFO Error from failed to create file for on client XXX because pendingCreates is non-null. 2006-12-25 06:50:49,917 INFO Error from failed to create file for on client XXX because pendingCreates is non-null. 2006-12-25 06:50:52,812 INFO Error from failed to create file for on client XXX because pendingCreates is non-null. I suspect this is due to the the fact that randomwriter doesn't use the PhasedFileSystem. -*-*- Unrelated note: can we add the 'examples' *Component* to this project here on jira? I've put in 'mapred' for this issue for now, please feel free to correct me. Thanks!",non_debt,-
hadoop,848,comment_0,+1 on the randomwriter using the phased file system.,non_debt,-
hadoop,848,comment_1,"The randomwriter already turns off speculative execution in its JobConf. It will only be a problem if the hadoop-site.xml overrides the setting. Therefore, this is a configuration issue rather than a problem with random writer.",non_debt,-
hadoop,876,summary,need a setting for random writer that generates compressable data and compressess it.,non_debt,-
hadoop,876,description,It would help debugging and testing if the random writer example had a switch to generate compressible data and write it compressed.,non_debt,-
hadoop,876,comment_0,This was fixed by HADOOP-1926.,non_debt,-
hadoop,920,summary,MapFileOutputFormat and use incorrect key/value classes in map/reduce tasks,non_debt,-
hadoop,920,description,"Let's assume a job uses different key/value class for the output of map tasks and for the final output of reduce tasks. When executing map tasks classes returned from / should be used, and when executing reduce tasks classes returned from / should be used. Currently both map and reduce tasks will use when using or they will always use when using This causes exceptions, because Mapper / Reducer implementations will output different key/value classes than expected.",non_debt,-
hadoop,920,comment_0,"Proposed fix, which uses different methods depending on whether we are in map or reduce task.",non_debt,-
hadoop,920,comment_2,"OutputFormats are only used when reducing, to generate the final output. They're not used when creating intermediate output. So the bug here is that MapFileOutputFormat calls methods should only be called by the MapReduce kernel when generating intermediate output and should not be called by an OutputFormat implementation. This bug was introduced by HADOOP-115. The proper fix I think is to undo that change to this file.",non_debt,-
hadoop,920,comment_3,Fixed by reverting an accidental change introduced by HADOOP-115.,non_debt,-
hadoop,922,summary,Optimize small reads and seeks,non_debt,-
hadoop,922,description,A seek on a DFSInputStream causes causes the next read to re-open the socket connection to the datanode and fetch the remainder of the block all over again. This is not optimal. A small read followed by a small positive seek could re-utilize the data already fetched from the datanode as part of the previous read.,design_debt,non-optimal_design
hadoop,922,comment_0,"If there is a forward seek within the current block, then do not re-issue the block-read request to the datanode.",non_debt,-
hadoop,922,comment_1,Did you mean to do this only for small seeks? The code does not enforce that. Even a 100MB seek will read 100MB data that should be skipped?,non_debt,-
hadoop,922,comment_2,Use skipBytes() instead of reading all the intervening data that is skipped.,non_debt,-
hadoop,922,comment_3,"A forward skip within a previously read data block just manipulates the underlying file pointer rather than re-reding the entire block from the datanode. This has been reviewed by Milind. His review comments were :"" Looks good to me. Can we ask the blockStream to skip diff bytes instead of reading them one by one ?""",non_debt,-
hadoop,922,comment_4,"In response to Raghu's comments: By ""small seeks"" I meant ""seeks within the current block"". The contents of this block was already fetched by the preceeding read call. If a datablock is 128MB then even a 100MB seek could trigger this particular optimization.",non_debt,-
hadoop,922,comment_6,"The call to skipTo() will still, in the worst case, cause an entire block to be streamed across the wire, using a lot of network bandwidth. Before we commit this I'd like to see some benchmarks showing that this is faster than closing and re-opening the connection. Note also that the stream buffering code already performs a similar optimization: if a 100k buffer is used, and one seeks within the buffer, then no i/o is performed on the underlying stream. So seeks of the underlying stream are generally at least a few k bytes away.",non_debt,-
hadoop,922,comment_7,"I agree with your comments. The amount of data cached by the receiving size of the TCP connection could possibly depend on the latency of transfer and the amount of memory available to the sender and received. By default, the TCP sending window size is usually 128KB and receiving windows size is 4MB. I propose that I change the above patch to trigger the optmization only if the skip length is <= 128KB.",non_debt,-
hadoop,922,comment_8,1,non_debt,-
hadoop,922,comment_9,One change from the previously submitted patch: allow the optimization only if the skip-length is within 128KB range. (The TCP receive window size is typically more than this limit).,non_debt,-
hadoop,922,comment_10,One change from the previously submitted patch: allow the optimization only if the skip-length is within 128KB range. (The TCP receive window size is typically more than this limit).,non_debt,-
hadoop,922,comment_11,"I just committed this. Thanks, Dhruba!",non_debt,-
hadoop,930,summary,Add support for reading regular (non-block-based) files from S3 in S3FileSystem,non_debt,-
hadoop,930,description,"People often have input data on S3 that they want to use for a Map Reduce job and the current S3FileSystem implementation cannot read it since it assumes a block-based format. We would add the following metadata to files written by S3FileSystem: an indication that it is block oriented and a filesystem version number Regular S3 files would not have the type metadata so S3FileSystem would not try to interpret them as inodes. An extension to write regular files to S3 would not be covered by this change - we could do this as a separate piece of work (we still need to decide whether to introduce another scheme - e.g. rename block-based S3 to ""s3fs"" and call regular S3 ""s3"" - or whether to just use a configuration property to control block-based vs. regular writes).",non_debt,-
hadoop,930,comment_0,"Here's a patch for a native S3 filesystem. * Writes are supported. * The scheme is s3n making it completely independent of the existing block-based S3 filesystem. It might be possible to make a general (read-only) S3 filesystem that can read both types, but I haven't attempted that here (it can go in another Jira if needed). * Empty directories are written using the naming convention of appending ""_$folder$"" to the key. This is the approach taken by S3Fox, and - crucially for efficiency - it makes it possible to tell if a key represents a file or a directory from a list bucket operation. * There's a new unit test for the contract of FileSystem to ensure that different implementations are consistent. Both S3 filesystems and HDFS are tested using this test. It would be good to add other filesystems later. * Renames are not supported as S3 doesn't support them natively (yet). It would be possible to support renames by getting the client to copy the data out of S3 then back again. * The Jets3t library has been upgraded to the latest version (0.6.0)",requirement_debt,requirement_partially_implemented
hadoop,930,comment_1,Second patch with the following changes: * Send Content-MD5 header to perform message integrity checks for writes. * Fix warnings from Jets3t to do with not closing streams. * Change property names to be independent of existing S3FileSystem: and * Findbugs and formatting fixes.,code_debt,low_quality_code
hadoop,930,comment_3,Fixed release audit warnings and a few formatting warnings from checkstyle.,code_debt,low_quality_code
hadoop,930,comment_5,Can someone validate that this code works for them?,non_debt,-
hadoop,930,comment_6,Merged with trunk.,non_debt,-
hadoop,930,comment_7,"-- Any reason you didn't use the mime type to denote directory files (as jets3t does)? {code:java} public static boolean isDirectory( S3Object object ) { return != null && MIME_DIRECTORY ); } -- I believe MD5 checksum should be set on s3 put (via header), and verified on s3 get. I see plenty of read failures because of checksum failures (though they could be side effects of stream reading timeouts in retrospect). This is especially useful if non Hadoop applications are dealing with the S3 data shared with Hadoop. -- Sometimes 'legacy' buckets have underscores, might consider trying to survive them.. {code:java} String userInfo = uri.getUserInfo(); // special handling for underscores in bucket names if( userInfo == null ) { String authority = uri.getAuthority(); String split[] = authority.split( ""[:@]"" ); if( split.length userInfo = split[ 0 ] + "":"" + split[ 1 ]; }",non_debt,-
hadoop,930,comment_8,"Thanks for the review Chris. It's to do with efficiency of listing directories. If you use mime type then you can't tell the difference between files and directories when listing bucket keys. So you have to query each key in a directory which can be prohibitively slow. But if you use the _$folder$ suffix convention (which S3Fox uses too BTW) you can easily distinguish files and directories. The code should be doing this. I agree that it's useful - in fact, the other s3 filesystem needs updating to do this too. Thanks for the tip. The code does detect this condition, but it might be nice to try to workaround as you say (perhaps emitting a warning). Have you done this elsewhere?",design_debt,non-optimal_design
hadoop,930,comment_9,"From what I can tell, returns an array of S3Object, where each instance already has any associated meta-data in a HashMap. Content-Type being one of them. So I think the penalty has been paid. Here is the jets3t code. are you seeing a different behavior or disabling meta-data in jets3t for performance reasons? Sorry if i seem little rusty on my jets3t api.. Sorry, didn't see where the checksum was being validated on a read. I see it in but not Does Jets3t do this automatically? If so cool. I believe those are the only two values that can be munged due to a underscore in the authority.",non_debt,-
hadoop,930,comment_10,I don't think all the fields in S3Object are populated - just those returned in the list keys response. See I think Jets3t does validate MD5 checksums on reads - but I'll double check.,non_debt,-
hadoop,930,comment_11,good catch.,non_debt,-
hadoop,930,comment_12,"New patch that works with trunk. This isn't true, Jets3t doesn't validate MD5 checksums on reads. In fact the stream is sent straight to the client, so it's not possible in general to validate the MD5 checksum - particularly when doing seeks, which use range GETs. Contrast this with S3FileSystem which retrieves data in blocks, so it would be easy to add checksum validate there (I've opened HADOOP-3494 for this). For this issue, I think we should just have write checksum validation. I've also created HADOOP-3495 to address supporting underscores in bucket names.",non_debt,-
hadoop,930,comment_13,"I just committed this. Thanks, Tom!",non_debt,-
hadoop,1034,summary,RuntimeException and Error not catched in,non_debt,-
hadoop,1034,description,"Only IOException is catched and logged (in warn). Every Throwable should be logged in error. Eg: a RuntimeException occurs in the writeBlock() method. The exception will not be logged, but simply ignored. The socket is closed silently and nothing and an non understandable exception will be thrown in the DFSClient sending the block....",design_debt,non-optimal_design
hadoop,1034,comment_0,Catch throwable and log it in error,non_debt,-
hadoop,1034,comment_1,"More generally, in a bunch of hadoop code, only IOException are caught. (eg: same issue on I think a little cleanup is needed, especially while catching an exception in a run() of a Thread...",code_debt,low_quality_code
hadoop,1034,comment_2,"Agreed. All threads should catch Throwable at the top level and log them. *Sigh* I thought we had gone through all of the threads and fixed that problem. If you see any more, please file them as bugs.",code_debt,low_quality_code
hadoop,1034,comment_3,"I just committed this. Thanks, Philippe!",non_debt,-
hadoop,1072,summary,VersionMismatch should be,non_debt,-
hadoop,1072,description,"extends IOException. It's name should follow the Java naming convention for Exceptions, and thus be",code_debt,low_quality_code
hadoop,1072,comment_1,"This is an incompatible change to a public API. So, if we choose to make it, we must deprecate the existing name for at least one release prior to removing it. Personally, I'm not convinced the naming hygiene is worth the hassle in this case, but I would not veto it.",code_debt,low_quality_code
hadoop,1072,comment_2,Already an IO class w that name.,non_debt,-
hadoop,1074,summary,fsck reports different number of files with differing dfs.data.dir directories,non_debt,-
hadoop,1074,description,"I run randomwriter twice on a formatted Namenode, each time only varying the number of directory elements in dfs.data.dir property. When dfs.data.dir has 1 element, fsck report the following: HEALTHY Total size: 47214240 B Total blocks: 760 (avg. block size 62124 B) Total dirs: 6 Total files: 80 Over-replicated blocks: 0 (0.0 %) Under-replicated blocks: 0 (0.0 %) Target replication factor: 3 Real replication factor: 3.0 The filesystem under path '/' is HEALTHY When dfs.data.dir has more than one entry, fsck reports the following: HEALTHY Total size: 35410680 B Total blocks: 570 (avg. block size 62124 B) Total dirs: 6 Total files: 60 Over-replicated blocks: 0 (0.0 %) Under-replicated blocks: 0 (0.0 %) Target replication factor: 3 Real replication factor: 3.0 The filesystem under path '/' is HEALTHY",non_debt,-
hadoop,1074,comment_0,This was due to a timing issue in my testing scripts.,non_debt,-
hadoop,1130,summary,Remove unused ClientFinalizer in DFSClient,code_debt,dead_code
hadoop,1130,description,The ClientFinalizer shutdown hook is not used. This can lead to severe memory leaks if you use the DFSClient in a dynamic class loading context (such as in a webapp) as the class is retained in the memory by the shutdown hook. It retains also the current ClassLoader and thus all classes loaded by the ClassLoader. (Threads put in the shutdown hook are never garbage collected).,code_debt,low_quality_code
hadoop,1130,comment_0,"Ack, I must have forgotten to actually add the client objects. Instead of just removing this finalizer, how about having a shutdown hook that calls the closeAll method from HADOOP-1131? The also needs a close method that calls the DFSClient.close.",non_debt,-
hadoop,1130,comment_1,"As hadoop may be used as a third party library, it shouldn't install shutdown hook by itself. Installing those must be the responsiblity of the application code and not the library code. Eg: If I write a standalone app with a main method, I will install the shutdown hooks. If I write a web application I will call the closeAll() method from a ContextListener... Is that really necessary to closeAll() everything when the jvm exits ? (Maybe I'm wrong but the JVM will itself close everything.)",non_debt,-
hadoop,1130,comment_2,Does the need updating in light of Owen's comments (and now HADOOP-1131 is resolved)?,non_debt,-
hadoop,1130,comment_3,"Jira keeps eating my comments on this issue. *sigh* Actually, I think you have this backwards. Since Hadoop may be used as a third party library, it must use finalizers and shutdown hooks to clean up properly. The problems with have an explicit close method that the user must call are: 1. Forcing users to remember to cleanup your library is error-prone. 2. Just because library Foo is done does not mean library Bar is done and Bar will lose when Foo calls close. That said, I think that with some relatively minor changes, we can get the effect that you want (minimal garbage after the usage of the FileSystem is done): 1. Fix HADOOP-1160 so that the FileSystem.closeAll actually closes the client. 2. Have FileSystem.closeAll clear the CACHE map via reset. 3. Have the DFSClient.close method remove the client from the ClientFinalizer. 4. Have the constructor add ""this"" to the ClientFinalizer. So after the closeAll call, you'll still have the DFSClient class loaded, but not much else. In particular, you'll have no instances of DFSClient left.",design_debt,non-optimal_design
hadoop,1130,comment_4,The closeAll() method should also remove ClientFinalizer from shutdown hooks to avoid the ClassLoader (so all Class(es) loaded by the ClassLoader) that have loaded hadoop being kept in memory.,code_debt,low_quality_code
hadoop,1130,comment_5,[ edit - removed grossly redundant exposition ] This patch clears entries from FileSystem.CACHE when there are no authorities for a given scheme and removes the shutdown hook when the cache is empty.,non_debt,-
hadoop,1130,comment_7,"Can Owen, Philippe or Tom please review this patch and indicate whether it is ripe for committing?",non_debt,-
hadoop,1130,comment_8,"Hi Chris, can you pl get this patch reviewed? Thanks.",non_debt,-
hadoop,1130,comment_9,1,non_debt,-
hadoop,1130,comment_10,I just committed this. Thanks Chris!,non_debt,-
hadoop,1147,summary,remove all @author tags from source,non_debt,-
hadoop,1147,description,"We should remove @author tags from the source code. We give contributors credit in at least three places (Jira, subversion and CHANGES.txt). Many files have been substantially re-written by a range of contributors and their @author tags are no longer accurate. Also, @author tags imply individual ownership, when we should rather strive for community ownership.",non_debt,-
hadoop,1147,comment_0,1,non_debt,-
hadoop,1147,comment_1,1,non_debt,-
hadoop,1147,comment_2,-1 I consider @author tags not as a recognition of achievements but as a reference to whom I should ask questions about the code. Removing will essentially make the code anonymous. I agree the tags should be revised to make them up to date. +1 for giving contributors credit in three places.,non_debt,-
hadoop,1147,comment_3,"Aren't 'svn log' and 'svn annotate' superior for this? And shouldn't all questions be on the developer list anyway, not directed to individuals, but to the group? The code should be anonymous. We want lots of developers to be familiar with every file, and want to discourage individual gatekeepers for files, no?",non_debt,-
hadoop,1147,comment_4,"+1 I was originally liked them, but they mostly end up creating noise. For example, I wrote the sort and word count examples that many people use for the start of their first map/reduce program and my name is in the @author. Someone didn't remove the comments and then handed off their program to someone else and it lead to confusion.",documentation_debt,low_quality_documentation
hadoop,1147,comment_5,"Subversion also prohibits author names in source files and has formalized the credit messages to make them tool parseable. Subversion is a much larger project, but it is interesting to see what they have done:",non_debt,-
hadoop,1147,comment_6,1,non_debt,-
hadoop,1147,comment_7,"Big +1 - I'm familiar with this system from my FreeBSD days as a committer, and it was working very well, it made the commit logs much more useful. FreeBSD went even further - there was a pre-commit script that would check the proper formatting of these comments, and refuse the commit if e.g. Reviewed By: was missing for a particular critical sub-tree, such as kernel source. We could formalize at least the format of Jira issue # references, and ""reviewed by"" - although I'm wary of including literal people's emails, which are an easy target for spammers.",non_debt,-
hadoop,1147,comment_8,"It looks like I am in the minority here, but still. Anonymous code is like public property: nobody cares, nobody takes responsibility. Been there. We are supporting Sun's code convention. They have author field in every file. Even if they don't have an author the field says @author unascribed",non_debt,-
hadoop,1147,comment_9,"Sorry Konstantin, but I agree with the majority here. I think the idea is that the code is collectively owned. The concept of collective ownership seems not to work on a large scale, but has been shown to work quite well on Open Source projects. Sun's code conventions don't recommend the use of @author, although it is shown in a code example. Netbeans puts it in by default, but you can change the template to turn this off.",non_debt,-
hadoop,1147,comment_10,I just committed this.,non_debt,-
hadoop,1192,summary,Du command takes a noticable longer time to execute on a large dfs than the 0.11 release,code_debt,slow_algorithm
hadoop,1192,description,"After installing 0.12.2, we notice that the du command takes a much longer time to execute than the previous release. Another problem is that dus prints a negative value.",code_debt,slow_algorithm
hadoop,1192,comment_0,The checksum patch did not overwrite the method getContentLength in This leads to the use of the default slower version of getContentLength defined in FileSystem. This patch fixed the du problem. It also fixed the dus problem by declaring the totalSize to be long.,code_debt,slow_algorithm
hadoop,1192,comment_1,"+1 Code looks fine. While reviewing this I noticed that FsShell.dus() seem to be doing one level recursion into the paths it wants to du. I would have thought we either need not recurse at all or to recurse recurse fully. This patch does not change this logic. Hairong is taking a look at this. If there is some issue here (either correctness or just code efficiency), that could be a different jira.",code_debt,complex_code
hadoop,1192,comment_2,"+1 Now, dfs -du works fine on our cluster.",non_debt,-
hadoop,1192,comment_4,"I just committed this. Thanks, Hairong!",non_debt,-
hadoop,1227,summary,Record IO C++ binding: cannot write more than one record to an XML stream and read them back,non_debt,-
hadoop,1227,description,"I tried just writing the same record twice and then reading it back twice, and got a segmentation fault. This works fine in the binary and csv cases.",non_debt,-
hadoop,1227,comment_0,"Resolving as Won't Fix, since the whole recordio component is now deprecated in favor of Avro (and technically ought to be removed in 0.22/0.23). Please see",non_debt,-
hadoop,1245,summary,"value for taken from jobtracker, not tasktracker",non_debt,-
hadoop,1245,description,"I want to create a cluster with machines with different numbers of CPUs. Consequently, each machine should have a different value for since my map tasks are CPU bound. When a new job starts up, the jobtracker uses its (single) value for to assign tasks. This means that each tasktracker gets the same number of tasks, regardless of how I configured that particular machine. The jobtracker should not consult its config for the value of It should assign tasks (or allow tasktrackers to request tasks) according to each tasktracker's value of Originally, I thought the behavior was slightly different, so this issue contained this text: After the first task finishes on each tasktracker, the tasktracker will request new tasks from the jobtracker according to the tasktracker's value for So after the first round of map tasks is done, the cluster reverts to a mode that works well for heterogeneous clusters.",non_debt,-
hadoop,1245,comment_0,"I too came across this problem the hard way (our weaker nodes started thrashing, because they were running too many jobs. In my case I don't even see it reverting to the desired behavior after the first round of jobs is finished, - it's always the jobtracker's config value. Maybe I'm using a different version of Hadoop? (it's 0.15 devel from svn). If anyone knows a way to run different number of tasks on different boxes, please let me know, because from what I see, there is no way to do it, which makes our hadoop cluster as lame as the lamest node in it.",non_debt,-
hadoop,1245,comment_1,"I've also run into this, and came up with a patch that involved: - adding a 'maxTasks' value to the TaskTrackerStatus (set by the local - modifying JobTracker to track totalTaskCapacity instead of per-node maxTasks, and to use the particular task tracker's maxTasks value when deciding whether to assign it another task If this seems like a reasonable approach, I can do more testing and provide a patch.",test_debt,lack_of_tests
hadoop,1245,comment_2,Here's a patch implementing the approach described in the last comment. It also includes web UI updates to display the per tasktracker max-tasks setting.,non_debt,-
hadoop,1245,comment_3,Patch looks reasonable. +1,non_debt,-
hadoop,1245,comment_4,See what hudson thinks...,non_debt,-
hadoop,1245,comment_6,"The patch is cool, I tested it on our hadoop cluster and it seems to be working. +1.",non_debt,-
hadoop,1245,comment_7,Fixing issue description to reflect reality as reported by others,non_debt,-
hadoop,1245,comment_8,"+1 This looks reasonable to me. Note that this is potentially incompatible, since previously folks could set the number of tasks per node globally at the jobtracker, now it is determined by the configuration of the tasktracker nodes. So we should probably either add a compatibility note (i.e., move this to the INCOMPATIBLE section of CHANGES.txt) or perhaps have a configuration parameter that enables the old behavior. I think a compatibility note is probably sufficient. Thoughts?",documentation_debt,outdated_documentation
hadoop,1245,comment_9,"It might be enough to use the value at the jobtracker as a default, and override with the tasktracker value if it's present (along with the compatibility note). If a tasktracker is configured differently than the jobtracker, it's more likely the configurer intended the tasktracker's value to be used, as opposed to the configurer expecting the tasktracker value to be ignored. So I doubt using the jobtracker as an overridable default will break anybody.",non_debt,-
hadoop,1245,comment_10,"Thanks. I can certainly modify the patch as Michael suggests, though there is a setting for in hadoop-default.xml, so it seems like a user would really have to go out of her way to not have any setting present on the tasktracker.",non_debt,-
hadoop,1245,comment_11,This looks fine.,non_debt,-
hadoop,1245,comment_12,"Rick, I forgot about that. So my suggestion would be rather useless.",non_debt,-
hadoop,1245,comment_13,"I just committed this (twice!). Thanks, Michael.",non_debt,-
hadoop,1251,summary,A method to get the InputSplit from a Mapper,non_debt,-
hadoop,1251,description,I need a way to get the InputSplit from a Mapper. This is effectively a stop gap until we get context objects and can fix this much better. *smile* I introduce a static method in the TaskTracker named getInputSplit() that returns the InputSplit.,design_debt,non-optimal_design
hadoop,1251,comment_1,"This method is not specific to TaskTracker, i.e., it should work fine with LocalRunner too, right? So there ought to be a better place to put it. JobConf? JobClient?",architecture_debt,violation_of_modularity
hadoop,1251,comment_2,This patch moves the getInputSplit from a static method on the TaskTracker to a non-static method on the Reporter. My intention is that getInputSplit will eventually move to the context method along with the rest of the Reporter methods. This patch also removes the Reporter subclass in the record io testing that was reimplementing Reporter.NULL.,non_debt,-
hadoop,1251,comment_3,Resubmit patch for automatic qa testing.,non_debt,-
hadoop,1251,comment_5,"I just committed this. Thanks, Owen!",non_debt,-
hadoop,1253,summary,and NPE in JobControl,non_debt,-
hadoop,1253,description,"jobs) in JobControl.java isn't synchronized and it resulted in a checkRunningState in Job.java can throw a if running = fails, leaving running null.",non_debt,-
hadoop,1253,comment_0,fixes the mentioned issues. also slipped in a small addJobs method to add more then one job at once.,non_debt,-
hadoop,1253,comment_2,I've just committed this. Thanks Johan!,non_debt,-
hadoop,1254,summary,TestCheckpoint fails intermittently,test_debt,flaky_test
hadoop,1254,description,TestCheckpoint started intermittently failing last night: This is probably caused by one of the changes introduced yesterday:,non_debt,-
hadoop,1254,comment_0,"For posterity, since Hudson builds are only kept for 30 days, the stack trace is:",non_debt,-
hadoop,1254,comment_1,"TestCheckpoint has one case where it creates a MiniDFSCluster but doesn't wait for it to be active. I'll file a patch for this. I wonder if this started showing up due to some new speed up or slow down in starting a NameNode and/or a DataNode, perhaps introduced by HADOOP-971 or HADOOP-1189...",code_debt,slow_algorithm
hadoop,1254,comment_2,This issue is a duplicate of HADOOP-1248 and HADOOP-1256. It is fixed by the patch for HADOOP-1256.,non_debt,-
hadoop,1311,summary,"Bug in newData, int offset, int length)",non_debt,-
hadoop,1311,description,"Current implementation: public void set(byte[] newData, int offset, int length) { setSize(0); setSize(length); 0, bytes, 0, size); } Correct implementation: public void set(byte[] newData, int offset, int length) { setSize(0); setSize(length); offset, bytes, 0, size); } please fix.",non_debt,-
hadoop,1311,comment_0,Please review and commit if appropriate.,non_debt,-
hadoop,1311,comment_2,"The final parameter to arraycopy should be 'length', not 'size', right?",non_debt,-
hadoop,1311,comment_3,"Yes. It should be length. Thanks, Srikanth",non_debt,-
hadoop,1311,comment_4,"But just before the call to arraycopy, we invoke setSize(length). This sets size = length. Thus, size and length are the same when the arraycopy is invoked.",non_debt,-
hadoop,1311,comment_5,Please review again.,non_debt,-
hadoop,1311,comment_7,"I just committed this. Thanks, Dhruba!",non_debt,-
hadoop,1352,summary,JobTracker consistently throws for some jobs accessed via the job-history,non_debt,-
hadoop,1352,description,"This is the exception from the jobtracker when I attempted to access the via the job history. It consistently threw this exception for that particular job whereas for other jobs in the history it worked fine.. 2007-05-13 18:51:07,343 WARN /: 79647",non_debt,-
hadoop,1352,comment_0,"This is the JobHistory.log content for that run of the JobTracker: Jobtracker Job JOBID=""job_0001"" USER=""hadoopqa"" Job JOBID=""job_0001"" TOTAL_MAPS=""8850"" TOTAL_REDUCES=""1"" Job JOBID=""job_0001"" Job JOBID=""job_0002"" JOBNAME=""sorter"" USER=""hadoopqa"" Job JOBID=""job_0002"" TOTAL_MAPS=""79650"" Job JOBID=""job_0002"" Job JOBID=""job_0003"" USER=""hadoopqa"" Job JOBID=""job_0003"" TOTAL_MAPS=""10560"" TOTAL_REDUCES=""1"" Job JOBID=""job_0003""",non_debt,-
hadoop,1352,comment_1,"I've tried to reproduce this without luck and even the actual jobhistory file didn't bring up the closing this for now, can reopen later if required.",non_debt,-
hadoop,1367,summary,Inconsistent synchronization of locked 50% of time,code_debt,multi-thread_correctness
hadoop,1367,description,line 556 The distFrom field of this class appears to be accessed inconsistently with respect to synchronization.,code_debt,multi-thread_correctness
hadoop,1367,comment_0,This is actually protected by the FSNamesystem global lock. Have to address this Deferriissue when we go to fine-grain-locking model. Deferring to next release.,defect_debt,uncorrected_known_defects
hadoop,1367,comment_1,I do not think it is a critical bug. Deferring it to 0.14.,defect_debt,uncorrected_known_defects
hadoop,1367,comment_2,The distFrom variable has been made a ThreadLocal. This should get rid of the findbugs warnings,code_debt,multi-thread_correctness
hadoop,1453,summary,exists() not necessary before DFS.open,non_debt,-
hadoop,1453,description,{{exists(f)}} adds extra namenode interaction that is not really required. Open is a critical DFS call.,code_debt,low_quality_code
hadoop,1453,comment_0,Attached simple patch:,non_debt,-
hadoop,1453,comment_1,I guess this is not enough. There is an exists() in as well. Should that be removed as well? :,non_debt,-
hadoop,1453,comment_2,This implies there are 3 extra RPCs to NameNode for typical DFS file open(). One in ChecksumFS and one each for file and .file.crc in DFS.,non_debt,-
hadoop,1453,comment_3,Attaching an updated patch. I think it is correct and can be checked in if anyone reviews it.,non_debt,-
hadoop,1453,comment_4,Both your patches are for whereas your comment says that there are *two* places where a redundant exists() call is made: and ChecksumFileSystem. Can you pl clarify?,code_debt,low_quality_code
hadoop,1453,comment_5,Thanks Dhruba. Now it is fixed. Please review 02.patch.,non_debt,-
hadoop,1453,comment_6,+1 code looks good.,non_debt,-
hadoop,1453,comment_9,"I just committed this. Thanks, Raghu!",non_debt,-
hadoop,1454,summary,Reducer doesn't track failed fetches and gets stuck,non_debt,-
hadoop,1454,description,"As mentioned in HADOOP-1452, but it deserves a separate issue, reducers retry forever to fetch data. At some time this should fail either the reducer or the corresponding mapper.",non_debt,-
hadoop,1454,comment_0,Duplicate of HADOOP-1158,non_debt,-
hadoop,1459,summary,"returns IP addresses rather than hostnames, which breaks 'data-locality' in map-reduce",non_debt,-
hadoop,1459,description,"via DFSClient.getHints (post HADOOP-894?) returns IP address of the datanodes instead of the hostnames which breaks mapping from task-tracker to datanodes in map-reduce i.e. the system cannot intelligently place maps on datanodes where blocks are present. I have verified that this affects trunk only, branch-0.13.0 seems ok.",non_debt,-
hadoop,1459,comment_0,Marking this issue as a blocker for 0.14 release because it breaks locality of map-reduce with DFS.,non_debt,-
hadoop,1459,comment_1,The getBlockLocations patch HADOOP-894 had a sideeffect that getHints started returning IP addresses instead of hostnames. This patch fixes this problem.,non_debt,-
hadoop,1459,comment_2,This allows the WebUI to display machine names as hostNames rather than IP address.,non_debt,-
hadoop,1459,comment_3,"Could you please remove redundant import if you change DatanodeInfo. I think this is the right fix for the time being, although I'm not happy that # we should keep 2 different identifications for the nodes and that # we have different ways of node identification in different components of hadoop. My proposition would be to return back to hostnames instead of ip addresses. But this of course belongs to a different issue.",code_debt,dead_code
hadoop,1459,comment_4,The namenode now serializes the hostName as part of DatanodeInfo. The namenode already had the hostName readily available. Another alternative would have been to make a getHostName() call on the DFSClient. But we did not adopt this approach because a getHostName() call for every replica of every block could be somewhat time-consuming.,non_debt,-
hadoop,1459,comment_5,"Note that this increases serialization cost for any DatanodeInfo tranfer, which is pretty much most RPC. This will needs a protocol version change since this won't work with prev clients/datanodes.",design_debt,non-optimal_design
hadoop,1459,comment_6,+1. Till now Namenode kept 'hostName' only for information purpose.,non_debt,-
hadoop,1459,comment_7,Please see HADOOP-985 for context why we moved to using IP addresses. It is just that not all Hadoop components moved to IPs.,non_debt,-
hadoop,1459,comment_8,This perticular issue was considered in HADOOP-985 : from :,non_debt,-
hadoop,1459,comment_9,Does this also affect fsimage version?,non_debt,-
hadoop,1459,comment_10,This does not affect fsimage version. DatanodeInfo is used to communicate over-the-wire whereas DatanodeImage is used to persistent info into fsImage. I di dnot change the serialization of DatanodeImage.,non_debt,-
hadoop,1459,comment_11,This patch should change protocol version.,non_debt,-
hadoop,1459,comment_12,The protocol version was already bumped up since the 0.13 release. hence I have not changed it in this patch.,non_debt,-
hadoop,1459,comment_13,"Protocol version should be changed any time the protocol is changing, which is what this patch does.",non_debt,-
hadoop,1459,comment_14,Bump up ClientProtocol version too.,non_debt,-
hadoop,1459,comment_15,+1 Should we file a separate issue for replacing ips by fully qualified hostnames?,non_debt,-
hadoop,1459,comment_16,"Yes, I like the idea where DFS keeps fully qualified hostnames and sends these to the map-reduce framework. Thanks, dhruba",non_debt,-
hadoop,1459,comment_17,In the medium term we should move all of Hadoop to use IP addresses instead of hostnames. I've filed the relevant bug in HADOOP-1487.,code_debt,low_quality_code
hadoop,1459,comment_18,"I just committed this. Thanks, Dhruba!",non_debt,-
hadoop,1488,summary,Remove all use of auto-progress threads in map/reduce,non_debt,-
hadoop,1488,description,"During the last rework of the shuffle, a lot of the stages of the shuffle had auto-progress threads added, leading to system lockups when the shuffle stalls. We need to add Progressables to the sort and fetching interfaces so that if any task gets stuck, it will eventually be killed by the framework.",design_debt,non-optimal_design
hadoop,1488,comment_0,I think the patch for HADOOP-1462 addresses this concern.,non_debt,-
hadoop,1536,summary,libhdfs tests failing,non_debt,-
hadoop,1536,description,"Starting today, 2 libhdfs tests are failing on Linux when I run ""ant -Dtest.output=yes package-libhdfs tar test-core test-libhdfs""",non_debt,-
hadoop,1536,comment_0,This is related to HADOOP-1283. This patch removed the implementation of file level locks from HDFS.,code_debt,dead_code
hadoop,1536,comment_1,HADOOP-932 was filed about removing the all of the locking interface. It probably time to fix it.,code_debt,dead_code
hadoop,1536,comment_2,Remove libhdfsLock and,non_debt,-
hadoop,1536,comment_3,The tests no longer fail with this patch. Moving to patch available.,non_debt,-
hadoop,1536,comment_5,The changes look fine. +1,non_debt,-
hadoop,1536,comment_6,I just committed this. Thanks Dhruba!,non_debt,-
hadoop,1545,summary,Maps hang when using native compression library to compress map out,non_debt,-
hadoop,1545,description,This issue is related to HADOOP-1193. I ran the same job. Each map of this job spawns a process which generates output to be cosumed by the map. The new native compression librar seems to hang some of the spawned processes. Some processes print using more than 100% CPU. Non-native library does not produce this problem.,non_debt,-
hadoop,1545,comment_0,"Hairong, after the investigations we have done into this and lack of closure on causes for this particular job's failure do you think it makes sense to move this bug to 0.15 and hence mark it as a non-blocker for now? Thanks!",non_debt,-
hadoop,1545,comment_1,1,non_debt,-
hadoop,1545,comment_2,"Thanks Hairong, I'm marking this *critical* for 0.15.0",non_debt,-
hadoop,1545,comment_3,Do we know the causes for this now ?,non_debt,-
hadoop,1545,comment_4,The last we checked there were a couple of application-level issues... I'm closing this for now. Please feel free to re-open or file a new jira.,non_debt,-
hadoop,1584,summary,Bug in readFields of GenericWritable,non_debt,-
hadoop,1584,description,"When getTypes() returns more than 127 entries, read of classes with index",non_debt,-
hadoop,1584,comment_0,Patch for trunk,non_debt,-
hadoop,1584,comment_2,"I just committed this. Thanks, Espen!",non_debt,-
hadoop,1586,summary,Progress reporting thread can afford to be slightly lenient towards exceptions other than ConnectException,design_debt,non-optimal_design
hadoop,1586,description,"Currently, in the loop of MAX_RETRIES (set to three) attempts are made to report progress/ping or All attempt failures are counted as critical. Here I am proposing a variant - treat only ConnectException exceptions are critical and treat the others as non-critical. The other exception could be the in the case of the two RPCs. The reason why I am proposing this is that since HADOOP-1462 went in, I have been seeing quite a few unexpected 65 deaths, and with some logging it appears that they happen, most of the time, due to the in the progress RPC call (before HADOOP-1462, the return value of progress would not be checked). And when the hack described above was put in, things improved considerably. One argument that one might make against the above proposal is that the tasktracker could be faulty, when a task is not able to successfully invoke an RPC on it even though it is able to connect. If this is indeed the case, even in the current scheme of things, the only resort is to restart the tasktracker (either manually, or, the JobTracker asks it to reinitialize), and in both the cases, normal behavior of the protocol will ensure that the child task will die (since the reinited tasktracker is going to return false for the progress/ping calls).",design_debt,non-optimal_design
hadoop,1586,comment_0,"This patch addresses the issue described, and also doubles the number of handlers for the server to 2*maxCurrentTasks.",non_debt,-
hadoop,1586,comment_1,i discovered an issue with the patch. Removing it and will submit another soon.,non_debt,-
hadoop,1586,comment_2,"Do we know why is being thrown? Is the TT too busy responding to the call? How about increasing the socket timeout? I'm not sure you want to treat and ConectException differently. What if the TT is hung, so that the former is thrown but not the latter - it might make sense for the Task to realize that and kill itself after 3 tries.",non_debt,-
hadoop,1586,comment_3,This issue is handled better in the related issue - HADOOP-1651,non_debt,-
hadoop,1664,summary,Hadoop DFS upgrade prcoedure,non_debt,-
hadoop,1664,description,"When upgrading from a July-9 to a July-25 nightly release, we are able to upgrade successfully on a single-node cluster, but failed on a 10 and a 200 node cluster. As it is not sure whether we made a mistake or not, I file this as an improvement. But going forward it is imperative that there is a safe and well-documented procedure to upgrade dfs without loss of data, including a rollback procedure and listing of operational procedures that are irreversibly destructive (hopefully an empty list).",non_debt,-
hadoop,1664,comment_0,"I am writing an admin guide for upgrading to Hadoop-0.14. will post it in couple of days. If you have any logs, please add them here. Upgrade and rollback procedure is same as before.",documentation_debt,low_quality_documentation
hadoop,1664,comment_1,"Datanode servers were apparently successful in upgrading: ... 2007-07-26 10:35:34,973 INFO Distributed upgrade for DataNode version -6 to current LV -7 is initialized. 2007-07-26 10:35:34,974 INFO Upgrading storage directory <hadoop-dir old LV = -5; old CTime = 1183153812398. new LV = -7; new CTime = 1185471333047 2007-07-26 10:36:58,098 INFO Upgrade of 10:36:58,587 INFO Opened server at 50010 ... but namenode server reported 0% upgrade long after that: 2007-07-26 10:43:04,818 INFO Upgrade still running. Avg completion on Datanodes: 0.00% with 0 errors. Even after 40 minutes no change in report status, namenode was still in safe mode, and if I wanted to force it to leave safe mode, it refused: hadoop dfsadmin -safemode leave safemode: Distributed upgrade is in progress. Name node is in safe mode.",non_debt,-
hadoop,1664,comment_2,"If possible, I would like to look at full log of a datanode and the namenode. There is new dfsadmin command 'upgradeProgress'.",non_debt,-
hadoop,1664,comment_3,"I will send you the location offline. BTW: I tried the dfsadmin command 'upgradeProgress' during that time, reporting 0.0% progress.",non_debt,-
hadoop,1664,comment_4,"Namenode log looks fine. It starts the CRC upgrade and is waiting for datanodes to start the same and join. But for some reason, datanodes don't start the CRC upgrade. I am not sure what was going on. If you ever able to reproduce this, please let me know. I am attaching relevant part of one of the datanode's log.",non_debt,-
hadoop,1664,comment_5,"We have never seen this behavior again. 0.14.x has gone through many upgrades, large and small.",non_debt,-
hadoop,1714,summary,fails on Windows,non_debt,-
hadoop,1714,description,"2007-08-13 18:48:42,036 INFO - Unpacking the tar file tar (child): Cannot execute remote shell: No such file or directory",non_debt,-
hadoop,1714,comment_0,The problem is with the above path. Should use 'cygpath'.,non_debt,-
hadoop,1714,comment_1,This patch fixes the path for 'tar' command. I have run the test on windows under cygwin and it passes.,non_debt,-
hadoop,1714,comment_2,+1 code review.,non_debt,-
hadoop,1714,comment_3,"I just committed this. Thanks, Raghu!",non_debt,-
hadoop,1773,summary,"Hadoop build (ant) hangs while setting up init target, build process hangs",non_debt,-
hadoop,1773,description,"Ant hangs during a build process, eventually (in hours) dies of heap exhaustion. The problematic lines in build.xml seem to be in the init target (fileset operations): - <touch 2:00 pm""- <fileset dir=""${conf.dir}"" <fileset </touch- <copy todir=""${conf.dir}"" verbose=""true""- <fileset dir=""${conf.dir}"" <mapper type=""glob"" from=""*.template"" to=""*""/- </copy- <copy verbose=""true""- <fileset <mapper type=""glob"" from=""*.template"" to=""*""/- </copy Commenting them out or deleting allows build to proceed to sucessful completion. Not being an expert, in either xml or ant, I'm not sure what exactly I'm missing with those lines gone, but at least I'm able to compile.",non_debt,-
hadoop,1773,comment_0,"I am using JDK 1.5.0_01 and Ant version 1.6.5 on Fedora Core 6 and it is the 32 bit version. Can you provide the exact procedure you followed, so that this problem can be reproduced? I had not problems in compiling or running Hadoop on a single node as specified on the Wiki page.",non_debt,-
hadoop,1773,comment_1,"Here's what I did yesterday: svn update rm -rf build ant -v -d What follows is the output of the last step; ant hangs after the last line. Hope this would be helpful. Apache Ant version 1.6.5 compiled on March 19 2007 Apache Ant version 1.6.5 compiled on March 19 2007 Buildfile: build.xml Adding reference: ant.PropertyHelper Detected Java version: 1.5 in: Detected OS: Linux Adding reference: ant.ComponentHelper Setting ro project property: ant.version -Setting ro project property: ant.file -Adding reference: ant.projectHelper Adding reference: ant.parsing.context Adding reference: ant.targets parsing buildfile with URI = Setting ro project property: ant.project.name -Adding reference: Hadoop Setting ro project property: ant.file.Hadoop -Project base dir set to: +Target: Adding reference: classpath Adding reference: test.classpath Adding reference: +Target: init +Target: record-parser +Target: +Target: +Target: compile-core-native +Target: compile-core +Target: compile-contrib +Target: compile +Target: compile-examples +Target: jar +Target: examples +Target: metrics.jar +Target: +Target: compile-core-test +Target: test-core +Target: test-contrib +Target: test +Target: test-cluster +Target: nightly +Target: checkstyle +Target: +Target: findbugs +Target: check-for-findbugs +Target: javadoc +Target: default-doc +Target: package +Target: tar +Target: clean +Target: deploy-contrib +Target: clean-contrib +Target: compile-libhdfs +Target: test-libhdfs +Target: clean-libhdfs +Target: doc-libhdfs +Target: package-libhdfs +Target: +Target: check-c++-makefiles +Target: +Target: compile-c++-utils +Target: +Target: compile-c++-pipes +Target: compile-c++ +Target: +Target: +Target: +Target: compile-ant-tasks +Target: ant-tasks +Target: clover +Target: clover.setup +Target: clover.info +Target: clover.check +Target: [property] Loading [property] Unable to find property file: [property] Loading [property] Unable to find property file: Setting project property: Name -Setting project property: name -Setting project property: version -Setting project property: final.name -Setting project property: year -Setting project property: libhdfs.version -Setting project property: src.dir -Setting project property: native.src.dir -Setting project property: examples.dir -Setting project property: anttasks.dir -Setting project property: lib.dir -Setting project property: conf.dir -Setting project property: docs.dir -Setting project property: contrib.dir -Setting project property: docs.src -Setting project property: c++.src -Setting project property: c++.utils.src -Setting project property: c++.pipes.src -Setting project property: -Setting project property: libhdfs.src -Setting project property: build.dir -Setting project property: build.classes -Setting project property: build.src -Setting project property: build.webapps -Setting project property: build.examples -Setting project property: build.anttasks -Setting project property: build.libhdfs -Setting project property: build.platform -Setting project property: build.native -Setting project property: build.c++ -Setting project property: build.c++.utils -Setting project property: build.c++.pipes -Setting project property: -Setting project property: build.docs -Setting project property: build.javadoc -Setting project property: build.encoding -Setting project property: install.c++ -Setting project property: -Setting project property: test.src.dir -Setting project property: test.build.dir -Setting project property: test.generated.dir -Setting project property: test.build.data -Setting project property: test.cache.data -Setting project property: test.log.dir -Setting project property: test.build.classes -Setting project property: test.build.testjar -Setting project property: test.build.javadoc -Setting project property: test.include -Setting project property: test.classpath.id -Setting project property: test.output -Setting project property: test.timeout -Setting project property: -Setting project property: -Setting project property: libhdfs.test.dir -Setting project property: web.src.dir -Setting project property: src.webapps -Setting project property: javadoc.link.java -Setting project property: javadoc.packages -Setting project property: dist.dir -Setting project property: javac.debug -Setting project property: javac.optimize -Setting project property: javac.deprecation -Setting project property: javac.version -Setting project property: javac.args -Setting project property: javac.args.warnings -Setting project property: clover.db.dir -Setting project property: clover.report.dir -[available] class was not found [available] Unable to load class to set property clover.present Condition false; not setting clover.enabled Could not load class for type image Could not load class for type wlrun Could not load class for type icontract Could not load class for type vajload Could not load class for type rexec Adding reference: classpath Could not load class for type image Could not load class for type wlrun Could not load class for type icontract Could not load class for type vajload Could not load class for type rexec Adding reference: test.classpath Could not load class for type image Could not load class for type wlrun Could not load class for type icontract Could not load class for type vajload Could not load class for type rexec Adding reference: Could not load class for type image Could not load class for type wlrun Could not load class for type icontract Could not load class for type vajload Could not load class for type rexec Could not load class for type stlist Could not load class for type telnet Could not load class for type netrexxc Could not load class for type ftp Could not load class for type starteam Could not load class for type stylebook Could not load class for type stlabel Could not load class for type splash Could not load class for type stcheckin Could not load class for type vajexport Could not load class for type stcheckout Could not load class for type ejbc Could not load class for type vajimport Could not load class for type wlstop Could not load class for type ddcreator fileset: Setup scanner in dir with patternSet{ includes: [**/*.jar] excludes: [**/excluded/] } Finding class Loaded from Finding class Loaded from Class java.lang.Object loaded from parent loader (parentFirst) Class loaded from ant loader (parentFirst) Class loaded from ant loader (parentFirst) Class java.lang.Throwable loaded from parent loader (parentFirst) Class loaded from parent loader (parentFirst) Class loaded from parent loader (parentFirst) Class java.io.IOException loaded from parent loader (parentFirst) Class loaded from parent loader (parentFirst) Class loaded from parent loader (parentFirst) Class loaded from parent loader (parentFirst) Class loaded from parent loader (parentFirst) Class loaded from parent loader (parentFirst) Class java.io.PrintStream loaded from parent loader (parentFirst) Finding class Loaded from Class loaded from ant loader (parentFirst) Class java.io.Writer loaded from parent loader (parentFirst) Class java.io.FileWriter loaded from parent loader (parentFirst) Class java.io.Reader loaded from parent loader (parentFirst) Class java.io.FileReader loaded from parent loader (parentFirst) Class java.lang.Exception loaded from parent loader (parentFirst) Finding class Loaded from Finding class Loaded from Class loaded from ant loader (parentFirst) Class loaded from ant loader (parentFirst) Class loaded from parent loader (parentFirst) Class java.lang.Class loaded from parent loader (parentFirst) Class loaded from parent loader (parentFirst) Class java.lang.String loaded from parent loader (parentFirst) Class java.io.File loaded from parent loader (parentFirst) Finding class Loaded from Class loaded from ant loader (parentFirst) Finding class Loaded from Class loaded from ant loader (parentFirst) Finding class Loaded from Class loaded from ant loader (parentFirst) Finding class Loaded from Class loaded from ant loader (parentFirst) +Datatype jsp-compile Setting project property: findbugs.home -Attempting to create object of type Adding reference: ant.executor Build sequence for target(s) `compile' is [clover.setup, clover.info, clover, init, record-parser, compile-c++-utils, compile-c++-pipes, compile-c++, compile-core, compile-contrib, compile-ant-tasks, compile] Complete build sequence is [clover.setup, clover.info, clover, init, record-parser, compile-c++-utils, compile-c++-pipes, compile-c++, compile-core, compile-contrib, compile-ant-tasks, compile, compile-examples, compile-core-test, test-core, test-contrib, test, jar, default-doc, javadoc, examples, deploy-contrib, ant-tasks, package, tar, nightly, test-cluster, checkstyle, clover.check, check-for-findbugs, compile-libhdfs, clean-libhdfs, findbugs, metrics.jar, test-libhdfs, doc-libhdfs, clean-contrib, clean, package-libhdfs, ] clover.setup: Skipped because property 'clover.enabled' not set. clover.info: clover: init: [touch] Setting millis to 33688800000 from datetime attribute Could not load class for type image Could not load class for type wlrun Could not load class for type icontract Could not load class for type vajload Could not load class for type rexec Could not load class for type stlist Could not load class for type telnet Could not load class for type netrexxc Could not load class for type ftp Could not load class for type starteam Could not load class for type stylebook Could not load class for type stlabel Could not load class for type splash Could not load class for type stcheckin Could not load class for type vajexport Could not load class for type stcheckout Could not load class for type ejbc Could not load class for type vajimport Could not load class for type wlstop Could not load class for type ddcreator fileset: Setup scanner in dir with patternSet{ includes: [**/*.template] excludes: [] }",non_debt,-
hadoop,1773,comment_2,"I carried out the same commands on the system I had mentioned earlier. I get a successful build. Can you also check the variables ANT_HOME, JAVA_HOME, CLASSPATH and PATH? Also, do you finally see a ""*build successful*"" when you compile after removing the offending lines from build.xml?",non_debt,-
hadoop,1773,comment_3,ANT_HOME is unset CLASSPATH is unset I do see BUILD SUCCESSFUL at the end.,non_debt,-
hadoop,1773,comment_4,"I can explain what those lines are trying to do though I have not been able to figure out the reason for the build process to hang. 1. The lines in between the <touch2. The lines between the <copy3. Same as 2 but for the contrib directory. When I try building Hadoop, I first see the commands for creating directories under the init tag. This just before the lines that you have commented out. But these are missing from the output that you have provided. Thanks Nilay",non_debt,-
hadoop,1773,comment_5,"Here's what I did today: svn update rm -rf build ant -v -d What follows is the output of the last step; ant hangs after the last line. Hope this would be helpful. Apache Ant version 1.6.5 compiled on March 19 2007 Apache Ant version 1.6.5 compiled on March 19 2007 Buildfile: build.xml Adding reference: ant.PropertyHelper Detected Java version: 1.5 in: Detected OS: Linux Adding reference: ant.ComponentHelper Setting ro project property: ant.version -Setting ro project property: ant.file -Adding reference: ant.projectHelper Adding reference: ant.parsing.context Adding reference: ant.targets parsing buildfile with URI = Setting ro project property: ant.project.name -Adding reference: Hadoop Setting ro project property: ant.file.Hadoop -Project base dir set to: +Target: Adding reference: classpath Adding reference: test.classpath Adding reference: +Target: init +Target: record-parser +Target: +Target: +Target: compile-core-native +Target: compile-core +Target: compile-contrib +Target: compile +Target: compile-examples +Target: jar +Target: examples +Target: metrics.jar +Target: +Target: compile-core-test +Target: test-core +Target: test-contrib +Target: test +Target: test-cluster +Target: nightly +Target: checkstyle +Target: +Target: findbugs +Target: check-for-findbugs +Target: javadoc +Target: default-doc +Target: package +Target: tar +Target: clean +Target: deploy-contrib +Target: clean-contrib +Target: compile-libhdfs +Target: test-libhdfs +Target: clean-libhdfs +Target: doc-libhdfs +Target: package-libhdfs +Target: +Target: check-c++-makefiles +Target: +Target: compile-c++-utils +Target: +Target: compile-c++-pipes +Target: compile-c++ +Target: +Target: +Target: +Target: compile-ant-tasks +Target: ant-tasks +Target: clover +Target: clover.setup +Target: clover.info +Target: clover.check +Target: [property] Loading [property] Unable to find property file: [property] Loading [property] Unable to find property file: Setting project property: Name -Setting project property: name -Setting project property: version -Setting project property: final.name -Setting project property: year -Setting project property: libhdfs.version -Setting project property: src.dir -Setting project property: native.src.dir -Setting project property: examples.dir -Setting project property: anttasks.dir -Setting project property: lib.dir -Setting project property: conf.dir -Setting project property: docs.dir -Setting project property: contrib.dir -Setting project property: docs.src -Setting project property: c++.src -Setting project property: c++.utils.src -Setting project property: c++.pipes.src -Setting project property: -Setting project property: libhdfs.src -Setting project property: build.dir -Setting project property: build.classes -Setting project property: build.src -Setting project property: build.webapps -Setting project property: build.examples -Setting project property: build.anttasks -Setting project property: build.libhdfs -Setting project property: build.platform -Setting project property: build.native -Setting project property: build.c++ -Setting project property: build.c++.utils -Setting project property: build.c++.pipes -Setting project property: -Setting project property: build.docs -Setting project property: build.javadoc -Setting project property: build.encoding -Setting project property: install.c++ -Setting project property: -Setting project property: test.src.dir -Setting project property: test.build.dir -Setting project property: test.generated.dir -Setting project property: test.build.data -Setting project property: test.cache.data -Setting project property: test.log.dir -Setting project property: test.build.classes -Setting project property: test.build.testjar -Setting project property: test.build.javadoc -Setting project property: test.include -Setting project property: test.classpath.id -Setting project property: test.output -Setting project property: test.timeout -Setting project property: -Setting project property: -Setting project property: libhdfs.test.dir -Setting project property: web.src.dir -Setting project property: src.webapps -Setting project property: javadoc.link.java -Setting project property: javadoc.packages -Setting project property: dist.dir -Setting project property: javac.debug -Setting project property: javac.optimize -Setting project property: javac.deprecation -Setting project property: javac.version -Setting project property: javac.args -Setting project property: javac.args.warnings -Setting project property: clover.db.dir -Setting project property: clover.report.dir -[available] class was not found [available] Unable to load class to set property clover.present Condition false; not setting clover.enabled Could not load class for type image Could not load class for type wlrun Could not load class for type icontract Could not load class for type vajload Could not load class for type rexec Adding reference: classpath Could not load class for type image Could not load class for type wlrun Could not load class for type icontract Could not load class for type vajload Could not load class for type rexec Adding reference: test.classpath Could not load class for type image Could not load class for type wlrun Could not load class for type icontract Could not load class for type vajload Could not load class for type rexec Adding reference: Could not load class for type image Could not load class for type wlrun Could not load class for type icontract Could not load class for type vajload Could not load class for type rexec Could not load class for type stlist Could not load class for type telnet Could not load class for type netrexxc Could not load class for type ftp Could not load class for type starteam Could not load class for type stylebook Could not load class for type stlabel Could not load class for type splash Could not load class for type stcheckin Could not load class for type vajexport Could not load class for type stcheckout Could not load class for type ejbc Could not load class for type vajimport Could not load class for type wlstop Could not load class for type ddcreator fileset: Setup scanner in dir with patternSet{ includes: [**/*.jar] excludes: [**/excluded/] } Finding class Loaded from Finding class Loaded from Class java.lang.Object loaded from parent loader (parentFirst) Class loaded from ant loader (parentFirst) Class loaded from ant loader (parentFirst) Class java.lang.Throwable loaded from parent loader (parentFirst) Class loaded from parent loader (parentFirst) Class loaded from parent loader (parentFirst) Class java.io.IOException loaded from parent loader (parentFirst) Class loaded from parent loader (parentFirst) Class loaded from parent loader (parentFirst) Class loaded from parent loader (parentFirst) Class loaded from parent loader (parentFirst) Class loaded from parent loader (parentFirst) Class java.io.PrintStream loaded from parent loader (parentFirst) Finding class Loaded from Class loaded from ant loader (parentFirst) Class java.io.Writer loaded from parent loader (parentFirst) Class java.io.FileWriter loaded from parent loader (parentFirst) Class java.io.Reader loaded from parent loader (parentFirst) Class java.io.FileReader loaded from parent loader (parentFirst) Class java.lang.Exception loaded from parent loader (parentFirst) Finding class Loaded from Finding class Loaded from Class loaded from ant loader (parentFirst) Class loaded from ant loader (parentFirst) Class loaded from parent loader (parentFirst) Class java.lang.Class loaded from parent loader (parentFirst) Class loaded from parent loader (parentFirst) Class java.lang.String loaded from parent loader (parentFirst) Class java.io.File loaded from parent loader (parentFirst) Finding class Loaded from Class loaded from ant loader (parentFirst) Finding class Loaded from Class loaded from ant loader (parentFirst) Finding class Loaded from Class loaded from ant loader (parentFirst) Finding class Loaded from Class loaded from ant loader (parentFirst) +Datatype jsp-compile Setting project property: findbugs.home -Attempting to create object of type Adding reference: ant.executor Build sequence for target(s) `compile' is [clover.setup, clover.info, clover, init, record-parser, compile-c++-utils, compile-c++-pipes, compile-c++, compile-core, compile-contrib, compile-ant-tasks, compile] Complete build sequence is [clover.setup, clover.info, clover, init, record-parser, compile-c++-utils, compile-c++-pipes, compile-c++, compile-core, compile-contrib, compile-ant-tasks, compile, compile-examples, compile-core-test, test-core, test-contrib, test, jar, default-doc, javadoc, examples, deploy-contrib, ant-tasks, package, tar, nightly, test-cluster, checkstyle, clover.check, check-for-findbugs, compile-libhdfs, clean-libhdfs, findbugs, metrics.jar, test-libhdfs, doc-libhdfs, clean-contrib, clean, package-libhdfs, ] clover.setup: Skipped because property 'clover.enabled' not set. clover.info: clover: init: [touch] Setting millis to 33688800000 from datetime attribute Could not load class for type image Could not load class for type wlrun Could not load class for type icontract Could not load class for type vajload Could not load class for type rexec Could not load class for type stlist Could not load class for type telnet Could not load class for type netrexxc Could not load class for type ftp Could not load class for type starteam Could not load class for type stylebook Could not load class for type stlabel Could not load class for type splash Could not load class for type stcheckin Could not load class for type vajexport Could not load class for type stcheckout Could not load class for type ejbc Could not load class for type vajimport Could not load class for type wlstop Could not load class for type ddcreator fileset: Setup scanner in dir with patternSet{ includes: [**/*.template] excludes: [] }",non_debt,-
hadoop,1773,comment_6,"I don't see any error on fedora_core_x64 jdk_1.5 ant_1.7 Quoting a code snippet from build.xml <touch 2:00 pm"" <fileset dir=""${conf.dir}"" <fileset </touch What is the significance of ""01/25/1971 2:00 pm"" .. if the idea is to stamp the files with an older date.. then a better thing to do is <touch millis=""0"" <fileset dir=""${conf.dir}"" <fileset </touch This may not fix the issue.. its just a suggestion to make the build file easier to understand",design_debt,non-optimal_design
hadoop,1773,comment_7,"Thanks Hirishikesh. Since this seems unrelated, can you file your suggestion as a new issue and attach a patch file? Instructions for creating patch files are here:",non_debt,-
hadoop,1773,comment_8,It just realized that I may have withheld one important piece of information: local repository and hadoop build are located on an NFS volume...,non_debt,-
hadoop,1773,comment_9,"Hi Yuri, Can you try this out ... Say you have a directory abc physically present on a machine X and you have exported this directory On machine Y you mount the abc directory on a mount point /mnt/testtouch The you login to machine Y as a normal user (who shall be firing the hadoop) build and then try to touch files /mnt/testtouch using the unix touch command ? IMO when touching files on a NFS share, the ""modified time"" will need to be changed in the file system of the machine which physically holds the files (and may be thats causing the blockage) Also can you try the build on the machine which physically holds the files (i.e. the machine from which the NFS share is exported) ?",non_debt,-
hadoop,1773,comment_10,Awaiting comments ....,non_debt,-
hadoop,1773,comment_11,"Hi Hrishikesh, Touching files on NFS using regular ""touch"" works fine (also works using the time string from build). I can't build on NFS server because it's out of my control, but here's what I did: 1. Copied the whole tree to a local dir 2. ant clean 3. rm build.xml; svn update build.xml 4. ant -v -d Here's clipped output: ... init: Setting project property: touch.temp.file - [touch] Creating /tmp/null2124908001 Could not load class for type image Could not load class for type wlrun Could not load class for type icontract Could not load class for type vajload Could not load class for type rexec Could not load class for type stlist Could not load class for type telnet Could not load class for type netrexxc Could not load class for type ftp Could not load class for type starteam Could not load class for type stylebook Could not load class for type stlabel Could not load class for type splash Could not load class for type stcheckin Could not load class for type vajexport Could not load class for type stcheckout Could not load class for type ejbc Could not load class for type vajimport Could not load class for type wlstop Could not load class for type ddcreator fileset: Setup scanner in dir /tmp/hadoop/conf with patternSet{ includes: [**/*.template] excludes: [] } It hangs after the last line.",non_debt,-
hadoop,1773,comment_12,"Hmm.. this is very strange... Infact I have been building hadoop on a NFS share all along and I never had any problems. My home directory gets auto mounted from the NFS server and the trunk branch is checked out in my home directory. You mentioned that the unix touch works as expected so the permissions part is not a problem, we can strike it off. Now the only 2 things that can affect this is ANT or the JVM. Can you try with ANT 1.7 and latest JDK ? Also can you mount the share onto some other machine ? and try a build there.",non_debt,-
hadoop,1773,comment_13,resolving as a can't reproduce now that trunk is on Maven,non_debt,-
hadoop,1803,summary,Generalize making contrib bin content executable in ant package target,non_debt,-
hadoop,1803,description,"In binary distributions of hadoop, hbase bin content are not executable.",non_debt,-
hadoop,1803,comment_0,Patch for that runs a chmod ugo+x on all contrib bin directories.,non_debt,-
hadoop,1803,comment_1,Works locally. Passing to hudson.,non_debt,-
hadoop,1803,comment_3,"I just committed this. Thanks, Michael!",non_debt,-
hadoop,1926,summary,Design/implement a set of compression benchmarks for the map-reduce framework,non_debt,-
hadoop,1926,description,"It would be nice to benchmark various compression codecs for use in the hadoop (existing codecs like zlib, lzo and in-future bzip2 etc.) and run these along with our nightlies or weeklies. Here are some steps: a) Fix HADOOP-1851 ( Map output compression codec cannot be set independently of job output compression codec) b) Implement a random-text-writer along the lines of to generate large amounts of synthetic textual data for use in sort. One way to do this is to pick a word randomly from till we get enough bytes per map. To be safe, we could store an array of Strings of a snap-shot of the words in c) Take a dump of wikipedia and/or the ebooks from Project Gutenberg and use them as non-synthetic data to run sort/wordcount against. For both b) and c) we should setup nightly/weekly benchmark runs with different codecs for reduce-outputs and map-outputs (shuffle) and track each. Thoughts?",non_debt,-
hadoop,1926,comment_0,"FYI, Lucene uses the wikipedia text for benchmarking. It keeps a copy on people.apache.org. For details, see:",non_debt,-
hadoop,1926,comment_1,Devaraj had a very good comment to add: we should also benchmark performance of sort with both {{RECORD}} compression and {{BLOCK}} compression.,non_debt,-
hadoop,1926,comment_2,Here is an implementation of a *randomtextwriter* which can generate random textual data in any output-format (e.g. etc.). This patch also enhances examples/Sort and test/SortValidator to ensure they can be used with randomtextwriter.,non_debt,-
hadoop,1926,comment_4,Fixed the javadoc oversight.,documentation_debt,low_quality_documentation
hadoop,1926,comment_6,"I just committed this. Thanks, Arun!",non_debt,-
hadoop,1961,summary,"-get, -copyToLocal fail when single filename is passed",non_debt,-
hadoop,1961,description,"In 0.14.1 and in trunk, when I try % hadoop dfs -get /user/knoguchi/aaa aaa get: Failed to rename tmp file to local destination ""aaa"". Remote source file is saved to This works. % hadoop dfs -get /user/knoguchi/aaa ./aaa or % hadoop dfs -get /user/knoguchi/aaa /home/knoguchi/aaa My guess. With change in HADOOP-1292, it now creates a tmp file when -copyToLocal. When destination path is passed without any directory, tmp file is created under '/tmp'. Otherwise, it uses the same directory as the destination path. In Java API for File.renameTo, it says "" The rename operation might not be able to move a file from one filesystem to another"", so renameTo call from /tmp/_tmpfile to /home/knoguchi can fail.",non_debt,-
hadoop,1961,comment_0,"[A in HADOOP-1292 says : I am not sure about the above comment either. When rename() fails wouldn't that be just another case of failure to copy? Once we use same filesystem as the intended destination, rename() won't fail. And if it does fail, we could just treat it as any other failure.",non_debt,-
hadoop,1961,comment_1,Fix is to pass instead of {{dst}} for Also this removes 'if conditional' around File.copy() since return is expected to be true and not handled when it is false anyway. This patch also removes the special treatment of rename(). It is strictly not required for this fix. Should I remove the change?,non_debt,-
hadoop,1961,comment_2,There are some more inconsistencies in copyToLocal() : For example : I will fix this as well. There is also an extra isDirectory() that is not required.,code_debt,low_quality_code
hadoop,1961,comment_3,"There are at least three versions of copyToLocal : one in FsShell, one in ChecksumFileSystem, and FileUtil. All of these implement same logic and recursion.. but are slightly different. FsShell and ChecksumFS versions in turn invoke FileUtil version. We should remove FsShell and ChecksumFS, at least in 0.15 or 0.16.",code_debt,duplicated_code
hadoop,1961,comment_4,"Attached patch keeps copyToLocal() in FsShell.java closer to 0.13 structure. Fixes following regressions : - copying a file with simple file name for destination as described in the description. - handling of globes : {{globePath()}} was invoked late and when it returns just one path, treated it as non-globed path, unless the destination was a directory (see example in earlier comment). Retains the following change between 0.13 and 0.14 : - {{bin/hadoop fs -get dir1 dir2 localdir}} # two dirs can be specified with a glob -- 0.13 copies _contents_ of dir1 and dir2 into localdir -- 0.14.x copies dir1 and dir2 into localdir (matches with regular cp) The following behaviour is new : - when rename() fails during the copy does not copy temp file to another temp file. - {{bin/hadoop fs -get dir1 localdir}} # and localdir exists -- 0.13 and 0.14.1 copy contents of dir1 into localdir -- 0.14.2 copies dir1 into localdir -- btw, when localdir does not exist, all versions copy contents of dir1 into localdir.",non_debt,-
hadoop,1961,comment_5,"Raghu, your patch cannot be applied to the current trunk. Could you update it?",non_debt,-
hadoop,1961,comment_6,This patch applies to 0-14 branch. I thought I tested it with trunk as well. I will attach an updated patch for trunk.,non_debt,-
hadoop,1961,comment_7,"Updated patch for trunk. If we want to retain the partially copied file, then fix I would suggest is to remove 'deleteOnExit' flag for the tmp file.",non_debt,-
hadoop,1961,comment_8,"+1 Patch looks good. Below are some minor thoughts. - I totally agree that there are redundant copyToLocal methods. See also HADOOP-1544. - In the case of rename failing, we know that the tmp file is good but cannot be renamed to dst. User can easily rename the tmp file. For the other exception cases, user don't know what to do to fix the problem. If we remove ""deleteOnExit"" flag, then we cannot easily tell whether the tmp file is perfect or not. - Should we not use deprecated API anymore? e.g.",code_debt,low_quality_code
hadoop,1961,comment_9,"yeah, I saw that. I just used what ever was there before. I think it will be removed when all the references to it are removed. With this fix rename should rarely fail. I think users will be wary of any error. The main concern that Dhruba mentioned is while users are copying very large file and they might might be ok with partial files. Retaining the file only in case of rename failure does not fix that problem.",non_debt,-
hadoop,1961,comment_11,I was dealing with multiple patches and totally missed running the unit tests for this. TestDFSShell faile because of the following change : There are no change the fix. Only TestDFSShell.java is changed. Also updated the patch to copy multiple directories.,test_debt,lack_of_tests
hadoop,1961,comment_13,I just committed this. Thanks Raghu!,non_debt,-
hadoop,2007,summary,Jobs use incorrect path to job.xml for different users,non_debt,-
hadoop,2007,description,"We run hadoop/hdfs under a generic username of 'hadoop'. When submitting a job as user 'fred', the job.xml file gets created correctly as I think the incorrect path gets created on line 133 in JobInProgress.java right before the call to copyToLocalFile. If I hardcode the correct path (just to test my theory) of in place of the call to the job kicks off and runs to completion as the user 'fred' with hadoop/hdfs running as the user 'hadoop'. Here's a portion of the output when the job is submitted by user 'fred': [fred@hdm01 ~]$ hadoop --config jar wordcount mytest/input/data output2 07/10/07 17:52:14 INFO Total input paths to process : 62 No such file or directory",non_debt,-
hadoop,2007,comment_0,Please configure mapred.system.dir to be a constant and somewhere outside of /tmp. I personally like to use something like /hadoop/system so that it is very clear that this is a framework directory.,non_debt,-
hadoop,2077,summary,Logging version number (and compiled date) at STARTUP_MSG,non_debt,-
hadoop,2077,description,This will help us figure out which version of hadoop we were running when looking back the logs.,non_debt,-
hadoop,2077,comment_0,Simple fix. I haven't gotten around to testing it much since svn.apache.org is _super_ slow...,test_debt,lack_of_tests
hadoop,2077,comment_1,"Minor change in formatting of the output, which now looks like:",code_debt,low_quality_code
hadoop,2077,comment_3,Slight modification to the log-msg:,non_debt,-
hadoop,2077,comment_4,+1 Thanks!,non_debt,-
hadoop,2077,comment_5,I just committed this.,non_debt,-
hadoop,2148,summary,Inefficient,code_debt,low_quality_code
hadoop,2148,description,"first verifies that the block is valid and then returns the file name corresponding to the block. Doing that it performs the data-node blockMap lookup twice. Only one lookup is needed here. This is important since the data-node blockMap is big. Another observation is that data-nodes do not need the blockMap at all. File names can be derived from the block IDs, there is no need to hold Block to File mapping in memory.",code_debt,slow_algorithm
hadoop,2148,comment_0,"there is no need to hold Block to File mapping in memory. It has the full path like ...."". Also existence of block to File mapping indicates that file is still valid. currently getBlockFile() is expected to throw IOException when there is no mapping. yes, removing double look up would be good.",non_debt,-
hadoop,2148,comment_1,"Yes I understand that the blockMap is there right now because of the subdirs and the volumes, but it would be better to have file names computable. This should probably belong to a different issue though.",non_debt,-
hadoop,2148,comment_2,This patch optimizes and so that they perform the data-node blockMap lookup only once. The patch is pretty straightforward. I don't think we should benchmark this.,code_debt,slow_algorithm
hadoop,2148,comment_4,This fixes findBugs warnings. I could not reproduce test timeout in This test has a lot of test cases. My suspicion is that if Hudson runs slow it could run out of time on this.,code_debt,low_quality_code
hadoop,2148,comment_6,+1 This patch looks good. It removes a duplicate block map look up.,code_debt,duplicated_code
hadoop,2148,comment_7,I verified both test failures. They are not related to the patch imo. In the first case took too long. It finished only 47 cases out of 71 in 13 minutes when the junit framework terminated it. In the second case the cluster fell into a infinite loop trying to replicate a block. Filed HADOOP-3050 to investigate it.,test_debt,expensive_tests
hadoop,2148,comment_8,I just committed this.,non_debt,-
hadoop,2181,summary,Input Split details for maps should be logged,non_debt,-
hadoop,2181,description,It would be nice if Input split details are logged someplace. This might help debugging failed map tasks,non_debt,-
hadoop,2181,comment_0,"Here is the proposed design for logging Inputsplits: 1. We can log input split details at the start of the job in initTasks(). 2. Log them in Job history also, once for each tip. Viewing split details on Web UI: 1. Input split details can be printed in a table in taskdetails.jsp whcih prints all the attempts of a tip. 2. Similarly, the split details are printed in a table on for viewing history.",non_debt,-
hadoop,2181,comment_1,Patch for proposed design. Thoughts?,non_debt,-
hadoop,2181,comment_2,Please check the history log file format is still compatible and make sure the JobHistory parser can parse the log file properly.,non_debt,-
hadoop,2181,comment_4,Yes. The log file format is compatible and also the job history parser parses the file properly.,non_debt,-
hadoop,2181,comment_5,Cancelling patch as there is a bug in task_details.jsp if there are no splits.,non_debt,-
hadoop,2181,comment_6,Patch with fix for jsp files for maps having no splits.,non_debt,-
hadoop,2181,comment_8,"I think this issue doesnt require a test case, since this adds logging and modifies jsp files to view input splits.",non_debt,-
hadoop,2181,comment_9,"Some comments: 1) The change in JobInProgress to do with wasRunning is problematic. In some cases, you might end up logging the split info more than once. 2) The method doesn't fit well in the StringUtils class. OTOH you could define it as a private method in TaskInProgress from where you call it.",design_debt,non-optimal_design
hadoop,2181,comment_10,Patch after incorporating review comments.,non_debt,-
hadoop,2181,comment_12,Patch in sync with the trunk,non_debt,-
hadoop,2181,comment_14,"I just committed this. Thanks, Amareshwari!",non_debt,-
hadoop,2204,summary,does not wait.,non_debt,-
hadoop,2204,description,This makes unit tests fail in unexpected ways. In DFSTestUtil.java :,non_debt,-
hadoop,2204,comment_0,Trivial fix.,non_debt,-
hadoop,2204,comment_2,I just committed this. Thanks Raghu!,non_debt,-
hadoop,2205,summary,Regenerate entire hadoop website since site.xml was changed by HADOOP-1917,non_debt,-
hadoop,2205,description,HADOOP-1917 changed but did not regenerate and to reflect those changes.,documentation_debt,outdated_documentation
hadoop,2205,comment_0,I'd also like to use this jira to fix 2 minor typos in the website introduced by HADOOP-1917.,documentation_debt,low_quality_documentation
hadoop,2205,comment_1,"Attached patch fixes some typos and broken links introduced by HADOOP-1917. I'll also ensure that website is updated, here is what my workspace looks like, after this patch:",documentation_debt,low_quality_documentation
hadoop,2205,comment_2,"+1 for the patch, and +1 for deploying other pages.",non_debt,-
hadoop,2205,comment_3,Thanks for the review Enis. I just committed this.,non_debt,-
hadoop,2208,summary,Reduce frequency of Counter updates in the task tracker status,non_debt,-
hadoop,2208,description,"Currently, We have counter updates from task tracker to job tracker on every heartbeat. Both counter name and the values are updated for every heartbeat. This can be improved by sending names and values for the first time and only the values after that. The frequency can be reduced by doing update only when the counters got changed.",design_debt,non-optimal_design
hadoop,2208,comment_0,One really simple option is to to add a {{Counters.clear}} method and call it from this will ensure that only updated counters are sent in every hearbeat.,non_debt,-
hadoop,2208,comment_1,"I jumped to a hasty conclusion. Forget it, won't work. The way to get that to work is to add a {{boolean}} {{updated}} flag to {{CounterRec}}, set that to true on every call and check and send only updated ones. The {{updated}} flag should be cleared via Thoughts?",non_debt,-
hadoop,2208,comment_2,"Actually, I would also cap sending the counters to once every 10 seconds and when the task changes state. That way, if you send an extra heartbeat to get a new task, you won't send the counters.",non_debt,-
hadoop,2208,comment_3,The attached patch sends updated counters. We mark counters as updated when they are created with a value or an increment operation is done. And sending the counters is capped at 1min.,non_debt,-
hadoop,2208,comment_5,"Sorry, this patch doesn't apply anymore. Could you please regenerate this?",non_debt,-
hadoop,2208,comment_6,patch in sync with trunk.,non_debt,-
hadoop,2208,comment_8,"I see a potential bug with this patch: the TaskTracker _caches_ counters sent by the child-task till it's sent out to the JobTracker via the heartbeat. Hence, we need to *merge* the ones received from the child in not just over-write them as-is today. Minor nit: the merge of the counters probably belongs to rather than ...",code_debt,low_quality_code
hadoop,2208,comment_9,Submiting patch after incorporating review comments,non_debt,-
hadoop,2208,comment_10,"Some comments: You don't have to call setSendCounters in JobInProgress.java, and, you probably should rename the APIs getSendCounters and setSendCounters to something more intuitive.",code_debt,low_quality_code
hadoop,2208,comment_11,Submit again after incorporating Devaraj's comments.,non_debt,-
hadoop,2208,comment_12,"I'm pretty worried about the approach of this patch. It takes it from always sending the current values for the counters to just sending the ones that changed. That doesn't seem like an optimization that is likely to be important. Have you run large jobs that show this is important? My concern is that sending the deltas makes the system very vulnerable to losing or duplicating a message. My preference would be to have a boolean in the TaskStatus whether it should be sending the counters or not, but always send the current values of all counters. I'd also recommend against the current sendCounters and doSendCounters. I think your original names were better: Maybe they should be something like:",code_debt,low_quality_code
hadoop,2208,comment_13,Cancelling patch while Owen's feedback gets incorporated...,non_debt,-
hadoop,2208,comment_14,submiting patch after incorporating Owen's comments,non_debt,-
hadoop,2208,comment_16,"I just committed this. Thanks, Amareshwari!",non_debt,-
hadoop,2254,summary,MiniMR tests timeout as a result of HADOOP-1281,non_debt,-
hadoop,2254,description,"The following tests timeout with the patch and do not if I reverse it: TestMiniMRClasspath I don't know, but this was reported by Hudson, and yet the patch was committed, why?",non_debt,-
hadoop,2254,comment_0,"I've reverted HADOOP-1281 which should fix this. 'ant test' with HADOOP-1281 works mostly, atleast on my machine but seems to break sporadically... I'll investigate.",non_debt,-
hadoop,2270,summary,Title: DFS submit client params overrides final params on cluster,non_debt,-
hadoop,2270,description,"hdfs client params over-rides the params set as final on hdfs cluster nodes. default valuesv of cleint side hadoop-site.xml values override the final prameters of hdfs hadoop-site.xml . oberved the following cases -: 1. and dfs.replication=2 marked final under hadoop-site.xml on hdfs cluster. When fsShel command ""hadoop dfs -put local_dir dest"" fired from submission host Files will still get replicated 3 times (default) instead of final dfs.replication=2. Similarly when ""hadoop dfs -rmr dfs_dir OR hadoop dfs -rm file_path "" fired from submit client the file/driectory diectly got deleted without being moved to /recycle. Here hadoop-site.xml on submit client does not specify dfs.trash.root, dfs.trash.interval and dfs.replication. Same is the case when we submit mapred JOB from client and job.xml dispalys default values which overrides the lsuter values. 2. and dfs.replication=2 marked final under hadoop-site.xml on hdfs cluster. And and dfs.replication=5 under hadoop-site.xml on submit client. When fsShel command ""hadoop dfs -put local_dir dest"" fired from submit client Files will get replicated 5 times instead of final dfs.replication=2. Similarly when ""hadoop dfs -rmr dfs_dir OR hadoop dfs -rm file_path "" fired from submit client the file/driectory diectly will be moved to /rubbish instead of /recycle. Same is the case when we submit mapred job from client, job.xml displays following values -: and dfs.replication=5",non_debt,-
hadoop,2270,comment_0,"Please read dfs.trash.root, dfs.trash.interval as fs.trash.root, fs.trash.interval",non_debt,-
hadoop,2270,comment_1,"I doubt this is still an issue, but it would be good for someone to verify. I'll mark this as a newbie jira for someone to look at, just in case...",non_debt,-
hadoop,2270,comment_2,I opened this very long back. Closing it,non_debt,-
hadoop,2402,summary,Lzo compression compresses each write from TextOutputFormat,non_debt,-
hadoop,2402,description,"Outputting with TextOutputFormat and Lzo compression generates a file such that each key, tab delimiter, and value are compressed separately.",non_debt,-
hadoop,2402,comment_0,"Inserted a 64k to Effects compression similar to block, lzo compressed SequenceFile (~20%). For comparison, lzop (command line compression utility backed by lzo lib) uses 256k blocks (and a different file format) and compresses the same 100M sample by 60%. As an aside, with this applied, Zip compressed text files are written approximately 10% faster; uncompressed text about 4% faster.",code_debt,slow_algorithm
hadoop,2402,comment_2,"I think this is probably the right approach, to not require the codecs themselves to return buffered streams. But the size of the buffer should be rather than a fixed 64k, no?",code_debt,low_quality_code
hadoop,2402,comment_3,"In the native libs, it looks like io.file.buffer.size determines the maximum size of the copy to the OutputStream from the buffer containing the compressed data. The GzipCodec and LzoCodec define their own properties and defaults for the size of this native buffer (both 64k). The reasoning went, if the buffer is larger than the native lib's buffer, it's still going to be blocked until that buffer's been flushed to the OutputStream. If the buffer is io.file.buffer.size (defaulting to 4k), then it's going to be giving the compression codec data 4k at a time. For Lzo, this means it will compress no more than 4k at a time, yielding even less than 20% compression. We could introduce a new property that sets the size of this buffer- or use the property given to Gzip/Lzo- but that's not very attractive, either. LzoCodec returns a stream wrapped in a but it doesn't provide any buffering. It ensures that no more than MAX_INPUT_SIZE (defaulting to 64k less the compression overhead) is compressed at once. This might be a better place to add some buffering, but then the codec will be returning a buffered stream.",non_debt,-
hadoop,2402,comment_4,"We can't compress multiple buffers together with the lzo codec? It only compresses buffer-at-a-time? If so, then it should do the buffering & set the buffer size, since this is an lzo-specific issue. I don't think it's a bug for a codec to return a buffered stream if a particular buffer size is required to get good performance from that codec. If it's impossible to get lzo to compress data across buffers, and 64k or larger is required to get good compression, then it should mandate that buffer size, perhaps adding a new configuration parameter. Separately, we should consider whether to (a) unilaterally add an io.file.buffer.size buffer in TextOutputFormat, since it helps other codecs, or (b) assume that all codecs return appropriately buffered streams, and add a buffer in the Zip codec if it improves performance. If a io.file.buffer.size buffer=4k gives somewhat improved Zip performance, and a 64k buffer gives even better performance, I think that's okay. Performance should improve a bit by increasing at the expense of chewing up more memory per open file. The default setting should be for decent performance with minimal memory use.",code_debt,slow_algorithm
hadoop,2402,comment_5,"This patch reworks the lzo codec to fill its nio buffer before spilling to the native code. It effects the following results for 5GB of random text: Write: || Format || Type || Time (s from trunk, pcnt change) || Compression (% from uncompressed) || | SEQ | BLK | -51 (-31.1%) | 54.9% (23.7% using trunk) | | SEQ | REC | +5 (+2.7%) | -3.6% (-3.6% using trunk) | | TXT | | -96 (-63.9%) | 54.8% (-12.3% using trunk) | Read: || Format || Type || Time (s from trunk, pcnt change) || | SEQ | BLK | +7 (+8.6%) | | SEQ | REC | +8 (+5.6%) | | TXT | | -83 (-39.1%) | The default extension should probably be something other than .lzo, since we're not compatible with lzop.",non_debt,-
hadoop,2402,comment_7,I don't think the timeout in contrib is related; trying again.,non_debt,-
hadoop,2402,comment_9,"Mostly looks ok, but there are too many unrelated white-space changes - hence, I'm cancelling this patch.",code_debt,low_quality_code
hadoop,2402,comment_10,"Removed most whitespace changes, except those reformatting sections with lines greater than 80 chars",code_debt,low_quality_code
hadoop,2402,comment_12,"I just committed this. Thanks, Chris!",non_debt,-
hadoop,2424,summary,lzop compatible CompressionCodec,non_debt,-
hadoop,2424,description,"LzoCodec currently outputs at most (default 64k)- less the compression overhead- bytes per write (HADOOP-2402) in the following format: lzop (lzo-backed command-line utility) writes blocks in the following format: There's an additional ~32 byte header to the file. I don't know of a standard, but the lzop source should suffice. Since we're using "".lzo"" as the default extension, it's worth considering being compatible with lzop, but not necessarily for all lzo-compressed blocks. For example, SequenceFiles should use the existing LzoCodec format.",non_debt,-
hadoop,2424,comment_0,"With the right header, the existing LzoCodec should be compatible with lzop. It would probably be better implemented as an anyway.",requirement_debt,requirement_partially_implemented
hadoop,2776,summary,Web interface uses internal hostnames on EC2,non_debt,-
hadoop,2776,description,"The web interface, for example uses internal hostnames when running on EC2. This makes it impossible to access from outside EC2. The slaves file has the public names listed. Resolving a public name inside EC2 returns the private IP (which would reverse to the internal DNS name).",non_debt,-
hadoop,2776,comment_0,"I get by this by setting up nph-proxy on the master Hadoop node, and use that to browse the hosts. Kind of a hack, but works well for me.",non_debt,-
hadoop,2776,comment_1,"Also note that the new EC2 scripts add SOCKS support: hadoop-ec2 proxy <cluster name By installing FoxyProxy or setting you socks firewall settings, you can browse your whole cluster. that said, Nate's suggestion might be less clunky... see:",design_debt,non-optimal_design
hadoop,2776,comment_7,Any more updates on this? Running Hadoop inside Amazon EC2 is super annoying because most of the links are broken (internal vs external addresses). This bug has been open for FOUR YEARS.,design_debt,non-optimal_design
hadoop,2776,comment_8,"I'm going to close this as won't fix. I don't think this is anything that we actually can fix here other than providing a complicated hostname mapping system for web interfaces. Part of the frustration I'm sure stems from a misunderstanding of what is actually happening: The slaves file is only used by the shell code to run ssh connections. It has absolutely zero impact on the core of Hadoop. Hadoop makes the perfectly valid assumption that the hostname the system tells us is a valid, network-connectable hostname. It is, from the inside of EC2. We have no way to know that you are attempting to connect from a completely different address that is being forwarded from some external entity. Proxying connections into a private network space is a perfectly valid solution.",defect_debt,uncorrected_known_defects
hadoop,2796,summary,For script option hod should exit with distinguishable exit codes for script code and hod exit code.,non_debt,-
hadoop,2796,description,"For hod script option, the exit code should distinguishable between hod exit code and script exit code. e.g. If script command contains the streaming command at end and that fails due to input path not found, its value exit cod will 5 which overlaps with hod exit code 5 which means ""job execution failure"" It would hod throws some distinguishable exit codes e.g For above examples 64 +5 =69 and we should this to get exact exit code of hod script command user should subtract 64 from exit code",non_debt,-
hadoop,2796,comment_0,"The proposed solution in the bug of adding a constant number to the script's exit code, in retrospect, seems like a bad idea. - It is not very intuitive. - There could be cases where because of the addition, some shells like bash which do modulo 256 on exit codes, could make the result become 0, which seems like a successful execution. - It causes an unreasonable dependency between HOD and user scripts, who need to remember this magic number. The requirements for this problem, to my understanding, are as follows: - Return a zero exit code for a completely successful operation (both hod and the script have worked fine) - Return a non-zero exit code for a failed operation (either hod or the script have failed). Users may not care for more than this. Did it work or not - In the event of a non-zero exit code where the user wants to know if his script failed, provide an easy, clear way to determine if it failed. On these lines, the attached patch does the following: - Returns a zero exit code on success. - Returns a non-zero exit code on failure of script or hod itself. - If the script returned a non-zero exit code, it writes the exit code from the script to a file 'script.exitcode' into the cluster directory. Users can simple check for this file's existence and determine if it is a script failure. - If it's a hod failure, no such file will exist.",design_debt,non-optimal_design
hadoop,2796,comment_1,"There are no test cases in this patch, because the commit of HADOOP-2848 missed committing the testHod.py file. This will cause a conflict now as the test cases should really be added to that file. Will submit test cases as part of a separate patch.",test_debt,lack_of_tests
hadoop,2796,comment_2,"+1 for the proposal. However, there is one problem with a corner case. If we do the following: * first run a script which returns with an error(and so the script.exitcode file exists once ""hod script"" command finishes), * and then run another ""hod script"" with the same cluster directory, but with invalid options (say the script.exitcode file will still be around, and hod returns a non-zero exit code; that the script ran and returned with an error, which in actuality is not the case. Other than that, tested the rest of the cases successfully. Barring the corner case, +1 for the fix in general.",non_debt,-
hadoop,2796,comment_3,Modified code to handle Vinod's corner case as well.,non_debt,-
hadoop,2796,comment_5,"The failure for unit tests is expected, as mentioned above.",non_debt,-
hadoop,2796,comment_6,"I just committed this. Thanks, Hemanth!",non_debt,-
hadoop,2850,summary,[HOD] Split the operations of allocation from provisioning in HOD.,non_debt,-
hadoop,2850,description,"Currently, the HOD allocation operation does the following distinct steps - allocate a requested number of nodes, [optionally] transfer a hadoop tarball and install it, then provision hadoop by bringing up the appropriate daemons. It would be nice to separate these layers so each of them could be done independently. This would lead to a very great flexibility in users using the cluster. For e.g. one could allocate a certain number of nodes, then have hod bring up one version, then bring it down, then repeat this again and so on.",architecture_debt,violation_of_modularity
hadoop,2850,comment_0,"+1 for this issue. This would effectively make the allocate a pure front for qsub. Of course, for most users, allocate and bringing up the daemons in a single step is what they'll want. That should still be made easy for them (a single command line arg perhaps).",non_debt,-
hadoop,2850,comment_1,"Moving out of the Hadoop 0.17 list, as we are running out of time.",non_debt,-
hadoop,2850,comment_2,hod contrib was removed,non_debt,-
hadoop,2851,summary,Bogus logging messages in Configration object constructors,code_debt,low_quality_code
hadoop,2851,description,The constructors for the Configuration object contains a superfluous logging message that logs an IOException whenever logging is enabled for the debug level. Basically both constructors have the statement: if { } I can' t see any reason for it to be there and it just ends up leaving bogus IOExceptions in log files. It looks like its an old debug print statement which has accidentally been left in.,code_debt,low_quality_code
hadoop,2851,comment_0,"looks like someone wants a way to get a stack trace of wherever a configuration is created. Sometimes that could be useful. As it is only done if the debug log is enabled, its cost is otherwise minimal. Recommend leaving as is, file as INVALID or WONTFIX.",non_debt,-
hadoop,2851,comment_1,"Since it is showing stack trace, how about change the log level from debug to trace?",non_debt,-
hadoop,2851,comment_2,use trace level instead of debug level.,non_debt,-
hadoop,2851,comment_3,Talked to Chris: these debug messages do not seem useful. So they are better removed. removed the debug messages and fixed all unchecked warning in Configuration.,code_debt,low_quality_code
hadoop,2851,comment_4,"Seem that this is not needed. Closing this as ""Won't Fix"".",non_debt,-
hadoop,2897,summary,[HOD] Remove script option from the core hod framework,non_debt,-
hadoop,2897,description,"Hod currently allows the user to specify and run a hadoop script after allocation, and deallocating as soon as the script is done. For e.g. hod -m 3 -z ~/hadoop.script allocates 3 nodes, and runs ~/hadoop.script, then deallocates This is a convenient way single line wrapper around 4-5 commands that users have to write themselves. We have this because: - hod 0.3 does not provide an easy way to combine these into a single operation, because of the HOD shell. - even in hod 0.4, users have to carefully write some error checking code to make sure their cluster is allocated successfully, before running the script and their HADOOP_CONF_DIR should be set correctly. - users can free up clusters as soon as they are done. The requirements make sense. But having this as part of the core hod interface seems incorrect. The interface should be an orthogonal set of commands that each just do one thing well. The script option should be converted to a simple wrapper that can be part of the hod project. This way, users can enjoy the benefits of not having to write such a script themselves, while the hod codebase can still be clean. One disadvantage if we change this is that users will need to remember one more command. But given hod 0.4 is a new interface anyway, it is better to address now, rather than later. And we can alleviate this a bit by making sure options are consistently named between hod and the wrapper script.",design_debt,non-optimal_design
hadoop,2897,comment_0,"In a discussion internally, it was decided to go with the approach of using the core hod interface itself for implementing this option, as opposed to a new command. This is in the interest of usability, and the fact that the script option is something existing users of HOD are quite used to. So, I am resolving this issue as 'Won't fix'",non_debt,-
hadoop,2958,summary,Test utility no longer works in trunk,non_debt,-
hadoop,2958,description,"Filebench no longer works in trunk, due to HADOOP-2391 performing a check for the existence of the output directory (it improperly sets it to the file location, which works due to URI.resolve semantics)",non_debt,-
hadoop,2958,comment_0,This patch fixes filebench and corrects a trivial bug in (min/max word counts are identical).,non_debt,-
hadoop,2958,comment_1,"I just committed this. Thanks, Chris!",non_debt,-
hadoop,2959,summary,"When a mapper needs to run a combiner, it should create one and reuse it, instead of creating one per partition per spill",design_debt,non-optimal_design
hadoop,2959,description,None,non_debt,-
hadoop,2959,comment_0,"I should have pounted out the use case why this is matter. When the combining logic (reducer logic) depends on some thing that is initialized in the configure method, and of the configure method call is relative expensive (say initialize a dictionary from a file on dfs), then such an optimization makes a huge difference.",design_debt,non-optimal_design
hadoop,2959,comment_1,"This is a feature, not a bug. The reduce (and combiner) are guaranteed they will get the inputs in sorted order. If you re-use a combiner, that is no longer true. If initialization is expensive, move it to an external/static class.",non_debt,-
hadoop,2959,comment_2,I don't think such a feature hold much value for combiners. And I don't think such a contract is ever documented clearly for combiners. Do you have any use cases for that feature for the combiner?,documentation_debt,low_quality_documentation
hadoop,2961,summary,[HOD] Hod expects port info though external host is not mentioned.,non_debt,-
hadoop,2961,description,"When external host is not specified in gridservice-mapred or hdfs sections, there is no point in HOD validating the port numbers or hostnames.",non_debt,-
hadoop,2961,comment_0,"Attaching patch. This also addresses HADOOP-2946 and HADOOP-2949. - When external host is not specified or set to False in either gridservice-mapred or hdfs sections and if any of is *missing*, HOD no longer complains. Earlier, it used to fail saying are not defined"", even though they are not used at all. - When external host is not specified or set to False in either gridservice-mapred or hdfs sections and any of or _host_ is *invalid*, HOD no longer complains. Earlier, it used to fail saying invalid or _host_ is specified , even though they are not used at all. HADOOP-2946: - Currently HOD client side *does not* check for the pkgs directory in gridservice-hdfs. But it *is* needed to be checked on the ringmaster where it *is* going to be used, irrespective of whether the external option is specified or not.. So, no fix needed for HADOOP-2946, leaving it as invalid. HADOOP-2949: - If tarball is specified, HOD no longer validates for the pkgs directory in gridservice-hdfs or mapred sections as these are not going to be used anyway. Earlier, it used to validate and fail with error code 6 (Ringmaster failure).",non_debt,-
hadoop,2961,comment_1,"The patch does not work. It throws a KeyError in bin/ringmaster if no tarball is specified, because we are assuming it will be available in the patched code.",non_debt,-
hadoop,2961,comment_2,"Reattaching patch, fixing the above issue.",non_debt,-
hadoop,2961,comment_3,"New patch handles the hostname and port related problems well. However, there are two issues with the patch for the ringmaster: - Consider a scenario where gridservice-hdfs is external. If the ringmaster node does not contain a valid pkgs option in the gridservice-hdfs section and no tarball is used, the current code will fail. It need not. Because the ringmaster code never uses the option. It only uses the option. This is what was intended in HADOOP-2946. - If tarball is not specified and gridservice-mapred has an invalid location, the failure is not recorded in the log file. This is a regression. Other than above two, I think the rest is fine.",non_debt,-
hadoop,2961,comment_4,Fixed this. Checked that this is not a regression. Reattaching patch fixing the first issue.,non_debt,-
hadoop,2961,comment_5,Looks like both issues are addressed. Looking at the code. +1 for the patch.,non_debt,-
hadoop,2961,comment_6,Resubmitting to see if Hudson picks it up now.,non_debt,-
hadoop,2961,comment_8,"I just committed this. Thanks, Vinod!",non_debt,-
hadoop,2963,summary,[HOD] hod temp-dir problems,non_debt,-
hadoop,2963,description,"Filed from couple of related internal bugs from Karam: - hod should validate directory provided for hod.temp-dir if exists then it must be a directory - If a tempdir path is specified with '~', it is not expanded as usual. - If there's a problem in creating a tempdir, hod exits with an error but does not deallocate the cluster.",non_debt,-
hadoop,2963,comment_0,hod contrib was removed,non_debt,-
hadoop,2965,summary,[HOD] Job logs which are fetched when a cluster is deallocated should have names corresponding to Hadoop jobs.,non_debt,-
hadoop,2965,description,From internal bug from Arkady.. Map reduce jobs have names. The log files use mysterious names like They should use real job names so that users can find the logs they need.,code_debt,low_quality_code
hadoop,2965,comment_0,hod contrib was removed,non_debt,-
hadoop,3004,summary,HOD allocate command does not accept '~',non_debt,-
hadoop,3004,description,hod allocate seems to throw error when I specify directory relative to my home [lohit@hod]$ hod allocate --hod.nodecount=5 error: bin/hod failed to start. error: invalid 'clusterdir' specified in section hod (--hod.clusterdir): ~/clusterdir error: 1 problem found. Check your command line options and/or your configuration file /hod/conf/hodrc,non_debt,-
hadoop,3004,comment_0,what happens if you just try withouth the quotes?,non_debt,-
hadoop,3004,comment_1,Same Error with or without quotes,non_debt,-
hadoop,3004,comment_2,"However, hod allocate --hod.clusterdir ~/clusterdir --hod.nodecount=5 works. *sigh* Will look at this to see why there's a difference between and --hod.clusterdir ~/clusterdir. These are meant to be the same as per python option parser, I thought.",non_debt,-
hadoop,3004,comment_3,this looks like the shell is expanding out the ~/clusterdir for you in the second case.,non_debt,-
hadoop,3004,comment_4,"I see the issue. Basically hod isn't doing anything special to normalize paths. It relies on the shell to do it. When we specify an option as the shell is not expanding it, while it is, when we specify it as . And this not just with hod. Try the same with standard linux commands like tar, cp etc. For e.g. doesn't work. So, closing this as not a bug.",non_debt,-
hadoop,3077,summary,[HOD] Minor changes to unit tests,non_debt,-
hadoop,3077,description,"HADOOP-2899 and HADOOP-2936 introduced minor problems in their unit tests that should be fixed. As per HADOOP-2899: There's an incorrect hardcoded error message. Also, a test data uses a user name which should be changed. As per HADOOP-2936: Temporary testing directories are being left behind after some tests run.",test_debt,low_coverage
hadoop,3077,comment_0,hod contrib was removed,non_debt,-
hadoop,3079,summary,Regular expressions does not work properly in dfs commands like bin/hadoop fs -ls,non_debt,-
hadoop,3079,description,"When i do $ hadoop fs -ls s* ls: Cannot access src: No such file or directory. It picks up the directory src in local file system. but when i do, $ hadoop fs -ls sc* /user/amarsri/scr <r <r <r 3It lists the directories properly. ""$ hadoop fs -ls s*"" used to work well earlier.",non_debt,-
hadoop,3079,comment_0,You need to escape the '*' so that the shell does not expand it first.,non_debt,-
hadoop,3079,comment_1,"I guess s* does not match anything in your current directory, that's why it works.",non_debt,-
hadoop,3079,comment_2,Using the escape character works. But is it the intended behavior that I should use escape character in first case and doesnt need to in second?,non_debt,-
hadoop,3079,comment_3,I think this is a duplicate of HADOOP-3147.,non_debt,-
hadoop,3079,comment_4,This is a duplicate of HADOOP-3147,non_debt,-
hadoop,3081,summary,The static method FileSystem.mkdirs() should use the correct permissions to create directory,non_debt,-
hadoop,3081,description,"There is a static method FileSystem.mkdirs() that create the directory first and then sets permissions in it. Instead, it should create the directory with the correct permissions. Otherwise, even if dfs.permission is set to off, one might see job submission failures with the following stack trace: Source) Source) at Method) Source)",non_debt,-
hadoop,3081,comment_0,"The static method is required to avoid umask, as is sometimes required, yet still have mkdirs implement POSIX conventions. So calling mkdirs(path, permissions) won't do what's desired here, will it? I don't see the exception in your stack trace. How does it fail?",non_debt,-
hadoop,3081,comment_1,"The non-static f, FsPermission permission) and the static fs, Path dir, FsPermission permission) look similar but they are different in semantic: - The non-static f, FsPermission permission) likes mkdir in Unix, it makes a directory with *umask*. - However, the static fs, Path dir, FsPermission permission) makes a directory with *absolute permission*. So I think it is better to change one of the method name (probably the static one), otherwise, it would be confusing.",code_debt,low_quality_code
hadoop,3081,comment_2,Is there a reason you don't just jettison the static one and use mkdir() followed by a chmod() like UNIX? The static one will break setuid/setgid if/when it is supported.,non_debt,-
hadoop,3081,comment_3,That's exactly what the static implementation does. It's just a utility that saves applications a few lines of code.,non_debt,-
hadoop,3087,summary,JobInfo session object is not refreshed in loadHistory.jsp if same job is accessed again.,non_debt,-
hadoop,3087,description,"JobInfo object is not refreshed in loadHistory.jsp if same job is accessed again. In loadhistory.jsp, the jobInfo object is stored as session attribute. And if the same job is accessed again, the object is not refreshed. This becomes a problem if first time the object accessed was during job running and it doesnt refresh even after completing the job.",design_debt,non-optimal_design
hadoop,3087,comment_0,This patch does a removal of session attribute in loadhistory.jsp if the job is incomplete sothat JobInfo object gets refrshed.,non_debt,-
hadoop,3087,comment_2,"I just committed this. Thanks, Amareshwari!",non_debt,-
hadoop,3108,summary,NPE in,non_debt,-
hadoop,3108,description,"Not sure if this is fixed in later release, but I'm seeing many NPE in the namenode log. Permission is disabled on this cluster.",non_debt,-
hadoop,3108,comment_0,"Oops, it throws NPE when the path does not exist. I will fix it.",non_debt,-
hadoop,3108,comment_1,Here is the codes I got. Konstantin will continue work on it.,non_debt,-
hadoop,3108,comment_2,"I extended Nicholas's patch. Found one more potential NPE related to getNode() method in FSDirectory. TestPermissions failed in its original version because it was catching but was receiving a RemoteException instead. I wrote a method, which throws the cause of the RemoteException if it is of the right type. I recommend using it instead of analyzing the exception message. I should say in general that handling of RemoteExceptions e.g. in connection with is terrible. I will file a separate jira. The test was not closing file system correctly, I fixed that. And also a couple of findBugs warnings related to ignoring return values of called methods. I used the same unwrapping for canMkdirs(), canCreate() and canOpen() methods: this test is based on FileSystem API, which should not know anything about RemoteException, because e.g. LocalFileSystem does not throw this, ever. These changes affected TestDFSPermission, which in turn led to changes in DFSClient methods responsible, which report permission violations. With all the changes TestPermissions now correctly tests the case of changing permissions and owner of missing files Nicholas introduced. All tests pass on my machine except for TestDFSShell. Something is wrong with getFileInfo(). Hudson is stuck again so I cannot verify on a different machine either. Please somebody take a look.",design_debt,non-optimal_design
hadoop,3108,comment_3,"This has been a long standing issue. Doug commented many times on this. If I remember correctly, his latest suggestion was something like to unwrap the exception as long as the inner exception class exists on the client. New jira will be good.",non_debt,-
hadoop,3108,comment_4,"For this patch, I would say it is safer to use only the test and leave DFSClient.java unchanged. The new jira might unwrap the exception in RPC client it self.",non_debt,-
hadoop,3108,comment_5,"Since this patch is meant for 16, I think its better if it only fixes the NPEs. Changing some of the IOExceptions to FileNotFound might be ok. But any other semantic changes probably belong in 17 or trunk. For example with the patch, deleting a file can throw IOException, where i think it should return false.",design_debt,non-optimal_design
hadoop,3108,comment_6,+1 we should minimize the last minute changes.,non_debt,-
hadoop,3108,comment_7,This is a patch for 0.16.2,non_debt,-
hadoop,3108,comment_8,This is a version with the testPermission changed to catch the case.,non_debt,-
hadoop,3108,comment_9,1,non_debt,-
hadoop,3108,comment_10,This patch applies both to trunk and 0.16. I corrected test failure in my previous patch.,non_debt,-
hadoop,3108,comment_11,I just committed this.,non_debt,-
hadoop,3159,summary,FileSystem cache keep overwriting cached value,non_debt,-
hadoop,3159,description,"Consider the following: Therefore, may keeps creating FileSystem and replaces the cached one.",non_debt,-
hadoop,3159,comment_0,check ugi if scheme is hdfs,non_debt,-
hadoop,3159,comment_2,-1 We shouldn't do hdfs-specific stuff in FileSystem.java.,non_debt,-
hadoop,3159,comment_3,"I agree with Doug, ""hdfs"" should not appear in FileSystem. Could you please elaborate what and how you are trying to fix the problem. It is not clear from the comments in the issue.",non_debt,-
hadoop,3159,comment_4,"One option would be to always invoke (without check for ""hdfs""). Howeverm the login() method could have a static variable that caches the last value. This way there will not perrormance impact even if login gets called many many times within the same process.",non_debt,-
hadoop,3159,comment_5,"Digressing a little bit, the fundamental confusion seems to be that the key used by the cache needs scheme, authority, and username, but it only requires scheme and authority for look up.. so tries derive username some how. In long term, hopefully the interface itself gets fixed.",design_debt,non-optimal_design
hadoop,3159,comment_6,"If the ugi is not found in conf, it will login for any file system. For better performance, a LOGIN_UGI will be stored in So actual login is done only once.",non_debt,-
hadoop,3159,comment_8,"+1 The new patch is acceptable. Longer-term it seems to me we need a static URI) method, and the possibility to register different login methods for different URI schemes, e.g., The static method would then, when the configuration has no login information invoke the login method of the class named for that scheme, if any.",design_debt,non-optimal_design
hadoop,3159,comment_9,"This patch is committed. Thank you, Nicholas.",non_debt,-
hadoop,3198,summary,ReduceTask should handle rpc timeout exception for getting recordWriter,non_debt,-
hadoop,3198,description,"After shuffling and sorting, the reduce task is ready for the final phase  reduce. The first thing is to create a record writer. That call may fail due to rpc timeout. timed out waiting for rpc response Source) at Method) Source) Then the whole reduce task failed, and all the work of shuflling and sorting is gone! The reduce task should handle this case better. It is worthwile to try a few times before it gives up. The stake is too high to give up at the first try.",non_debt,-
hadoop,3198,comment_0,Add simple re-try logic.,non_debt,-
hadoop,3198,comment_1,Some comments. 1) Declare a _private static final_ for {{MAX_DFS_RETRIES}} and initialize it to 10. Use this in the for loop. 2) Remove extra spaces after {{reporter}} (line 14 of the patch) 3) Sleeping for 1 sec needs to be argued. Btw a log message is required before waiting. 4) Some extra code slipped in (regarding the log message). 5) After 10 retries we should throw the exception rather than silently coming out of the loop (leading to null pointer exception). +_Points to ponder_+ Can we do a timeout based stuff where we wait for _shuffle-run-time / 2_ before bailing out and having multiple retries within this timeout. This will somehow make sure that we dont kill the reducer too early.,code_debt,low_quality_code
hadoop,3198,comment_2,"This is the wrong place to do this. In particular, I believe the HDFS client already does a retry on exists. If it doesn't, it should. If the rpc timeout is coming out of the record writer creation times out, it means that it already failed several times. -1",non_debt,-
hadoop,3198,comment_3,"HDFS client has a retry on exists. It is likely that it tried and failed the several times. That is perhaps fine for exists call in general. However, for this particular call in getRecordWriter in reduce rask, the cost of failure is too expensive. Thus, reduce task has to do something special. In this sense, I think it is reduce task's responsibility to further re-try. I am open for any suggestions to fix the problem. However, I am not convenced that re-try at rpc level is the right answer.",design_debt,non-optimal_design
hadoop,3198,comment_4,"BTW, why the output format class bothers to check the existence of the output dir (see",non_debt,-
hadoop,3198,comment_5,This will lead to very unmaintainable code. We absolutely do not want to have nested retries for different contexts.,code_debt,low_quality_code
hadoop,3198,comment_6,"I saw a jos, 13% of its reducers were failed due to this rpc timeout problem. That is seriously flawed.",non_debt,-
hadoop,3211,summary,Utility to enforce current replication policy on already exitsting blocks,non_debt,-
hadoop,3211,description,"If the replica placement policy changes, or if the cluster topology changes, existing block replicas will not be deliberately moved to implement the new policy or to reflect the new topology. The re-replicator and the re-balancer always implement the current policy, but a healthy block may never attract the attention of either of those facilities. In practice, all replicas of a block have been found within a single rack (as allowed by the previous _random rack_ placement policy. The {{fsck}} facility reports such examples. The re-placer utility could follow the model of the re-balancer. Or even be part of the re-balancer. An alternative implementation would scan the {{fsck}} report for curious blocks, and rename a replica block file on the data node, so as to hide it. The re-replicator would be expected to create a new replica properly placed. (This is a fix up that any administrator could implement.) (1761414)",non_debt,-
hadoop,3211,comment_0,"The balancer does this today. The BlockManager and its services do not, however.",non_debt,-
hadoop,3286,summary,Gridmix jobs' output dir names may collide,design_debt,non-optimal_design
hadoop,3286,description,"Gridmix jobs use time suffix to differentiate output dir names. The current time granularity of second may not be sufficient, causing multiple jobs having the same output dir.",design_debt,non-optimal_design
hadoop,3286,comment_0,Attached the patch fixes the potential output dir name collision. It also fixed a bug in script maxentToSameCluster and a bug in webdatasort.large,non_debt,-
hadoop,3286,comment_2,patch generated for the latest trunk,non_debt,-
hadoop,3286,comment_4,Reviewed the patch. +1,non_debt,-
hadoop,3286,comment_5,"I just committed this. Thanks, Runping",non_debt,-
hadoop,3337,summary,Name-node fails to start because DatanodeInfo format changed.,non_debt,-
hadoop,3337,description,"HADOOP-3283 introduced a new field ipcPort in DatanodeInfo, which was not reflected in the reading/writing file system image files. Particularly, reading edits generated by the previous version of hadoop throws the following exception: and startup fails.",non_debt,-
hadoop,3337,comment_0,reverted the accidental changes of FSEditLog format in HADOOP-3283.,non_debt,-
hadoop,3337,comment_1,3334_20080501.patch => 3337_20080501.patch,non_debt,-
hadoop,3337,comment_2,"This patch works on my old file system image. Minor comments, please - remove import of UTF8 - provide comments on the 2 new methods *FSEditLog() explaining what they are for.",code_debt,low_quality_code
hadoop,3337,comment_3,incorporated Konstantin's comments.,non_debt,-
hadoop,3337,comment_4,Wouldn't this affect readFields() and write() of DatanodeDescriptor (used everywhere : RPCs etc) ? This patch looks like a problematic hack. I think this needs to be fixed better. If EditLog requires to read and write differently these different serialization should used there instead of everywhere.,design_debt,non-optimal_design
hadoop,3337,comment_5,DatanodeDescriptor is not used in RPC only DatanodeInfo.,non_debt,-
hadoop,3337,comment_6,"I agree. We will remove storing DatanodeDescriptor to FSEditLog In HADOOP-3329 soon. Therefore, I don't want to introduce layout change or protocol change in this patch.",non_debt,-
hadoop,3337,comment_7,"Does it mean the current patch is ok? But we should not have wrong implementations of Writable interface for DatanodeDescriptor, right? Could you describe the fix (and may be problem)? Was this stored before HADOOP-3283?",non_debt,-
hadoop,3337,comment_8,"Sure it is. Even if it is not, I don't think its a good practice to silently break the contract because we think the contract is not used (yet), (especially for widely used interfaces like Writables) Example use: returns {{LocatedBlocks}}, if you trace its implementation, you will see that LocatedBlock is created using DatanodeDescriptor (around .. so etc do get called out side of FSEditLog.",design_debt,non-optimal_design
hadoop,3337,comment_9,"Actually this is not a potential problem, rather real one. Most unit tests fail with this patch.",non_debt,-
hadoop,3337,comment_10,"Raghu, thank you for pointing out that DatanodeDescriptor is sent in RPC in some hidden way. created a subclass to fix this bug.",non_debt,-
hadoop,3337,comment_11,use static methods instead of subclass.,non_debt,-
hadoop,3337,comment_12,"DatanodeDescriptor is not sent over RPC and is not supposed to. You can never get DatanodeDescriptor on the other end. DatanodeDescriptor is sort of a name-node private class. Although the actual class is DatanodeDescriptor, rpc serializes the base class DatanodeInfo using its Writable implementation and sends the latter over the network. The problem here is that the serialization intended for DatanodeDescriptor (which is only serialized to disk) is mixed with the serialization of DatanodeInfo (which should be used only for rpc). We have been through this before. I think we should introduce 2 new static methods in the DatanodeDescriptor that would provide serialization to disk.",code_debt,low_quality_code
hadoop,3337,comment_14,Tested manually. The 4 new javac warnings are due to the use of UTF8 for backward compatibility.,non_debt,-
hadoop,3337,comment_15,I just committed this. Thanks Nicholas!,non_debt,-
hadoop,3375,summary,Lease paths are sometimes not removed from,non_debt,-
hadoop,3375,description,"In LeaseManager, - does not remove paths from sortedLeasesByPath - removeLease(...) should do no matter lease.hasPath() returns true or false.",non_debt,-
hadoop,3375,comment_0,"Also, lease related structure should store names and paths as String instead of It will prevent unnecessary converting between String and",design_debt,non-optimal_design
hadoop,3375,comment_1,fixed sortedLeasesByPath and removed,non_debt,-
hadoop,3375,comment_3,Nicholas reports that this may result in the loss of a lease record for every task that creates a file. This is a new problem not present in 0.16.,non_debt,-
hadoop,3375,comment_4,Not a 0.17 problem after all (only trunk). I'm sorry for the confusion.,non_debt,-
hadoop,3375,comment_5,+1 code looks good. Is there a way to enhance an existing unit test to catch this situation?,test_debt,lack_of_tests
hadoop,3375,comment_6,added a test,non_debt,-
hadoop,3375,comment_8,reverted the change of removing StringBytesWritable since it needs more thought.,non_debt,-
hadoop,3375,comment_9,I like the latest patch better since it does not affect the disk format. Changing the StringBytesWritbale is bets done in a separate patch.,non_debt,-
hadoop,3375,comment_10,"Yes, I plan to create another issue for removing StringBytesWritbale which was introduced in HADOOP-1283.",non_debt,-
hadoop,3375,comment_11,updated javadoc,non_debt,-
hadoop,3375,comment_13,Two tests failed but none of them is related to the patch. - See HADOOP-3419 for TestFsck - See HADOOP-3354 for TestEditLog,non_debt,-
hadoop,3375,comment_14,+1. Code looks good.,non_debt,-
hadoop,3375,comment_15,I just committed this. Thanks Nicholas!,non_debt,-
hadoop,3375,comment_17,Created HADOOP-3461,non_debt,-
hadoop,3377,summary,Use instead of,non_debt,-
hadoop,3377,description,"A minor cleanup. In the TaskRunner, we can read : ""When hadoop moves to JDK1.5, replace with String#replace"" This patch do so, removing ~30 lines.",code_debt,low_quality_code
hadoop,3377,comment_1,"I just committed this. Thanks, Brice",non_debt,-
hadoop,3394,summary,Distributed Lucene Index For Hadoop,non_debt,-
hadoop,3394,description,"Here is the current prototype implementation of a distributed free text index using Hadoop based on Doug Cutting's design: There has also been some discussion about this on the Hadoop Wiki: This work is not finished, so it is not intended for inclusion yet. For a description of the contribution and its current status see the report in doc/index.html in the attached archive that gives some details of the implementation. This work was designed as a contrib contribution. However, as there are at least two other projects (Bailey and Katta) with similar goals it seemed a good idea to make this code available for discussion.",requirement_debt,requirement_partially_implemented
hadoop,3394,comment_0,Current implementation of the Distributed Lucene index on 15 May 2008.,non_debt,-
hadoop,3394,comment_1,This work has now been superseded by Katta - see,non_debt,-
hadoop,3394,comment_2,"Closing this issue, it is no longer relevant.",non_debt,-
hadoop,3468,summary,Compile error: cannot access,non_debt,-
hadoop,3468,description,"[javac] Compiling 496 source files to [javac] cannot access [javac] bad class file: [javac] class file has wrong version 50.0, should be 49.0 [javac] Please remove or make sure it appears in the correct subdirectory of the classpath. [javac] import [javac] ^ [javac] Note: Some input files use or override a deprecated API. [javac] Note: Recompile with -Xlint:deprecation for details. [javac] 1 error",non_debt,-
hadoop,3468,comment_0,"Are you using Java 1.5 or Java 1.6? If this only happens with Java 1.5, is it time for HADOOP-2325?",non_debt,-
hadoop,3468,comment_1,"In case it's not obvious, the simple fix for this is to revert HADOOP-3246 (r661473).",non_debt,-
hadoop,3468,comment_2,It seems that it requires Java 1.6. Mine is 1.5.,non_debt,-
hadoop,3468,comment_3,Attaching new required Jar files created after compiling Mina FTP server jars with Java 1.5. Verified that there are no compilation errors when compiling hadoop under Java 1.5 after replacing the required jars for HADOOP-3246 with the one attached here. I have also replaced the required jars in HADOOP-3246. Sorry for the the compilation problems and inconvenience caused.,non_debt,-
hadoop,3468,comment_4,"I just committed this. Thanks, Ankur!",non_debt,-
hadoop,3468,comment_6,Voted out of the release notes list.,non_debt,-
hadoop,3477,summary,release tar.gz contains duplicate files,build_debt,build_others
hadoop,3477,description,None,non_debt,-
hadoop,3477,comment_0,"Simple patch, one line.",non_debt,-
hadoop,3477,comment_1,"More details: == tar tzvf |awk '{print $6}'|sort|uniq -c|grep -v '1 ' 2 2 2 2 2 2 == This occurs because those files are included twice in build.xml, because there is no explicit exclude on those dirs, like there is for the others. The attached patch(1-line) adds such an exclude. This is keeping me from using the downloaded tarball as the orig.tar.gz for a debian package.",build_debt,build_others
hadoop,3477,comment_2,"I just committed this. Thanks, Adam!",non_debt,-
hadoop,3477,comment_3,Could you apply the fix to 0.16 as well(creating 0.16.5)? Nutch isn't using 0.17 yet.,non_debt,-
hadoop,3477,comment_4,"I guess this is a regression, since it was introduced in 0.16.0 by HADOOP-2494. Unless there are objections, I will merge it into the 0.16 and 0.17 branches.",non_debt,-
hadoop,3491,summary,Name-node shutdown causes in ResolutionMonitor,non_debt,-
hadoop,3491,description,catches a general Exception and logs them no matter what. It should explicitly catch the and exit gracefully.,code_debt,low_quality_code
hadoop,3491,comment_0,"Attached patch, catches the exception and logs a one line warning instead of whole stack trace.",non_debt,-
hadoop,3491,comment_1,"No tests, I manually checked logs and dont see strack dump as earlier.",non_debt,-
hadoop,3491,comment_2,"I think should be caught inside the loop so that the ResolutionMonitor could check fsRunning and decide what to do based on its value. And there should be no messages higher than debug level, imo.",non_debt,-
hadoop,3491,comment_3,"Thanks Konstantin, Second try with your comments.",non_debt,-
hadoop,3491,comment_4,[exec],non_debt,-
hadoop,3491,comment_5,1,non_debt,-
hadoop,3491,comment_6,I just committed this. Thank you Lohit.,non_debt,-
hadoop,3500,summary,"decommission node is both in the ""Live Datanodes"" with ""In Service"" status, and in the ""Dead Datanodes"" of the dfs namenode web ui.",non_debt,-
hadoop,3500,description,"try to decommission a node by the following the steps: (1) write the hostname of node which will be decommissioned in a file (the exclude file) (2) specified the absolute path of the exclude file as a configuration parameter dfs.hosts.exclude. (3) run ""bin/hadoop dfsadmin -refreshNodes"". It is surprising that the node is found both in the ""Live Datanodes"" with ""In Service"" status, and in the ""Dead Datanodes"" of the dfs namenode web ui. When copy new data to the HDFS, its Used size is increasing as other un-decommissioned nodes. Obviously it is in service. Restarting the HDFS or waiting a long time(two day) havn't make the decommission yet. the more strange thing, If nodes are configured as the include nodes by similar steps, then these include nodes and the exclude node are all only in the ""Dead Datanodes"" lists. I did many times tests in both 0.17.0 and 0.15.1. The results is same. So i think there maybe bugs.",non_debt,-
hadoop,3500,comment_0,I suspect the browser hiccuped and you got two instances of the same bug. (HADOOP-3499),non_debt,-
hadoop,3500,comment_1,oh yes. How do I delete one of them? I just could edit.,non_debt,-
hadoop,3501,summary,deprecate InMemoryFileSystem,non_debt,-
hadoop,3501,description,"As of HADOOP-2095, InMemoryFileSystem is no longer used. Its design was optimized for a particular application and it is thus not a good general-purpose RAM-based FileSystem implementation, so it ought to be removed.",code_debt,dead_code
hadoop,3501,comment_0,Patch to deprecate InMemoryFileSystem.,non_debt,-
hadoop,3501,comment_1,+1 We can fix the test-case which uses the InMemoryFileSystem at a latter point.,non_debt,-
hadoop,3501,comment_3,"This is expected, since this patch adds a deprecation. These same tests are failing for other patches and are unrelated to this patch.",non_debt,-
hadoop,3501,comment_4,This patch is w.r.t the trunk.,non_debt,-
hadoop,3501,comment_5,Pushing through hudson,non_debt,-
hadoop,3501,comment_6,"I just committed this. Thanks, Doug!",non_debt,-
hadoop,3505,summary,omissions in HOD documentation,documentation_debt,low_quality_documentation
hadoop,3505,description,"There's a couple HOD limitations that really trip up the unwary. Two I've encountered are that hod can't take relative paths on the command line, and that if you pass hod a tarball with a modified /conf, then the cluster will fail mysteriously to be initialized. I don't see any references to either in the documentation, and it'd be great to write this down.",documentation_debt,low_quality_documentation
hadoop,3505,comment_0,"Actually changes in Hadoop 0.18 now allow HOD to take relative paths on the command line. Likewise, if an incorrect conf is present, the errors are reported back to the client. However, we should document that the tarball should not contain a modified conf, so that users know this proactively. In addition to the above, the following changes are identified after reviewing the bugs fixed for HOD in Hadoop 0.18: - HADOOP-3376: Hyperlink Maui in the documentation, crediting Cluster Resources for the software. Also, Torque and Maui should be capitalized in the documentation. - HADOOP-3464: Add 2 new error scenarios in the trouble shooting section. One should describe the error that comes when ringmaster fails, the other when the jobtracker fails. For the latter, we should mention that administrators can review other log messages in the ringmaster log to see which other machines had problems bringing up the jobtracker, apart from the one that is reported on the command line. - HADOOP-3483: Remove any limitations we have specified about creating a cluster directory. Now, we create it automatically. - HADOOP-3184: Document in the Config guide about parameter.",documentation_debt,low_quality_documentation
hadoop,3505,comment_1,"Made documentation changes, attaching patch.",non_debt,-
hadoop,3505,comment_3,The attached patch makes minor editorial modifications to the Vinod's changes and adds one point about incorrect configuration in the hadoop tarball.,non_debt,-
hadoop,3505,comment_4,Corrected two minor errors in the earlier patch.,non_debt,-
hadoop,3505,comment_5,"Running through Hudson. As this is a documentation patch, it will not have test cases.",non_debt,-
hadoop,3505,comment_6,+1 for the changes.,non_debt,-
hadoop,3505,comment_8,"I committed this to trunk. Thanks, Vinod !",non_debt,-
hadoop,3505,comment_9,Merged to branch-0.18,non_debt,-
hadoop,3505,comment_11,Torque hyperlink is required in Pre-requistites section of hod admin guide.,non_debt,-
hadoop,3560,summary,Archvies sometimes create empty part files.,code_debt,low_quality_code
hadoop,3560,description,Archvies creates a bunch of empty part files sometimes which are not necessary.,code_debt,low_quality_code
hadoop,3560,comment_0,this patch fixes the splits created in archive map reduce jobs so that splits are evenly distributed amongst the maps. The sync marker made the splits to be non uniform amongst the maps. The sync marker is called on each record append in the filesplits file.,non_debt,-
hadoop,3560,comment_2,i have not added tests since the test require creation of large input files.,test_debt,lack_of_tests
hadoop,3560,comment_3,"I just committed this. Thanks, Mahadev!",non_debt,-
hadoop,3621,summary,HoD should include timestamp in all error messages,non_debt,-
hadoop,3621,description,"In order to debug and correlate errors reported by HoD with the logs of torque etc, it is essential that HoD include a timestamp in all the error messages it prints.",non_debt,-
hadoop,3621,comment_0,hod contrib was removed,non_debt,-
hadoop,3627,summary,HDFS allows deletion of file while it is stil open,non_debt,-
hadoop,3627,description,"This was a single node cluster, so my DFSClient was from same machine. In a terminal I was writing to a HDFS file, while on another terminal deleted the same file. Deletion succeeded, and the write client failed. If the write was still going on, then the next block commit would result in exception saying, the block does not belong to any file. If the write was about to close, then we get an exception in completing a file because getBlocks fails. Should we allow deletion of file? Even if we do, should the write fail?",non_debt,-
hadoop,3627,comment_0,"I think the semantics is actually the best that we can do with the current protocols. I wouldn't want a windows-like semantics where a writer anywhere can keep you from deleting a file. I think it would make sense to introduce fileid's at some point so that renames while you are writing work in the unix-style, with the name and contents being completely separate from each other. That is a much bigger change to the name node though... I'd propose that we make this wont-fix.",non_debt,-
hadoop,3627,comment_1,"We seek unix-like behavior when feasable, and the unix behavior here would be that the write would not fail. So permitting deletion is not the bug. The error in the writer is perhaps a bug, but fully supporting the unix notion of unlinked files that disappear when the last reader or writer is closed might prove difficult, and is not the subject of this issue anyway. +1",non_debt,-
hadoop,3627,comment_2,"Thanks Owen, Doug. I should have explained background of this issue as well. We saw this case (running 0.17), when coupled with NameNode dropping request when it is under load caused the write to hang. On trunk, namenode would no longer drop requests, and we agree that we are not changing semantics of deleted while writing. I will close this as wont fix.",non_debt,-
hadoop,3649,summary,in,non_debt,-
hadoop,3649,description,A job-submission failed with:,non_debt,-
hadoop,3649,comment_0,"1. Looks like there is bug in removing corrupted blocks from the corrupted block map. We do not remove corrupted replicas until the valid replicas are fully re-replicated on other nodes. When they do the corrupted replicas can and should be removed from the data-nodes. So actually checks whether there is enough healthy replicas and invalidates corrupted replicas by: - removing corrupted locations from the block's list of locations, and - calling which is supposed to remove it from the set of corrupted. But has a condition under which it removes the block from the corruptReplicasMap only if the block does not belong to the main blocksMap. This particularly means that once in the corruptReplicasMap the block stays there until the file is removed. The comes from which assumes that the set of corrupted replicas is always a subset of all block replicas. Due to the bug in it is not the case because corrupt replicas are not in the block's location list, but are still in the corruptReplicasMap. 2. In I see boolean variable ""gotException"" which is set to false and never changes. I think there was an intention to set it to true in the catch{} section. But may the right thing to do is just to remove the variable and the call of from this method because will be called within if it is successful. Promoting this to a blocker for 0.18",non_debt,-
hadoop,3649,comment_1,"By writing a testcase we were able to reproduce this. The attached patch fixes this issue as described by Konstantin. Also, when a bad replica is reported, it is added to neededReplication.",non_debt,-
hadoop,3649,comment_2,# This does not work if you have 2 out of 3 replicas corrupt. When the first replication happens you will unconditionally remove the block from the corrupt replica map not waiting for the second replication to complete. We should make a test case for that. # needs JavaDoc explaining what it tests. # Could you please remove redundant import of ByteBuffer in # You should not corruptReplica() twice (the same one) in,test_debt,lack_of_tests
hadoop,3649,comment_3,"Thanks Konstantin. I changed the test case and made few more fixes. This new patch, tests both 1 and 2 corrupt replica cases. The tests poll until the corrupted blocks reaches health state and corresponding entries are removed from corruptReplicasMap. I ran the test in a loop for 20 times and seem to work fine.",non_debt,-
hadoop,3649,comment_4,"This looks right. I made some minor changes to the patch: # removed method which is not used anywhere, and # moved into FSNamesystem, which imo it belongs to based on the structure of our name-node class hierarchy.",code_debt,dead_code
hadoop,3649,comment_6,I just committed this. Thank you Lohit.,non_debt,-
hadoop,3654,summary,Log4J logging of stack trace may deadlock JRockit in TestFileSystem,non_debt,-
hadoop,3654,description,"This is being added as a bugrep so that other people can find it, and the workaround 1. On my machine TestFileSystem will hang, even overnight -even though the build was set with a timeout. 2. halting the build left a JVM running; it was not being killed. 3. Under the IDE, the main thread appears hung in the native library call to get a stack trace, somewhere inside Log4J 4. the IDE could not halt the build, and could not be shut down cleanly either The fix for this problem was to edit and switch to a log4J log pattern that did not print the line of the code %-5p %c %x - %m%n Given that working out a stack trace can be an expensive call, and that it can apparently hang some JVMs, perhaps it should not be the default.",design_debt,non-optimal_design
hadoop,3654,comment_0,"-this turns off line/file logging, and so stops deadlocks on BEA JRockit(R) (build compiled mode). It may make test runs slightly faster too, as every log statement with line numbers requires an exception to be created and its stack extracted. -the old appender is there to be switched on in emergency -thread and extra context logging have been inserted as thread info is useful for separating different sources of messages.",non_debt,-
hadoop,3654,comment_1,tuned log4j parameters,non_debt,-
hadoop,3654,comment_2,I should add that the file to patch is not but,non_debt,-
hadoop,3654,comment_4,I think the line numbers are really useful and would hate to turn them off by default. How about doing the reverse of your patch where there is a configuration that doesn't have line numbers that can be selected?,non_debt,-
hadoop,3654,comment_5,-a switch to turn numbers off would work for me; something you could set in build.properties would be ideal. Maybe we could have >1 log4j.properties compatible file in the src/test classpath; and somehow make the choice of which to run tests on configurable. I'll check out the log4j docs next week to see how to do this.,non_debt,-
hadoop,3654,comment_6,"With up to date JRockit not being public, and the Oracle/Sun merger going to make for some interesting JDK futures, marking this as a LATER. If the JRockit codebase moves to being the core JDK, it will surface again.",non_debt,-
hadoop,3836,summary,TestMultipleOutputs will fail if it is ran more than one times,non_debt,-
hadoop,3836,description,"TestMultipleOutputs will success after ""ant clean"" but it will fail if running it more than one times.",non_debt,-
hadoop,3836,comment_0,The test output says The output directory was not cleaned up after the test is done.,code_debt,low_quality_code
hadoop,3836,comment_1,"patch for testcase that cleans up at setup and teardown. strange that the failure was not happening in Mac OS X, nor in the Hudson servers.",non_debt,-
hadoop,3836,comment_2,"I tried the patch. The test still fails. The problem is that there is a typo in line 180 (after the patch). ""reader.close()"" should be It works fine after fixed the typo. Also, the before is not needed.",documentation_debt,low_quality_documentation
hadoop,3836,comment_4,"Got Tsz, new patch fixes it, Thxs.",non_debt,-
hadoop,3836,comment_6,+1 the new patch looks good. I tested it manually. It does not seem to have any problem.,non_debt,-
hadoop,3836,comment_7,"I just committed this. Thanks, Alejandro and Nicholas!",non_debt,-
hadoop,3849,summary,doesn't wait for the heartbeat-interval if it doesn't have events to fetch,non_debt,-
hadoop,3849,description,"The 'notify' done by causes the to immediately run to the JobTracker On a 3500 node cluster, I saw that each TaskTracker calls multiple times per-second. This caused the JobTracker's RPC queues to back-up resulting in each RPC spending more than 120s in the queue - leading to shuffle proceeding very very slowly.",code_debt,slow_algorithm
hadoop,3849,comment_0,"Invalid, FetchStatus will regulate itself to ensure it doesn't slam the JobTracker.",non_debt,-
hadoop,3861,summary,Make MapFile.Reader and Writer implement java.io.Closeable,non_debt,-
hadoop,3861,description,Both MapFile.Reader and Writer have a close() method with the right signature. They just need to declare that they implement Closeable.,non_debt,-
hadoop,3861,comment_0,Patch implementing the change.,non_debt,-
hadoop,3861,comment_2,"+1 This looks good to me. Thanks, Tom!",non_debt,-
hadoop,3861,comment_3,"I just committed this. Thanks, Tom!",non_debt,-
hadoop,3905,summary,Create a generic interface for edits log.,non_debt,-
hadoop,3905,description,Create a generic interface that would cover all edit log operations and be useful for implementing journaling functionality on different storage sources.,non_debt,-
hadoop,3905,comment_0,HADOOP-3860 concluded particularly that edits log currently presents one of the main bottlenecks for the name-node performance. Currently we can save edits records only into a file and are bound by hard drive performance. We may try to stream edits e.g. into a database or another high performance system optimized for writes. My primary focus though is to make it possible to stream records into a secondary name-node so that the latter could always keep an up todate namespace state with a perspective of building an HA solution based on that.,non_debt,-
hadoop,3905,comment_1,"In the patch. # I substantially simplified what used to be called by using existing hadoop class instead of a pair of classes import and # is now an abstract class, and is its implementation for storing edits in a file. # Introduced and its implementation called for reading edits from a file. # The rest of the {{FSEditLog}} remained unchanged. The idea here is that one should write implementations of and for a different type of persistent storage and the rest of the logic for the edits log should remain unchanged. Since I don't have other implementations ready yet it is hard to predict what else should be changed or abstracted. FSImage and FSEditLog heavily depend on the storage directory file names so may be that should be changed somehow in the future.",design_debt,non-optimal_design
hadoop,3905,comment_2,+1 this looks good to me.,non_debt,-
hadoop,3905,comment_3,I fixed one JavaDoc warning.,documentation_debt,low_quality_documentation
hadoop,3905,comment_5,I just committed this.,non_debt,-
hadoop,3925,summary,Configuration paramater to set the maximum number of mappers/reducers for a job,non_debt,-
hadoop,3925,description,The JobTracker can be prone to a denial-of-service attack if a user submits a job that has a very large number of tasks. This has happened once in our cluster. It would be nice to have a configuration setting that limits the maximum tasks that a single job can have.,design_debt,non-optimal_design
hadoop,3925,comment_0,Will this be the case with a (custom made) scheduler? Controlling the max number of maps in a job might be too restrictive. I think we should control the execution of these tasks via the scheduler. Something like a fair scheduler might help.,non_debt,-
hadoop,3925,comment_1,"Dhruba, what are the symptoms of the DOS? Does the JT lockup & stop responding to the Or, are you worried that a single large job would starve others?",non_debt,-
hadoop,3925,comment_2,One of our users submitted a job that has a million mappers and million reducers. The JobTracker was runnign with 3GB heap. It went into 100% CPU usage (probably GC). Never came back to life even after 10 minutes. Is there a way (in the current release) to prevent this from happening?,design_debt,non-optimal_design
hadoop,3925,comment_3,Fixed by HADOOP-4018,non_debt,-
hadoop,3957,summary,Fix javac warnings in DistCp and the corresponding tests,code_debt,low_quality_code
hadoop,3957,description,There are a few javac warning in DistCp and TestCopyFiles.,code_debt,low_quality_code
hadoop,3957,comment_0,fixed unchecked warnings. Removed some not-so-useful methods and the use of deprecated API.,code_debt,dead_code
hadoop,3957,comment_1,"It passed all tests locally. The following is the ""ant test-patch"" results. The javadoc warning is due to HADOOP-3949.",non_debt,-
hadoop,3957,comment_2,"New ""ant test-patch"" result after ""svn update""",non_debt,-
hadoop,3957,comment_3,+1 Looks good,non_debt,-
hadoop,3957,comment_4,"I just committed this. Thanks, Nicholas",non_debt,-
hadoop,3995,summary,"renameTo(src, dst) does not restore src name in case of quota failure.",non_debt,-
hadoop,3995,description,"How to reproduce : set up quota such that 'hadoop -mv src dst' results in a 'quota excceeded' exception. * Now try : {{hadoop -mv src dst/diffname}} ** This fails as expected ** buf 'src' does not exist after this, you will find 'diffname' directory in its place I think fix should go into 0.18.1",non_debt,-
hadoop,3995,comment_0,A simple fix is attached. This adds a test case in TestQuota,non_debt,-
hadoop,3995,comment_2,+1 The patch looks good.,non_debt,-
hadoop,3995,comment_3,Javadoc warnings are unrelated.,non_debt,-
hadoop,3995,comment_4,I just committeted this.,non_debt,-
hadoop,3999,summary,Dynamic host configuration system (via node side plugins),non_debt,-
hadoop,3999,description,"The MapReduce paradigma is limited to run MapReduce jobs with the lowest common factor of all nodes in the cluster. On the one hand this is wanted (cloud computing, throw simple jobs in, nevermind who does it) On the other hand this is limiting the possibilities quite a lot, for instance if you had data which could/needs to be fed to a 3rd party interface like Mathlab, R, BioConductor you could solve a lot more jobs via hadoop. Furthermore it could be interesting to know about the OS, the architecture, the performance of the node in relation to the rest of the cluster. (Performance ranking) i.e. if i'd know about a sub cluster of very computing performant nodes or a sub cluster of very fast disk-io nodes, the job tracker could select these nodes regarding a so called job profile (i.e. my job is a heavy computing job / heavy disk-io job), which can usually be estimated by a developer before. To achieve this, node capabilities could be introduced and stored in the DFS, giving you a1.) basic information about each node (OS, ARCH) a2.) more sophisticated infos (additional software, path to software, version). a3.) PKI collected about the node (disc-io, cpu power, memory) a4.) network throughput to neighbor hosts, which might allow generating a network performance map over the cluster This would allow you to b1.) generate jobs that have a profile (computing intensive, disk io intensive, net io intensive) b2.) generate jobs that have software dependencies (run on Linux only, run on nodes with MathLab only) b3.) generate a performance map of the cluster (sub clusters of fast disk nodes, sub clusters of fast CPU nodes, between nodes) From step b3) you could then even acquire statistical information which could again be fed into the DFS Namenode to see if we could store data on fast disk subclusters only (that might need to be a tool outside of hadoop core though)",non_debt,-
hadoop,3999,comment_0,"First implementation on the way, extending the global config structure with information acquired by plugins",non_debt,-
hadoop,3999,comment_1,"1. This would be good if it could be easily extended; rather than than a hard coded set of values, clients could add other (key,value) info for schedulers to use. Things like for cycle-scavenging task-trackers, and other extensions that custom schedulers could use. It could also integrate with diagnostics. 2. There's a danger here in trying to do a full grid scheduler. Why Danger? Hard to get right, there are other tools and products that can do a lot of this. Hadoop likes to push work near the data and works best if the work is all Java. 3. Developers are surprisingly bad about estimating workload, especially if you have a few layers between you and the MR jobs. The best metric for how intensive a job will be is ""what was like last time"".",design_debt,non-optimal_design
hadoop,3999,comment_2,"Thanks a lot for your comment! Regarding 1.) im just implementing some sort of plugin system which allows us to load arbitrary plugin classes that have to implement the CapabilityPlugin class. Its working with Maps so the plugins are quite free in what the put into it as results. This is necessary since many benchmarks are available under a non-apache license only (i.e. scimark2) and in this way they can still be used. Furthermore i think is makes sense to define which ""key(s)"" from the CapabilitiyPlugin are supposed to be your relevant keys for your scheduler (i.e. the value combined w/ and the might be interesting, other combinations for others - a good default setting is important here but should be tweakable - at least for testing). The plugin system should also be able to handle shell scripts/tools since some benchmarks (i/o etc) are nearly impossible in java. Furthermore this system can also hold software info as well as other at the same time. it will have aging (since we dont want to do some (.i.e. performance) tests on every start) and serialization of the data. I assume this system fits into other domains (beside sw/hw) as well. Regarding 2.) I see this danger as well ... anyway i think it still makes a lot of sence if you can assume you have a special tool onsite you can use (as we have - using a lot of biological add ons - which you dont want to reinvent ;). Further down the road, if we see superclouds that need to handle multiple customers with different needs / specs / service levels we also should be able to differ between nodes (i call it individualized nodes at this point) Looking at smaller setups / test setups with a lot of heterogeneousity (as we have here) we could be better of, if we can make the scheduler stop using machines for workload which are needed otherwise. Regarding the not only the scheduler has to know about specs of the nodes, also the dfs could make use of it (actually should prefer fast IO machines eventually) For friends I often use the metaphor : different people are living in the cloud, i.e. workers, scientists, housewifes. so why give mathematical problems to the housewife and ironing jobs to the scientists? Regarding 3.) (and 2) maybe the performance system is - in the beginning - more usable for core-developers and performance tweakers than for my biologist neighbors who just were forced to develop in java.",non_debt,-
hadoop,3999,comment_3,"The basic capability plugin system is done so far but i have some structural problems/questions which you hopefully might be able to help me out with: Status : - I currently do the execution of the collector-plugins into both, the DataNode startup as well as the TaskTracker startup. - The results should be persisted locally with a timestamp so that expensive plugins (like searching for a binary, harddisk performance checks etc) are not run too often. Questions: - Where should i put the configuration to be available throughout the cluster (especially to the namenode and to the jobtracker). Would DatanodeInfo be a good place? - Would it make sense to merge the capabilities with the generic conf structure? - Plugins (.class, shell and perl scripts) currently reside in I am not quite happy with that and not yet sure where to place them in the build stack. any recommendations? Maybe ?",non_debt,-
hadoop,3999,comment_4,"- Nodes collect local information (functions / performance indicators / other) via plugins - We assume the job scheduler knows the information of these (now individualized) nodes - Cloud might be logically split up into several sections, functional ones, providing some special software or having some special capability - The scheduler can now provide different quality (Service Levels, Software) as well as quantity levels (Performance, BW) to the customer. - Customer now can submit different ""profiled"" jobs. Regarding the profile they submitted they can be charged at different cost.",non_debt,-
hadoop,3999,comment_5,Closing this as stale. Much of this functionality has since been added to YARN and HDFS. Holes are slowly being closed!,non_debt,-
hadoop,4025,summary,Add hama site link to hadoop related project,non_debt,-
hadoop,4025,description,"The Hama team which is develop a parallel matrix computational package based on Hadoop would like to add our site link to hadoop related project. Please review this, Thanks.",non_debt,-
hadoop,4025,comment_0,Here's the patch.,non_debt,-
hadoop,4025,comment_1,Submit for review.,non_debt,-
hadoop,4025,comment_3,"I just committed this. Thanks, Edward! I'm assuming it's cached on the site for a while.",non_debt,-
hadoop,4025,comment_4,Thanks Johan. : ),non_debt,-
hadoop,4066,summary,fuse-dfs readme cleanup and point to wiki instead,non_debt,-
hadoop,4066,description,stop having to duplicate changes in both the readme and wiki This is a change to the README only and as such should not require any QA or anything and obviously no unit tests :),documentation_debt,low_quality_documentation
hadoop,4066,comment_0,"That's it - done. I assume it can be committed without submitting it as a patch to hudson?? thanks, pete",non_debt,-
hadoop,4066,comment_1,I think it is better to keep this document in the README file. The reasons being: 1. The README is version-controlled and can change from version to version. 2. All contrib modules are *required* to have a README file.,documentation_debt,low_quality_documentation
hadoop,4066,comment_2,"Wiki is *not* an acceptable place for system documentation. In particular, it is not versioned or released.",documentation_debt,low_quality_documentation
hadoop,4066,comment_3,I filed to cleanup the README and will delete the version specific stuff from the wiki.,documentation_debt,low_quality_documentation
hadoop,4182,summary,Streaming Documentation Update,non_debt,-
hadoop,4182,description,"When Text input data is used with streaming, every line is expected to end with a newline. Hadoop results are undefined if input files do not end in a newline. (The results will depend on how files are assigned to mappers.) Example: In streaming if mapper = xargs cat reducer = cat and the input is a two line, where each line is symbolic link in HDFS link1\n link2\n EOF link1 points to a file which contains This is line1EOF link2 points to a file which contains This is line2EOF Now running a streaming job such that, there is only one split, will produce results: This is line1This is line2\t\n But if there were two splits, the result will be This is line1\t\n This is line2\t\n So in summary, the output depends on the factor that how many mappers were invoked. As a caution, it should be recorded in Streaming wiki that users always put a new line at the end of each line to get away with such problems.",documentation_debt,low_quality_documentation
hadoop,4182,comment_0,"There isn't much that streaming can do. In the first case, your application gives the streaming framework: line1line2EOF In the second case you give it: line1EOF in one map and line2EOF in the second map streaming needs line based data, so the entire input is treated as a line. Leading to the differences that you observed.",non_debt,-
hadoop,4182,comment_1,"I agree with you that it is a problem at the application / user level. I only wanted to put a simple comment somewhere on the Hadoop Wiki that says that a line must end with an end of line delimiter. If not, user might get different behaviors as I explained earlier. This simple comment can keep a user from accidental un-expected results.",documentation_debt,low_quality_documentation
hadoop,4182,comment_2,"I updated the wiki documentation of the page as follows. Line 28: -Default Map input format: a line is a record in UTF-8 - the key part ends at first TAB, the rest of the line is the value +Default Map input format: a line is a record in UTF-8. Every line must end + with an 'end of line' delimiter. The key part ends at first TAB, the rest + of the line is the value",non_debt,-
hadoop,4231,summary,Hive: converting complex objects to JSON failed.,non_debt,-
hadoop,4231,description,Can not set int field xxx to java.lang.String,non_debt,-
hadoop,4231,comment_0,"The problem was that ReduceSinkOperator was trying to convert everything to String. It should not be doing that - instead the Serializer should take care of that if necessary (like does). In the old code reduceSinkOperator was passing <object.toString(), objectInspector converts objects to JSON string only if they are NOT primitive objects, so this does not affect primitive objects like String. The logic in makes sure the data is written as ""\\N"" if it is a String type and it's a null. Once we have the DynamicSerDe working, we will use that to make sure we can pass complex objects from map to reduce (and serialize complex objects).",non_debt,-
hadoop,4231,comment_2,Fixed as part of JIRA-4230,non_debt,-
hadoop,4245,summary,KFS: Update the kfs jar file,non_debt,-
hadoop,4245,description,Please update the kfs jar file in hadoop/lib to the one in this jira (kfs-0.2.2.jar).,non_debt,-
hadoop,4245,comment_0,An updated jar is included in this attachment.,non_debt,-
hadoop,4245,comment_1,this issue should also contain a patch file that updates with the new kfs jar name otherwise Hudson will fail the contrib unit tests.,non_debt,-
hadoop,4245,comment_2,A patch file eclipse classpath is included.,non_debt,-
hadoop,4245,comment_3,This was tested locally. I ran tests and we used this to test it against the latest kfs deployement using the jar.,non_debt,-
hadoop,4245,comment_4,I just committed this. Thank you Sriram.,non_debt,-
hadoop,4300,summary,"When fs.trash.interval is set to non-zero value, the deleted files and directory which are in .Trash are not getting removed from there after <fs.trash.interval>",non_debt,-
hadoop,4300,description,"Set fs.trash.interval to non zero value(say 1), touch a file (say file.txt) and delete it. The expected behavior would be that file.txt is moved to .Trash and also file.txt is removed from .Trash after 1min. But the observed behavior is that, even though file.txt is being moved to .Trash, it is not removed from .Trash after 1min.",non_debt,-
hadoop,4300,comment_0,"Ramya, how are you setting the interval to 1 minute? in hodrc or hadoop-site.xml? If this is truly broken, why is this marked ""minor""?",non_debt,-
hadoop,4300,comment_1,"I haven't tested on 0.19, but we've been using the Trash feature on our clusters for some time . Could you check if Namenode config also has the non-zero fs.trash.interval? - Client uses 'fs.trash.interval' to determine whether to delete files or move them to Trash. - Namenode uses 'fs.trash.interval' for running the expunge thread to clean up the users' Trash directories. So your files should be deleted between namenode's 'fs.trash.interval to 'fs.trash.interval * 2' period.",test_debt,lack_of_tests
hadoop,4300,comment_2,"To explain it in more detail, here is how I carried out the test: 1) Allocate a cluster through HOD 2) Append 3) ssh to the machine where name node is running and append the same above line in <hodring.temp-dir4) Create some files on HDFS and delete it. The files are getting deleted and being moved to .Trash but not being removed from .Trash after <fs.trash.interval>",non_debt,-
hadoop,4300,comment_3,"Wait... just to be clear, _after_ the cluster was allocated, you updated the config file of the running namenode? If so, that's not supported and the parameter needs to be given to HOD prior to allocation.",non_debt,-
hadoop,4300,comment_4,"Sorry for the mistake Chris. Koji pointed out the right way to execute it. Here is what I followed: 1) Allocate a cluster through HOD and pass to HOD. 2) Append 3) Create some files on HDFS and delete it. The files are getting deleted and being moved to .Trash but not being removed from .Trash after <fs.trash.interval Interestingly, I set up a stand alone mode and the files were removed from .Trash after But the same behavior was not seen in distributed mode.",non_debt,-
hadoop,4300,comment_5,Please mark it as a blocker for 0.19 if required,non_debt,-
hadoop,4300,comment_6,Verified the issue with Milind and confirming it as a blocker for 0.19.,non_debt,-
hadoop,4300,comment_7,Are you using HOD to create an HDFS cluster? It's not clear from your description that you are. If you allocate a map/reduce only cluster through HOD set fs.trash.interval for your HOD allocation then that setting will not affect a static external HDFS.,non_debt,-
hadoop,4300,comment_8,"Sorry, for not being clear. I am using HOD to create a HDFS cluster. I am not making use of the static external HDFS.",non_debt,-
hadoop,4300,comment_9,"It is difficult to imagine circumstances where this would be true. I tried to reproduce this on a small, non-HOD cluster using (0.19, r704262) and trash worked as expected. It looks like this issue is with HOD or its configuration.",non_debt,-
hadoop,4303,summary,Hive: trim and rtrim UDFs behaviors are reversed,non_debt,-
hadoop,4303,description,Currently trim removes trailing spaces where as rtrim removes spaces from both ends of the string.,non_debt,-
hadoop,4303,comment_0,This patch has all the fixes for hive bugs that we have made in the internal tree. Incorporation of this patch will allow us to close out all the 0.19 bugs open so far.,non_debt,-
hadoop,4303,comment_1,changing this to fix for 0.19,non_debt,-
hadoop,4303,comment_2,submitting.,non_debt,-
hadoop,4303,comment_3,1,non_debt,-
hadoop,4303,comment_4,1,non_debt,-
hadoop,4303,comment_5,Fixed as part of JIRA-4230,non_debt,-
hadoop,4303,comment_6,Fixed as part of JIRA-4230,non_debt,-
hadoop,4303,comment_7,Fixed in 0.19.1,non_debt,-
hadoop,4303,comment_8,Fixed in 0.19.0,non_debt,-
hadoop,4324,summary,Need a way to get the inforserver port of a name node,non_debt,-
hadoop,4324,description,"To test that services shut down cleanly, I need to know the port that the namenode brings up an info server on, which means that its assigned port value needs to be exported from the namesystem. I can see two ways to do this, and wish some recommendations of the best approach 1. extract the port value when the FSNameSystem comes up, and add it to that classes Conf; add a method to get that Conf so that its state can be read. 2. save the port value to a member variable in FSNameSystem and provide a public method to get at it. My preference is for #1, as it is consistent with how the NameNode saves its port value, and doesn't change any APIs.",non_debt,-
hadoop,4324,comment_0,looking in the wrong place; getInfoPort() does it. It may still be useful to push out to the Conf instance though.,non_debt,-
hadoop,4324,comment_1,"I'm generally not a fan of using the Configuration for output. A Configuration is a generic input mechanism for metadata, not a read/write blackboard for state.",non_debt,-
hadoop,4343,summary,Adding user and service-to-service authentication to Hadoop,non_debt,-
hadoop,4343,description,"Currently, Hadoop services do not authenticate users or other services. As a result, Hadoop is subject to the following security risks. 1. A user can access an HDFS or M/R cluster as any other user. This makes it impossible to enforce access control in an uncooperative environment. For example, file permission checking on HDFS can be easily circumvented. 2. An attacker can masquerade as Hadoop services. For example, user code running on a M/R cluster can register itself as a new TaskTracker. This JIRA is intended to be a tracking JIRA, where we discuss requirements, agree on a general approach and identify subtasks. Detailed design and implementation are the subject of those subtasks.",non_debt,-
hadoop,4343,comment_0,Here is the authentication design I plan to implement.,non_debt,-
hadoop,4343,comment_1,More details on the delegation token design. h4.,non_debt,-
hadoop,4343,comment_2,"An additional benefit of using Hadoop proprietary delegation tokens for delegation, as opposed to using Kerberos TGT/Service tickets, is that Kerberos is only used at the ""edge"" of Hadoop. Delegation tokens don't depend on Kerberos and can be coupled with non-Kerberos authentication mechanisms (such as SSL) used at the edge.",non_debt,-
hadoop,4343,comment_3,Required to support security in 0.23 right?,non_debt,-
hadoop,4343,comment_4,"Eli, the authentication feature is there in 20-security and in trunk. It was just not closed.",non_debt,-
hadoop,4402,summary,Namenode is unaware of FS corruption,non_debt,-
hadoop,4402,description,"I think the name node is not told when there is block corruption. I found a huge number of files corrupted when I restarted my namenode. Digging through the datanode logs, I saw the following: 2008-10-13 03:30:44,266 INFO Reporting bad block to namenode. 2008-10-13 03:57:11,447 WARN First Verification failed for Exception : Block is not valid. 2008-10-13 03:57:11,448 WARN Second Verification failed for Exception : Block is not valid. 2008-10-13 03:57:11,448 INFO Reporting bad block to namenode. So, node099 found a bad block. However, if I grep the namenode information for that block: 20:21:20,002 INFO BLOCK* 20:21:32,150 INFO BLOCK* blockMap updated: 172.16.1.110:50010 is added to size 67108864 20:21:32,151 INFO BLOCK* blockMap updated: 172.16.1.99:50010 is added to size 67108864 05:05:26,898 INFO BLOCK* ask 172.16.1.99:50010 to replicate to datanode(s) 172.16.1.18:50010 05:05:40,742 INFO Error report from 172.16.1.99:50010: Can't send invalid block 05:12:43,759 WARN timed out block To summarize: - Block is allocated and written successfully to node100, then replicated to node099. - Name node asks node099 to replicate block to node018 - Name node is told it can't send invalid block to node018! A few minutes later, the times out - No new replications are launched!!! - Block is found to be corrupted on node099 a few days later. Data node claims to inform the namenode of this, but nothing is listed in the namenode logs. - Block is suspiciously missing on node110 as well Perhaps there are a few bugs here? 1) Name node doesn't get notified of the corrupted blocks - even though the datanode claims to! 2) On replication failure, no new replicas are created. 3) Corruption events are much, much too common. We have a specific dataset which the namenode now claims is mostly-corrupted (100/167 files); before I restarted the namenode, we had jobs run against it continuously for the entire weekend. The jobs were all successful, and the binary data format does internal integrity checks as it reads the files. If the corruption was real, jobs would have failed. I'm concerned that the systems of Hadoop are seriously busted for me.",non_debt,-
hadoop,4402,comment_0,"Ah - mystery solved. It turns out that there actually was a large-scale corruption event that took out half the hadoop data nodes, but no one realized it until the above recurred. We have a ""scratch cleanup"" script which did a ""rm"" on all of the hadoop data directories for about 100 nodes. So, starting at midnight, all the block verification attempts started failing because there were no blocks to read; when I restarted the namenode, the data nodes suddenly had new block reports saying they all had no blocks. Please close.",non_debt,-
hadoop,4436,summary,S3 object names with arbitrary slashes confuse NativeS3FileSystem,code_debt,low_quality_code
hadoop,4436,description,"Consider a bucket with the following object names: * / * /foo * foo//bar NativeS3FileSystem treats an object named ""/"" as a directory. Doing an ""fs -lsr"" causes an infinite loop. I suggest we change NativeS3FileSystem to handle these by ignoring any such ""invalid"" names. Thoughts?",code_debt,low_quality_code
hadoop,4436,comment_0,"Is this only a problem for S3 files that are written using other tools, or can you cause this problem by writing using NativeS3FileSystem? The discussion in HADOOP-3257 may be relevant here too.",non_debt,-
hadoop,4436,comment_1,I only noticed this with existing buckets written by other tools.,non_debt,-
hadoop,4436,comment_2,"closing as WONTFIX...s3n, s3a and openstack clients all assume that a ""/"" paths represent directory delimiters in paths, an assumption that goes pretty deep. Sorry",non_debt,-
hadoop,4436,comment_3,Closing old tickets that are already part of a release.,non_debt,-
hadoop,4576,summary,Modify pending tasks count in the UI to pending jobs count in the UI,non_debt,-
hadoop,4576,description,"The UI for capacity scheduler displays 'pending tasks' counts. However the capacity scheduler does not update these counts to be the actual values for optimization purposes, for e.g. to avoid walking all pending jobs on all heartbeats. Hence this information is not very accurate. Also, while 'running tasks' counts are useful to compare against capacities and limits, 'pending tasks' counts do not add too much user value. A better count to display would be the number of running and pending jobs.",code_debt,low_quality_code
hadoop,4576,comment_0,"Attaching patch, which displays running jobs and waiting jobs instead of running and waiting task counts for maps and reduces. Waiting jobs and running jobs are computed after getting all jobs which are present in the scheduler and checking their status, this is done to incorporate changes which would be made in",non_debt,-
hadoop,4576,comment_1,Attaching output of ant test-patch:,non_debt,-
hadoop,4576,comment_2,"Attaching new patch, this patch does not iterate thro' job list to find count of waiting jobs,instead introduces a counter in the QueueInfo object in JobQueueManager to maintain a count of waiting jobs which is incremented and decremented (by This would mean that in we just need to iterate thro' running jobs queue to find number of running tasks for that queue.",non_debt,-
hadoop,4576,comment_3,Attaching output of ant test-patch :,non_debt,-
hadoop,4576,comment_4,"Attaching a new patch merged with current trunk. Plus have removed the variable which I introduced in previous patch, it was not required as we can find out number of waiting jobs directly from size of the jobList instead of having a counter. Also added a note in scheduling information stating that scheduling information can be off at the maximum by polling interval of the job initialization poller which does clean up of the waiting jobs.",non_debt,-
hadoop,4576,comment_5,"Comments : : 1) No need to import {{AtomicInteger}}. Also plz remove extra diffs. : 1) You cant simply change the constructor def. Overload it and deprecate the other if needed. 2) Extra diffs w.r.t. 3) Use {{StringUtils}} for formatting time. 1) The ordering doesnt seem right. You submit 5 jobs, try to assign tasks (which should be no-op) and then you init the jobs. 2) Shouldnt we also check/test the timing issue that after _poll-interval_ units of time the values are correct. Something like - add jobs - check the queue-sched-info - allow the jobs to be inited by the poller i.e wait for _poller-interval_ time - check again to see if the change is made. 2) Plz mark the start and end of a new sub-test using comments",code_debt,low_quality_code
hadoop,4576,comment_6,"Attaching latest patch incorporating Amar's comments. With respect to the test cases: Following is the order we are testing the scheduling information: - Submit 5 jobs to a queue. - Check the waiting jobs count, it should be 5. - Then run this initializes first two jobs in the queue but does not raise the job status changed event. - Check once again the waiting queue, it should be 5 jobs again. - Then raise status change events. - Assign one task to a task tracker. - Check waiting job count, it should be 4 now. - Then pick an initialized job but not scheduled job and fail it. - Run the poller, since poller is responsible for removing, failed jobs and scheduled jobs from job queue maintained by job queue manager, there is no requirement for raising status changed event as the job is cleared without it by the poller once job fails before it gets scheduled. With respect to removal of the jobs which are scheduled and failing is taken care by the JobQueueManager. - Check the waiting job count should now be 3. - Now fail a job which has not been initialized at all. - Run the poller, so that it can clean up the job queue. - Check the count, the waiting job count should be 2. I hope this answers the doubt with regarding test case, I have also mentioned the steps as inline comment in the test case.",non_debt,-
hadoop,4576,comment_7,+1. Looks good.,non_debt,-
hadoop,4576,comment_8,Attaching patch with merging with today's trunk.,non_debt,-
hadoop,4576,comment_9,"I think it is not necessary to deprecate the constructor of SchedulingInfo. It is a private static inner class and hence nobody should be affected ? It would just add more code to maintain. Can you please submit a new patch removing the deprecation and just changing the api ? Please also remember to remove the checks for null in the toString method, as the object is not expected to be null after this change. BTW, I spoke to Amar about this and we agree on this point now.",code_debt,dead_code
hadoop,4576,comment_10,Changing visibility of the constructor and removing the deprecation. ant test-patch output for the patch is :,code_debt,dead_code
hadoop,4576,comment_11,Attaching screenshot of the scheduling information after the patch has been applied.,non_debt,-
hadoop,4576,comment_12,The ant test-contrib passed on the local machine.,non_debt,-
hadoop,4576,comment_13,"I just committed this. Thanks, Sreekanth !",non_debt,-
hadoop,4576,comment_15,Edit release note for publication.,non_debt,-
hadoop,4603,summary,Installation on Solaris needs additional PATH setting,non_debt,-
hadoop,4603,description,"A default installation as outlined in the docs won't start on Solaris 10 x86. The ""whoami"" utility is in path ""/usr/ucb"" on Solaris 10, which isn't in the standard PATH environment variable unless the user has added that specifically. The documentation should reflect this. Solaris 10 also seemed to throw NPEs if you didn't explicitly set the IP address to bind the servers to. Simply overriding the IP address fixes the problem.",documentation_debt,outdated_documentation
hadoop,4603,comment_0,Stack traces of any NPEs would be handy.,non_debt,-
hadoop,4603,comment_1,"Uses sh, id, sed, cut instead of whoami",non_debt,-
hadoop,4603,comment_2,"Currently, we don't require sh, cut and sed. These changes may break existing systems. BTW, we do assume bash.",non_debt,-
hadoop,4603,comment_3,"ok, good to know... bash might be missing on systems like Solaris and BSD though and/or in a different path.",non_debt,-
hadoop,4603,comment_4,"I think a better change is for Hadoop to simply try /usr/ucb/whoami if exec whoami fails. This code executes only once, so I don't see any downside of trying multiple locations for better user experience. Another possibility is to use JNA and make a libc call from within Java.",non_debt,-
hadoop,4603,comment_5,A patch to call /usr/ucb/whoami is at,non_debt,-
hadoop,4603,comment_6,Patch extracted from Git,non_debt,-
hadoop,4603,comment_7,"Calling /usr/ucb/whoami directly is definitely the way to go, IMO. It is going to exist on all but the most hardened of Solaris boxes.",non_debt,-
hadoop,4603,comment_8,"Hey Allen, if you can +1, we can commit changes similar to Kohsuke's into the trunk to at least have whoami working on Solaris (its a start). Let me know if there really is no better way for sure.",non_debt,-
hadoop,4603,comment_9,"This is essentially fixed by using the native libraries in newer releases of Hadoop. The problem is that the native code is completely non-portable and the committer community has shown no real desire to make that code portable. I don't believe we should encourage folks to run without the native code because the performance is likely to be seriously horrendous. (My anecdotal experience says the NN in 0.20.20x is 10-15% slower compared to 0.20.2). At this point, I'd close this as won't fix, just like I did all of my portability JIRAs (including some with patches). I think pretending that we care about portability is sort of silly at this point when it has been demonstrated over and over that we don't.",design_debt,non-optimal_design
hadoop,4603,comment_10,"Now I really am going to close this as Won't Fix, especially since we finally removed the requirement for whoami.",non_debt,-
hadoop,4608,summary,Examples -Driver does not check first argument.,non_debt,-
hadoop,4608,description,"hadoop@vldb ~/hadoop $ bin/hadoop jar An example program must be given as the first argument. Valid program names are: aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files. aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files. grep: A map/reduce program that counts the matches of a regex in the input. join: A job that effects a join over sorted, equally partitioned datasets multifilewc: A job that counts words from several files. pentomino: A map/reduce tile laying program to find solutions to pentomino problems. pi: A map/reduce program that estimates Pi using monte-carlo method. randomtextwriter: A map/reduce program that writes 10GB of random textual data per node. randomwriter: A map/reduce program that writes 10GB of random data per node. sleep: A job that sleeps at each map and reduce task. sort: A map/reduce program that sorts the data written by the random writer. sudoku: A sudoku solver. wordcount: A map/reduce program that counts the words in the input files. An example program must be given as the first argument. Method)",non_debt,-
hadoop,4608,comment_0,"IMO, In this case the exit is enough.",non_debt,-
hadoop,4608,comment_1,Current trunk also affected.,non_debt,-
hadoop,4608,comment_2,Submit my patch. Can anyone review my code? Thanks in advance.,non_debt,-
hadoop,4608,comment_4,"I just committed this. Thanks, Edward! This isn't a blocker, so only applying to trunk.",non_debt,-
hadoop,4611,summary,Documentation for Tool interface is a bit busted,documentation_debt,low_quality_documentation
hadoop,4611,description,"The documentation for the Tool interface will not work out of the box. It seems to have taken the Sort() implementation in examples, but has ripped out some important information. 1) args[1] and args[2] should probably be args[0] and args[1], as most MapReduce tasks don't take the first argument that examples.jar takes 2) int run() needs to actually return an int 3) and are deprecated. 4) the call to ToolRunner.run() in main() should take ""new MyApp()"" instead of ""Sort()"" as an argument More generally, a working implementation of Tool in the docs would be handy.",documentation_debt,low_quality_documentation
hadoop,4611,comment_0,"Tool is still broken, but for different reasons. I'll file a different jira for that.",non_debt,-
hadoop,4634,summary,220 javac compiler warnings,code_debt,low_quality_code
hadoop,4634,description,"""ant test-patch"" on trunk for a zero size patch file. Then, it said,",non_debt,-
hadoop,4634,comment_0,This problem begins from build #3572. See - - -,non_debt,-
hadoop,4634,comment_1,I am investigating if this was caused by HADOOP-4634,non_debt,-
hadoop,4634,comment_2,"Or rather, HIVE-6",non_debt,-
hadoop,4634,comment_3,"Hive is moved to a subproject at if these warnings were created by Hive, then this issue can be safely closed.",non_debt,-
hadoop,4634,comment_4,Build #3578 seems back to normal.,non_debt,-
hadoop,4710,summary,"Chukwa - Add duplicate detection, and implement virtual offset of the log file to checkpoint file",non_debt,-
hadoop,4710,description,"Each data stream has been sent to Chukwa with sequence id, and this sequence id is used as the guide line for tracking duplicate chunk data in Chukwa. However, the check point file does not include the virtual offset. This means when collector crashed, sequence id is reset to zero. Chukwa Agent needs to keep track of the sequence id in the check point file in order to recover from a crash.",non_debt,-
hadoop,4710,comment_0,This has been resolved by 4709 patch.,non_debt,-
hadoop,4710,comment_1,This is integrated by patch for HADOOP-4709.,non_debt,-
hadoop,4719,summary,The ls shell command documentation is out-dated,documentation_debt,outdated_documentation
hadoop,4719,description,"Current ls output is but the doc says ""dirname <dir> modification_time modification_time permissions userid groupid"". See",non_debt,-
hadoop,4719,comment_0,Did not run tests . Change in documentation .,test_debt,lack_of_tests
hadoop,4719,comment_1,"""&amp;lt;number of replicas&amp;gt;"" should be changed to since the ls output does not contain '<' and '>' anymore.",documentation_debt,outdated_documentation
hadoop,4719,comment_2,Canceling patch to fix issue raised by Nicholas,non_debt,-
hadoop,4719,comment_3,Did not run tests . Just document change.,non_debt,-
hadoop,4719,comment_4,The patch does not work.,non_debt,-
hadoop,4719,comment_5,Canceling patch to resubmit correct patch.,non_debt,-
hadoop,4719,comment_6,Submitting correct patch . Output of ant test docs [,non_debt,-
hadoop,4719,comment_8,"Technically, the owner and group are optional in the output (in support of FileSystems like KFS: HADOOP-4335). I have no strong convictions about including this point in the docs; we can open another issue if it causes confusion. +1",non_debt,-
hadoop,4719,comment_9,"I committed this. Thanks, Ravi",non_debt,-
hadoop,4849,summary,Document service level authorization - HADOOP-4348,non_debt,-
hadoop,4849,description,Document service level authorization (HADOOP-4348) via forrest.,non_debt,-
hadoop,4849,comment_0,Documentation for service-level authorization (HADOOP-4348).,non_debt,-
hadoop,4849,comment_1,1,non_debt,-
hadoop,4849,comment_2,I just committed this. In anticipation of HADOOP-4920 I didn't check-in the generated docs into trunk/docs.,non_debt,-
hadoop,4858,summary,to add appropriate reference to the dependent library files in the chukwa/build.xml file,non_debt,-
hadoop,4858,description,While going through the chukwa/build.xml 's package-hadoop target found that we are trying to copy jsp-api.jar from the hadoop/lib dir but actually the jsp-api. jar resides in chukwa/lib directory. For more details see comment -Giri,non_debt,-
hadoop,4858,comment_0,Chukwa moved out long ago.,non_debt,-
hadoop,4878,summary,After introduction of ivy ant test-patch always returns -1 score,non_debt,-
hadoop,4878,description,After dependency management is moved to ivy : ant test-patch always returns a -1 score. The reason reported by the target is modification of the Eclipse classpath.,non_debt,-
hadoop,4878,comment_0,Output of ant test-patch on an empty patch on the current trunk (revision 726981) -1 on test is expected but not for Eclipse classpath.,non_debt,-
hadoop,4878,comment_1,i've disabled patch testing until this is resolved. Giri is looking at the issue now.,non_debt,-
hadoop,4878,comment_2,"created a patch to add path's of ivy resolved lib dir's to file and added the same path to test-patch.sh script. This patch adds changes to the patch testing script itself. Hence this patch cannot be tested using the test-patch target. I ve tested is manually and it works just fine. Thanks, Giri",non_debt,-
hadoop,4878,comment_3,1,non_debt,-
hadoop,4878,comment_4,"I just committed this. Thanks, Giridharan!",non_debt,-
hadoop,4884,summary,Change Date format pattern for Time Series graph,non_debt,-
hadoop,4884,description,The tool tip of Time series chart for Chukwa is formatting date as: day/month/year hour:minute:second The date format should change to: year/month/day hour:minute:second,code_debt,low_quality_code
hadoop,4884,comment_0,Change tool tip format to Year/Month/Day Hour:Minute/Second for Time Series Graph,non_debt,-
hadoop,4884,comment_1,+1 match the standard HICC format,non_debt,-
hadoop,4884,comment_2,"I committed this. Thanks, Eric",non_debt,-
hadoop,4889,summary,Chukwa RPM needs to chown user/group in build stage instead of post install stage.,non_debt,-
hadoop,4889,description,When running rpm -V the rpm shows: bash-3.1# rpm -V chukwa-0.1.1-1 .....UG. .....UG. /grid/0/chukwa/bin .....UG. .....UG. This means the User and Group permission are mismatched with what RPM provided. The build script needs to change to chown file ownership during build time instead of chown in the post installation script.,non_debt,-
hadoop,4889,comment_0,- move chown operation from post install into rpm package phase.,non_debt,-
hadoop,4889,comment_1,+1 need that in order to run rpm verified,non_debt,-
hadoop,4889,comment_2,"I committed this. Thanks, Eric",non_debt,-
hadoop,4941,summary,"Remove getBlockSize(Path f), getLength(Path f) and getReplication(Path src)",non_debt,-
hadoop,4941,description,Remove the following - public long getBlockSize(Path f) throws IOException - public long getLength(Path f) throws IOException - public short getReplication(Path src) throws IOException,non_debt,-
hadoop,4941,comment_0,remove the deprecated methods.,code_debt,dead_code
hadoop,4941,comment_1,+1. Patch looks good.,non_debt,-
hadoop,4941,comment_3,I got a compile error {{fs.getLength(p)}} should be changed to,non_debt,-
hadoop,4941,comment_4,"Is it already in I tried ""ant tar"". Everything can be compiled. I tested the patch in my machine. Only TestMapReduceLocal failed. See HADOOP-4907.",non_debt,-
hadoop,4941,comment_5,I just committed this.,non_debt,-
hadoop,4941,comment_6,Looks like there was some problem with my build area. +1 for the patch.,non_debt,-
hadoop,4941,comment_8,Editorial pass over all release notes prior to publication of 0.21.,non_debt,-
hadoop,4985,summary,IOException is abused in FSDirectory,code_debt,low_quality_code
hadoop,4985,description,"In FSDirectory, quite a few methods are unnecessary declared with ""throws IOException"". In some cases, it can just be removed without causing any compilation problem. In some other cases, it can be replaced with a specific subclass like",code_debt,low_quality_code
hadoop,4985,comment_0,"remove unnecessary ""throws IOException""",code_debt,dead_code
hadoop,4985,comment_1,"reverted the changes of try-catch, which need more thought.",non_debt,-
hadoop,4985,comment_2,"- FSDirectory empty line change. - Javadoc for should not remove {{@throws IOException}} but rather replace it with {{@throws - file: "" ...)}} should be does not exist: "" ...)}} I once tried to unify this in the code, but it's now back with all different messages. - Additionally we can remove IOException in FSEditLog.close() and then and then",documentation_debt,low_quality_documentation
hadoop,4985,comment_3,incorporated Konstantin's comments.,non_debt,-
hadoop,4985,comment_4,+1. This looks good.,non_debt,-
hadoop,4985,comment_5,No new tests added since there are only declaration changes but no code logic change.,non_debt,-
hadoop,4985,comment_6,Tested locally. All tests Passed except HADOOP-4907: TestMapReduceLocal.,non_debt,-
hadoop,4985,comment_7,I just committed this.,non_debt,-
hadoop,4997,summary,workaround for tmp file handling on DataNodes in 0.18 (HADOOP-4663),non_debt,-
hadoop,4997,description,"This is a temporary work around issues discussed in HADOOP-4663. The proposal is to remove all the files under tmp directory, thus bringing the behavior back to 0.17. The main cost is that sync() will not be supported. This is incompatible with 0.18.x, but not with 0.17 because of this reason.",design_debt,non-optimal_design
hadoop,4997,comment_0,"when the datanode restarts, remove all blocks from the tmp directory of the datanode. However, fsync does not throw an exception.",non_debt,-
hadoop,4997,comment_1,"Dhruba, we should also remove the unit tests that test the tmp directory does not get removed after restarting the cluster.",test_debt,expensive_tests
hadoop,4997,comment_2,The two junit tests that would fail with this patch are and,non_debt,-
hadoop,4997,comment_3,Thanks Hairong. The updated patch disables the test Hairong mentioned with a very explicit comment that it is a temporary change and it has the original patch Dhruba attached.,non_debt,-
hadoop,4997,comment_4,"Hi Hairong, I saw ur comment just now about the unit tests. Thanks for fixing it.",non_debt,-
hadoop,4997,comment_5,+1 on Raghu's patch. It is critical for 0.18.,non_debt,-
hadoop,4997,comment_6,test-patch :  - JavaDoc failures are unrelated to this patch (they are for,non_debt,-
hadoop,4997,comment_7,I just committed this to 0.18 only.,non_debt,-
hadoop,4997,comment_8,Could you please file a bug against CyclicIterator.java,non_debt,-
hadoop,4997,comment_9,done : HADOOP-5077.,non_debt,-
hadoop,5076,summary,chukwa metrics file get overwritten when process launch,non_debt,-
hadoop,5076,description,In the log4j appender always rewrite the file instead of append to the log file. This should be changed to append to ensure the metrics log file is streamed correctly.,code_debt,low_quality_code
hadoop,5076,comment_0,Duplicate of HADOOP-5100,non_debt,-
hadoop,5097,summary,Remove static variable JspHelper.fsn,non_debt,-
hadoop,5097,description,"There is another static FSNamesystem variable, fsn, declared in JspHelper. We should remove it.",code_debt,dead_code
hadoop,5097,comment_0,"- change JspHelper.fsn from ""static"" to ""private final"" - change a few non-static methods in JspHelper to static",code_debt,low_quality_code
hadoop,5097,comment_1,added javadoc and re-organized some methods.,code_debt,low_quality_code
hadoop,5097,comment_2,"# Remove comment in FSNamesystem /** NameNode RPC address */ # I do not very much like that you add JspHelper as a NameNode members. I understand the intention is not to instantiate objects on the web UI, but may be we should rather consider making most of the methods / fields of JspHelper static instead of constructing the object. In any case it is better not to introduce helper variables in NamNode and DataNode.",code_debt,low_quality_code
hadoop,5097,comment_3,changed all JspHelp methods to static and removed /** NameNode RPC address */,code_debt,low_quality_code
hadoop,5097,comment_4,should be done in static. Thank Suresh for catching this.,code_debt,low_quality_code
hadoop,5097,comment_6,+1 for the patch if findbugs and test failures are fixed.,non_debt,-
hadoop,5097,comment_7,There were no new findbugs warnings. The problem is that hdfsproxy cannot be compiled after the changes. I guess test-patch was confused. fixed the problem with hdfsproxy.,non_debt,-
hadoop,5097,comment_8,Passed all tests locally.,non_debt,-
hadoop,5097,comment_10,revert some changes in FileDataServlet so that it won't affect hdfsproxy.,non_debt,-
hadoop,5097,comment_11,+1. New patch looks good,non_debt,-
hadoop,5097,comment_13,The failed Chukwa tests are not related to the changes here. I just committed this.,non_debt,-
hadoop,5111,summary,Generic mapreduce classes cannot be used with Job::set* methods,non_debt,-
hadoop,5111,description,"The set\* methods on a Job take {{Class}} instances whose parameterized type is an unbounded type (e.g. {{Class<? extends Attempting to pass a {{Class}} whose parameterized types are not explicit will cause compile-time errors, as in HADOOP-5065.",non_debt,-
hadoop,5111,comment_0,+1 Retrying Hudson.,non_debt,-
hadoop,5111,comment_2,I committed this.,non_debt,-
hadoop,5130,summary,TaskTracker seems to hold onto the assigned task for a long while before launching it,non_debt,-
hadoop,5130,description,I saw atleast a couple of instances where the task assigned to the TaskTracker is launched several minutes after the receipt of the LaunchTaskAction:,non_debt,-
hadoop,5130,comment_0,I also saw this behavior many times. And there are cases that these tasks got killed for not reporting progress for 10 mins.,non_debt,-
hadoop,5130,comment_1,"To clarify - the task did eventually launch and succeed, just the latency is too high (order of several minutes).",non_debt,-
hadoop,5130,comment_2,Promoting this to a blocker for 0.20.0.,non_debt,-
hadoop,5130,comment_3,Arun confirmed that this problem was due to the IPV4/V6 issue. This is discussed here - Adding to fixes the problem.,non_debt,-
hadoop,5141,summary,Resolving json.jar through ivy,non_debt,-
hadoop,5141,description,using json.jar (snapshot) from mvn repository and resolve the chukwa dependency through ivy,non_debt,-
hadoop,5141,comment_0,"I dont think SNAPSHOT release can be relied on to stay there; its very dangerous to release against them 1. There are two json releases in the main repository; the 20080701 version is fairly recent 2. Over in restlet, they put their own version 2.0 up in their own artifact repository 3. The central repository people say it's OK to put a new release up if that's what is needed, because the JSON team never do official releases themselves.",non_debt,-
hadoop,5141,comment_1,"I agree that we shouldn't use a SNAPSHOT. Of the 3 options outlined, is option 3 possible? How do we go about getting that done?",non_debt,-
hadoop,5141,comment_2,"1. check that the 20080701 version isn't what is actually needed 2. Someone creates a new release from the JSON source code; gives it a new version -20090201- for example 3. we take the previous POM file as a template; update it for the new version 4. submit it to the repository people as a JIRA issue; link to this one to show the reason for the new JAR 5. if that isn't enough, email the repository mailing list (I'm on it) I can help with steps 3-5",non_debt,-
hadoop,5141,comment_3,"Giri, did you check this? Steve, this sounds like we just make a release of JSON trunk whenever we want and then request it gets posted. That sounds like a strange release process. How's that different than a snapshot release?",non_debt,-
hadoop,5141,comment_4,"It doesn't have any specific dependency on the version, all we need is the latest version, since the vesion of json.jar that we are using currently is missing some classes.",architecture_debt,using_obsolete_technology
hadoop,5141,comment_5,"@Giri, if you check out and build the latest version, I'll help push it into the repository -steve",non_debt,-
hadoop,5141,comment_6,"Steve, I 've the json.jar created with the code's from Let me know if you you want the jar to be an attachment to this jira or as an email ? Tnx!",non_debt,-
hadoop,5141,comment_7,"stick it up as an attachment and I'll work on the POM, let everyone review that I will submit it to the repo team with a datestamp version 20090211",non_debt,-
hadoop,5141,comment_8,"Nigel said no different, except that by giving it a datestamp it gets pushed into the main repository forever. the build becomes replicable. With snapshot, you are saying ""build and ship the most recent version of some library, even if it is different from last week"".",non_debt,-
hadoop,5141,comment_9,attached json.jar built from,non_debt,-
hadoop,5141,comment_10,"This is what I propose for the POM file; it's based on the last one off the repository, and has no dependencies. Have a look at it, if we're happy I will try and get it added to the repository",non_debt,-
hadoop,5141,comment_11,"submitted to the repository managers, bugrep is",non_debt,-
hadoop,5141,comment_12,"FWIW, note that over at Sling we are maintaining our own variant of the json.org library:",non_debt,-
hadoop,5141,comment_13,what have you changed? And what is your release schedule?,non_debt,-
hadoop,5141,comment_14,"Steve, Thanks for adding json.jar to the mvn repo. I tried to findout the jar from but, couldn't find it out.. Could you please tell me if I'm referring to the right location?",non_debt,-
hadoop,5141,comment_15,"oops, I'm able to find it out.. will upload the patch soon .. Thanks Steve!",non_debt,-
hadoop,5141,comment_16,Giri -thank you for doing all the heavy lifting; I just edited a POM and opened a bug report on someone else's JIRA,non_debt,-
hadoop,5141,comment_17,closing this as wontfix as chukwa is no more a hadoop contrib comp.,non_debt,-
hadoop,5298,summary,Unit test fails out on trunk,non_debt,-
hadoop,5298,description,From: Regression Failing for the past 1 build (Since #760 ) Took 1 min 10 sec. Error Message expected:<8> but was:<9>,non_debt,-
hadoop,5298,comment_0,test log from build 760,non_debt,-
hadoop,5298,comment_1,check uri got filtered at least once instead of exactly once.,non_debt,-
hadoop,5298,comment_2,Tested locally.,non_debt,-
hadoop,5298,comment_3,"+1 Makes sense. It looks like HADOOP-4695 is addressing the same issue, but uses a different approach. Is the difference significant? The two tests are nearly identical and could probably share more code than they currently do.",non_debt,-
hadoop,5298,comment_4,"There is no difference in the logic. I agree that two tests are similar. The codes were duplicated in HADOOP-4284. It needs more works to refactor the codes now. Since these are only tests, I suggest we leave it and do the refactoring in the future.",code_debt,duplicated_code
hadoop,5298,comment_5,I committed this to 0.20 and above.,non_debt,-
hadoop,5329,summary,failed in Hudson,non_debt,-
hadoop,5329,description,"failed in Hudson, from [build to the latest, [build",non_debt,-
hadoop,5329,comment_0,The test no longer fails in Hudson.,non_debt,-
hadoop,5402,summary,TaskTracker ignores most RemoteExceptions from heartbeat processing,design_debt,non-optimal_design
hadoop,5402,description,The code in looks like:,non_debt,-
hadoop,5402,comment_0,This is probably stale.,non_debt,-
hadoop,5465,summary,Blocks remain under-replicated,non_debt,-
hadoop,5465,description,"Occasionally we see some blocks remain to be under-replicated in our production clusters. This is what we obeserved: 1. Sometimes when increasing the replication factor of a file, some blocks belonged to this file do not get to increase to the new replication factor. 2. When taking meta save in two different days, some blocks remain in under-replication queue.",non_debt,-
hadoop,5465,comment_0,"Thank Koji for his tireless investigation on this issue. When this situation occurs, the source DataNode of the block shows abnormal behavior. No blocks gets replicated from this node or no block gets removed from this node. Digging into the problem, we seet that NameNode sends the DataNode an empty replication request, i.e. a replication request with no blocks and targets as parameters, on every heartbeat reply, thus preventing sending the node any replication or deletion request. More suspiciously DataNode notifies NameNode that it has 1 replication in progress although its jstack shows that it has no replication (data transfer) thread alive.",non_debt,-
hadoop,5465,comment_1,"Two bugs in DFS contributed to the problem: (1). DataNode does not sync on modification to the counter ""xmitsInProgress"", which keeps track of the number of replication in progress. When two threads update the counter concurrently, race condition may occurs. The counter may change to be a non-zero value when no replication is going on. (2). Each DN is configured to have at most 2 replications in progress. When DN notifies NN that it has 1 replication in progress, NN should be able to send one block replication request to DN. But NN wrongly interprets the counter as the number of targets. When it sees that the block is scheduled to 2 targets but DN can only take 1, it sends an empty replication request to DN. As a result, blocking all replications from this DataNode. If the DataNode is the only source of an under-replicated block, the block will never get replicated. Fixing either (1) or (2) could fix the problem. I think (1) is more fundamental so I will fix (1) in this jira and file a different jira to fix (2).",code_debt,multi-thread_correctness
hadoop,5465,comment_2,This implies that all the blocks that remained under replicated have only one replica and only on this specific datanode. Was that the case?,non_debt,-
hadoop,5465,comment_3,The previous patch synced the counter on a wrong object. This patch uses AtomicInteger to guarantee atomic modification.,non_debt,-
hadoop,5465,comment_4,"Yes, most of the blocks have only one source. Those are the kind of blocks that initially triggers a DataNode into this state. But we could and our clusters do have under-replicated blocks that have two replicas and all its sources are in this state. The only exception is a block in our clusters that has two sources, one in this state but the other is replicating. This block is still under investigation.",non_debt,-
hadoop,5465,comment_5,"Thanks Hairong. Since a rare race condition is suspected, I thought there would be very few datanodes hitting such a race condition.",non_debt,-
hadoop,5465,comment_6,"On a cluster with thousands of machines, we saw 5% of the nodes were in this state. It turns out that the other source that is replicating has a corrupt copy of the block.",non_debt,-
hadoop,5465,comment_7,"Since this is such a vital stat, may be better to decrement at the top of finally block (so that some other runtime exception does not cause this situation again).",non_debt,-
hadoop,5465,comment_8,The patch incorporates Raghu's comment.,non_debt,-
hadoop,5465,comment_9,1,non_debt,-
hadoop,5465,comment_11,Attach a patch to 0.18.,non_debt,-
hadoop,5465,comment_12,I've just committed this.,non_debt,-
hadoop,5465,comment_14,This jira is too trivial to add a unit test.,non_debt,-
hadoop,5559,summary,BackupStorage should not use,non_debt,-
hadoop,5559,description,"In BackupStorage.java, HADOOP-5119 already involves a lot of codes. It is better to fix this problem in a separated issue.",non_debt,-
hadoop,5559,comment_0,This was fixed as a part of HADOOP-2413.,non_debt,-
hadoop,5561,summary,Javadoc-dev ant target runs out of heap space,non_debt,-
hadoop,5561,description,"The default configuration for the ant task javadoc-dev does not specify a maxmemory and, after churning for a while, fails with an OOM exception:",design_debt,non-optimal_design
hadoop,5561,comment_0,Patch explicitly sets the maxmemory in the javadoc ant task to a reasonable 512 MB. This solves the issue and the javadoc task completes quickly on my machine. Not unit-tested as is a build configuration change.,non_debt,-
hadoop,5561,comment_1,submitting patch,non_debt,-
hadoop,5561,comment_3,"would recommend having an Ant property javadoc.memory that is set to 512m in the build file, but can be overridden by people with problems (or 64 bit JVMs) without having to patch the build file. It could also be used by all build files",design_debt,non-optimal_design
hadoop,5561,comment_4,1,non_debt,-
hadoop,5561,comment_5,Updated patch to use javadoc.maxmemory property and have all the javadoc targets use it. Tested by calling all the javadoc targets and verifying they're using the property.,non_debt,-
hadoop,5561,comment_6,submitting new patch,non_debt,-
hadoop,5561,comment_7,"+1 I committed this. Thanks, Jacob",non_debt,-
hadoop,5657,summary,Validate data passed through TestReduceFetch,non_debt,-
hadoop,5657,description,"While TestReduceFetch verifies the reduce semantics for reducing from in-memory segments, it does not validate the data it reads. Data corrupted during the merge will not be detected.",design_debt,non-optimal_design
hadoop,5657,comment_0,"The following are now validated in the reduce: * Each map produces one record for each of 4096 small keys * Includes unique large records, each straddled by a pair of small records from another map (to detect corruption from the merge) * Changes some parameters for to make intermediate merges with in-memory data occur occasionally",non_debt,-
hadoop,5657,comment_2,Emits only two additional records per map; less expensive key updates. Detects known-bad case when merging a combination of in-memory and on-disk segments.,design_debt,non-optimal_design
hadoop,5657,comment_3,1,non_debt,-
hadoop,5657,comment_4,It'd help code readability if some comments are added on why two values are emitted per map and the logic the testcase is employing for validation (verbose comments for the arithmetic is what I mean *smile*).,code_debt,low_quality_code
hadoop,5657,comment_6,Adds some extra comments; no functional changes,code_debt,low_quality_code
hadoop,5657,comment_7,I committed this.,non_debt,-
hadoop,5701,summary,"With fair scheduler, long running jobs can easily occurpy a lot of task slots",design_debt,non-optimal_design
hadoop,5701,description,"Current fair scheduler implementation favor long running jobs since once a task slot is assigned to a job, the fair scheduler is not able to reclaim it.",non_debt,-
hadoop,5701,comment_0,"Consider that a cluster have 2000 map slots and jobs submitted in the following sequence: |1:00pm|JobA|1500 maps, each map runs 24 hours| |1:30pm|JobB|1000 maps, each map runs 2 hours| |1:40pm|JobC|3000 maps, each map runs 10 minutes| Then, all 1500 maps in JobA got scheduled and only 500 map slots remained in the cluster at 1pm. 30 minutes later, JobB came and only 500 maps slots got scheduled. At 1:40pm, JobC came but no maps got scheduled until some maps in JobB finished 2 hours later. In this cases, JobA always has 75% of the capacity, JobB and JobC never able to obtain 1/N of the capacity. If JobA has 2000 maps, other jobs have to wait for maps in JobA to finish and have no progress in 24 hours.",design_debt,non-optimal_design
hadoop,5701,comment_1,"In other words, fair share scheduler cannot really allocate fair shares without preemption.",non_debt,-
hadoop,5701,comment_2,HADOOP-4665 will prevent this from happening. Do you want any additional features on top of that?,non_debt,-
hadoop,5701,comment_3,"Matei, thank you for fixing the problem. We should close this as duplicated.",non_debt,-
hadoop,5701,comment_4,"HADOOP-4665 is not committed yet, but when it is, it should solve the problem.",non_debt,-
hadoop,5771,summary,Create unit test for LinuxTaskController,non_debt,-
hadoop,5771,description,Add unit tests to test functionality introduced by HADOOP-4490,non_debt,-
hadoop,5771,comment_0,"Attaching patch which adds unit test for HADOOP-4490. Following is approach used in running unit tests. HADOOP-4490 unit tests require following extra parameter to be passed alongwith the ant test target. * - path to built task controller executable * - ugi of the user as who we should be executing the task. If any of the above two parameter is not passed the HADOOP-4490 test cases would not be executed and test case would be successful. An example for running a HADOOP-4490 test case is as follows For pipes related test cases you would have to pass compile.c++ flag. Following are the test cases added to the patch: * :- Core test case which launches a {{SleepJob}} and {{WordCount}} job as a different user. * :- Test case which checks if the process trees of launched tasks are cleaned up properly. Currently, fails with trunk when HADOOP-5420 is commited would work fine. * :- Runs a pipes job as a different user. * :- Runs a streaming job as different user.",non_debt,-
hadoop,5771,comment_1,Attaching patch incorporating Vinod's offline comments: * Renamed to * Added apache license header to all new test cases. * Cleaning up configuration file when test case completes. * Added documentation.,non_debt,-
hadoop,5771,comment_2,"Attaching the new patch incorporating Vinod's offline comment. * Removed unused imports. * Added new assertion to check if the passed user actually ran the job. * Modified, {{TrApp}} to stop check if the local job runner and map output start, so that {{TrApp}} can be run on a {{MiniMRCluster}} * Changed documentation, added it to javadoc and release note of JIRA issue.",documentation_debt,outdated_documentation
hadoop,5771,comment_3,Patch looked good overall. Am uploading a new patch with slight modifications to the above patch with: - Better java code comments - Better code formatting and code-cleanup - And a small fix in because the earlier code was making the streaming jobs only run with LocalJobRunner.,code_debt,low_quality_code
hadoop,5771,comment_4,The changes to patch looks fine to me. +1 to patch.,non_debt,-
hadoop,5771,comment_5,Output from ant-test patch is:,non_debt,-
hadoop,5771,comment_7,Attaching new patch merging with latest changes in trunk.,non_debt,-
hadoop,5771,comment_9,The test failure in capacity scheduler is not related to this patch.,non_debt,-
hadoop,5771,comment_10,"I just committed this to trunk. Thanks, Sreekanth !",non_debt,-
hadoop,5771,comment_11,"Also, thanks to Vinod for review and modifications on the last patch.",non_debt,-
hadoop,5771,comment_13,Y! distribution patch.,non_debt,-
hadoop,5771,comment_14,Editorial pass over all release notes prior to publication of 0.21. Routine.,non_debt,-
hadoop,5771,comment_15,Attaching latest ydist patch with just testcases merged.,non_debt,-
hadoop,5771,comment_16,Attaching patch for yahoo hadoop 20.,non_debt,-
hadoop,5775,summary,HdfsProxy Unit Test should not depend on HDFSPROXY_CONF_DIR environment,build_debt,build_others
hadoop,5775,description,"as war target read user-certs.xml and from If a user set this environment and have some files in it, it could potentially cause the unit test to fail if the conf files does not match what the unit test needs.",build_debt,build_others
hadoop,5775,comment_0,"use a separate ""testwar"" ant target to load the conf files for testing purpose. war target remains the same.",non_debt,-
hadoop,5775,comment_1,solved in HDFS-447,non_debt,-
hadoop,5824,summary,remove OP_READ_METADATA functionality from Datanode,non_debt,-
hadoop,5824,description,Operation OP_READ_METADATA exists on the datanode streaming interface. But it is not used by any client code and is currently not protected by access token. Should it be removed?,code_debt,dead_code
hadoop,5824,comment_0,+1 for removing it if it is not used.,code_debt,dead_code
hadoop,5824,comment_1,I like this too. Should we also remove OP_READ_METADATA from,non_debt,-
hadoop,5824,comment_2,"Yes, I attached a new patch to that effect. thanks.",non_debt,-
hadoop,5824,comment_3,Added a new patch that deprecates the variable for now.,non_debt,-
hadoop,5824,comment_4,+1 patch looks good.,non_debt,-
hadoop,5824,comment_6,"The failed tests are not related. I have committed this. Thanks, Kan!",non_debt,-
hadoop,5824,comment_7,Patch for hadoop-20.,non_debt,-
hadoop,5849,summary,JMX Metrics For JobTracker,non_debt,-
hadoop,5849,description,The Job Tracker Should have JMX Metrics for monitoring performance * JobsSubmitted: counter * JobsSuccessful: counter * JobsFailed: counter * TotalTasksCreated: counter * counter * TotalTasksFailed These can be used to monitor the JobTracker performance.,non_debt,-
hadoop,5866,summary,Move DeprecatedUTF8 to o.a.h.hdfs,non_debt,-
hadoop,5866,description,"HADOOP-5823 added {{DeprecatedUTF8}} class as a wrapper for UTF8. Though UTF8 is deprecated, it is used in many places and most likely will continue to be used for quite sometime. My initial thought was that other packages might want to use the wrapper {{DeprecatedUTF8}}. But the current suggestion (discussed in HADOOP-5823) is to move the class to o.a.h.hdfs. Alternately we could have just use ""@SuppressWarnings"" rather than introducing a new class.",non_debt,-
hadoop,5866,comment_0,The attached patch just moves DeprecatedUTF8 from o.a.h.io to o.a.h.hdfs. It is declared public since it is used in multiple packages under hdfs.,non_debt,-
hadoop,5866,comment_1,Forgot to add the new file?,non_debt,-
hadoop,5866,comment_2,Thanks Nicholas. corrected patch is attached.,non_debt,-
hadoop,5866,comment_3,1,non_debt,-
hadoop,5866,comment_4,Thanks Nicholas for the review. Updated patch is attached. The previous one was incomplete. NamespaceInfo.java was not updated. ant test-patch:,non_debt,-
hadoop,5866,comment_5,I just committed this.,non_debt,-
hadoop,5891,summary,"If dfs.http.address is default, SecondaryNameNode can't find NameNode",non_debt,-
hadoop,5891,description,"As detailed in this blog post: if dfs.http.address is not configured, and the 2NN is a different machine from the NN, the 2NN fails to connect. In the 2NN should notice a ""0.0.0.0"" dfs.http.address and, in that case, pull the hostname out of fs.default.name. This would fix the default configuration to work properly for most users.",non_debt,-
hadoop,5891,comment_0,"this whole problem of bootstrapping a cluster where machines don't know who they are is pretty brittle right now. In an ideal world, even the NN would be able to work out its name/address and share it with the rest, but failing that, having everything else work out the details by asking the NN would be handy. It would also be good if everything provided (in the same process and via JMX) a list of (service, address, port) for all the different things that the node runs. I try to reverse engineer that, but it adds more scheduling problems (don't start the downstream nodes until the NN and JT are live), and for some reason jetty comes up bonded to 0:0:0:0:0:1 on one machine, which is particularly irritating. so: +1 to this, I can see the BackupNode having the same problem on scaled up, as it needs to know both the NN and 2N addresses (note addresses, not hostnames. Maybe we should open this up to a general ""nodes to come up better on an under-configured network"" bugrep which those of us who do underconfigure their networks can deal with.",design_debt,non-optimal_design
hadoop,5891,comment_1,"Steve: I agree that service location can be improved across the board. However, I don't think it's necessarily a good idea to overload the NameNode as a service name daemon. Personally, I'd prefer to use something like ZooKeeper here. Obviously there needs to be at least one host that is in a ""well-known"" location, which could be configured by default as a ""hadoop-zk"" hostname which has multiple A records pointing to all of the ZK nodes. Anyway, I agree that we should work towards the ideal goal, but I'd like to have that discussion in a new JIRA. This one is a very simple fix whereas that one could be pretty significant.",design_debt,non-optimal_design
hadoop,5891,comment_3,"As usual, the failing test is unrelated (capacity scheduler).",non_debt,-
hadoop,5891,comment_4,+1. code looks good.,non_debt,-
hadoop,5891,comment_5,I just committed this. Thanks Todd!,non_debt,-
hadoop,5891,comment_6,From Email exchange: This code change is already exercised by TestCheckpoint.,non_debt,-
hadoop,5935,summary,Hudson's release audit warnings link is broken,non_debt,-
hadoop,5935,description,"For example, on HADOOP-5170 the link gives a 404. This makes it hard to work out which file or files are causing a problem.",non_debt,-
hadoop,5935,comment_0,this patch fixes the releaseaudit warnings link.,non_debt,-
hadoop,5935,comment_1,"Nigel, Could you please review this patch? tnx!",non_debt,-
hadoop,5935,comment_2,1,non_debt,-
hadoop,5935,comment_3,I just committed this!,non_debt,-
hadoop,5935,comment_4,Looks like this change was lost during the project split. See link in HADOOP-6138 for example.,non_debt,-
hadoop,5935,comment_5,committed with the latest trunk code.,non_debt,-
hadoop,5940,summary,trunk eclipse-plugin build fails while trying to copy commons-cli jar from the lib dir,non_debt,-
hadoop,5940,description,None,non_debt,-
hadoop,5940,comment_0,this fixes eclipse-plugin failure.,non_debt,-
hadoop,5940,comment_1,tested the patch locally : tnx!,non_debt,-
hadoop,5940,comment_2,"+1 for the patch..once the build passes, this can be committed.",non_debt,-
hadoop,5940,comment_3,I just committed this,non_debt,-
hadoop,6009,summary,S3N listStatus incorrectly returns null instead of empty array when called on empty root,non_debt,-
hadoop,6009,description,"Null means the directory does not exist, which is obviously not the case.",non_debt,-
hadoop,6009,comment_0,Fix with unit test.,non_debt,-
hadoop,6009,comment_1,+1 looks good to me.,non_debt,-
hadoop,6009,comment_3,Re-generated patch for trunk.,non_debt,-
hadoop,6009,comment_5,I've just committed this. Thanks Ian!,non_debt,-
hadoop,6075,summary,fails with NPE,non_debt,-
hadoop,6075,description,Here is the error,non_debt,-
hadoop,6075,comment_0,Closing as stale.,non_debt,-
hadoop,6106,summary,Provide an option in to timeout commands that do not complete within a certain amount of time.,non_debt,-
hadoop,6106,description,In MAPREDUCE-211 we came across a need to provide an option to timeout commands launched via the The use case is for the health check script being developed in MAPREDUCE-211. We would like the TaskTracker thread to not be blocked by a problematic script or in instances where fork()+exec() has hung (which apparently has been observed in large clusters).,non_debt,-
hadoop,6106,comment_0,"Code was being reviewed in MAPREDUCE-211. Sreekanth, can you please put up the latest patch here ?",non_debt,-
hadoop,6106,comment_1,Attaching Shell timeout feature patch.,non_debt,-
hadoop,6106,comment_2,New patch which Sreekanth and I worked on together.,non_debt,-
hadoop,6106,comment_3,"The patch contains the following changes: - Converted the timedOut variable to an atomic boolean, as it was being accessed from the timer task as well as from the - Creating the Timer only if the timeout interval is - Setting completed variable at exactly the same places as the previous code in order not to change contract. - Cancelling timer in the finally block of the code. - Refactored the constructors of to all reach one constructor. Sreekanth, can you please run ant test and test-patch so I can commit this ?",non_debt,-
hadoop,6106,comment_4,Attaching latest patch fixing findbugs warning. * Changing to private static class.,non_debt,-
hadoop,6106,comment_5,Output from ant test-patch Release audit is flagged because of changes to {{Shell}} and checking javac warnings does not point to any of the changes which were made in this patch. All tests passes successfully on local box.,non_debt,-
hadoop,6106,comment_6,"Sigh. Found one more problem. In the timer task timeout, the variable timedout must be set up before the process.destroy, because the exception would be thrown asynchronously when the process is destroyed.",non_debt,-
hadoop,6106,comment_7,Attaching patch as per Hemanth's comment. Running ant test and test-patch again.,non_debt,-
hadoop,6106,comment_8,output from ant test-patch All test cases passed locally.,non_debt,-
hadoop,6106,comment_9,+1 for the changes.,non_debt,-
hadoop,6106,comment_10,"I just committed this. Thanks, Sreekanth !",non_debt,-
hadoop,6106,comment_12,"I had a chat with Owen and Giri about how to get this dependency jar into the HDFS and MapReduce sub projects. Basically the current school of thought (until IVY is fixed to automate this) is to take the latest built binary from Hudson and commit it to the HDFS and MapReduce sub projects - making an entry in changes.txt referencing this JIRA. We are running HDFS and MapReduce unit tests with the latest jar to make sure tests work fine. Once that's done, we'll commit it.",non_debt,-
hadoop,6106,comment_13,HDFS tests passed with the new jars.,non_debt,-
hadoop,6106,comment_14,"Mapreduce tests also ran, except for some test case failures that are already logged. The jars can be committed to HDFS and Map/Reduce subprojects now.",non_debt,-
hadoop,6134,summary,New Hadoop Common Site,non_debt,-
hadoop,6134,description,"New Hadoop Common Site Set up site, initial pass. May need to add more content. May need to update some links.",documentation_debt,low_quality_documentation
hadoop,6134,comment_0,Patch for hadoop-6134. Apply this patch to: Note: No new test code requried; changes to documentation only.,non_debt,-
hadoop,6134,comment_1,Patch submitted.,non_debt,-
hadoop,6134,comment_3,"I just committed this. Thanks, Corinne!",non_debt,-
hadoop,6145,summary,No error message for deleting non-existant file or directory.,non_debt,-
hadoop,6145,description,"If non-existant path or src is provided with rm/rmr option then no error message is displayed command: hadoop dfs -rm <srcdfs displays ""rm: <srcwhile it should display ""No such file or directory"".",non_debt,-
hadoop,6145,comment_0,I can't reproduce this. I'm running off the same build as I believe you are. I'll need more information to track this down further.,non_debt,-
hadoop,6145,comment_1,"Moved to Common as bug was tracked to FsShell, which is in Common.",non_debt,-
hadoop,6145,comment_2,Attaching patch for v20,non_debt,-
hadoop,6145,comment_3,"Attaching patch for trunk. Problem was the trash method was throwing FileNotFound, which was not being handled. There are three different places where the non-existence of the file being deleted could conceivably be handled. Fixed by just checking for the file's existence at the beginning and exiting if not found. No unit test because the affected unit test is TestHDFSCLI, which is in HDFS project. Manually tested and it works. The reason this wasn't detected previously is that it only manifests itself when the trash feature is enabled, which apparently it isn't on the minidfscluster that powers TestCLI. We should probably looking at running Test*CLI with both trash on and trash off.",test_debt,lack_of_tests
hadoop,6145,comment_4,submitting patch. Hudson should pick up the correct one.,non_debt,-
hadoop,6145,comment_5,"also, the trunk patch has a quick fix to bring the rm documentation in line with what was committed in HADOOP-6139.",documentation_debt,outdated_documentation
hadoop,6145,comment_7,srcFs.exists(src) and both call We could call once in FsShell.delete(..) and save a rpc.,code_debt,low_quality_code
hadoop,6145,comment_8,canceling patch,non_debt,-
hadoop,6145,comment_9,attaching new v20 patch,non_debt,-
hadoop,6145,comment_10,Attaching new Common patch for hudson.,non_debt,-
hadoop,6145,comment_11,submitting patch,non_debt,-
hadoop,6145,comment_13,+1 patch looks good.,non_debt,-
hadoop,6145,comment_14,"I have committed this to 0.20 and above. Thanks, Jakob!",non_debt,-
hadoop,6151,summary,The servlets should quote html characters,non_debt,-
hadoop,6151,description,"We need to quote html characters that come from user generated data. Otherwise, all of the web ui's have cross site scripting attack, etc.",non_debt,-
hadoop,6151,comment_0,"I believe the transforms should be: 1. & - 2. < - 3. 4. ' - 5. ""- As long as we do those transforms, any html that the user includes in their data will just be treated as literal text rather than html commands.",non_debt,-
hadoop,6151,comment_1,This patch introduces an input filter for all of the servlets and jsp pages that quotes all of the html active characters in the parameters. This means that all of the cross site scripting attacks based on bad urls should be fixed. I'll file a follow up jira to fix the vector where the values in the job need to be quoted.,non_debt,-
hadoop,6151,comment_3,I forgot the --no-prefix..,non_debt,-
hadoop,6151,comment_5,"* The unit test should use JUnit4 test annotations instead of JUnit3 TestCase * looks useful for debugging, but should probably be left out * The static \*Bytes fields should be final * The @return docs for ""needsQuoting"" could be more explicit",code_debt,low_quality_code
hadoop,6151,comment_6,Messed up the JavaDoc. Now fixed.,documentation_debt,low_quality_documentation
hadoop,6151,comment_8,This patch addresses Chris' comments.,non_debt,-
hadoop,6151,comment_9,1,non_debt,-
hadoop,6151,comment_11,I just committed this.,non_debt,-
hadoop,6151,comment_14,This patch is for 0.20. (not to be committed),non_debt,-
hadoop,6182,summary,Adding Apache License Headers and reduce releaseaudit warnings to zero,code_debt,low_quality_code
hadoop,6182,description,As of now rats tool shows 111 RA warnings [rat:report] Summary [rat:report] - [rat:report] Notes: 18 [rat:report] Binaries: 118 [rat:report] Archives: 33 [rat:report] Standards: 942 [rat:report] [rat:report] Apache Licensed: 820 [rat:report] Generated Documents: 11 [rat:report] [rat:report] JavaDocs are generated and so license header is optional [rat:report] Generated files do not required license headers [rat:report] [rat:report] 111 Unknown Licenses [rat:report] [rat:report],code_debt,low_quality_code
hadoop,6182,comment_0,"with this patch the releaseaudit warnings is down to 1. [rat:report] 1 Unknown Licenses [rat:report] [rat:report] Unapproved licenses: [rat:report] this is an empty file. I'm not sure why we have this empty java files. If we can remove this empty java file, we can get rid of this releaaseaudit warning. Thanks,",code_debt,dead_code
hadoop,6182,comment_1,"Looks like LengthFileChecksum was removed in HADOOP-3981, although the source file wasn't deleted. It should be possible to delete it since it is no longer used. It should be deleted from the 0.20 branch too.",non_debt,-
hadoop,6182,comment_3,+1 code review.,non_debt,-
hadoop,6182,comment_4,"Thanks Nigel, I just committed this!",non_debt,-
hadoop,6182,comment_6,"I've had to revert this patch, it causes all unit-tests to fail since some of the xml config files are invalidated by the presence of the license headers (e.g. HADOOP-6195). This patch shouldn't have been committed since hudson did catch the problem...",non_debt,-
hadoop,6182,comment_7,"Looks like the original patch inserted text blocks into the XML files ahead of the block: <?xml version=""1.0""? <?xml-stylesheet type=""text/xsl"" I corrected this for checkstyle in HADOOP-6185, but because the error is pervasive, it fails a wide variety of unit tests.",code_debt,low_quality_code
hadoop,6182,comment_8,Let's remove the file separately once the build is stable.,code_debt,dead_code
hadoop,6182,comment_9,"Updated the patch, and moved the XML comment blocks so that they function correctly.",non_debt,-
hadoop,6182,comment_10,Ran Test-patch locally: Also ran unit tests and all passed.,non_debt,-
hadoop,6182,comment_11,This adjusts the previous version of the patch to put the commented license in the appropriate location in all XML files.,non_debt,-
hadoop,6182,comment_12,Trunk is passing the unit tests (see [build I have removed from 0.19 and above.,non_debt,-
hadoop,6188,summary,TestHDFSTrash fails because of TestTrash in common,non_debt,-
hadoop,6188,description,None,non_debt,-
hadoop,6188,comment_0,"Test is using Java File.list() method which works fine for LocalFileSystem. But the same test is being run in HDFS project with DFS and it failes. We need to change it to , which works for both.",non_debt,-
hadoop,6188,comment_1,"Ran testHDFSTrash on HDFS project.. run-test-hdfs: [delete] Deleting directory [mkdir] Created dir: [delete] Deleting directory [mkdir] Created dir: [junit] Running [junit] 2, 0, Errors: 0, 5.526 sec",non_debt,-
hadoop,6188,comment_2,I'd suggest to declare method's parameters final: it will improve readability and make testing easier.,code_debt,low_quality_code
hadoop,6188,comment_3,test-patch: [exec],non_debt,-
hadoop,6188,comment_4,"test-core: run-test-core: [mkdir] Created dir: [mkdir] Created dir: [copy] Copying 1 file to [junit] Running [junit] 1, 0, Errors: 0, 0.308 sec [junit] Running [junit] 13, 0, Errors: 0, 1.244 sec [junit] Running [junit] 3, 0, Errors: 0, 0.495 sec [junit] Running [junit] 1, 0, Errors: 0, 0.435 sec [junit] Running [junit] 2, 0, Errors: 0, 1.202 sec [junit] Running [junit] 1, 0, Errors: 0, 0.213 sec [junit] Running [junit] 2, 0, Errors: 0, 0.674 sec [junit] Running [junit] 1, 0, Errors: 0, 0.405 sec [junit] Running [junit] 1, 0, Errors: 0, 5.323 sec [junit] Running [junit] 3, 0, Errors: 0, 1.224 sec [junit] Running [junit] 2, 0, Errors: 0, 0.056 sec [junit] Running [junit] 5, 0, Errors: 0, 0.859 sec [junit] Running [junit] 4, 0, Errors: 0, 1.096 sec [junit] Running [junit] 2, 0, Errors: 0, 0.567 sec [junit] Running [junit] 8, 0, Errors: 0, 0.088 sec [junit] Running [junit] 2, 0, Errors: 0, 2.221 sec [junit] Running [junit] 1, 0, Errors: 0, 0.571 sec [junit] Running [junit] 3, 0, Errors: 0, 0.861 sec [junit] Running [junit] 4, 0, Errors: 0, 0.093 sec [junit] Running [junit] 3, 0, Errors: 0, 0.081 sec [junit] Running [junit] 29, 0, Errors: 0, 2.435 sec [junit] Running [junit] 1, 0, Errors: 0, 0.23 sec [junit] Running [junit] 1, 0, Errors: 0, 0.264 sec [junit] Running [junit] 35, 0, Errors: 0, 2.494 sec [junit] Running [junit] 1, 0, Errors: 0, 2.639 sec [junit] Running [junit] 1, 0, Errors: 0, 2.008 sec [junit] Running [junit] 2, 0, Errors: 0, 6.843 sec [junit] Running [junit] 1, 0, Errors: 0, 0.273 sec [junit] Running [junit] 1, 0, Errors: 0, 1.085 sec [junit] Running [junit] 4, 0, Errors: 0, 0.208 sec [junit] Running [junit] 4, 0, Errors: 0, 0.364 sec [junit] Running [junit] 3, 0, Errors: 0, 0.236 sec [junit] Running [junit] 5, 0, Errors: 0, 0.584 sec [junit] Running [junit] 1, 0, Errors: 0, 0.073 sec [junit] Running [junit] 3, 0, Errors: 0, 0.79 sec [junit] Running [junit] 3, 0, Errors: 0, 0.271 sec [junit] Running [junit] 1, 0, Errors: 0, 0.641 sec [junit] Running [junit] 1, 0, Errors: 0, 6.456 sec [junit] Running [junit] 2, 0, Errors: 0, 0.267 sec [junit] Running [junit] 9, 0, Errors: 0, 1.209 sec [junit] Running [junit] 1, 0, Errors: 0, 0.046 sec [junit] Running [junit] 3, 0, Errors: 0, 0.234 sec [junit] Running [junit] 3, 0, Errors: 0, 0.079 sec [junit] Running [junit] 3, 0, Errors: 0, 0.081 sec [junit] Running [junit] 4, 0, Errors: 0, 0.222 sec [junit] Running [junit] 1, 0, Errors: 0, 0.04 sec [junit] Running [junit] 6, 0, Errors: 0, 22.523 sec [junit] Running [junit] 1, 0, Errors: 0, 0.524 sec [junit] Running [junit] 3, 0, Errors: 0, 2.854 sec [junit] Running [junit] 25, 0, Errors: 0, 6.889 sec [junit] Running [junit] 3, 0, Errors: 0, 0.562 sec [junit] Running [junit] 25, 0, Errors: 0, 6.668 sec [junit] Running [junit] 25, 0, Errors: 0, 0.273 sec [junit] Running [junit] 19, 0, Errors: 0, 0.217 sec [junit] Running [junit] 25, 0, Errors: 0, 3.59 sec [junit] Running [junit] 25, 0, Errors: 0, 3.614 sec [junit] Running [junit] 19, 0, Errors: 0, 3.937 sec [junit] Running [junit] 1, 0, Errors: 0, 6.799 sec [junit] Running [junit] 1, 0, Errors: 0, 55.548 sec [junit] Running [junit] 1, 0, Errors: 0, 12.042 sec [junit] Running [junit] 19, 0, Errors: 0, 3.815 sec [junit] Running [junit] 4, 0, Errors: 0, 0.971 sec [junit] Running [junit] 9, 0, Errors: 0, 3.778 sec [junit] Running [junit] 9, 0, Errors: 0, 0.394 sec [junit] Running [junit] 2, 0, Errors: 0, 0.253 sec [junit] Running [junit] 4, 0, Errors: 0, 0.658 sec [junit] Running [junit] 3, 0, Errors: 0, 15.36 sec [junit] Running [junit] 1, 0, Errors: 0, 12.436 sec [junit] Running [junit] 4, 0, Errors: 0, 34.981 sec [junit] Running [junit] 1, 0, Errors: 0, 1.38 sec [junit] Running [junit] 3, 0, Errors: 0, 0.145 sec [junit] Running [junit] 1, 0, Errors: 0, 0.045 sec [junit] Running [junit] 7, 0, Errors: 0, 0.128 sec [junit] Running [junit] 1, 0, Errors: 0, 0.535 sec [junit] Running [junit] 1, 0, Errors: 0, 3.086 sec [junit] Running [junit] 6, 0, Errors: 0, 0.068 sec [junit] Running [junit] 5, 0, Errors: 0, 0.202 sec [junit] Running [junit] 2, 0, Errors: 0, 0.14 sec [junit] Running [junit] 2, 0, Errors: 0, 0.064 sec [junit] Running [junit] 2, 0, Errors: 0, 0.989 sec [junit] Running [junit] 4, 0, Errors: 0, 0.464 sec [junit] Running [junit] 1, 0, Errors: 0, 0.416 sec [junit] Running [junit] 1, 0, Errors: 0, 0.059 sec [junit] Running [junit] 6, 0, Errors: 0, 0.27 sec [junit] Running [junit] 2, 0, Errors: 0, 2.144 sec [junit] Running [junit] 3, 0, Errors: 0, 7.744 sec [junit] Running [junit] 1, 0, Errors: 0, 1.305 sec [junit] Running [junit] 3, 0, Errors: 0, 4.283 sec [junit] Running [junit] 6, 0, Errors: 0, 0.081 sec",non_debt,-
hadoop,6188,comment_5,+1 on the new patch. I also manually tested that with the new jar TestHDFSTrash no longer fails. I'm not overly concerned with the non-final parameters as the function is just a few lines and the variables aren't being abused. Of note is the necessity of checking for null from listStatus. This is to avoid the fact that the listStatus implementation differs between and I've opened HDFS-538 to address this.,code_debt,low_quality_code
hadoop,6188,comment_6,"I have committed this. Thanks, Boris!",non_debt,-
hadoop,6198,summary,FileSystem filtering should work on FileStatus rather than Path objects,non_debt,-
hadoop,6198,description,"There's an avoidable overhead in listing/globbing items with some property (e.g. owned by user foo, only files, files larger than _n_ bytes, etc.). Internally, the Path is extracted from a FileStatus object and passed to the PathFilter; simply passing the FileStatus object would allow one to filter on the information in the status object.",design_debt,non-optimal_design
hadoop,6198,comment_0,I'd argue that we may want to support both interfaces. The fact that the current path-based filtering retrieves a FileStatus object first is an implementation detail and it is conceivable that a different FS implementation may be more efficient to support path-based filtering than file-status based filtering.,code_debt,slow_algorithm
hadoop,6198,comment_1,"Granted, but supporting two filtering interfaces will likely cause maintenance headaches and be more of a burden to a FS implementor; I'd rather pick an API and not support all possible variants. If a FileSystem is more efficient with path-based filtering, it can still work with FileStatus objects, either populating them lazily, filling them with defaults (what many shims do anyway), or even failing if a user queries unsupported data. Since globStatus returns FileStatus objects, any implementation will need to construct them for the set of accepted Paths, anyway. Given that the API seems biased toward FileStatus objects, I'd rather endure a penalty for the hypothetical FS that doesn't return this information, rather than maintain two separate filtering APIs.",design_debt,non-optimal_design
hadoop,6198,comment_2,I am not sure why it would be a burden for FS implementors. listStatus would be implemented by FileSystem and FS implementors should only override them when the default implementation is less efficient (and the implementors have the incentive of making them fast).,code_debt,slow_algorithm
hadoop,6198,comment_3,"Is the proposal to retain two versions of globStatus, one taking a PathFilter and another taking e.g. StatusFilter? This seems like a premature optimization, given that we have no example of a FileSystem for which there is a significant performance difference, or any performance improvement. If one wants to implement a FileSystem that only supports filtering by Path, or that optimizes that particular call, then there are plenty of other ways to effect this without adding a separate method to the core API. Your point on possible performance gain is taken, but it doesn't seem like the right tradeoff, particularly without a concrete use case. We'll have to keep PathFilter around for at least another release for but I'd like to deprecate it.",non_debt,-
hadoop,6198,comment_4,"+1 We've generally tried to switch FileSystem APIs from using Path to FileStatus whereever possible, deprecating and eventually removing Path-based methods, and this just seems like a case we've missed.",non_debt,-
hadoop,6220,summary,HttpServer wraps by IOExceptions if interrupted in startup,non_debt,-
hadoop,6220,description,"Following on some discusson on mapred-dev, we should keep an eye on the fact that Jetty uses sleeps when starting up; jetty can be a big part of the delays of bringing up a node. When interrupted, the exception is wrapped by an IOException, the root cause is still there, just hidden. If we want callers to distinguish from IOEs, then this exception should be extracted. Some helper method to start an http daemon could do this -catch the IOE, and if there is a nested interrupted exception, rethrow it, otherwise rethrowing the original IOE",non_debt,-
hadoop,6220,comment_0,This turns an interrupt into an includes the original exception as the cause. No test. This is a tricky one to set up a test for as you need to block the startup and then interrupt the starting thread.,test_debt,lack_of_tests
hadoop,6220,comment_2,"unmarking as incompatible. Any is still turned into an IOException, only now it is a subclass to say ""we were interrupted"". Anything that looks for an IOE will get the same experience as before",non_debt,-
hadoop,6220,comment_3,"resubmitting, no obvious reason why this should fail TestUTF8; HADOOP-6479 adds better diagnostics if Hudson is still failing.",non_debt,-
hadoop,6220,comment_5,"Would adding to the throws list of HttpServer::start, and rethrowing, be clearer? should be reserved for, well, interrupted I/O. Have you seen wrapped in logs or other evidence that would support this change? It is difficult to evaluate this without corresponding changes to the callers. What one might do with this is speculative in its current form.",code_debt,low_quality_code
hadoop,6220,comment_6,"Seen in trying to shut down nodes as they come up; it's nice to be able to differentiate ""we interrupted you"" from ""something went wrong"". Changing the signature? It's more dramatic, but yes, more explicit.",non_debt,-
hadoop,6220,comment_7,in sync with trunk,non_debt,-
hadoop,6220,comment_8,"this is the existing patch, instead of just throwing an IOException when startup is interrupted, it gets converted to an",non_debt,-
hadoop,6220,comment_11,No tests as it's so hard to recreate this situation in a test; you need one thread sleeping and another interrupting.,test_debt,lack_of_tests
hadoop,6220,comment_12,patch against trunk,non_debt,-
hadoop,6220,comment_14,+1 voting by self no tests as there is no easy way to generate the race condition.,test_debt,lack_of_tests
hadoop,6220,comment_15,Committed revision 1177051.,non_debt,-
hadoop,6229,summary,Atempt to make a directory under an existing file on LocalFileSystem should throw an Exception.,non_debt,-
hadoop,6229,description,This task is sub task of HDFS-303 Actually HDFS throws in this case (in ). So I guess we should do the same.,non_debt,-
hadoop,6229,comment_0,Does throwing a (a new class) make more sense in this case?,non_debt,-
hadoop,6229,comment_1,I second Nicholas' suggestion.,non_debt,-
hadoop,6229,comment_2,Took Nikolas' suggestion. Created new class Turns out mapred already has such class. After this patch is committed I will remove mapred's one and make it use this one (from common). Also I will need to change HDFS to use this new exception instead of FileNotFound.,code_debt,low_quality_code
hadoop,6229,comment_3,"Ran hadoop-common test, hadoop-hdfs and hadoop-mapreduce with new hadoop-common jar Also created a new hadoop-mapreduce jars (with the new core jars) and ran hadoop-hdfs test with them.",non_debt,-
hadoop,6229,comment_4,I believe the original code isn't defensive enough and there's a possibility for NPE to be thrown if {{Path f}} happens to be {{null}}. I'd add an extra check for this as the very first line of the method. Looks good otherwise.,code_debt,low_quality_code
hadoop,6229,comment_6,Forgot to add,non_debt,-
hadoop,6229,comment_8,"+1 This will be tested by the test being introduced in HDFS-303, right? Minor nit: we tend not to add serialVersionUID fields. (Any warnings in IDEs should be turned off.)",code_debt,low_quality_code
hadoop,6229,comment_9,removed serialVersionUID. Added some unit test in any case for hadoop-common.,non_debt,-
hadoop,6229,comment_11,+1 patch looks good,non_debt,-
hadoop,6229,comment_12,I've just committed this. Thanks Boris! Could you update the release note with the new behaviour please?,documentation_debt,outdated_documentation
hadoop,6229,comment_15,Editorial pass over all release notes prior to publication of 0.21. Bug.,non_debt,-
hadoop,6279,summary,Add JVM memory usage to JvmMetrics,non_debt,-
hadoop,6279,description,"The JvmMetrics currently publish memory usage from the MemoryMXBean. This is useful, but doesn't include the total heap size (eg as displayed in the JT Web UI). It would be nice to expose as part of JvmMetrics. It seems that (used by the JT for ""memory used"") is the same as the 'memHeapCommittedM' which already exists.",non_debt,-
hadoop,6279,comment_0,"Does not include unit tests - this is an existing open issue HADOOP-3634. This patch is a one-liner, though. Manual verification steps: 1) Launch daemon with configured in 2) Go to /metrics on web UI 3) Verify totalMemoryM is present",test_debt,lack_of_tests
hadoop,6279,comment_2,"+1 I committed this. Thanks, Todd!",non_debt,-
hadoop,6283,summary,The exception meessage in is not clear,code_debt,low_quality_code
hadoop,6283,description,"When a file is not found, shows the following error message. It looks like that there was a uncaught",non_debt,-
hadoop,6283,comment_0,throws if the file does not exist.,non_debt,-
hadoop,6283,comment_1,also changed the exception messages.,non_debt,-
hadoop,6283,comment_2,+1. Code looks good.,non_debt,-
hadoop,6283,comment_3,"Thanks, Dhruba for the review. No new tests will be added for this since it only changes exception messages.",non_debt,-
hadoop,6283,comment_5,I have committed this.,non_debt,-
hadoop,6297,summary,Hadoop's support for zlib library lacks support to perform flushes (Z_SYNC_FLUSH and Z_FULL_FLUSH),non_debt,-
hadoop,6297,description,"The zlib library supports the ability to perform two types of flushes when deflating data. It can perform both a Z_SYNC_FLUSH, which forces all input to be written as output and byte-aligned and resets the Huffman coding, and it also supports a Z_FULL_FLUSH, which does the same thing but additionally resets the compression dictionary. The Hadoop wrapper for the zlib library does not support either of these two methods. Adding support should be fairly trivial. An additional deflate method that takes a fourth ""flush"" parameter, and a modification to the native c code to accept this fourth parameter and pass it along to the zlib library. I can submit a patch for this if desired. It should be noted that the native SUN Java API is likewise missing this functionality, as has been noted for over a decade here:",architecture_debt,using_obsolete_technology
hadoop,6297,comment_0,Kevin - would you mind submitting a patch? It would be very helpful/welcome!,non_debt,-
hadoop,6297,comment_1,"Exposes the ability to pass a flush level through to the zlib deflate function call. This is achieved by: * Creating a FlushLevel enumeration that mirrors the zlib values. * Adding a 'flushLevel' parameter to the compress function that takes a value in this enumeration, and sets a local private variable to the integer corresponding to this enumeration. (If the finish boolean is true, then FINISH is used in place of the flushLevel parameter, which matches current behavior. * Create a new compress function that matches the old signature that just calls the new compress with NO_FLUSH for the flush level, which should maintain existing behavior. * Make the native code wrapper check for the private flushLevel variable and pass that along to the underlying deflate call. Also added a TestZlib unit test that tests the three possible flush levels.",non_debt,-
hadoop,6297,comment_3,"Inexplicably, the test it failed is the test I added. It looks like the native library isn't getting initialized properly by the testing machine, for some reason. I can run the test just fine locally using ant test. Also, I'm confused by the audit warning. What does that warning mean?",code_debt,low_quality_code
hadoop,6297,comment_4,"The test should pass even if the native libraries aren't loaded, as some platforms (Windows, MacOS) don't come with the native libs installed. The error in the unit test: may be a problem with Hudson, but you may want to be certain your patch works on a clean checkout of trunk. It means that the patch adds files without headers licensing them to Apache. TestZlib.java, in this case. Small nits on the testcase: * Please use the JUnit4 conventions, rather than JUnit3; instead of extending TestCase, import the org.junit classes and use annotations (see examples in src/test, e.g. TestCodec) * This can simply call {{Assert.fail()}} rather than using the boolean, or even better, just allow the exception to escape the method",code_debt,low_quality_code
hadoop,6297,comment_5,"Revised version of the previous patch. The only file that's different is the unit test: a) There's now a license block at the top b) It's now in JUnit4 format c) It's now three separate test functions d) The tests bail out early (without failing) if returns false. I suspect that Hudson will bail out of all the tests early, unfortunately, if it's just not loading the natvie zlib library as I suspect. I don't think there's much I can do about that, however.",non_debt,-
hadoop,6297,comment_7,"This bug has been sitting in limbo for months, now. Are there further steps I need to take to get my patched looked over by a reviewer and hopefully committed soon?",non_debt,-
hadoop,6297,comment_10,What's the Hadoop-side use case for this functionality? ie who is the new user of the added function call?,non_debt,-
hadoop,6297,comment_11,"A user creating a customized compression scheme using zlib that requires syncing. For example, a compression method that generates fixed-sized output blocks.",non_debt,-
hadoop,6297,comment_12,"Hrm, I guess what I don't understand is why we haven't needed this method for existing file formats like SequenceFile that do block-based compression?",non_debt,-
hadoop,6297,comment_13,"SequenceFile just compresses blocks of input into variable output block sizes, this is different from having fixed-size output blocks. The theory is that if the compressed block size is fixed, and an even divisor of the HDFS block size, then a naive 'split at the HDFS block boundaries' will work without having to do any seqing around at the start of each mapper. Theoretically you get less start-of-mapper overhead and less reading from blocks that might not be rack local. I'm honestly not certain anymore that it's the best approach. I have my scheme set up using a little JNI code I threw together that provides full zlib support, and the overall performance gains over sequence files are fairly negligible. It's still functionality that's missing from the Hadoop code that would be easy to add, though. (Oracle is finally fixing this issue in the Java Zlib implementation as part of Java 7.)",non_debt,-
hadoop,6297,comment_14,"Canceling this patch as it is over a year old. Kevin, if you are still interested in adding in this functionality please up-merge and post a new patch, I would be happy to review it and commit it for you. If you have given up on the patch please indicate it in the comments and I can close the JIRA for you if you like.",non_debt,-
hadoop,6297,comment_15,"As JDK 7 provides support for this functionality in the basic Java zlib implementation, there is no longer need for this patch.",non_debt,-
hadoop,6304,summary,Use where possible in RawLocalFileSystem,non_debt,-
hadoop,6304,description,Using where possible in RawLocalFileSystem when g & o perms are same saves a lot of 'fork' system-calls.,non_debt,-
hadoop,6304,comment_0,Emergency bug-fix to yahoo hadoop20 distribution - I'll upload one for trunk shortly.,non_debt,-
hadoop,6304,comment_1,Arun - it would be good to get this into trunk for the 0.22 release.,non_debt,-
hadoop,6304,comment_2,"I am -1 on this patch. As seen in MAPREDUCE-2238, this pattern is dangerous as it temporarily drops directories into u-x or u-r territory which can screw up other processes working inside. I think HADOOP-7110 is the right solution for performance, and forking as we're doing now is fine for people who don't care about performance.",design_debt,non-optimal_design
hadoop,6304,comment_3,I disagree with your assessment of the dangers seen in MAPREDUCE-2238. See my comment in HADOOP-7110. I also disagree with your aggressive -1-ing without proper discussions and proposing alternatives within the same context.,non_debt,-
hadoop,6304,comment_4,"Apologies, I certainly didn't mean to come off as aggressive. To me, ""-1"" means ""this shouldn't be committed as is until changes are made"" with the same force as +1 means ""commit away as is"". I suppose it has become a loaded term in this community. The alternative proposed is HADOOP-7110.",non_debt,-
hadoop,6304,comment_5,"Todd, this was a stop-gap. Anyway, a foggy day, could you please explain to me the link between using and MAPREDUCE-2238. Thanks. I agree that HADOOP-7110 will obviate a need for this. But, please be careful with JNI...",non_debt,-
hadoop,6304,comment_6,"Hey Arun. MAPREDUCE-2238 was caused by the use of these same APIs in What was happening was something like the following: - Thread A: 755) - Thread B: 755) The way they got interleaved was something like: - B: set userlogs/attempt_x to 000 - A: set userlogs/ to 000 - B: try to restore permissions on attempt_x, but fail since it can't traverse the path - A: set userlogs/ back to 755 - The attempt_x directory is left at 000 or some other ""incomplete"" permissions where any following Hudson runs won't be able to delete it. This same problem can happen in a real cluster too. So I think the pattern used in this patch (same as the one in is dangerous because it's not atomic. This was just one manifestation of the issue. I agree JNI is difficult but this is a very simple use of it. No buffer management, no complicated errors to handle, no strange system APIs. Let's have the JNI discussion over in HADOOP-7110 though.",code_debt,multi-thread_correctness
hadoop,6304,comment_7,"Ah, I was looking at each set* and thinking they are fine since each is a JNI call individually... thanks for the explanation. +1 for marking this invalid in favour of HADOOP-7110.",non_debt,-
hadoop,6304,comment_8,To be fixed via HADOOP-7110.,non_debt,-
hadoop,6313,summary,Expose flush APIs to application users,non_debt,-
hadoop,6313,description,"Earlier this year, Yahoo, Facebook, and Hbase developers had a roundtable discussion where we agreed to support three types of flush in HDFS (API1, 2, and 3) and the append project aims to implement API2. Here is a proposal to expose these APIs to application users. 1. Three flush APIs * API1: flushes out from the address space of client into the socket to the data nodes. On the return of the call there is no guarantee that that data is out of the underlying node and no guarantee of having reached a DN. New readers will eventually see this data if there are no failures. * API2: flushes out to all replicas of the block. The data is in the buffers of the DNs but not on the DN's OS buffers. New readers will see the data after the call has returned. * API3: flushes out to all replicas and all replicas have done posix fsync equivalent - ie the OS has flushed it to the disk device (but the disk may have it in its cache). 2. Support flush APIs in FS * supports API1 * FSDataOutputStream implements Syncable interface defined below. If its wrapped output stream (i.e. each file system's stream) is Syncable, and hsync() call its wrapped output stream's hflush & hsync. * In each file system, if only hflush() is implemented, hsync() by default calls hflush(). If only hsync() is implemented, hflush() by default calls flush().",non_debt,-
hadoop,6313,comment_0,This patch 1. defines Syncable interface 2. makes FSDataOutputStream to implement Syncable interface 3. makes of RawLocalFileSystem to implement the Syncable interface and also makes it a 4. implement a unit test to test 2 and 3.,test_debt,lack_of_tests
hadoop,6313,comment_1,The reason that I made this change is that of RawLocalFileSystem implements Syncable in the trunk although there is a bug. My patch fixed the bug. But another option is to remove the Syncable implementation in RawLocalFileSystem because I guess most users use LocalFileSystem anyway.,non_debt,-
hadoop,6313,comment_2,This patch removes Syncable implementation in RawLocalFileSystem and makes the default implementation of hflush & hsync to be flush in FSDataOutputStream.,code_debt,duplicated_code
hadoop,6313,comment_3,1,non_debt,-
hadoop,6313,comment_5,+1 on patch.,non_debt,-
hadoop,6313,comment_6,Thank Sanjay and Stack for your review. Here is a patch that adds a @Deprecated annotation to to remove the javac warning.,non_debt,-
hadoop,6313,comment_7,Here is the new ant test-patch result: [exec] +1 release audit. The applied patch does not increase the total number of release audit warnings.,non_debt,-
hadoop,6313,comment_9,I committed the patch to both trunk and 21. Thank you Hairong.,non_debt,-
hadoop,6321,summary,Hadoop Common - Site logo,non_debt,-
hadoop,6321,description,"Hadoop Common - Site Logo Update the logo (see attached jpg). Image has elephant + common. With this update, Site logo and Documentation logo will be the same.",non_debt,-
hadoop,6321,comment_0,Site logo for Hadoop Common,non_debt,-
hadoop,6321,comment_1,No patch file required. Put attached jpg file in this directory:,non_debt,-
hadoop,6321,comment_2,No new test code; required; changes to documentation only.,non_debt,-
hadoop,6321,comment_4,"I committed this. Thanks, Corinne!",non_debt,-
hadoop,6364,summary,add a 'hostname' or 'identity' parameter,non_debt,-
hadoop,6364,description,There was a bit of talk about HDFS-34 last night at Apachecon. One of the points brought up was the difficulty in assuming that the 'hostname' was externally available. Perhaps what needs to happen instead is to follow what httpd and a few other apps do. That is provide a way for the service to know what hostname to advertise itself as and use in all communications. This could also help solve the multi-nic problems (HADOOP-6210) faced when using hadoop in a HA environment.,design_debt,non-optimal_design
hadoop,6364,comment_1,"This sorta-kinda exists in the undocumented configuration slave.host.name, right?",documentation_debt,low_quality_documentation
hadoop,6364,comment_2,I don't know. I've never tried it since it is undocumented. :),documentation_debt,low_quality_documentation
hadoop,6364,comment_3,A quick grep through the source says this is only supported in the TaskTracker and DataNode.,non_debt,-
hadoop,6364,comment_4,"# It might make sense to have options for the namenode and job tracker, something with different config names for each, so a single configuration can define both. # It could still be important for some people to say ""come up on all interfaces, localhost included"", which could imply it should be a list. # There's also the problem that the namenode has historically been very fussy about what hostname was used to refer to it",code_debt,low_quality_code
hadoop,6364,comment_5,Hadoop isn't focused on servers with multiple interfaces. Closing as won't fix.,non_debt,-
hadoop,6374,summary,JUnit tests should never depend on anything in conf,architecture_debt,violation_of_modularity
hadoop,6374,description,The recent change to mapred-queues.xml that causes many mapreduce tests to break unless you delete out of your build tree is bad. We need to make sure that nothing in conf is used in the unit tests. One potential solution is to copy the templates into build/test/conf and use that instead.,architecture_debt,violation_of_modularity
hadoop,6374,comment_0,"Similar issue has been created for Map/Reduce (MAPREDUCE-1369, see Issue Links). Perhaps, a similar issue should be also created for HDFS.",non_debt,-
hadoop,6374,comment_1,"Please review the submitted patch. The patch is the same as in MAPREDUCE-1369, attached at 2010-01-11 04:04 AM (with different line numbers), and details are the same as described in the comment to MAPREDUCE-1369 added at 11/Jan/10 04:12 AM.",non_debt,-
hadoop,6374,comment_3,Please review the submitted patch. Thanks.,non_debt,-
hadoop,6374,comment_4,Please review the submitted patch. Thanks.,non_debt,-
hadoop,6374,comment_5,Please review the submitted patch. Thanks.,non_debt,-
hadoop,6374,comment_6,+1 patch looks good. I see that a similar JIRA has been open for MapReduce. I think the same needs to be done for HDFS subproject. I also have ran all Common's tests and they seem to be just fine with the patch in place. I'm going to commit it.,non_debt,-
hadoop,6374,comment_7,I've just committed this to the trunk. Thanks Anatoli!,non_debt,-
hadoop,6405,summary,Update Eclipse configuration to match changes to Ivy configuration,non_debt,-
hadoop,6405,description,"The file doesn't match the Ivy configuration, so I've updated it to use the right version of commons-logging",non_debt,-
hadoop,6405,comment_0,Patch to update the Eclipse configuration,non_debt,-
hadoop,6405,comment_1,+1 on the patch. I've run and && and compile-core-test && ant eclipse-files and have gotten working Eclipse project. I'm going to commit it.,non_debt,-
hadoop,6405,comment_2,I've just committed this. Thanks Edwin!,non_debt,-
hadoop,6413,summary,Move TestReflectionUtils to Common,non_debt,-
hadoop,6413,description,The common half of MAPREDUCE-1209,non_debt,-
hadoop,6413,comment_0,Patch moves the non-MR-dependent part to common.,non_debt,-
hadoop,6413,comment_2,The comment is for this patch and for MAPREDUCE-1209 as well: I'd suggest to perform the conversion of the test cases to JUnit 4 while you're at it anyway.,non_debt,-
hadoop,6413,comment_3,Updated to JUnit 4 (annotation) style,code_debt,low_quality_code
hadoop,6413,comment_5,+1 patch looks good.,non_debt,-
hadoop,6413,comment_6,I've just committed this. Thanks Todd!,non_debt,-
hadoop,6435,summary,Make RPC.waitForProxy with timeout public,non_debt,-
hadoop,6435,description,"The public RPC.waitForProxy() method waits for Long.MAX_VALUE before giving up, ignores all interrupt requests. This is excessive. The version of the method that is package scoped should be made public. Interrupt swallowing is covered in HADOOP-6221 and can be done as a separate patch",code_debt,low_quality_code
hadoop,6435,comment_0,"This is the patch against HEAD. Created via git, which I am still coming to terms with, apologies if it does not work. the git patch is on github",non_debt,-
hadoop,6435,comment_2,patch generated with the HEAD~1 and HEAD declarations in the correct order,non_debt,-
hadoop,6435,comment_4,I've just committed this. Thanks Steve!,non_debt,-
hadoop,6445,summary,distcp -p doesn't work when run on destination,non_debt,-
hadoop,6445,description,"From what I can tell, when distcp -p hdfs://src hdfs://dest is run on the destination grid, the permissions, etc, are not applied.",non_debt,-
hadoop,6457,summary,Set Hadoop User/Group by System properties or environment variables,non_debt,-
hadoop,6457,description,"Hadoop User/Group can be set by System properties or environment variables. For example, in environment variables, export HADOOP_USER=test export HADOOP_GROUP=user or in your MapReduce, ""test""); ""user"");",non_debt,-
hadoop,6457,comment_0,"Do note that users can be in more than one group, so hadoop.group.name should probably be hadoop.group.names, and be comma-delimited.",code_debt,low_quality_code
hadoop,6457,comment_1,"Also note that this isn't going to be compatible with security for obvious reasons. If you look at the patches that I've uploaded on HADOOP-6299, you'll see that we are moving toward using the JAAS interface for getting the user. Can you explain more of what you are trying to accomplish?",non_debt,-
hadoop,6457,comment_2,What makes this better than setting hadoop.job.ugi?,non_debt,-
hadoop,6462,summary,"contrib/cloud failing, target ""compile"" does not exist",non_debt,-
hadoop,6462,description,"I'm not seeing this mentioned in hudson or other bugreports, which confuses me. With the addition of a from HADOOP-6426, contrib/build.xml won't build no more: The following error occurred while executing this line: Target ""compile"" does not exist in the project ""hadoop-cloud"". What is odd is this: the final patch of HADOOP-6426 does include the stub <target> files needed, yet they aren't in SVN_HEAD. Which implies that a different version may have gone in than intended.",non_debt,-
hadoop,6462,comment_0,"Patch to fix the missing targets. The final patch from HADOOP-6426 was not the one committed, since there were problems getting Hudson to run the new unit tests, so this was broken off into another issue (HADOOP-6451). Sorry for the confusion.",non_debt,-
hadoop,6462,comment_1,"looks good to me, seeing how hudson handles it",non_debt,-
hadoop,6462,comment_3,I've just committed this.,non_debt,-
hadoop,6492,summary,Make avro serialization APIs public,non_debt,-
hadoop,6492,description,Some avro-specific serialization methods need to be public for MAPREDUCE-815,non_debt,-
hadoop,6492,comment_0,Attaching a patch that makes some APIs public. No tests because this is a trivial visibility-only change.,non_debt,-
hadoop,6492,comment_2,+1. I'll commit this later today barring objections.,non_debt,-
hadoop,6492,comment_3,New patch for this issue which includes the AvroFSInput class suggested in MAPREDUCE-815,non_debt,-
hadoop,6492,comment_5,This patch requires Avro 1.3.,non_debt,-
hadoop,6492,comment_6,Avro 1.3-specific dependencies had been inadvertently put in patch #2. Trying again with those removed.,build_debt,over-declared_dependencies
hadoop,6492,comment_8,"I just committed this, but without the AvroFsInput class. Trivial as it is, it should probably be submitted as a separate issue, with some unit tests. Thanks, Aaron!",test_debt,lack_of_tests
hadoop,6492,comment_9,Submitted AvroFsInput as HADOOP-6497,non_debt,-
hadoop,6517,summary,Ability to add/get tokens from,non_debt,-
hadoop,6517,description,We need to be able to get and set tokens in the user's UGI object.,non_debt,-
hadoop,6517,comment_0,here is a preliminary patch,non_debt,-
hadoop,6517,comment_1,"The methods were already there, but they were storing the information in the UGI itself rather than the JAAS Subject. This patch fixes the behavior and adds a test to detect the problem.",non_debt,-
hadoop,6517,comment_3,I have some difficulty dealing with parameter <Ident extends TokenIdentifier>. Changing it to <?> or <? extends TokenIdentifier>. See the uploaded patch.,non_debt,-
hadoop,6517,comment_5,I just committed this. Thanks Owen and Kan!,non_debt,-
hadoop,6525,summary,Support secure clients connecting to insecure servers,non_debt,-
hadoop,6525,description,"It would be useful to allow clients that have security turned on to talk to servers that have security turned off. (This does *not* mean protocol compatibility between versions, but rather with the same version with different configurations.)",non_debt,-
hadoop,6525,comment_0,"I believe this is the basics of HDFS-3905 and its related JIRAs. So I'll close this as a dupe. If anyone feels otherwise, feel free to open a new JIRA.",non_debt,-
hadoop,6536,summary,behavior is not defined when we pass a symlink as the argument,non_debt,-
hadoop,6536,description,"deletes contents of sym-linked directory when we pass a symlink. If this is the behavior, it should be documented as so. Or it should be changed not to delete the contents of the sym-linked directory.",documentation_debt,low_quality_documentation
hadoop,6536,comment_0,"Since when symlink to a file is sent as param to fullyDelete(), it deletes the symlink only(and not the file pointed to by symlink), I think fullyDelete() should only delete the symlink even when symlink to a dir is passed as param(and should not delete the dir pointed to by symlink).",non_debt,-
hadoop,6536,comment_1,"Based on a simple test I did on Linux, it appears that removal of a symlink (whether to file or directory) only removes the link. If I treat as a Java equivalent for ""rm -r"", then it seems to me that there's a bug in the implementation of It should only delete the link and not contents of it. IOW, I am +1 for Ravi's proposal.",non_debt,-
hadoop,6536,comment_2,+1 Ravi's proposal. I would also add that fullyDelete should delete dangling links (it currently does but we should add a test). The current behavior is due to java making it difficult to identify symlinks.,test_debt,lack_of_tests
hadoop,6536,comment_3,"One more issue. Currently comes out stopping deletion of other files/directories if it is unable to delete a file/dir(say because of not having permissions to delete that file/dir) anywhere under myDir. This is because we return from method if the recursive call ""if(!fullyDelete()) {return false;}"" fails at any level of recursion. Shouldn't it continue with deletion of other files/dirs continuing in the for loop instead of returning false here ? We can just set a boolean to false in this if() and can continue the for loop sothat return value of fullyDelete() is correct always. For example, if we have myDir/subdir2/file2 and if fullyDelete() tries to delete file1 first and could not delete it(because subDir1 has nonwritable permissions), then fullyDelete() currently comes out without deleting file2 and subdir2. I guess fullyDelete() should delete as many files as possible(similar to 'rm -rf'). Thoughts ?",non_debt,-
hadoop,6536,comment_4,"+1 for this. But this is a separate issue, so I am creating a new ticket for this.",non_debt,-
hadoop,6536,comment_5,Filed HADOOP-6631 for this.,non_debt,-
hadoop,6536,comment_6,Attaching patch for trunk. This patch is on top of HADOOP-6631.,non_debt,-
hadoop,6536,comment_7,Attaching new patch to apply to the latest trunk as HADOOP-6631 has gone in.,non_debt,-
hadoop,6536,comment_8,"Took a look at the latest patch, the changes seem fine.",non_debt,-
hadoop,6536,comment_9,Allowing the patch to go through Hudson...,non_debt,-
hadoop,6536,comment_11,Changes look good. Can you add a test for deleting dangling links also?,test_debt,lack_of_tests
hadoop,6536,comment_12,Attaching patch adding more testcases.,non_debt,-
hadoop,6536,comment_13,Patch looks good.,non_debt,-
hadoop,6536,comment_15,The failed test TestTrash passes on my local machine. Allowing to go through Hudson again...,non_debt,-
hadoop,6536,comment_17,No javadoc warning is introduced in this patch.,non_debt,-
hadoop,6536,comment_18,I just committed this. Thanks Ravi!,non_debt,-
hadoop,6538,summary,"Set to ""simple"" by default",non_debt,-
hadoop,6538,description,"The default value of is ""kerberos"". It makes sense for that to be ""simple"" since not all users have kerberos infrastructure set up, and it is inconvenient to have it set it to ""simple"" manually.",build_debt,build_others
hadoop,6538,comment_0,Straightforward patch.,non_debt,-
hadoop,6538,comment_1,+1 on the patch. I think it's good to default to no authentication.,non_debt,-
hadoop,6538,comment_2,"The documentation in the XML looks wrong - it's copied from the setting above. Also, the code in should be updated to reflect this default (right now it defaults to kerberos)",documentation_debt,low_quality_documentation
hadoop,6538,comment_3,"Thanks for catching that, Todd. Attached the updated patch.",non_debt,-
hadoop,6538,comment_4,"The description in the XML still reads ""Is service-level authorization enabled?"" which is copied from the conf item above. It should instead describe the options ""simple"" vs ""kerberos""",documentation_debt,low_quality_documentation
hadoop,6538,comment_5,Argh!! Stupid of me. Updated patch.,non_debt,-
hadoop,6538,comment_6,"+1, looks good to me pending hudson. Thanks for the changes.",non_debt,-
hadoop,6538,comment_7,retrying hudson,non_debt,-
hadoop,6538,comment_8,"ant tests passed with the last patch. test-patch passed with the last patch but it complained about ""no tests included"". I manually tested the patch by checking that in mapreduce, without setting any config for authentication, the value of the parameter is ""simple"" with this modified common jar. I will commit this.",non_debt,-
hadoop,6538,comment_10,I just committed this.,non_debt,-
hadoop,6538,comment_13,Patch for an earlier version of Hadoop. Not for commit.,non_debt,-
hadoop,6556,summary,Cleaup some of the methods on FileContext.,non_debt,-
hadoop,6556,description,Cleanup some of the methods of FileContext,code_debt,low_quality_code
hadoop,6556,comment_0,"Additional changes needed to FileContext methods: * isFile, isDir - should it throw notFoundException or return false if the path does not exist? * deleteOnExit should return void instead of boolean.",non_debt,-
hadoop,6556,comment_1,FileContext#delete and FileContext#copy should return void instead of boolean. On failure an appropriate exception should be thrown that details the failure instead of returning false. There could be other methods in FileContext too that need similar changes.,non_debt,-
hadoop,6556,comment_2,..) should fail on any errors on any of the supplied paths.,non_debt,-
hadoop,6556,comment_3,"Another comment from HADOOP-6537 - listStatus(all variations), globStatus(??) - add the UnresolvedLinkE",non_debt,-
hadoop,6556,comment_4,"isFile, isDir no longer exist. deleteOnExit: I am afraid we cannot be sure how much applications use it and expect return value from the ecosystem. Seeing that it is a 7 years old ticket and it is an incompatible change I would close it as Won't fix.",non_debt,-
hadoop,6584,summary,Provide Kerberized SSL encryption for webservices,non_debt,-
hadoop,6584,description,"Some web services should be authenticated via SSL backed by Kerberos, both to provide cross-cluster secure communication and to provide intra-cluster server authentication for services such as the image transfer and balancer.",non_debt,-
hadoop,6584,comment_0,Patch for review.,non_debt,-
hadoop,6584,comment_2,"Prelim Y20 patch for review, not for commit.",non_debt,-
hadoop,6584,comment_3,"oh, and the prelim Y20 patch incorporates HDFS-1004, which is the HDFS component of this patch... life in a post-split world...",non_debt,-
hadoop,6584,comment_4,updated prelim patch.,non_debt,-
hadoop,6584,comment_5,Looks good.,non_debt,-
hadoop,6584,comment_6,small iteration,non_debt,-
hadoop,6584,comment_7,Already went stale...,non_debt,-
hadoop,6584,comment_8,Small patch to fix javadoc warnings introduced into Y20 branch. Not to be committed.,code_debt,low_quality_code
hadoop,6584,comment_9,uploading a Common patch for the current trunk,non_debt,-
hadoop,6584,comment_11,"Patch c6584-02.patch was simply a port of Jakob's original patch for Y20. The javadoc warnings are unrelated since they are due to KerberosName.java and SecurityUtil.java, neither of which this patch modifies. This patch adds a Kerberos functionality and we currently don't have a framework to unit test it. However, I have manually verified it by deploying a single host cluster with suitable Kerberos infrastructure. I also ran ""ant test"" and it passed.",non_debt,-
hadoop,6584,comment_12,"This looks good to me - thanks, Kan for the forward port! - but should be reviewed by another committer before commit to get a fresh pair of eyes on it.",non_debt,-
hadoop,6584,comment_13,Looks good. +1,non_debt,-
hadoop,6584,comment_14,I've committed this. Resolving as fixed.,non_debt,-
hadoop,6589,summary,Better error messages for RPC clients when authentication fails,design_debt,non-optimal_design
hadoop,6589,description,"Currently when authentication fails, RPC server simply closes the connection. Sending certain error messages back to the client may help user debug the problem. Of course, those error messages that are sent back shouldn't compromise system security.",design_debt,non-optimal_design
hadoop,6589,comment_0,"this patch introduced a mechanism for RPC server to send status, exception class and error message to RPC client during the SASL authentication process.",non_debt,-
hadoop,6589,comment_2,"I just committed this. Thanks, Kan!",non_debt,-
hadoop,6589,comment_3,Here is the patch for Yahoo 0.20s branch.,non_debt,-
hadoop,6632,summary,Support for using different Kerberos keys for different instances of Hadoop services,non_debt,-
hadoop,6632,description,"We tested using the same Kerberos key for all datanodes in a HDFS cluster or the same Kerberos key for all TaskTarckers in a MapRed cluster. But it doesn't work. The reason is that when datanodes try to authenticate to the namenode all at once, the Kerberos authenticators they send to the namenode may have the same timestamp and will be rejected as replay requests. This JIRA makes it possible to use a unique key for each service instance.",non_debt,-
hadoop,6632,comment_0,"One error message we observed. 2010-03-03 07:33:50,542 INFO IPC Server listener on 8020: readAndProcess threw exception GSS initia te failed [Caused by GSSException: Failure unspecified at GSS-API level (Mechanism level: Request is a replay (34))]. Count of bytes read: 0 GSS initiate failed [Caused by GSSException: Failure unspecified at GSS-API level (Mechanism level: Request is a replay (34))] Caused by: GSSException: Failure unspecified at GSS-API level (Mechanism level: Request is a replay (34)) ... 4 more Caused by: KrbException: Request is a replay (34) ... 7 more",non_debt,-
hadoop,6632,comment_1,"A patch for Yahoo 0.20S branch. Not for commit. This patch assumes a service's Kerberos principal name is of the form where servicename is the service type (for example, dn for DataNodes, and tt for TaskTrackers), hostname is the fully-qualified domain name of the host where the service is running on, and REALM is Kerberos realm that the service belongs to. To support the convenience of having the same conf on every host, a service's Kerberos principal name can be configured as where ${FQDN} will be substituted at runtime with the fully-qualified domain name of the host that the service is running on.",non_debt,-
hadoop,6632,comment_2,"New patch for hadoop-20. The hostname patter is changed to _HOST, and the renewer for delegation tokens is changed to shortnames.",non_debt,-
hadoop,6632,comment_3,A minor fix for the MR side to reuse filesystem handles,code_debt,low_quality_code
hadoop,6632,comment_4,A port for trunk.,non_debt,-
hadoop,6632,comment_6,The javadoc warnings are unrelated to this patch. Manually verified the feature on a single node cluster.,non_debt,-
hadoop,6632,comment_7,Uploading a new patch that simply merges with latest trunk changes. No semantic change from previous patch.,non_debt,-
hadoop,6632,comment_9,"I just committed this. Thanks, Kan & Jitendra!",non_debt,-
hadoop,6632,comment_13,It looks like the 6632.mr.patch portion was committed to ydist but not trunk - was this intentional?,non_debt,-
hadoop,6632,comment_14,"Yes this was intentional. The mr patch seemed like a hack and that's why we didn't commit it to trunk, and instead raised MAPREDUCE-1824 to discuss that... BTW, the problem which the mr patch attempted to address would be significantly less once we have HADOOP-6706 committed that does retries in case of failures due to the false replay attack detection by the rpc servers. MAPREDUCE-1824 takes a low priority..",non_debt,-
hadoop,6632,comment_15,"Thanks, Deveraj. That makes sense.",non_debt,-
hadoop,6639,summary,FileSystem.get(..) may be blocked for a long time,non_debt,-
hadoop,6639,description,"When FileSystem cache is enabled, FileSystem.get(..) will call which is a synchronized method. If the lookup fails, a new instance will be initialized. Depends on the FileSystem subclass implementation, the initialization may take a long time. In such case, the FileSystem.Cache lock will be hold and all calls to FileSystem.get(..) by other threads will be blocked for a long time. In particular, the initialization may take a long time since there are retries. It is even worst if the socket timeout is set to a large value.",code_debt,slow_algorithm
hadoop,6639,comment_0,This is the same as HADOOP-6640.,non_debt,-
hadoop,6644,summary,util.Shell method name - should use common naming convention,code_debt,low_quality_code
hadoop,6644,description,util.Shell method name - should use common naming convention,code_debt,low_quality_code
hadoop,6644,comment_0,also included these issues: - Put back the constructor for Server in Server.java - auditLOG should be private in Server and should be all uppercase,non_debt,-
hadoop,6644,comment_1,"for previous version, not for commit",non_debt,-
hadoop,6644,comment_3,1,non_debt,-
hadoop,6644,comment_4,committed to trunk.,non_debt,-
hadoop,6655,summary,SLA related changes in hadoop-policy.xml have misleading property descriptions,non_debt,-
hadoop,6655,description,"In the patch introduced my HADOOP-4348 proposed modifications of read on more than one occasion: It is either should read ""separated by a semicolon"" or the given example has to be changed.",non_debt,-
hadoop,6655,comment_0,"Kos, the ACL proposed in HADOOP-4348 actually used a space character as the user-group list separator. The same AccessControlList is used for other features like queue administration and I know of code throughout MapReduce that hardcodes this same assumption for the lack of good API to deconstruct an ACL or modify an already constructed ACL. Are you proposing we change the character separator to a semi-coloon? Any reason for this? Now?",non_debt,-
hadoop,6655,comment_1,I'm proposing to close this JIRA because I've misread the test in the config file. Nevermind.,non_debt,-
hadoop,6658,summary,Exclude Public elements in generated Javadoc,code_debt,low_quality_code
hadoop,6658,description,"Packages, classes and methods that are marked with the or annotation should not appear in the public Javadoc. Developer Javadoc generated using the ""javadoc-dev"" ant target should continue to generate Javadoc for all program elements.",code_debt,low_quality_code
hadoop,6658,comment_0,Patch for review.,non_debt,-
hadoop,6658,comment_2,It would be great if the jdiff stuff could ignore private/limited stuff too...,non_debt,-
hadoop,6658,comment_3,"Here's a new patch that excludes private elements from JDiff as well. I tested this by running ""ant api-report"" and visually checking that a new class with the Private annotation (FtpConfigKeys) did not appear.",non_debt,-
hadoop,6658,comment_5,New patch which excludes annotations that are marked Private or LimitedPrivate. Also removes Javadoc and RAT warnings.,documentation_debt,low_quality_documentation
hadoop,6658,comment_7,Test failures are unrelated. I think this is ready to be committed.,non_debt,-
hadoop,6658,comment_8,+1 Looks good to me. I also saw that you okayed this use of LGPL'd JDiff on legal-discuss@.,non_debt,-
hadoop,6658,comment_9,"Yes, here's the relevant",non_debt,-
hadoop,6658,comment_10,"New version of the doclets that takes a option to specify the stability: -unstable (the default), -evolving, -stable. For example, to exclude evolving APIs you would specify -stable. This is useful for using JDiff to examine compatibility differences.",non_debt,-
hadoop,6658,comment_12,"Test failures were due to a xerces jar being pulled onto the classpath for jdiff. It's not needed for compilation, or for the targets that use jdiff, since they use the xerces jar bundled with jdiff.",non_debt,-
hadoop,6658,comment_14,"Tests now pass, but annotation processing doesn't seem to be working with the new classes. This patch excludes them from annotation processing, since we don't need fault injection for these tools.",non_debt,-
hadoop,6658,comment_16,"I'd like to commit this soon, since it makes it easier to check API compatibility changes in HADOOP-6668 and MAPREDUCE-1623.",non_debt,-
hadoop,6658,comment_17,I've just committed this.,non_debt,-
hadoop,6709,summary,Re-instate deprecated FileSystem methods that were removed after 0.20,non_debt,-
hadoop,6709,description,"To make the FileSystem API in the 0.21 release (which should be treated as a minor release) compatible with 0.20 the deprecated methods removed in HADOOP-4779 need to be re-instated. They should still be marked as deprecated, however.",code_debt,low_quality_code
hadoop,6709,comment_0,Patch to re-instate the removed methods. No extra tests.,non_debt,-
hadoop,6709,comment_2,+1 Looks good to me. Nit: please put @Deprecated for getBlockSize and getLength on their own line to be consistent with the other methods.,code_debt,low_quality_code
hadoop,6709,comment_3,I've just committed this. (I fixed the nit.),non_debt,-
hadoop,6709,comment_4,More than a Nit. We have to leave the deprecation in. The goal of this Jira is to merely reinstate deleted methods. Old deprecation should remain.,code_debt,low_quality_code
hadoop,6709,comment_5,"To clarify, the nit was placement of the @Deprecated annotation (so that it was on its own line) rather than whether it was included or not.",non_debt,-
hadoop,6727,summary,Remove from public FileContext APIs,code_debt,low_quality_code
hadoop,6727,description,"HADOOP-6537 added to the throws clause and java docs of FileContext public APIs. FileContext fully resolves symbolic links, exception is only used internally by FSLinkResolver. I'll attach a patch which updates the APIs and javadoc.",code_debt,low_quality_code
hadoop,6727,comment_1,"Just updating throws clauses and java docs, hence the lack of tests.",code_debt,low_quality_code
hadoop,6727,comment_2,"API change so should be in 21, also an easy change.",non_debt,-
hadoop,6727,comment_3,Attached patch merges with trunk.,non_debt,-
hadoop,6727,comment_5,I've just committed this. Thanks Eli!,non_debt,-
hadoop,6730,summary,Bug in FileContext#copy and provide base class for FileContext tests,non_debt,-
hadoop,6730,description,"Thanks to Eli, He noticed that there is no test for FileContext#Copy operation. On further investigation with the help of Sanjay we found that there is bug in *FileStatus dstFs = should be in try...catch block.",test_debt,low_coverage
hadoop,6730,comment_0,Adding information about bug in Will upload patch soon with bug fix and Tests for copy.,non_debt,-
hadoop,6730,comment_1,The call to getFileStatus is outside the try block to make sure is thrown per the interface no?,non_debt,-
hadoop,6730,comment_2,Attaching patch for review.,non_debt,-
hadoop,6730,comment_3,"Hey Ravi, per the previous comment I left the getFileStatus outside the try block so the is now swallowed silently and is thrown up to copy (whose API wants This deserves a comment. The patch just has util classes - perhaps you forgot to add the test class in your diff?",test_debt,lack_of_tests
hadoop,6730,comment_4,"operation was failing with file not found exception if destination file does not exits , this was happening due to the fact that getFileStatus was out of try block. I have included test runner with util base. Please check that there are two classes added with this patch - and",non_debt,-
hadoop,6730,comment_5,+1 Looks good to me.,non_debt,-
hadoop,6730,comment_6,"Review of the patch: * Lines 47-52: These notes aren't necessary since the test writers should know about @Before and @After. Also, the provided methods (setUp() and tearDown()) conflict with the provided names. * Lines 55 & 56: These values should be all capitalized and final. Alternatively, it may be good to provide these values via functions so that implementing classes may override as needed. * Line 64: catch should be on the same line as closing brace, per our coding style * Line 70: fc should not be declared static. It is the responsibility of each implementing class to provide an instance of it. Also, fc should probably be protected * Line 91: Please provide assert message for assertion failure * Since this is a test that the copy method worked, we should prove it by comparing the contents of the copied file with the original. * Lines 135-138: This code: is a no-op and should be removed. Nits: * Rather than Assert.assertTrue() you can do a static import of * Line 57: Remove extra line * Line 47: ""Since this a junit 4"" ->""Since this is a junit 4 test""",code_debt,low_quality_code
hadoop,6730,comment_7,Thanks for reviewing patch Jakob. Uploading updated patch.,non_debt,-
hadoop,6730,comment_8,Updated patch with changes. 1. Removed unnecessary tearDown method. 2. Changed file read write to bytes from String.,code_debt,dead_code
hadoop,6730,comment_9,Updated patch. Removed static from fc initialization. Removed invalid comments.,documentation_debt,low_quality_documentation
hadoop,6730,comment_10,Submitting patch for hudson queue.,non_debt,-
hadoop,6730,comment_12,1,non_debt,-
hadoop,6730,comment_13,"I have committed this. Thanks, Ravi. Resolving as fixed.",non_debt,-
hadoop,6734,summary,wrongly calls during initialisation,non_debt,-
hadoop,6734,description,"Reason: If a bucket is created with specified, will fail. Symptoms: If a bucket has will fail with Error. A detailed descrioption from a blog",non_debt,-
hadoop,6734,comment_0,HADOOP-4422 removed bucket creation from S3 filesystems. Does this fix the problem for you?,non_debt,-
hadoop,6734,comment_1,"Sure. I had to drop calling S3 straight from Hadoop code anyhow (due to us using S3 and Amazon Elastic MapReduce in EU). I don't know what's Hadoop policy on making patches to older releases, so I won't mark this ""won't fix"".",non_debt,-
hadoop,6772,summary,Utilities for system tests specific.,non_debt,-
hadoop,6772,description,Common utilities for system tests. 1. A method for restarting the daemon with new configuration. public static void 2. A method for restarting the daemon with default configuration. public void restart() throws Exception; 3. A method for waiting until daemon is stop. public void throws Exception; 4. A method for waiting until daemon is start. public void throws Exception;,non_debt,-
hadoop,6772,comment_0,Initial patch.,non_debt,-
hadoop,6772,comment_1,"this won't do for sure... Also, Common doesn't need {{system-test.xml}} to be present because its content is only used by HDFS and MR clusters.",non_debt,-
hadoop,6772,comment_2,Uploaded the patch by addressing cos comments.,non_debt,-
hadoop,6772,comment_3,"Looks better, but isn't applicable because has been generated incorrectly...",non_debt,-
hadoop,6772,comment_4,Fixed and uploaded the latest patch.,non_debt,-
hadoop,6772,comment_5,+1 patch looks good. Let's verify,non_debt,-
hadoop,6772,comment_7,I have just committed this. Thank you Vinay.,non_debt,-
hadoop,6772,comment_9,Patch for yahoo distribution security branch.,non_debt,-
hadoop,6794,summary,Move configuration and script files post split,non_debt,-
hadoop,6794,description,None,non_debt,-
hadoop,6794,comment_0,See,non_debt,-
hadoop,6794,comment_2,"The following need to be added to the patch. - configuration property is not present in common-project but is present and needed in mapreduce. - Nine settings for 'Job Summary Appender' are present and needed in mapreduce project but are not there in common project. - Two settings for mapreduce task logs are present and needed in mapreduce project but are not there in common project. -- -- - One more thing: copying this file over from mapreduce will not work because six settings for 'Security appender' are in common but not present in the mapreduce project's log4j.properties file. May be we should also split both the above files per project. But that may be long term, will open a separate ticket if you agree. Other issues - Minor: bin/rcc still refers to hadoop-core at one place. - when i run `ant binary` in hdfs bin directory isn't getting copied into the packaged binary, neither are the hdfs script files copied. - when i run `ant binary` in mapreduce, files aren't getting copied into the packaged bin directory Verified the rest of the changes in all the three patches, they look good. Still couldn't get the cluster up with different installation directories, will be trying in the meanwhile..",design_debt,non-optimal_design
hadoop,6794,comment_3,"Vinod, many thanks for the thorough review. I've made fixes for all the errors you pointed out. Common's is now a merged superset of the log4j.properties - I think we can split it out further later as needed. I tried running with different installation directories, and successfully ran a job in pseudo-distributed mode. However, I did notice that the web UIs were not working, which I think will be fixed as a part of HADOOP-6461, although I'm still investigating that.",non_debt,-
hadoop,6794,comment_4,Updated to pick up webapps from the classpath. With this change (and HADOOP-6461) the web UIs work correctly.,non_debt,-
hadoop,6794,comment_6,"The latest patches for the three projects look good, Tom. I could use HADOOP-6461 to successfully bring up single node clusters of both hdfs and mapreduce. I ran a simple mapreduce job too. +1. Thanks for taking this up!",non_debt,-
hadoop,6794,comment_7,"Thanks for the review, Vinod. I plan to commit these three patches soon.",non_debt,-
hadoop,6794,comment_8,"A minor change that loosens the jar glob pattern at the top-level from to {{hadoop-*.jar}}. This change supports an installation where all the scripts, configuration and jars are in common directories, as produced by HADOOP-6342. I successfully ran a job on a pseudo-distributed cluster in this mode.",non_debt,-
hadoop,6794,comment_9,"A minor change to the patches which makes HADOOP_COMMON_HOME, HADOOP_HDFS_HOME, and HADOOP_MAPRED_HOME default to HADOOP_HOME if they are not set.",non_debt,-
hadoop,6794,comment_11,I've just committed this.,non_debt,-
hadoop,6826,summary,Revert FileSystem create method that takes CreateFlags,non_debt,-
hadoop,6826,description,"As discussed in HDFS-609 and HADOOP-5438 we should back out the FileSystem create() method that takes a set of CreateFlag objects, until the interface has been agreed upon and fully tested.",non_debt,-
hadoop,6826,comment_0,"Here's a patch that removes the public create method. It is not a straight revert of HADOOP-5438 since in the intervening time since this was committed the codebase has been split into three projects, and FileContext has been introduced which uses some of the same underlying code. The HDFS part of this patch is in HDFS-609.",non_debt,-
hadoop,6826,comment_2,There are no new tests since this patch removes code. The javadoc warning is an unrelated warning due to sun.security.krb5.* imports (from HADOOP-6526 and HADOOP-6603?) - is anyone looking at this?,non_debt,-
hadoop,6826,comment_3,"This revert-patch looks good to me, +1",non_debt,-
hadoop,6826,comment_4,I've committed this.,non_debt,-
hadoop,6826,comment_5,This broke compilation of mapreduce trunk.,non_debt,-
hadoop,6826,comment_6,Raised MAPREDUCE-1885 in mapreduce project to fix the compilation issue because of this FileSystem api change.,non_debt,-
hadoop,6826,comment_7,"Tom/Dhruba, I think this commit should be reverted. FileSystem.create() is a public api and hence cannot be removed like this. Ideally it should be deprecated. Do you agree?",non_debt,-
hadoop,6826,comment_8,"hi amar, this API never made it to any ""release"" of Hadoop. Better to remove it now than to ship a release with a bad API.",code_debt,low_quality_code
hadoop,6826,comment_9,"Ravi, I missed that this affected MapReduce: sorry about that! Thanks for opening an issue, and creating a patch to fix MapReduce trunk. Amar, This reverts an unreleased API, so we don't need to deprecate it. In fact, the API was broken in some cases (HDFS-609) so we shouldn't release it.",non_debt,-
hadoop,6839,summary,[Herriot] Implement a functionality for getting the user list for creating proxy users.,non_debt,-
hadoop,6839,description,"Develop a new method for getting the user list. Method signature is public ArrayList<StringAdd new attribute in system-test.xml file for getting userlist path. For submitting the jobs as different user a proxy user id is needed. So,get the available users from a userlist and then pass the user as proxy instead of hardcoding user id in a test.",code_debt,low_quality_code
hadoop,6839,comment_0,Patch for yahoo dist security branch.,non_debt,-
hadoop,6839,comment_1,Patch for trunk.,non_debt,-
hadoop,6839,comment_2,changes to system-test.xml is with other JIRA call out the dependency.,non_debt,-
hadoop,6839,comment_3,"As I have asked on another JIRA I perhaps missing a point of having this functionality in place. I'd appreciate a better info on this. Also, similar to MAPREDUCE-1896 HDFS' needs to be changed if this feature is going to be committed.",non_debt,-
hadoop,6839,comment_4,Looks good overall. Please fix the formatting (white space are inconsistent) and make exception and error messages more meaningful. Submit the patch for verification whenever is ready.,code_debt,low_quality_code
hadoop,6839,comment_5,Addressed the cos comments and attached new patches.,non_debt,-
hadoop,6839,comment_6,"- has to be unquoted if you want to print the value of the constant. + LOG.error(""Multi users list path not passed "" + + ""for "");",non_debt,-
hadoop,6839,comment_8,Addressed the cos comments and attached latest patch for trunk.,non_debt,-
hadoop,6839,comment_9,"I can see there are 6 javadoc warnings and all of them are related to warning: So, I don't think the patch could raise the number of java doc warning considering the scope of the patch.",documentation_debt,low_quality_documentation
hadoop,6839,comment_10,+1 patch looks good. I'll commit it by COB today if hear no objections.,non_debt,-
hadoop,6839,comment_11,I have just committed this. Thanks Vinay.,non_debt,-
hadoop,6869,summary,Functionality to create file or folder on a remote daemon side,non_debt,-
hadoop,6869,description,"Functionality for creating either files or folders in task attempt folder while job is running. The functionality covers the following methods. 1. public void path, String fileName, boolean local) throws IOException; It uses to create a file with full permissions. 2. public void path, String fileName, FsPermission permission, boolean local) throws IOException; It uses to create a file with given permissions. 3. public void path, String folderName, boolean local) throws IOException; It uses to create a file with full permissions. 4. public void path, String folderName, FsPermission permission, boolean local) throws IOException; It uses to create a folder with given permissions.",non_debt,-
hadoop,6869,comment_0,Initial patch for yahoo dist security branch.,non_debt,-
hadoop,6869,comment_1,Patch for trunk.,non_debt,-
hadoop,6869,comment_2,1,non_debt,-
hadoop,6869,comment_3,"- Looks good with one nit. I'd suggest to have methods with fewer arguments simply call ones with more args. E.g. in this case instead of duplicating their exact implementation. You have done similar in already. - Also, in you already have a ref to the filesystem so you can just call its {{createFile}} methods instead of implementing your own logic just call to have new file created for you. - also you are writing some content into this file which seems to be an inadvertent side-effect. Doesn't look like a good idea.",code_debt,low_quality_code
hadoop,6869,comment_4,In continuation of the comment above: since there's {{getFS()}} method which exposes filesystem API from a particular daemon then creating additional 4 wrappers around the standard API doesn't make much sense to me. These wrappers are too obvious and then will simply crowd Herriot API without adding much value.,design_debt,non-optimal_design
hadoop,6869,comment_5,"Hi Cos, I do agree with you. However my requirement is creating the user define files and folders in task attempt id folder with different kind of permissions while job is running. Later while cleaning up, i just wanted make sure whether all the user defined files and folders are cleaned up or not for that job. So that I have defined the above methods for fulfilling my requirement. As my understand, i don't think so, the FileSystem is providing the no such straight forward and createNewFolder) like you mentioned. In the four methods, the first method creates the files with full permission.Suppose if user wants to give his own permissions while creating file in that case he can use the second method.Same way for folder creation also. I will remove this part in the code.",code_debt,dead_code
hadoop,6869,comment_6,This reasoning sounds like an argument to move this functionality up the stack to the MR cluster concrete implementation. But let's suppose they are useful enough and let them be in the Common. I do understand that. What I have said is that you need a generic enough method which does most of needed functionality (i.e. create a file with specified permissions) and a wrapper around it which creates a file with all permissions. The implementation of the latter is essentially a call to the former with *all* permissions being passed. This is already done in this very patch for the implementation of methods. Look at your own code.,code_debt,low_quality_code
hadoop,6869,comment_7,"Cos, Please look at my patch once again because I have not duplicated the exact implementation for create file. I have created a wrapper around the actual method with full permission like which i coded for create folder.Might be you have missed to look at that portion in my patch.",non_debt,-
hadoop,6869,comment_8,Addressed cos comments.,non_debt,-
hadoop,6869,comment_9,"I am glad we have understood each other :) +1 patch looks good. Let's verify. Also, please make sure that it works in real cluster setup because we can't verify it in Hudson yet.",non_debt,-
hadoop,6869,comment_11,java doc warnings are not related to this patch.,non_debt,-
hadoop,6869,comment_12,I'll commit it later today if won't hear otherwise.,non_debt,-
hadoop,6869,comment_13,I have just committed this. Thanks Vinay.,non_debt,-
hadoop,6877,summary,Common part of HDFS-1178,non_debt,-
hadoop,6877,description,This is the Common part of HDFS-1178.,non_debt,-
hadoop,6877,comment_0,1,non_debt,-
hadoop,6877,comment_1,Trying to get Hudson's attention.,non_debt,-
hadoop,6877,comment_3,The javadoc warnings are unrelated. No new test needed since this patch simply removes a utility method.,non_debt,-
hadoop,6877,comment_4,"I've committed this. Resolving as fixed. Thanks, Kan.",non_debt,-
hadoop,6879,summary,Provide SSH based (Jsch) remote execution API for system tests,non_debt,-
hadoop,6879,description,com.jcraft  jsch 0.1.42 version needs to be included in the build. This is needed to facilitate implementation of some system (Herriot) testcases . Please include this in ivy. jsch is originally located in,build_debt,under-declared_dependencies
hadoop,6879,comment_0,patch for 0.20.1.xxx,non_debt,-
hadoop,6879,comment_1,"Please have the trunk patch as well. For trunk, I think, this jar file needs to go to test->system profile, not master.",non_debt,-
hadoop,6879,comment_2,1,non_debt,-
hadoop,6879,comment_3,patch for trunk,non_debt,-
hadoop,6879,comment_4,One minor change,non_debt,-
hadoop,6879,comment_5,"Apparently there's no Ivy system profile in Common (unlike Hdfs and MR). However, this dependency needs to go to test instead of master.",build_debt,build_others
hadoop,6879,comment_6,"Moving over (and fixing issues in the original patch) proposed functionality from MAPREDUCE-1882 and refitting original patch for the trunk. Remote exec API needs some extra work to be done on it especially in the part of getting user's identities, but I just want to have it in the same place for now.",non_debt,-
hadoop,6879,comment_7,"Patch for y20, not to commit here.",non_debt,-
hadoop,6879,comment_8,The patch isn't final yet because there's not way to run framework validation tests from Ant,test_debt,lack_of_tests
hadoop,6879,comment_9,"Used wrong ivy profile, apparently",non_debt,-
hadoop,6879,comment_10,Base implementing method should use {{portNumber}} variable instead of a constant.,code_debt,low_quality_code
hadoop,6879,comment_11,Updated based on Sharad's comments.,non_debt,-
hadoop,6879,comment_12,"Oops, published wrong files.",non_debt,-
hadoop,6879,comment_13,Minor nits: 1. The method needs to be implemented or removed: 2. There shouldn't be any @author tags in the javadoc. Other than that patch looks fine.,code_debt,dead_code
hadoop,6879,comment_14,Addressing Sharad's comments and adding Apache license boiler-plate to the test class.,non_debt,-
hadoop,6879,comment_15,Same as before to make sure that {{test-patch}} picks up the correct patch.,non_debt,-
hadoop,6879,comment_16,Looks like Hudson is semi-dead again... Here's the results of {{test-patch}} javadoc warning is unrelated: they all seem to be caused by security code,non_debt,-
hadoop,6879,comment_17,I have just committed it.,non_debt,-
hadoop,6885,summary,Fix java doc warnings in Groups and,documentation_debt,low_quality_documentation
hadoop,6885,description,There are a couple java docs warnings in Groups and,documentation_debt,low_quality_documentation
hadoop,6885,comment_0,Patch attached.,non_debt,-
hadoop,6885,comment_2,"Eli- Funny, I don't get these as warnings when I run {{ant javadoc}}. I do however, get: Could you update the patch to fix these warnings as well and I'll +1?",non_debt,-
hadoop,6885,comment_3,Canceling patch post-review.,non_debt,-
hadoop,6885,comment_4,Updated the patch to fix the IPC javadoc warnings. The earlier warnings are seen when using javadoc-dev. Both targets build cleanly on trunk with this patch. Thanks Jakob.,documentation_debt,low_quality_documentation
hadoop,6885,comment_5,"+1. Verified javadoc is fixed. Hudson is completely AWOL and this is a minor documentation patch, so I've committed it. Resolving as fixed. Thanks, Eli.",non_debt,-
hadoop,6906,summary,FileContext copy() utility doesn't work with recursive copying of directories.,non_debt,-
hadoop,6906,description,None,non_debt,-
hadoop,6906,comment_0,Attaching a regression test. Running TestFcLocalFsUtil after applying this patch verifies that a simple recursive copy of a directory from local file system to the same FS fails. I could verify the same failure with HDFS to HDFS copy also.,non_debt,-
hadoop,6906,comment_1,"Attaching a patch to address this issue. Please review this patch. It does the following: - Fixes the main bug which incorrectly constructs path names of contents of a directory while copying (FileContext.java +2012 with the patch) - Modifies to avoid null errors when using file:/// URIs which don't have an authority part. - LocalFs extends ChecksumFs and *has* a RawLocalFs object. RawLocalFs is backed by RawLocalFileSystem which doesn't know about .crc files. Because of this, files written via LocalFs automatically write .crc files transparently, but listing of files in a LocalFs doesn't hide .crc files. The modified test fails because of this. I changed ChecksumFs to override listStatus() and swallow .crc files. Please correct me if I am wrong.",non_debt,-
hadoop,6906,comment_2,The problem with .crc files indeed is a valid one - HADOOP-6872.,non_debt,-
hadoop,6906,comment_3,"Regarding HADOOP-6872 - just a quick question: Why not provide a method in FilterFs and do the filtering there? Sounds more natural to me, since it is already a filtering fs. Other subclasses of FilterFs might benefit from this as well. Minor detail, just curious why it is in ChecksumFS now.",non_debt,-
hadoop,6906,comment_4,"I think that AbstractFileSystem should have an API listStatus(Path, PathFilter).",non_debt,-
hadoop,6906,comment_5,"+1 for the fix. as per the listStatus(Path, PathFilter) api, it would be good to have it, but again this fix should neways be there.",non_debt,-
hadoop,6906,comment_6,I just committed this. thanks vinod!,non_debt,-
hadoop,6906,comment_9,sorry I forgot to mention I ran ant test and and test-patch. They both pass!,non_debt,-
hadoop,6921,summary,metrics2: metrics plugins,non_debt,-
hadoop,6921,description,"This jira tracks the porting of builtin metrics sink plugins (file, ganglia) for the new metrics framework. Whether or not ganglia 3.0/3.1 plugins will be ported depends on the outcome of the discussion (proposed in the parent issue: HADOOP-6728) on backward compatibility (at some cost/limitations of course.)",non_debt,-
hadoop,6921,comment_0,"Based on the community discussion outlined in HADOOP-6728, we're keeping the metrics v1 for now, so GangaliaContext is not ported (by me. But others should feel free to port it :)) FileContext is already ported to FileSink in the metrics v2 as part of the framework.",non_debt,-
hadoop,6925,summary,BZip2Codec incorrectly implements read(),non_debt,-
hadoop,6925,description,HADOOP-4012 added an implementation of read() in BZip2InputStream that doesn't work correctly when reading bytes > 0x80. This causes EOFExceptions when working with BZip2 compressed data inside of sequence files in some datasets.,non_debt,-
hadoop,6925,comment_0,Patch fixes the read() implementation to correctly mask with 0xff before upcasting to int. Also augments the unit test to check the single-byte read() function - the new test fails before this patch.,test_debt,low_coverage
hadoop,6925,comment_1,"This may fix HADOOP-6852, as well. (The unit test in question is available as a standalone patch in MAPREDUCE-1927, which is awaiting review.) If so, I can update the unit test to uncomment the relevant bzip2 test. I'll make a note to check this soon...",non_debt,-
hadoop,6925,comment_2,+1 Looks good. Nice find.,non_debt,-
hadoop,6925,comment_3,#NAME?,non_debt,-
hadoop,6925,comment_4,Committed to trunk and branch-0.20. Thanks Todd.,non_debt,-
hadoop,6925,comment_5,"Forgot to mention that I verified TestCodec passed on trunk and the merge to branch-0.21. Previous comment has a typo, meant *branch-0.21* not branch-0.20.",documentation_debt,low_quality_documentation
hadoop,6925,comment_7,"Nope, concatenation is still broken. Ah, well.",non_debt,-
hadoop,6936,summary,broken links in,documentation_debt,low_quality_documentation
hadoop,6936,description,has links to : both of which are 404 as of time of filing this issue.,documentation_debt,low_quality_documentation
hadoop,6936,comment_0,another 404: has links to:,non_debt,-
hadoop,6936,comment_1,Not sure where went to or was replaced by: the closest existing page I could find was:,non_debt,-
hadoop,6936,comment_2,Not sure where went to or what it was replaced by. The closest existing page I could find was:,non_debt,-
hadoop,6936,comment_3,A possible replacement as a link target:,non_debt,-
hadoop,6936,comment_4,Please feel free to go ahead replacing the links. You may sign up a Wiki account and edit it directly.,non_debt,-
hadoop,6936,comment_5,"Fixed the FAQ links. The HowToConfigure page is highly outdated. I do not know if it still serves a purpose beyond the tutorial (which explains how to too). It could be possibly set up to redirect to a current documentation. But as Nicholas notes, this is all doable by anyone signed into the Wiki (free account signups, just gotta enter captchas). Filing a JIRA for this is not required.",documentation_debt,outdated_documentation
hadoop,6936,comment_6,"Just for a history lesson ref. here: There used to be hadoop-*.xml files once upon a time. Its now split over to core-*, hdfs-*, mapred-* files (* - Closing as the HowToConfigure link has also been updated by me. Although it needs more love in general (We should switch to confluence its more encouraging).",non_debt,-
hadoop,6936,comment_7,"Failed to add, link that has proper sublinks is:",non_debt,-
hadoop,6950,summary,Suggest that HADOOP_CLASSPATH should be preserved in,non_debt,-
hadoop,6950,description,"HADOOP_CLASSPATH tends to be used to add to bin/hadoop's classpath. Because of the way the comment is written, administrator's who customize hadoop-env.sh often inadvertently disable user's abilities to use it, by not including the present value of the variable. I propose we change the commented out suggestion code to include the present value.",non_debt,-
hadoop,6950,comment_0,Patch attached. This is a comment/doc change.,non_debt,-
hadoop,6950,comment_1,+1 Seems reasonable. Thanks Philip.,non_debt,-
hadoop,6950,comment_2,I've committed this. Thanks Philip.,non_debt,-
hadoop,6970,summary,SecurityAuth.audit should be generated under /build,non_debt,-
hadoop,6970,description,"SecurityAuth.audit is generated under currently root project directory whenever I run anything, and is not being cleaned up by the clean target. It should be created under build directory instead.",non_debt,-
hadoop,6970,comment_0,"Actually SecurityAuth.audit is a log file and should be generated under logs directory. Current problem is with the configuration, which by default creates the file in the current directory. Attached patch should fix that. If one is running test - no SecurityAuth.audit will be created at all. But if you start namenode - then the file will be created in the logs directory.",non_debt,-
hadoop,6970,comment_2,+1 Worked for me. It looks like the same patch applies cleanly to 0.22. Should not be a problem committing. We probably need similar jiras for hdfs and mapreduce.,non_debt,-
hadoop,6970,comment_5,"Ok, I will commit it to trunk and 0.22. I don't think this is needed for hdfs or mapreduce because they don't have log4j conf file.",non_debt,-
hadoop,6988,summary,Add support for reading multiple hadoop delegation token files,non_debt,-
hadoop,6988,description,"It would be nice if there were a way to specify multiple delegation token files via the environment variable and the configuration value. I suggest a colon-separated list of paths, each of which is read as a separate delegation token file.",non_debt,-
hadoop,6988,comment_0,This seems like a great way to inject tokens into a process that it shouldn't have.,non_debt,-
hadoop,6988,comment_1,"At a minimum, -1 on the environment variable. Shouldn't HADOOP_CLIENT_OPTS be sufficient for passing extra -D params? We have an abundance of environment variables that users can't handle as it is.",code_debt,low_quality_code
hadoop,6988,comment_2,"Thanks for the comments, Allen. The environment variable already exists in hadoop trunk; it's just only capable of specifying a single path to a token file. I'd like to be able to specify multiple token files via this variable. This is really only for convenience, as it's entirely possible to stuff multiple delegation token objects into a single credentials object, which is then serialized to a file. I considered creating a tool which would be capable of merging multiple delegation token files into one, but this seemed like a cleaner solution.",non_debt,-
hadoop,6988,comment_3,"Grr. I really wish we'd stop creating pet environment variables. This is ridiculous. Can we remove this env var as part of this JIRA? What takes precendence the env var or the jobconf setting? What is the interaction? If the answer is ""we have to look at the code"" then we've failed. It makes much more sense to have to support a comma delimited set (to be consistent with the rest of the job conf. Never mind that colon is the traditional directory delimiter on OS X.)",code_debt,low_quality_code
hadoop,6988,comment_4,"The environment variable should *not* be multi-valued. It is used to communicate the job's token store to sub-processes of the task. Since a task can't be in more than one job, there isn't any need. What is the use case for having multiple token files? The rest of the lists use commas, so this should be the same. Wouldn't it be easier to write a tool that allows you to combine multiple token files together into a single one?",code_debt,low_quality_code
hadoop,6988,comment_5,"Allen: I agree with Owen - it doesn't make sense to remove this environment variable. It is primarily used internally, though its use is documented in hdfs_user_guide.xml and The interaction between this and the conf var is that all tokens specified via either method are added to a single credentials object - nothing's going to take precedence. Owen: the motivation for this change is not with regard to passing delegation tokens from job to tasks, but rather with submitting jobs in the first place, which is another use for I'd like to be able to specify multiple tokens (gotten from fetchdt, or via other means) which a job could use to, for example, authenticate to multiple NNs and JTs. I considered creating a tool which would be capable of merging multiple delegation token files into one, but this seemed like a cleaner solution. Good point on making this comma-separated. I'll definitely do that.",non_debt,-
hadoop,6988,comment_6,"PLEASE, DO NOT REMOVE THIS VARIABLE: Oozie relies on this variable to be able to dispatch actions. Oozie uses a launcher MR job to start all those action types. This launcher MR job uses the variable to propagate the delegation token to the code that launches the corresponding action.",non_debt,-
hadoop,6988,comment_7,Adding support for being interpreted as a comma-separated list of paths to delegation token files.,non_debt,-
hadoop,6988,comment_8,"Same patch, this time with -p0.",non_debt,-
hadoop,6988,comment_9,"I'm sorry, but it is completely short sighted to have a single use env var like this. If we need to modify the tasks environment for something else, are we going to introduce another environment variable? How many are too many?",design_debt,non-optimal_design
hadoop,6988,comment_10,"Being able to specify both MR and HDFS delegation token files upfront when submitting a job seems reasonable, avoids requiring all clients kinit or use a tool to merge token files. Env variable philosophy aside (Allen, want to open a jira with an alternative?) does anyone object to modifying this existing variable so more than one file can be specified?",non_debt,-
hadoop,6988,comment_11,The intent of the environment variable is *not* for job submission. I really don't see any value in making it multi-valued.,non_debt,-
hadoop,6988,comment_12,"Even if it's not intended for job submission, it's documented as being the preferred way to pass token files to bin/hadoop for the purpose of running HDFS commands. From the HDFS user guide: Thus, this change could be useful for any HDFS command which is capable of communicating with multiple distinct NNs.",non_debt,-
hadoop,6988,comment_13,"Although it is true that can be used to make normal hdfs commands work, the intent for having this was to support security for Map/Reduce tasks, and, hadoop streaming apps that internally invoke command-line hdfs operations (as Owen had pointed out earlier). If you want to pass multiple tokens during job submission, the preferred approach would be to write the tokens into a file (using the Credentials class's utilities), and then point to that file. Thinking about it, wouldn't the option of defining in the job configuration work for you. The JobClient will automatically get delegation tokens from those namenodes and all tasks of the job can use those tokens..",non_debt,-
hadoop,6988,comment_14,"Thanks for the thoughtful comments, Devaraj. As I said earlier, this is really only for convenience, as it's entirely possible to stuff multiple delegation token objects into a single credentials object, which is then serialized to a file. I considered creating a tool which would be capable of merging multiple delegation token files into one, but this seemed like a cleaner solution. Rather than having every script/job/program that wants to pass multiple delegation token files first invoke some command to merge them, just specify them all via the method that already exists. The problem with specifying for my particular use-case is that delegation tokens can't be fetched if the application which is submitting the job is only authenticated via a delegation token in the first place. That said, I see this issue as being largely orthogonal from the core question of whether or not it is reasonable to want to specify multiple delegation token files via the system that already exists.",non_debt,-
hadoop,6988,comment_15,"Looks like we won't be reaching consensus on this issue. Closing out as it has long ago gone stale. Thanks for the thoughtful comments, everyone.",non_debt,-
hadoop,7007,summary,update the hudson-test-patch target to work with the latest test-patch script.,non_debt,-
hadoop,7007,description,The hudson-test-patch target has to be updated to work with the current test-patch.sh script. Since the callback login in the test-patch.sh is removed. by hadoop-7005,non_debt,-
hadoop,7007,comment_0,updating the jira with the right set to version and component,non_debt,-
hadoop,7007,comment_1,"- is for the build.xml for common, hdfs and mapreduce projects - is just for the common project, as test-patch.sh is reference by hdfs and mr by svn:extenals.",non_debt,-
hadoop,7007,comment_2,"+1, looks good. Note that the unified patch (HADOOP-7007.patch) shouldn't be applied.",non_debt,-
hadoop,7007,comment_3,Patch committed.,non_debt,-
hadoop,7013,summary,Add boolean field isCorrupt to BlockLocation,non_debt,-
hadoop,7013,description,"This is needed to allow to notify the calling application when returning a BlockLocation that corresponds to a corrupt block. Currently, this happens when there are no uncorrupted replicas of a requested block.",non_debt,-
hadoop,7013,comment_0,Review board:,non_debt,-
hadoop,7013,comment_1,"The patch looks good except for an incompatible change. BlockLocation is not used in any wire-protocol, but for some reason it is Writable. So if by any chance somebody serialize this object in a file etc, then this patch makes it incompatible. I would suggest that this patch does not change the serialization format. Since Hadoop does not use it over the wire, it won't do any harm.",non_debt,-
hadoop,7013,comment_2,"On a second thought, Hadoop does not have a story for Writable backward compatibility yet. So this patch does not need to take care of this either. Instead we could simply mark this jira as incompatible. So a +1 from me.",non_debt,-
hadoop,7013,comment_3,ant test-path results: Tests are included with HDFS-1483. The findbugs warning are not caused by this patch (see MAPREDUCE-2172 for details).,non_debt,-
hadoop,7013,comment_4,+1 Patch looks good to me.,non_debt,-
hadoop,7013,comment_5,"I've just committed this. Thanks, Patrick!",non_debt,-
hadoop,7057,summary,IOUtils.readFully and IOUtils.skipFully have typo in exception creation's message,documentation_debt,low_quality_documentation
hadoop,7057,comment_1,For apparent reasons - typo correction - patch doesn't provide any tests. I'm going to commit this for it is trivial.,documentation_debt,low_quality_documentation
hadoop,7057,comment_2,I have just committed this.,non_debt,-
hadoop,7072,summary,Remove java5 dependencies from build,non_debt,-
hadoop,7072,description,As the first short-term step let's remove JDK5 dependency from build(s),non_debt,-
hadoop,7072,comment_0,Trunk fix. and docs now works without -Djava5.home,non_debt,-
hadoop,7072,comment_1,Checking,non_debt,-
hadoop,7072,comment_2,Thx Cos. FWIW the precommit testing doesn't work so well when the patch changes the test-patch.sh Note that changing the params to test-patch.sh will require a synchronized update across all 3 project build files since this script is shared (via svn externals) across projects. I'll review the patch tomorrow.,non_debt,-
hadoop,7072,comment_3,"Thanks, Nigel: I am aware about the externals. I will do the rest of the projects tomorrow: was a long day already.",non_debt,-
hadoop,7072,comment_5,Anyone cares to review?,non_debt,-
hadoop,7072,comment_6,+1. Looks good. Will need to do this for the top level site too,non_debt,-
hadoop,7072,comment_7,"Thanks for the review, Nige. Let's do top-level site as a separate JIRA. I'll open one and link it here.",non_debt,-
hadoop,7072,comment_8,the same for 0.20,non_debt,-
hadoop,7072,comment_9,"I have committed this to the trunk and 0.21, 0.22 branches",non_debt,-
hadoop,7072,comment_10,Fixing incorrect count of parameters.,non_debt,-
hadoop,7072,comment_11,"should this have gone to 20 and 21? It's listed as an improvement, not a bug fix, which I think is a correct description. Also, does Nigel want this in 22? As RM, he should be pulling things in or asking explicitly for them to be included in the branch.",non_debt,-
hadoop,7072,comment_12,"The fix has been reviewed by Nigel (RM for 0.22) as it is (e.g. with 0.20.3 as an oldest affected branch). I am not going to commit to 0.20 though because I am not sure if we are going to make any more 0.20 release - thus this fix doesn't seem important enough to be included there. However, I have added the patch if someone needs it.",non_debt,-
hadoop,7072,comment_15,test-patch.sh has a problem. See HADOOP-7120.,non_debt,-
hadoop,7091,summary,reloginFromKeytab() should happen even if TGT can't be found,non_debt,-
hadoop,7091,description,"HADOOP-6965 introduced a getTGT() method and prevents reloginFromKeytab() from happening when TGT is not found. This results in the RPC layer not being able to refresh TGT after TGT expires. The reason is RPC layer only does relogin when the expired TGT is used and an exception is thrown. However, when that happens, the expired TGT will be removed from Subject. Therefore, getTGT() will return null and relogin will not be performed. We observed, for example, JT will not be able to re-connect to NN after TGT expires.",non_debt,-
hadoop,7091,comment_0,"Attaching a patch that allows reloginFromKeytab() to happen when getTGT() returns null. Also, making getTGT() a synchronized method.",non_debt,-
hadoop,7091,comment_1,+1 pending test results.,non_debt,-
hadoop,7091,comment_3,"Todd, thanks for the review. Can't write unit tests for it, but we have verified the patch on clusters at Yahoo.",non_debt,-
hadoop,7091,comment_4,I've committed this based on Todd's review. Resolving as fixed.,non_debt,-
hadoop,7091,comment_7,"Hi Kan, should this have gone in the 0.22 branch as well?",non_debt,-
hadoop,7091,comment_8,Yes. Can you help me get it committed? Thanks!,non_debt,-
hadoop,7091,comment_9,Committed to 0.22 branch and moved it in CHANGES.txt,non_debt,-
hadoop,7098,summary,tasktracker property not set in conf/hadoop-env.sh,non_debt,-
hadoop,7098,description,"For all cluster components, except TaskTracker the OPTS environment variable is set like this in hadoop-env.sh: export HADOOP_<COMPONENT The provided patch fixes this.",non_debt,-
hadoop,7098,comment_0,"+1 The lack of a definition looks like an oversight (these variables were introduced in HADOOP-2551). is already honoured by bin/mapred, so no changes are needed there. Bernd, have you tested this manually? There's currently no easy way to write an automated test for this change.",test_debt,lack_of_tests
hadoop,7098,comment_1,"I did test this manually indeed. The patch is in active use in my env. There are things like which might help setting up a test, but just for this small improvement it seem like overkill.",test_debt,lack_of_tests
hadoop,7098,comment_3,Good to know. Regenerating patch from top level so Hudson can check it.,non_debt,-
hadoop,7098,comment_5,I've just committed this. Thanks Bernd!,non_debt,-
hadoop,7118,summary,NPE in,non_debt,-
hadoop,7118,description,"In HADOOP-7082 I stupidly introduced a regression whereby will throw an NPE if it is called before any .get() call is made, since the properties member is not initialized. This is causing a failure in on my box, but doesn't appear to trigger any failures in the non-contrib tests since .get() is usually called first. This JIRA is to fix the bug and add a unit test for writeXml in common (apparently it never had a unit test)",test_debt,lack_of_tests
hadoop,7118,comment_1,"Woops, patch was against yesterday's common tree, didn't apply. Take two.",non_debt,-
hadoop,7118,comment_3,"+1 LGTM. I would also convert the test to JUnitv4 but one can't have everything, I guess ;)",test_debt,expensive_tests
hadoop,7118,comment_4,"Committed, thanks for review Cos.",non_debt,-
hadoop,7154,summary,Should set MALLOC_ARENA_MAX in hadoop-config.sh,non_debt,-
hadoop,7154,description,"New versions of glibc present in RHEL6 include a new arena allocator design. In several clusters we've seen this new allocator cause huge amounts of virtual memory to be used, since when multiple threads perform allocations, they each get their own memory arena. On a 64-bit system, these arenas are 64M mappings, and the maximum number of arenas is 8 times the number of cores. We've observed a DN process using 14GB of vmem for only 300M of resident set. This causes all kinds of nasty issues for obvious reasons. Setting MALLOC_ARENA_MAX to a low number will restrict the number of memory arenas and bound the virtual memory, with no noticeable downside in performance - we've been recommending MALLOC_ARENA_MAX=4. We should set this in hadoop-env.sh to avoid this issue as RHEL6 becomes more and more common.",non_debt,-
hadoop,7154,comment_0,"We should also consider adding a ""ulimit -u <some high number Without this option set, I find I get ""Unable to create native thread"" errors under any heavy MR load. Clearly we to wrap this stuff so that it won't cause problems on non-RHEL6 operating systems.",non_debt,-
hadoop,7154,comment_1,Attached patch only does the MALLOC_ARENA_MAX bit. The ulimit fix is tougher since it appears to not be very portable.,non_debt,-
hadoop,7154,comment_2,1,non_debt,-
hadoop,7154,comment_3,Committed to trunk and branch-22. Thanks Tom for reviewing.,non_debt,-
hadoop,7154,comment_7,"Committed to branch-1.1, where it is also very important. Merged to branch-1, and branch-1.0. Thanks, Todd!",non_debt,-
hadoop,7154,comment_8,changed title to reflect the change actually done in the patch.,non_debt,-
hadoop,7154,comment_9,"For the record, the memory issue also arises on ubuntu 12 64-bits. 'export MALLOC_ARENA_MAX=4' also fixes it on ubuntu 12.",non_debt,-
hadoop,7154,comment_10,"I was very confused by this discussion and dug into it a bit more; here's what I learned. The takeaway is, ARENA_MAX=4 is a win for Java apps. # Java doesn't use {{malloc()}} for object allocations; instead it uses its own directly {{mmap()}}ed arenas. # however, a few things such as direct {{ByteBuffer}}s do end up calling malloc on arbitrary threads. There's not much thread locality in the use of such buffers. As a result, the glibc arena allocator is using a lot of VSS to optimize a codepath that's not very hot. So decreasing the number of arenas is a win, overall, even though it will increase contention (the malloc arena locks are pretty cold so this doesn't matter much) and potentially increase cache churn. But fewer arenas should decrease total cache footprint by increasing reuse.",design_debt,non-optimal_design
hadoop,7154,comment_11,"I wonder if fixes everything More test on my ubuntu12 laptop dev env (with export MALLOC_ARENA_MAX=4 in .bashrc): [1] (Pi Estimator) and [2] (RandomWriter) complete successfully. [3] DistributedShell (see 'is running beyond virtual memory limits' message): 12/07/25 14:23:15 INFO Got application report from ASM for, appId=3, clientToken=null, failed 1 times due to AM Container for exited with exitCode: 143 due to: Container is running beyond virtual memory limits. Current usage: 82.8mb of 128.0mb physical memory used; 873.6mb of 268.8mb virtual memory used. Killing container. Dump of the process-tree for : |- PID PPID PGRPID SESSID CMD_NAME SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE |- 31510 28157 31510 31510 (bash) 0 1 17031168 369 /bin/bash -c -Xmx128m --container_memory 10 --num_containers 1 --priority 0 --shell_command ls --shell_args / 1 |- 31514 31510 31510 31510 (java) 139 10 899014656 20830 -Xmx128m --container_memory 10 --num_containers 1 --priority 0 --shell_command ls --shell_args / .Failing this attempt.. Failing the application., appMasterHost=, appQueue=default, appMasterRpcPort=0, appTrackingUrl=, appUser=echarles Looking at the Yarn tmp files, I see that MALLOC_ARENA_MAX=4 is effectively exported: more | grep MALLOC export Thx, Eric [1] hadoop jar pi 100 100 [2] hadoop jar randomwriter randomwriter-out [3] hadoop jar -jar -shell_command pwd -num_containers 1",non_debt,-
hadoop,7154,comment_12,Closed upon release of Hadoop-1.0.4.,non_debt,-
hadoop,7154,comment_13,"- I know this bug is pretty old, but do you mind doing me the favor of explaining this statement: Perhaps I am revealing too much of my naivety, but what issues the vmem size presents nor the reasons are necessarily obvious to me. The reason I ask is not directly related to this JIRA nor even Hadoop. I am just trying to learn more about the glibc change and its potential impacts. I've noticed high virtual memory size in another Java-based application (a Zabbix agent process if you care) and I'm struggling slightly to decide if I should worry about it. presents what appears to me to be a rational explanation as to why the virtual memory size shouldn't matter too much. I could push on Zabbix to implement a change to set MALLOC_ARENA_MAX and I feel relatively confident the change wouldn't hurt anything but I'm not sure it would actually help anything either. The Zabbix agent appears to be performing fine and the only reason I noticed the high vmem size was because someone pointed me to this JIRA and I did an audit looking for processes with virtual memory sizes that looked suspicious. I guess the biggest problem I have with the affect the glibc change has on reported vmem size is that it seems to make vmem size meaningless where previously you could get some idea about what a process was doing from its vmem size but your comment suggests maybe there are other things I should be concerned about as well. If you could share those with me I would greatly appreciate it and perhaps others will benefit as well. Thanks!",defect_debt,uncorrected_known_defects
hadoop,7154,comment_14,"Ok, so after further consideration I think my last comment/question was probably somewhat silly. I think the problems the high vmem sizes present to Hadoop are probably obvious to many as Todd originally suggested. I feel sort of dumb for not realizing more quickly. MapReduce (and YARN) monitor virtual memory sizes of task processes and kill them when they get too big. For example, controls the max virtual memory size of a map task. WIthout MALLOC_ARENA_MAX this would be broken since tasks would have super inflated vmem sizes.  - do I have that about right? Are there other types of problems you were noticing? Basically it seems any piece of software that tries to make decisions based on process vmem size is going to be messed up by the glibc change and likely has to implement MALLOC_ARENA_MAX. For some reason the fact that Hadoop was making such decisions was escaping me when I made my last comment.",non_debt,-
hadoop,7154,comment_15,"yep, you got it. The issues are specific to how Hadoop monitors its tasks, etc. Using lots of vmem on a 64-bit system isn't problematic in and of itself.",non_debt,-
hadoop,7154,comment_16,Thanks for the confirmation Todd!,non_debt,-
hadoop,7154,comment_17,"There might be other environment settings that should be tuned besides MALLOC_ARENA_MAX. The [mallopt man (""man mallopt"") contains an important notice about dynamic mmap threshold in glibc malloc. More information . I'd suggest disabling dynamic mmap threshold by setting these environment variables (besided MALLOC_ARENA_MAX): That would prevent memory fragmentation by keeping using mmap for memory allocations that are over 128K. (the default before dynamic mmap behaviour was introduced)",non_debt,-
hadoop,7154,comment_18,"A note about MALLOC_ARENA_MAX: MALLOC_ARENA_MAX is broken on glibc < 2.15 (like Ubuntu 10.04) . The fix was made for 2.16 and backported to 2.15 . MALLOC_ARENA_MAX doesn't work on Ubuntu 10.04 because of [this The same bug seems to be reported to Redhat as . Other reports: , , . This is the commit to glibc fixing the bug: (backport for 2.15 is ).",non_debt,-
hadoop,7161,summary,Remove unnecessary oro package from dependency management section,build_debt,over-declared_dependencies
hadoop,7161,description,"Best I can tell we never use the ""oro"" dependency, but it's been in ivy.xml forever. Does anyone know any reason we might need it?",build_debt,over-declared_dependencies
hadoop,7161,comment_0,"oro is a transitive dependency from commons-net. Feel free to remove it from ivy.xml though, as it'll get pulled in anyway :)",build_debt,over-declared_dependencies
hadoop,7161,comment_1,"-01 Remove Jakarta Oro from pom - remove oro from dependency management HADOOP-8278 removed Oro from the dependency list for hadoop-common, but left it in the dependency management section of hadoop-project. commons-net stopped using oro in 2.x (and we rely on a much later version).",build_debt,over-declared_dependencies
hadoop,7161,comment_3,no new tests because it's a pom-only change.,non_debt,-
hadoop,7161,comment_4,"+1, committing this.",non_debt,-
hadoop,7161,comment_5,"Committed this to trunk, branch-2, and branch-2.8. Thanks  for the contribution!",non_debt,-
hadoop,7165,summary,filter) is not redefined in FilterFs,non_debt,-
hadoop,7165,description,"filter) is not redefined in FilterFs. So if a job client uses a FilterFs to talk to NameNode, it does not trigger the bulk location optimization.",non_debt,-
hadoop,7165,comment_0,Here is a straightforward fix.,non_debt,-
hadoop,7165,comment_1,+1 patch looks good.,non_debt,-
hadoop,7165,comment_2,Rebased patch for trunk.,non_debt,-
hadoop,7165,comment_5,+1 lgtm. Committing this.,non_debt,-
hadoop,7165,comment_6,"Thanks to  for the original contribution,  for upmerging the patch, and  for additional review! I committed this to trunk and branch-2.",non_debt,-
hadoop,7170,summary,Support UGI in FileContext API,non_debt,-
hadoop,7170,description,The FileContext API needs to support UGI.,non_debt,-
hadoop,7170,comment_0,This is duplicate of HADOOP-7171.,non_debt,-
hadoop,7191,summary,BackUpNameNode is using 100% CPU and not accepting any requests.,non_debt,-
hadoop,7191,description,"In our environment, after 3 days long run Backup NameNode is using 100% CPU and not accepting any calls. *Thread dump* ""IPC Server Responder"" daemon prio=10 nid=0x3b2a runnable RUNNABLE at Method) locked at Looks like we are running into similar issue like this Jetty one.",non_debt,-
hadoop,7191,comment_0,Same thing is seen in MR. And I see it too for the RPC.,non_debt,-
hadoop,7191,comment_1,As per the patch in DIRMINA-678,non_debt,-
hadoop,7191,comment_3,"I've seen this in DataNode this time, similar to as described long time ago in HADOOP-3132. Thousands of DataXceiver threads are deadlocked in epollWait. Like below. Ramkrishna, is your patch intended to address this java bug and providing a workaround? Is there a way to test the condition (in any form)? Also we should probably rename this jira, as it has more general application.",non_debt,-
hadoop,7191,comment_4,"Afraid I'm going to wontfix this, Backup NN its not in active use any more.",non_debt,-
hadoop,7197,summary,Support for non-standard ssh port for slaves,non_debt,-
hadoop,7197,description,"I was trying to add a slave that ran sshd in a non-standard port (eg. 2222 in stead of 22), when I noticed that there was no way to support another port through the configuration for a single node. Supporting a different port for all the slaves is possible through the HADOOP_SSH_OPTS variable, but not for a single slave.",non_debt,-
hadoop,7197,comment_0,Changing the IFS variable to the new line permits the inclusion of ssh flags in the slaves file such as -p <port>,non_debt,-
hadoop,7197,comment_1,You can change ~/.ssh/config to point to a different port per host.,non_debt,-
hadoop,7197,comment_2,I think ~/.ssh/config changes make it a better approach for the site on the whole as well.,non_debt,-
hadoop,7294,summary,FileUtil uses wrong stat command for FreeBSD,non_debt,-
hadoop,7294,description,"I get next exception when try to use append: 2011-05-16 17:07:54,648 ERROR infoPort=50075, Failed to get link count on file message=null; error=stat: illegal option -- c; exit value=1 It seems that FreeBSD is treated like UNIX and so calls 'stat -c%h', while FreeBSD is much more like Mac (since they have same BSD roots): $ stat --help stat: illegal option -- - usage: stat [-FlLnqrsx] [-f format] [-t timefmt] [file ...] $ stat -f%l a_file 1",non_debt,-
hadoop,7294,comment_0,"Vitalli, AFAIK, most of the shell calls were written with Linux in mind, and that platform is what we recommend to use as well (so far). Would you have a solution that would work on all platforms for this? A patch would be great as well, but just tips otherwise would be nice too!",non_debt,-
hadoop,7294,comment_1,"Actually there is support for Windows and Mac in the code, so the thing to be changed is to rename Mac to BSD and treat FreeBSD as BSD. In general, I'd prefer it to be configurable.",non_debt,-
hadoop,7294,comment_2,"In general, we try to follow BSD semantics",non_debt,-
hadoop,7294,comment_3,"This is patch for 0.21 release. I did a branch in my own svn, so revisions are off.",non_debt,-
hadoop,7294,comment_5,"Canceling the patch as it is rather old. If you still want this patch to go in you are going to also need a patch that supports trunk, eventually.",non_debt,-
hadoop,7294,comment_6,Here is trunk commit from,non_debt,-
hadoop,7294,comment_7,Here is trunk patch from,non_debt,-
hadoop,7294,comment_8,"I've just committed a potentially conflict MAC specific patch to this file, HADOOP-7680, which hard codes in {{/usr/bin/stat}} as the path for stat. Vitalli, can we be confident that this path will always be the location of stat in BSD unix? It may be safer to have an explicit BSD case that is separate from the rest. We'll also need to retain the OS_TYPE_MAC enum for compatibility. There's no use of it in hadoop other than the {{HardLink}} file, but that doesn't mean others don't use it. The options then # Add a new OS enum for BSD, give it its own command (I prefer this) # Add a new OS enum for BSD, share the action with the MAC option # when BSD is found, just say its OS_TYPE_MAC even though it is the other way around I prefer the first",non_debt,-
hadoop,7294,comment_12,This issue is resolved by HADOOP-8811 Current code to get stat command is checking for FreeBSD: Closing this issue as duplicate of HADOOP-8811,non_debt,-
hadoop,7323,summary,Add capability to resolve compression codec based on codec name,non_debt,-
hadoop,7323,description,"When setting up a compression codec in an MR job the full class name of the codec must be used. To ease usability, compression codecs should be resolved by their codec name (ie 'gzip', 'deflate', 'zlib', 'bzip2') instead their full codec class name. Besides easy of use for Hadoop users who would use the codec alias instead the full codec class name, it could simplify how HBase resolves loads the codecs.",non_debt,-
hadoop,7323,comment_0,"The codec alias is taken from the Codec short class name (classname without package) after removing the 'Codec' postfix if present, else the full short class name is used as alias. The patch adds to the an alias lookup fallback if the codec class has not been found (this preserves existing behavior). The alias lookup is case-insensitive.",non_debt,-
hadoop,7323,comment_1,This looks good. The only nit I noticed is that the javadoc on codecsByName in is incorrect.,documentation_debt,low_quality_documentation
hadoop,7323,comment_2,"Thanks Tom. I've updated the Javadocs. I've also improved the alias resolution to resolve to the short class name, both with 'codec' ending and without 'codec' ending. This means that the aliases for GzipCodec are 'gzip' and 'gzipcodec' (both case insensitive)",non_debt,-
hadoop,7323,comment_4,1,non_debt,-
hadoop,7323,comment_5,"Thinking about this more, overloading may be misleading, so it might be better to add a new method called getCodecByName() which returns codecs based on class name or alias. There are only a couple of callers of (in HDFS) so it doesn't make much difference in terms of changing code to use the new method. To take advantage of the new method expressions of the form should be replaced with This mainly applies in the MapReduce project. We should also add a method at the same time, since sometimes only the class is needed.",non_debt,-
hadoop,7323,comment_6,"Tom, Thank for your feedback. Attached is patch that includes your suggestions.",non_debt,-
hadoop,7323,comment_8,And now a patch with all files :),non_debt,-
hadoop,7323,comment_10,"I've just committed this. Thanks, Alejandro!",non_debt,-
hadoop,7358,summary,Improve log levels when exceptions caught in RPC handler,design_debt,non-optimal_design
hadoop,7358,description,"When a server implementation throws an exception handling an RPC, the Handler thread catches it and logs it before responding with the exception over the IPC channel. This is currently done at INFO level. I'd like to propose that, if the exception is an unchecked exception, it should be logged at WARN level instead. This can help alert operators when they might be hitting some kind of bug.",design_debt,non-optimal_design
hadoop,7358,comment_0,Simple patch attached. No test case since it's just a log level change.,non_debt,-
hadoop,7358,comment_1,+1 pending hudson,non_debt,-
hadoop,7358,comment_4,"Todd, could you provide a patch for trunk as well.",non_debt,-
hadoop,7358,comment_5,"Hi Konstantin, Onbehalf of todd,I have just put the patch for trunk. Can you please take a look! Thanks Uma",non_debt,-
hadoop,7358,comment_7,I just committed this to trunk and 0.22 branch. Thank you Todd and Uma.,non_debt,-
hadoop,7358,comment_14,Marking for 23.,non_debt,-
hadoop,7358,comment_15,Trunk and 0.22 but not 0.23?,non_debt,-
hadoop,7358,comment_17,Merged to branch-0.23.,non_debt,-
hadoop,7375,summary,Add resolvePath method to FileContext,non_debt,-
hadoop,7375,description,None,non_debt,-
hadoop,7375,comment_0,AFS#getFileStatus is now called instead of which means fixRelativePart is no longer used to make the path absolute in FileContext relative to the working dir before passing the path to AFS right? Nit: lines 568 and 2231 need indenting. Otherwise looks great.,code_debt,low_quality_code
hadoop,7375,comment_2,+1 with Eli's comments addressed.,non_debt,-
hadoop,7375,comment_3,No sure if your question is with regards to all the methods in FileContext or the single method I changed. I have changed the FileContext#resolve to call AFS#resolvePath(p) which currently calls,non_debt,-
hadoop,7375,comment_4,"Ah, never mind, I thought it was previously calling FC#getFileStatus. +1 feel free to address the nits directly in the commit since it's just indentation.",code_debt,low_quality_code
hadoop,7375,comment_5,I've committed this. Thanks Sanjay.,non_debt,-
hadoop,7390,summary,VersionInfo not generated properly in git after unsplit,non_debt,-
hadoop,7390,description,"The version information generated during the build of common when running from git has revision and branch Unknown. I believe this started after the unsplit: revision=""Unknown"", branch=""Unknown"", user=""tgraves"", date=""Tue Jun 14 13:39:10 UTC 2011"", The script which generates the package-info.java file with the version info looks for the presence of .git directory and that is now a level up instead of in the common directory.",non_debt,-
hadoop,7390,comment_3,+1 downloaded and tested patch and it works now. Thanks!,non_debt,-
hadoop,7390,comment_4,"+1 - worked like a charm. I've just committed this. Thanks a lot, Todd.",non_debt,-
hadoop,7390,comment_7,This is a problem in 0.22 as well. Need to apply same patch.,non_debt,-
hadoop,7390,comment_8,Just committed this to 0.22 branch. Thank you Todd.,non_debt,-
hadoop,7483,summary,Multiple Java installed confuses Hadoop Debian package installer,non_debt,-
hadoop,7483,description,"When openjdk and sun java are both installed on the machine, Hadoop debian package can not determine correct Java to use.",non_debt,-
hadoop,7483,comment_0,"See HADOOP-6605. In summary: IMO Hadoop should auto-detect/prefer the Sun JDK 1.6 over others because it is required, however people veto'd that and would rather require users explicitly set JAVA_HOME (or remove the other versions of java installed on the host).",non_debt,-
hadoop,7483,comment_1,"Well, as allen said on HADOOP-6605, ""My technical objection is that there is a high likelihood of getting this wrong."" There's a serious risk of the JAVA_HOME autodetector becoming a major maintenance mess. This is the beginning.",design_debt,non-optimal_design
hadoop,7483,comment_2,"I don't see how this is the beginning. JAVA_HOME auto-detection is orthogonal to a java package dependency. Regardless of whether we auto-detect JAVA_HOME we need to deal with multiple javas installed on a host - even if we create a package dependency there may be others installed as well. CDH has had JAVA_HOME detection for years and it hasn't been a major maintenance mess, but that's a separate discussion.",non_debt,-
hadoop,7483,comment_3,"Should we move this issue to Apache Bigtop (TLP now)? I have not heard of such a problem existing in it currently, in its java-home-detector. So I'd rather close it out since Bigtop has, I think, superceded this upstream packaging effort overall.",non_debt,-
hadoop,7483,comment_4,"Things are always easier when you only run on a tiny set of platforms. :) Anyway, closing as stale.",non_debt,-
hadoop,7513,summary,mvn-deploy target fails,non_debt,-
hadoop,7513,description,"When executing mvn-deploy target, the build fails. hadoop-common and deploy, but the test jar does not. property staging is not set and/or set to false, meaning when you try to deploy a snapshot build. The error reads: Invalid reference: 'hadoop.core.test'.",non_debt,-
hadoop,7513,comment_0,Here is a larger snippet from the log: clean-sign: sign: Skipped because property 'staging' not set. signanddeploy: Skipped because property 'staging' not set. simpledeploy: ... [artifact:deploy] An error has occurred while processing the Maven artifact tasks. [artifact:deploy] Diagnosis: [artifact:deploy] [artifact:deploy] Invalid reference: 'hadoop.core.test' BUILD FAILED Invalid reference: 'hadoop.core.test' at,non_debt,-
hadoop,7513,comment_1,This is not applicable to trunk due to HADOOP-6671,non_debt,-
hadoop,7513,comment_2,"+1 patch looks good. Unfortunately, I can't validate it because maven wagon pluging doesn't exist locally and can't be in installed automatically because of some mvn repository issues not related to the patch. Will be committing it.",non_debt,-
hadoop,7513,comment_3,I have committed this. Thanks Joep!,non_debt,-
hadoop,7517,summary,hadoop common build fails creating docs,non_debt,-
hadoop,7517,description,post hadoop-6671 merge executing the following command fails on creating docs $MAVEN_HOME/bin/mvn clean verify findbugs:findbugs -DskipTests -Pbintar -Psrc -Pnative -Pdocs,non_debt,-
hadoop,7517,comment_0,It looks like Forrest isn't installed on the build machine.,non_debt,-
hadoop,7517,comment_1,I've just run your mvn command line invocation (except for -Pnative as I'm on Mac) and it worked. As Tom says it may be that Forrest is not installed. Can you please verify that?,non_debt,-
hadoop,7523,summary,Test fails due to,non_debt,-
hadoop,7523,description,"Test fails due to Here is the error message: Test set: 1, 0, Errors: 1, Skipped: 0, 0.232 sec <<< FAILURE! 0.075 sec <<< ERROR! boolean) Method) Method) This prevents a clean build.",non_debt,-
hadoop,7523,comment_0,"Updated the static inner class DontCheck in with an empty method delSrc, Path src, Path dst, boolean to fix the A code review is requested for this patch.",non_debt,-
hadoop,7523,comment_1,Patch looks good.,non_debt,-
hadoop,7523,comment_2,+1 This fixes the failing test and all other tests pass. Output from test-patch:,non_debt,-
hadoop,7523,comment_3,"I've just committed this. Thanks, John!",non_debt,-
hadoop,7525,summary,Make arguments to test-patch optional,non_debt,-
hadoop,7525,description,"Currently you have to specify all the arguments to test-patch.sh, which makes it cumbersome to use. We should make all arguments except the patch file optional.",non_debt,-
hadoop,7525,comment_0,It probably should use,non_debt,-
hadoop,7525,comment_1,Here's a patch for this. * I checked and test-patch.sh already uses * I removed the curl option since it wasn't used. * I also added a that allows the workspace to have uncommitted changes in it. This is useful if you need to move some files around in SVN before applying your patch. It's also useful for testing changes to test-patch.sh itself. Usage:,non_debt,-
hadoop,7525,comment_2,"+1, this change looks great, Tom. I only tested the script *without* the {{--jenkins}} option, but that appears to have worked flawlessly.",non_debt,-
hadoop,7525,comment_3,I've just committed this. I'll work on getting the Jenkins job running.,non_debt,-
hadoop,7551,summary,LocalDirAllocator should incorporate LocalStorage,non_debt,-
hadoop,7551,description,"The is not aware of (introduced in MAPREDUCE-2413) - it always considers the configured local dirs, not just the ones that happen to be good. Therefore if there's a disk failure then *every* call to get a local path will result in doing a disk check of *all* the configured local dirs. It seems like LocalStorage should be a private class to LocalAllocator so that all users of LocalDirAllocator benefit from the disk failure handling and all the various users of LocalDirAllocator don't have to be modified to handle disk failures. Note that LocalDirAllocator already handles faulty directories.",non_debt,-
hadoop,7551,comment_0,is actually updating the savedLocalDirs everytime conf is changed. So the main concern raised in the above description i.e. every call to confChanged is not resulting into disk checks (except once per configuration change i.e. once per new bad disk).,non_debt,-
hadoop,7551,comment_1,I mean the concern seems to be not valid.,non_debt,-
hadoop,7551,comment_2,"Per MAPREDUCE-3011 the if the conf given to the LocalDirAllocator is always updated by callers (eg with input based on LocalStorage), as is currently the case, it doesn't need to be aware. It would be good if LocalDirAllocator didn't need to be given a new conf but combining LocalDirAllocator and LocalStorage is probably too invasive for a stable release. Closing as won't fix.",non_debt,-
hadoop,7609,summary,Debian package shows invalid hdfs user,non_debt,-
hadoop,7609,description,"First time install debian package on Debian machine, there is a error message showing: invalid hdfs user. invalid mapred user. Looks like the users are not created during the installation. Not sure if this is EC2 related or debian related. Investigating...",non_debt,-
hadoop,7609,comment_0,"Giri, I verified this issue is isolated to the AMI image that you used on EC2, not an actual problem in Debian package.",non_debt,-
hadoop,7609,comment_1,This is not a real problem.,non_debt,-
hadoop,7610,summary,/etc/profile.d does not exist on Debian,non_debt,-
hadoop,7610,description,"As part of post installation script, there is a symlink created in to source Therefore, users do not need to configure HADOOP_* environment. Unfortunately, /etc/profile.d only exists in Ubuntu. [Section 9.9 of the Debian states: Hence the default environment setup should skip for Debian.",non_debt,-
hadoop,7610,comment_0,Detect /etc/profile.d prior to making hadoop-env.sh as part of default environment.,non_debt,-
hadoop,7610,comment_2,+1 looks good,non_debt,-
hadoop,7610,comment_3,"Thanks Eric, I just committed this patch to trunk and 20-security branch",non_debt,-
hadoop,7610,comment_9,Closed upon release of 0.20.205.0,non_debt,-
hadoop,7620,summary,All profiles should build the javadoc,non_debt,-
hadoop,7620,description,"Currently, the default profile doesn't generate the javadoc, which gives the developer a false sense of security. Leaving the forrest stuff in the doc profile makes sense.",design_debt,non-optimal_design
hadoop,7620,comment_0,"Javadocs take time, if you are doing a build for a devel test, you don't want to wait for javadocs.",non_debt,-
hadoop,7620,comment_1,"They don't take that much time and the developer absolutely should check the result of their changes to the javadoc. Having the equivalent of -DskipTests=true is fine, but by default it should generate the javadoc.",non_debt,-
hadoop,7620,comment_2,"I agree with Alejandro. Build turnaround time is a big pain, and especially on non-SSD, or even worse, NFS, the javadoc build takes quite a while. Lots of iops to create all those .html files. To check that the javadoc changes don't introduce issues, we have test-patch.",design_debt,non-optimal_design
hadoop,7620,comment_3,Closing as won't fix. test-patch has been much better about testing the javadoc for a while now.,non_debt,-
hadoop,7632,summary,NPE in copyToLocal,non_debt,-
hadoop,7632,description,[todd@c0309 hadoop-trunk-home]$ ./bin/hadoop fs -copyToLocal /tmp/meta/ copyToLocal: Fatal internal error,non_debt,-
hadoop,7632,comment_0,"Hi Todd, By looking the trace, I think the problem exist here. In PathData, it is just ignoring the and returning null. Thanks Uma",non_debt,-
hadoop,7632,comment_1,It's actually a bit more than that. The recursion is handled incorrectly in some cases. I've got a really old patch for this that I can put once 205 work is complete.,non_debt,-
hadoop,7632,comment_2,Thanks Daryn!,non_debt,-
hadoop,7632,comment_3,same issue,non_debt,-
hadoop,7634,summary,Cluster setup docs specify wrong owner for task-controller.cfg,documentation_debt,low_quality_documentation
hadoop,7634,description,The cluster setup docs indicate task-controller.cfg must be owned by the user running TaskTracker but the code checks for root. We should update the docs to reflect the real requirement.,documentation_debt,outdated_documentation
hadoop,7634,comment_0,Patch attached.,non_debt,-
hadoop,7634,comment_1,"+1. Note that the file can actually be owned by anyone, it just can't be writable by anyone but root. The error message displayed is correct:",non_debt,-
hadoop,7634,comment_2,"If the file can be owned by anyone should we update the error message to not indicate ""Must be owned by root""?",non_debt,-
hadoop,7634,comment_3,"Sorry, Eli - I should have said ""group-owned by anyone."" You are right that it must be owned by root. The error message really is completely correct: ""Must be owned by root and not writable by group or other.""",non_debt,-
hadoop,7634,comment_4,Thanks atm. I've committed this.,non_debt,-
hadoop,7634,comment_5,Closed upon release of Hadoop-1.1.0.,non_debt,-
hadoop,7640,summary,PluginDispatcher should identify which class could not be found,non_debt,-
hadoop,7640,description,"Right now, you get a rather generic: ""Unable to load plugins"" This is usually due to class not found issues. It would be helpful to identify the specific class that could not be found.",non_debt,-
hadoop,7640,comment_0,Depends on HADOOP-5640 which isn't in trunk.,non_debt,-
hadoop,7641,summary,Add Apache License to template config files,non_debt,-
hadoop,7641,description,Files in don't have Apache Software License in the header.,non_debt,-
hadoop,7641,comment_0,Add Apache License to config files.,non_debt,-
hadoop,7641,comment_2,"Patch looks good to me except that it's now 2011, not 2010. :) I'll commit it once that's fixed.",non_debt,-
hadoop,7641,comment_3,Updated year to 2011.,non_debt,-
hadoop,7641,comment_5,"I've just committed this. Thanks a lot for getting this taken care of, Eric.",non_debt,-
hadoop,7684,summary,jobhistory server and secondarynamenode should have init.d script,non_debt,-
hadoop,7684,description,The current set of init.d scripts can start/stop process for: namenode datanode jobtracker tasktracker It is missing init.d scripts for: secondarynamenode jobhistory,non_debt,-
hadoop,7684,comment_0,Added init.d script for branch-20-security,non_debt,-
hadoop,7684,comment_1,Updated patch for branch-20-security branch.,non_debt,-
hadoop,7684,comment_2,Updated secondary namenode and history server init.d script for trunk.,non_debt,-
hadoop,7684,comment_4,"Eric, 0.23 has RM, NM & MR JobHistoryServer...",non_debt,-
hadoop,7684,comment_5,- Updated the trunk version to match yarn-daemon.sh. - Added script for node manager and resource manager.,non_debt,-
hadoop,7684,comment_7,rpm package doesnt seem to include the historyserver and secondarynamenode init scripts. looks like and init.d script should be added to the list of init.d scripts in the spec file.,code_debt,low_quality_code
hadoop,7684,comment_8,Updated patch to include changes in .spec file for 20-security branch.,non_debt,-
hadoop,7684,comment_9,Updated trunk to reflect changes required in rpm spec file.,non_debt,-
hadoop,7684,comment_11,"Committed to 0.20-security and 0.20.205. Thanks, Eric!",non_debt,-
hadoop,7684,comment_12,+1 verified the patch with rpm installation on a single node cluster. singlenode setup script requires a fix to create the required mapred dirs in the hdfs fs. Will file a separate jira for that. filed jira - HADOOP-7692 deb pkg installation fails on ubuntu as the groupid 114 is used by gdm. jira: HADOOP-7691,non_debt,-
hadoop,7684,comment_13,"I just committed this to trunk and branch 0.23, thanks Matt and Giri.",non_debt,-
hadoop,7684,comment_21,Closed upon release of 0.20.205.0,non_debt,-
hadoop,7766,summary,"The auth to local mappings are not being respected, with webhdfs and security enabled.",non_debt,-
hadoop,7766,description,reloads the KerberosName statically and overrides the auth to local mappings.,non_debt,-
hadoop,7766,comment_0,This jira is common component of HDFS-2411.,non_debt,-
hadoop,7766,comment_2,1,non_debt,-
hadoop,7766,comment_6,Merged to 0.23.,non_debt,-
hadoop,7766,comment_10,This is committed.,non_debt,-
hadoop,7769,summary,TestJMXJsonServlet is failing,non_debt,-
hadoop,7769,description,"The ""testQury"" (sic) test is failing with",non_debt,-
hadoop,7769,comment_0,The refactor in HADOOP-7704 caused the failure I think. Adding a flush fixes the test.,non_debt,-
hadoop,7769,comment_1,+1 looks good to me.,non_debt,-
hadoop,7769,comment_3,I've just committed this. (The test failure Jenkins is reporting is due to HADOOP-7768. I successfully ran the unit test this is fixing on my machine.),non_debt,-
hadoop,7786,summary,Remove HDFS-specific configuration keys defined in FsConfig,non_debt,-
hadoop,7786,description,"HADOOP-4952 added a couple HDFS-specific configuration values to common (the block size and the replication factor) that conflict with the HDFS values (eg have the wrong defaults, wrong key name), are not used by common or hdfs and should be removed. After removing these I noticed the rest of FsConfig is only used once outside a test, and isn't tagged as a public API, I think we can remove it entirely.",code_debt,low_quality_code
hadoop,7786,comment_0,No test included because this is basically a config key value change.,non_debt,-
hadoop,7786,comment_1,"Hey Abel, thanks a lot for the patch. It looks pretty good to me, but lets see what the QA bot has to say. One question - since, this patch changes the value of did you check if that is referenced anywhere in the Common project? If so, where? And did you try running the common unit tests with this patch as well? Since this is an HDFS JIRA, the QA bot will only run the HDFS tests, not the common ones.",non_debt,-
hadoop,7786,comment_3,"Hey Aaron, is not being used in the Common project.",non_debt,-
hadoop,7786,comment_4,Note: is only being used in the Test in the Mapreduce project. I ran that test all is good :),non_debt,-
hadoop,7786,comment_5,"Thanks a lot for checking on that, Abel. That seems fine to change. The patch application failed with the following error ""Looks like this is a cross-subproject patch. Not supported!"" This happened because this change affects both the Common and HDFS subprojects. Unfortunately, the QA bot can't presently handle this, so you'll need to run test-patch yourself and paste the output here as a comment. Sorry about that. :/",non_debt,-
hadoop,7786,comment_6,I've filed HADOOP-7416 to address this limitation of test-patch.,non_debt,-
hadoop,7786,comment_7,"Hey Aaron, I'm trying to run the Ant test-patch task but I'm not sure what args I should be passing the task. Can you provide me with a sample command for the sh script or Ant task? thanks, - Abel",non_debt,-
hadoop,7786,comment_8,"Hey Abel, I use the following bash function: and apache-forrest-0.8 and findbugs-1.3.9. More at",non_debt,-
hadoop,7786,comment_9,"Hey Eli, thanks for the function. Not sure if my environment is properly setup, I tried running test-patch and got the following error: [exec] * [8/24] [0/0] 0.838s 0b linkmap.pdf",non_debt,-
hadoop,7786,comment_10,Are you using forrest 0.8 or 0.9? I think I've seen this problem using forrest 0.9 which is fairly new and apparently has some kind of problem with our build environment.,non_debt,-
hadoop,7786,comment_11,That's definitely Forrest 0.9. See: HADOOP-7394,non_debt,-
hadoop,7786,comment_12,"Hey Guys, your absolutely right, I was using forest 0.9. I ran the test-patch with forest 0.8 and that seemed to resolved the issue. However looks like I'm getting the same error as the QA Bot: Any idea how to get around this?",non_debt,-
hadoop,7786,comment_13,"I think ""dfs.block.size"" introduced in HADOOP-4952 was just a mistake. I think the intention was to have it ""dfs.blocksize"", same as in HDFS. I see a bunch of other file system keys in FsConfig, that duplicate their HDFS counterparts, but the naming is consistent across them. So I assume that extra dot between block and size was a typo. Having said that I think it is not a good idea to duplicate key definitions. The reason is exactly the typos or inconsistent renaming of those properties or the default values. E.g. in common = 32 MB}} while in HDFS = 64 MB}} This is really messy. The only method from {{FsConfig}} that is used in the code is I propose to remove everything else from {{FsConfig}} in order to avoid confusion. The rational behind this is that {{FsConfig}} should only contain keys that are specified in core-site.xml. The keys that belolng to hdf-site.xml shoud be described in {{DFSConfigKeys}}. It also looks that Tom's documentation change HDFS-671 describes keys consistently with this assumption.",code_debt,duplicated_code
hadoop,7786,comment_14,"I agree with Konst, we should remove the overlap between FsConfig and DFSCOnfig.",code_debt,duplicated_code
hadoop,7786,comment_15,Patch attached.,non_debt,-
hadoop,7786,comment_17,"Eli, the patch looks great. One thing: Instead of It should be +1 other than that. Findbugs come from protobuf packages untouched here. Don't understand why jenkins reported them as new.",code_debt,low_quality_code
hadoop,7786,comment_18,"Thanks Konst. Fixed this and committed to trunk, 22 and 23.",non_debt,-
hadoop,7796,summary,HADOOP-7773 introduced 7 new findbugs warnings,code_debt,low_quality_code
hadoop,7796,description,Code Warning Se stored into non-transient field Se stored into non-transient field Se Class defines non-transient non-serializable instance field request_ Se stored into non-transient field Se Class defines non-transient non-serializable instance field response_ Dodgy Warnings Code Warning UCF Useless control flow in UCF Useless control flow in,non_debt,-
hadoop,7796,comment_0,"Eli, I tried many ways to turn the findbugs warnings off. This might be related to the build setup.",non_debt,-
hadoop,7796,comment_1,I didn't see a JIRA was filed for this. I submitted a patch under:,non_debt,-
hadoop,7796,comment_2,has been committed by Suresh Srinivas so this JIRA should be considered fixed.,non_debt,-
hadoop,7796,comment_3,Fixed by HADOOP-7883.,non_debt,-
hadoop,7815,summary,Map memory mb is being incorrectly set by,non_debt,-
hadoop,7815,description,"HADOOP-7728 enabled task memory management to be configurable in the However, the default value for is being set incorrectly.",non_debt,-
hadoop,7815,comment_0,"Patch for 0.20.205 Arpit, can you please review this?",non_debt,-
hadoop,7815,comment_1,+1 looks good.,non_debt,-
hadoop,7815,comment_2,+1 change looks good.,non_debt,-
hadoop,7815,comment_3,I just committed this to and,non_debt,-
hadoop,7815,comment_4,Closed upon release of version 1.0.0.,non_debt,-
hadoop,7863,summary,"Apply HADOOP-7424 ""Log an error if the topology script doesn't handle multiple args"" to 0.23 branch",non_debt,-
hadoop,7863,description,"Merging HADOOP-7777 to 0.23.1 fails because this (minor) patch isn't in there. Rather than generate a new patch for 0.23.1, it would be easier to apply the existing patch to the 0.23.x branch and move the change log entries",non_debt,-
hadoop,7863,comment_0,+1 sounds good,non_debt,-
hadoop,7863,comment_2,Existing patch applied; only conflict was in CHANGES.TXT,non_debt,-
hadoop,7863,comment_12,"Closing as duplicate of HADOOP-7424, mucks up release notes and CHANGES.txt.",non_debt,-
hadoop,7911,summary,Exception while connecting grails app to hadoop!,non_debt,-
hadoop,7911,description,"Hii there, I am new to hadoop and I was trying to connect it to my grails application. I tested it with java program and it worked successfully. Then I moved on for grails app. Configuring mysql is easy and I can easily connect with it. But when I tried to connect via grails app.... In datasource driverClassName = and url = gorm-hbase plugin installed. My master server was running successfully. But its port number is different and changes if I restart hbase. (think I have done some novice mistake) Please help me out I got this Error 2011-12-12 10:54:52,032 [main] ERROR - Error executing bootstraps: Error creating bean with name 'hbase.admin': Instantiation of bean failed; nested exception is Could not instantiate bean class Constructor threw exception; nested exception is localhost:56297 Error creating bean with name 'hbase.admin': Instantiation of bean failed; nested exception is Could not instantiate bean class Constructor threw exception; nested exception is localhost:56297 Source) Source) Caused by: Could not instantiate bean class Constructor threw exception; nested exception is localhost:56297 ... 23 more Caused by: localhost:56297 ... 23 more",non_debt,-
hadoop,7911,comment_0,Hi Arvind. Please contact the hbase-users mailing list or spring support with this issue - it's not a Hadoop bug but rather some problem with your setup or configuration.,non_debt,-
hadoop,7911,comment_1,"Hii Todd, I did not know. But if you say...that could really be a problem of configuration. Sorry, If I wasted your time. And If you have any good reference of connecting a grails app to hadoop...please tell me. Thanks :)",non_debt,-
hadoop,7915,summary,possible Case for,non_debt,-
hadoop,7915,comment_0,This bug is outdated as the lines of code mentioned in the description does not exist anymore in StreamUtil.java. I have marked it as wont fix.,non_debt,-
hadoop,7915,comment_1,I meant to say it should be marked as a wontfix.,non_debt,-
hadoop,7915,comment_2,Thanks  for the catch. I'll close this issue.,non_debt,-
hadoop,7919,summary,[Doc] Remove hadoop.logfile.* properties.,non_debt,-
hadoop,7919,description,"The following only resides in core-default.xml and doesn't look like its used anywhere at all. At least a grep of the prop name and parts of it does not give me back anything at all. These settings are now configurable via generic Log4J opts, via the shipped log4j.properties file in the distributions.",documentation_debt,low_quality_documentation
hadoop,7919,comment_3,Javadoc issues aren't from this patch.,non_debt,-
hadoop,7919,comment_4,"Since this was trivial and I double-checked the inexistence of these props, I went ahead and committed it to trunk.",non_debt,-
hadoop,7919,comment_10,This is safe to backport to branch-0.23. Will backport in couple of days if there are no objections.,non_debt,-
hadoop,7919,comment_11,Backported to 0.23,non_debt,-
hadoop,7924,summary,FailoverController for client-based configuration,non_debt,-
hadoop,7924,description,Basic FailoverController to coordinate fail-over using a client-based config (ie fail-over from NameNode x to NameNode y).,non_debt,-
hadoop,7924,comment_0,Patch attached. Introduces a basic FailoverController that can be used for manual failover (no monitoring or leader election) and wires it into the CLI.,non_debt,-
hadoop,7924,comment_1,"- maybe we should rename FailoverController to or even just ManualFailover? IMO has always had a connotation of being some kind of daemon. - in the preFailoverCheck, I'd expect us to support the case where ""svc1"" is down -- ie the admin has noticed the active crashed, and therefore wants to initiate failover to svc2. We should treat this the same as a failure in calling transitionToStandby - i.e fence and continue. - {{svc1}} and {{svc2}} aren't very descriptive parameter names - maybe {{fromSvc}} and {{toSvc}} or {{oldActive}} and {{newActive}}? The javadoc in {{failover(...)}} is also incorrect here for {{svc2Name}} - in no need to duplicate the standard exception javadoc from its parent class",code_debt,low_quality_code
hadoop,7924,comment_2,"This could be used for automatic failover as well though right? I was thinking this class would eventually get a main method and become a daemon (the static methods would still support manual failover). If not, maybe rename HAFailover? Updated patch coming that addresses your other feedback.",non_debt,-
hadoop,7924,comment_3,"Thanks for the good feedback. Updated patch attached, addresses your comments, also adds an ""haadmin"" hook into bin/hadoop. #1 Lemme know if my comment wrt naming made sense. #2 Agree. Was originally thinking in this case the admin would just make the standby active directly, but we need to run through the whole failover in this case for fencing. Updated code and tests. For sanity, I ran bin/hadoop haadmin failover X Y and verified that it works (and prints something reasonable) when X isn't running, and fails when Y isn't yet running (and A remains active of course). Note that this means if we failover from X to Y and there's a typo in X then you might unexpectedely end up with two actives (added Another jira for warning if X is not specified as one of the options in dfs.ha.namenodes or we add a flag that's required to failover from a service we can't connect to? #3 Done. #4 Done. Also fixed up the other HA exceptions to be consistent.",documentation_debt,low_quality_documentation
hadoop,7924,comment_4,"in I think it's clearer to structure the code like: rather than trying to collapse both exceptions into one throw. Also, should log the exception thrown by getServiceState. - In {{failover()}}, I think you probably want to catch all Throwables in another catch clause - eg what if it's in some bad state and your failover attempt caused it to crash, which would give your IPC a - indentation - I think better to not abbreviate ""first"" and ""second"" - - Can you add some javadoc to -- it's strange that this is a test case... it's more like you're showing that a particular user error can cause a problem, rather than showing something about the bug, right? Or else it should be a test case that fails, with an @Ignore explaining why it fails, maybe? - Just to confirm, the manual test you mentioned was done with two NNs in a running HA cluster?",documentation_debt,outdated_documentation
hadoop,7924,comment_5,"#1 Done #2 Added missing catch Exception in transitionToActive #3-4 Done #5 Just removed the test. Think the typo case I mentioned above is worth worrying about or a non-issue since we let people explicitly transition more than one NN to active (and have to handle that fallout)? #6 The ""cluster"" was running on one host, though yes, the failover is done between two independent NN processes running with HA enabled using a shared dir.",non_debt,-
hadoop,7924,comment_6,"I think what you said earlier (""Another jira for warning if X is not specified as one of the options in dfs.ha.namenodes or we add a flag that's required to failover from a service we can't connect to?"") makes sense -- you shouldn't allow people to specify NNs on this command when they aren't listed in your config. Another JIRA seems OK, since the cross-project nature is a little annoying..",non_debt,-
hadoop,7924,comment_7,+1 on the patch,non_debt,-
hadoop,7924,comment_8,Thanks for the reviews Todd. I've committed this to branch.,non_debt,-
hadoop,7924,comment_9,And filed HADOOP-7958 to require both services be present in the config.,non_debt,-
hadoop,7936,summary,There's a Hoop README in the root dir of the tarball,non_debt,-
hadoop,7936,description,The Hoop README.txt is now in the root dir of the tarball.,non_debt,-
hadoop,7936,comment_1,"+1, that should fix it.",non_debt,-
hadoop,7936,comment_2,committed to trunk and branch-0.23,non_debt,-
hadoop,7946,summary,Testing hadoop using scripting languages,non_debt,-
hadoop,7946,description,"Hi friends, I am writing the map-reduce using scripting languages(Ruby).I am planning to write unit test cases in ruby.In ruby i can test the input and expected output by using local file system but i am feeling it is not right way.Is there any way to test real map-reduce running on hdfs . Can you suggest me any testing framework is there for scripting languages.",test_debt,lack_of_tests
hadoop,7946,comment_0,"Hello Arun, Please use the user mailing lists for MapReduce for your usage related questions. This JIRA project here is to track development activities of the project instead. Head over here: and subscribe + shoot a mail to mapreduce-user@, am sure there are loads of people ready to help you out with your issue at any time if you use the right channels! Resolving JIRA as Invalid.",non_debt,-
hadoop,7946,comment_1,Thanx for your comments,non_debt,-
hadoop,7971,summary,"hadoop <job/queue/pipes> removed - should be added back, but deprecated",non_debt,-
hadoop,7971,description,The mapred subcommands were removed from the /bin/hadoop command. I believe for backwards compatibility at least some of these should have stayed along with the deprecated warnings.,non_debt,-
hadoop,7971,comment_0,+1 for adding pipes/job/queue back to bin hadoop. We do need it for backwards compatiblity.,non_debt,-
hadoop,7971,comment_1,This should do it. Still testing.,non_debt,-
hadoop,7971,comment_2,"Tested it out, seems fine.",non_debt,-
hadoop,7971,comment_4,Looks like Prasthant;s already got a patch on MAPREDUCE-3605.,non_debt,-
hadoop,7971,comment_5,Assigning it to prashanth since he already had a patch.,non_debt,-
hadoop,7971,comment_6,Updated with better error msg and exit status .,code_debt,low_quality_code
hadoop,7971,comment_7,+1 looks good.,non_debt,-
hadoop,7971,comment_8,I just committed this. Thanks Prashanth & Mahadev!,non_debt,-
hadoop,7971,comment_15,Thank's ACM and Mahadev.,non_debt,-
hadoop,7974,summary,TestViewFsTrash incorrectly determines the user's home directory,non_debt,-
hadoop,7974,description,"HADOOP-7284 added a test called TestViewFsTrash which contains the following code to determine the user's home directory. It only works if the user's directory is one level deep, and breaks if the home directory is more than one level deep (eg user hudson, who's home dir might be /usr/lib/hudson instead of /home/hudson). Seems like we should instead search from the end of the path for the last slash and use that as the base, ie ask the home directory for its parent.",non_debt,-
hadoop,7974,comment_0,Patch that uses a get-parent call instead of hacking with strings.,code_debt,low_quality_code
hadoop,7974,comment_3,+1 javadoc failures are unrelated,non_debt,-
hadoop,7974,comment_4,I've committed this and merged to 23. Thanks Harsh!,non_debt,-
hadoop,7974,comment_15,It's been fixed for a while.,non_debt,-
hadoop,7974,comment_16,TestViewFsTrash occasionally fails. Filed HADOOP-8110. I am not sure if this issue causes it. It would be great if you can take a look.,non_debt,-
hadoop,7985,summary,maven build should be super fast when there are no changes,code_debt,slow_algorithm
hadoop,7985,description,"I use this command ""mvn -Pdist -P-cbuild -DskipTests install"" to build. Without ANY changes in code, running this command takes 1:32. It seems to me this is too long. Investigate if this time can be reduced drastically.",code_debt,slow_algorithm
hadoop,7985,comment_0,In * Touchz.java should be renamed to Touch.java to avoid compilation everytime. * Record IO generated test files which were compiled everytime. We could modify RccTask.java to doCompile only when sourceFile is newer than the destination file These are just 2 seconds of the 24 seconds needed to build hadoop-common. So obviously not substantial. Will keep looking. I'm beginning to think running from JARs might not be the best idea to have a quick dev cycle. Maybe I should try running from the target/classes directories and skip building the jar altogether.,design_debt,non-optimal_design
hadoop,7985,comment_1,all protobuf compilation sections in pom.xml can be modified like this I am now trying to optimize mvn -P-cbuild -DskipTests -X compile Even here jsps are being compiled into java files every time unnecessarily,design_debt,non-optimal_design
hadoop,7985,comment_2,Thanks to Jason (who's too shy to comment on this JIRA =P hahaha),non_debt,-
hadoop,7985,comment_3,Just a lil' bit of improvement. Still a long ways to go,design_debt,non-optimal_design
hadoop,8011,summary,How to use distcp command betwen 2 cluster that different version,non_debt,-
hadoop,8011,description,I have tow cluster 1.0 and 0.2 how to use distcp to copy betwen 2 cluster this is error: Copy failed: Call to cluster1 failed on local exception: Source) Caused by:,non_debt,-
hadoop,8011,comment_0,"Try the ""Copying between versions of HDFS"" instructions at",non_debt,-
hadoop,8053,summary,"""har://hdfs-/foo"" is not a valid URI",non_debt,-
hadoop,8053,description,None,non_debt,-
hadoop,8053,comment_0,"HADOOP-6560 added a check which requires HAR URIs to include a '-' after the underlying scheme even if a host is not specified. Unfortunately this appears to violate the grammar for the host field defined in RFC-2396. Here are the relevant bits from section 3.2.2: Note that the toplabel field can't end in a ""-"", which implies the host field can't end a in a ""-"". Generates:",non_debt,-
hadoop,8053,comment_1,"Hi Carl, ""har://hdfs-/foo"" actually is also an invalid Hadoop FileSystem path. The har path format is ""har://<scheme In HADOOP-6560, I changed it from throwing to IOException. I think it is fine if getting URISyntaxException.",non_debt,-
hadoop,8053,comment_2,I believe this is not a problem. Please feel free to reopen if not.,non_debt,-
hadoop,8084,summary,Protobuf RPC engine can be optimized to not do copying for the RPC request/response,non_debt,-
hadoop,8084,description,None,non_debt,-
hadoop,8084,comment_0,Straightforward patch. This is relevant to mapreduce too.,non_debt,-
hadoop,8084,comment_1,The patch makes sense to me. Have you done any benchmarks? Perhaps using the tool introduced in HADOOP-8070?,non_debt,-
hadoop,8084,comment_2,No I haven't done benchmarks. But it seemed to make sense to avoid a copy if it could be avoided.,code_debt,slow_algorithm
hadoop,8084,comment_3,"[oops, put comment in the wrong field.. duh] patch looks good, marking patch available so Hudson runs",non_debt,-
hadoop,8084,comment_5,Committed to trunk. Thanks Aaron and Todd for the reviews.,non_debt,-
hadoop,8084,comment_8,"Ddas, Can you merge this to the PB branch?",non_debt,-
hadoop,8084,comment_10,Is that branch (branch-0.23-PB) scheduled to be merged with 0.23? This issue is currently marked for 0.24. I am okay merging this fix with that branch but wanted to double-check...,non_debt,-
hadoop,8084,comment_11,"Check with Nicholas, he's been maintaining the PB branch.",non_debt,-
hadoop,8084,comment_13,"Hi Devaraj and Mahadev, I will merge this to 0.23-PB. Thanks.",non_debt,-
hadoop,8084,comment_15,I have merged this to 0.23.,non_debt,-
hadoop,8100,summary,share web server information for http filters,non_debt,-
hadoop,8100,description,This is a simple fix which shares the web server bind information for consumption down stream for 3rd party plugins.,non_debt,-
hadoop,8100,comment_0,This JIRA applies to trunk as well. Would you please provide a PATCH for trunk? On the patch: For the key names I'd remove 'authentication' as these properties can be used for different things than authentication. Testcase is missing.,test_debt,lack_of_tests
hadoop,8100,comment_1,"No, I won't.",non_debt,-
hadoop,8100,comment_2,"Nice attitude, thanks",non_debt,-
hadoop,8100,comment_3,"Oh, forgot: ... is the reason why it has authentication in it. This will allow anyone who is writing against to use it. While I'm planning on replacing that locally, others may not want to do that.",non_debt,-
hadoop,8117,summary,Upgrade test build to Surefire 2.12,non_debt,-
hadoop,8117,description,"Surefire 2.9, which we're using currently, has a few annoying bugs. In particular, if a test exits with a non-zero exit code, it doesn't report the test as failed.",non_debt,-
hadoop,8117,comment_0,"attached patch upgrades to 2.11. Even though 2.12 is supposedly released, it doesn't seem to be in the mvn repos. I verified that 2.11 does indeed trigger a test failure for the case described in HDFS-3019.",non_debt,-
hadoop,8117,comment_1,1,non_debt,-
hadoop,8117,comment_3,2.11 has some annoying bugs I've ran into. I suggest 2.12 It is in the maven central repo:,non_debt,-
hadoop,8117,comment_4,"Ah, my local m2 had a negative cache entry. mvn -U fixed it. New patch updates to 2.12 after all.",non_debt,-
hadoop,8117,comment_6,"+1, the updated patch looks good to me. Thanks for taking care of this, Todd.",non_debt,-
hadoop,8117,comment_7,Committed to trunk for now. I'll wait til after tonight's trunk build to commit to branch-2 - leaving this open so I don't forget.,non_debt,-
hadoop,8117,comment_13,"Test run seemed to go OK last night, so I committed this to branch-2 as well. Thanks.",non_debt,-
hadoop,8124,summary,Remove the deprecated Syncable.sync() method,code_debt,dead_code
hadoop,8124,description,The Syncable.sync() was deprecated in 0.21. We should remove it.,code_debt,dead_code
hadoop,8124,comment_0,"- removes Syncable.sync(); - removes the deprecated - removes unnecessary ""throws IOException"" declarations.",code_debt,dead_code
hadoop,8124,comment_2,+1 for the patch.,non_debt,-
hadoop,8124,comment_3,I have committed this.,non_debt,-
hadoop,8168,summary,empty-string owners or groups causes in,non_debt,-
hadoop,8168,description,"In we set the member variable {{lineFormat}}, which is used by {{ProcessPath()}} to print directory entries. Owners and groups are formatted using the formatting conversion {{%-Xs}}, where X is the max length of the owner or group. However, when trying this with an S3 URL, I found that the owner and group were empty (""""). This caused X to be 0, which means that the formatting conversion is set to {{%-0s}}. This caused a to be thrown when the formatting string was used in {{ProcessPath()}}. Formatting conversions are described here: The specific exception thrown (a subtype of is described here:",non_debt,-
hadoop,8168,comment_0,"Make sure that for {{%-Xs}} formatting conversions, {{X}} is greater than 0.",non_debt,-
hadoop,8168,comment_1,Note that a similar 0-length check for groups and owners is present in Hadoop 1.0:,non_debt,-
hadoop,8168,comment_2,"Sorry, correct line number is :",non_debt,-
hadoop,8168,comment_3,HADOOP-4335 seems to be the same bug.,non_debt,-
hadoop,8168,comment_5,I think you meant {{Math.min}}. Although I'd suggest maybe something like this to avoid spurious whitespace:,code_debt,low_quality_code
hadoop,8168,comment_6,Ignore the -Math.min-. I'm tired.,non_debt,-
hadoop,8168,comment_7,"Hi Daryn, Your fix looks a bit cleaner than mine, thanks! -Eugene",code_debt,low_quality_code
hadoop,8168,comment_8,"+1, built & tested with S3",non_debt,-
hadoop,8168,comment_9,"Eugene, just noticed you did not update the patch to Daryn's suggestion. Would you mind uploading a new patch?",non_debt,-
hadoop,8168,comment_10,"Eugene, I'm uploading a patch tweaking your patch to Daryn's comments. Hope this is OK with you. I've tested with local/hdfs/s3 filesystem.",non_debt,-
hadoop,8168,comment_11,+1 looks good to me,non_debt,-
hadoop,8168,comment_12,Thanks Eugene. Committed to trunk and branch-2.,non_debt,-
hadoop,8209,summary,Add option to relax build-version check for branch-1,non_debt,-
hadoop,8209,description,"In 1.x DNs currently refuse to connect to NNs if their build *revision* (ie svn revision) do not match. TTs refuse to connect to JTs if their build *version* (version, revision, user, and source checksum) do not match. This prevents rolling upgrades, which is intentional, see the discussion in HADOOP-5203. The primary motivation in that jira was (1) it's difficult to guarantee every build on a large cluster got deployed correctly, builds don't get rolled back to old versions by accident etc, and (2) mixed versions can lead to execution problems that are hard to debug. However there are also cases when users know they two builds are compatible, eg when deploying a new build which contains the same contents as the previous one, plus a critical security patch that does not affect compatibility. Currently deploying a 1 line patch requires taking down the entire cluster (or trying to work around the issue by lying about the build revision or checksum, yuck). These users would like to be able to perform a rolling upgrade. In order to support this, let's add an option that is off by default, but, when enabled, makes the DN and TT version check just check for an exact version match (eg ""1.0.2"") but ignore the build revision (DN) and the source checksum (TT). Two builds still need to match the major, minor, and point numbers, but nothing else.",design_debt,non-optimal_design
hadoop,8209,comment_0,"+1 We use this practice. This makes it possible to do a rolling restart of DataNodes without taking down service by bouncing the NameNode. This is most useful when the change scope is restricted to the DN. If HA is backported to branch-1 we could handle most NN changes similarly: Upgrade the NNs one at a time with manual failover for no downtime. One issue remaining is that modification of a NN<- It is also possible to do this with TaskTrackers, but this will fail currently running tasks on the TT. Even so we can still stage in a TT bugfix release, just more slowly. Bouncing the JobTracker remains a big deal, but the maintenance window for that becomes very short if everything else has been rolled out ahead of time. With some ""HA JT"" option for branch-1 (Corona?) this might also have no downtime.",non_debt,-
hadoop,8209,comment_1,"Patch attached. Adds new config option to relax the version check to just the version number. Aside from the new DN and TT tests that cover the current/default behavior and the new behavior, I tested on a cluster and verified that (1) DNs/TTs with different revisions can not join by default, and (2) using the new flag they can (and the new log message for this case is appropriate).",non_debt,-
hadoop,8209,comment_2,"Here are test-patch results. This doesn't introduce new findbugs, a null patch has 8 as well (HADOOP-7847).",non_debt,-
hadoop,8209,comment_3,"Patch looks pretty good to me, Eli. Just a few small comments: # Not obvious to me why we have these static version methods in the Storage class, which themselves just delegate to static methods of the VersionInfo class. # Recommend adding additional detail to the AssertionErrors, including the revisions and versions that didn't match. # Recommend adding an explanation to the DN log message about why the communication is being allowed, e.g.: ""... because versions match exactly ('"" + version + ""') and is enabled."" Ditto for TT. # Similarly the log message explaining why communication isn't being allowed might mention whether the check failed because of strict revision checking, or relaxed version checking. # Why call the new method ""getInfoVersion"" in JobTracker? getVersion, as was done in Storage, seems to make more sense to me. # In I don't think you actually test that different revisions are still disallowed by default, since you change both the revision and version simultaneously in the test.",code_debt,low_quality_code
hadoop,8209,comment_4,"Thanks for the review ATM. Updated patch, and re-tested. #1 Yea, was following the existing method but better to use VersionInfo directly, done. #2-4 Done #5 Because JT#getVersion exists (for MXBean#getVersion), in the updated patch I've addressed this via new NN/JT methods getBuildVersion which return the version, I renamed to getFullVersion to clear up the distinction between the build's version and what was called the ""build version"". #6 Fixed, took the same assert from and changed the polarity",non_debt,-
hadoop,8209,comment_5,"I reviewed the delta, and it largely looks good. One tiny nit: looks like you variously spelled the word ""disallow"" either as ""dissallow"" or ""dissalow"". Patch looks good otherwise - +1. Please do also run the branch-1 test suite on the latest patch before committing.",code_debt,low_quality_code
hadoop,8209,comment_6,"Thanks ATM. Will fix the spelling misstake, running the full suite now.",documentation_debt,low_quality_documentation
hadoop,8209,comment_7,"* I thought we were focusing on rolling upgrades for Hadoop 2 not Hadoop 1 given that wire compatibility is only in Hadoop 2. * The jira title should be ""Add option to relax build-version check for branch-1""",non_debt,-
hadoop,8209,comment_8,"Sanjay, See [this on HDFS-2983. For v1 this only enables rolling upgrade when there's an *exact version match* (eg v1.0.2), this is still very useful though as it allows people to perform rolling upgrades for a security patch or an EBF that doesn't affect compatibility (most don't). Updated the jira tile.",non_debt,-
hadoop,8209,comment_9,"Ran all the tests. There are 5 MR failures on branch-1, confirmed they all fail on a clean tree and filed MAPREDUCE-4142 for them.",non_debt,-
hadoop,8209,comment_10,"Rather than renaming, maybe add getFullVersion() and deprecate getBuildVersion()? Also, is this change needed on trunk as well?",non_debt,-
hadoop,8209,comment_11,"Thanks for the feedback Tom. Looked at this again and I think it's better to leave VersionInfo as is (not rename getBuildVersion to getFullVersion) and just make match, eg should return and add that returns (there's already a getVersion method for MXBean). Sound good?",non_debt,-
hadoop,8209,comment_12,"Forgot to mention, leaving VersionInfo as is means we don't need to do anything for trunk.",non_debt,-
hadoop,8209,comment_13,Patch attached. Minor change from the last one.,non_debt,-
hadoop,8209,comment_14,Agree that not changing VersionInfo is better. +1 from me if Jenkins comes back OK.,non_debt,-
hadoop,8209,comment_15,"Thanks Tom, re-ran test-patch and the tests.",non_debt,-
hadoop,8209,comment_16,"Thanks for the reviews ATM and Tom, I've committed this to branch-1.",non_debt,-
hadoop,8209,comment_17,Closed upon release of Hadoop-1.1.0.,non_debt,-
hadoop,8210,summary,Common side of HDFS-3148,non_debt,-
hadoop,8210,description,"Common side of HDFS-3148, add necessary DNS and NetUtils methods. Test coverage is in the HDFS-3148 patch.",non_debt,-
hadoop,8210,comment_0,Patch attached (for trunk).,non_debt,-
hadoop,8210,comment_2,Think the change to bump commons-net in needs its own change.,non_debt,-
hadoop,8210,comment_3,"Given the extensive use of IPs in this patch, is this going to impact the ability to use hostnames as a token's service?",non_debt,-
hadoop,8210,comment_4,Why would the local client interface used impact security?,non_debt,-
hadoop,8210,comment_5,"I may not be properly understanding the intent of the change, hence my request for clarification. I haven't fully researched the related jiras. I'll quickly explain host based tokens so you can easily respond. As you probably already know, the default token implementation always resolves the host to an ip for the token's service. This is fine if the user explicitly specified an ip (maybe due to dns errors, or no dns entries), but the host based tokens preserve the exact hostname or ip the user specified. Preserving the hostname shields the user against ip changes which is very important, for example, when a local or remote NN's ip may change during an upgrade. That said, will this change be used exclusively by the client to select an outgoing network interface? That will be fine. My concern is if the client starts always using ips for a remote host since the token will probably have to contain the ip -- depending how/where these new methods are used. Resolving an ip back to a hostname is insufficient since the user may have provided a CNAME and a reverse lookup will return the A name. If the CNAME's ip changes, the client will not detect it which defeats the host based tokens. Thanks in advance for taking the time to explain.",non_debt,-
hadoop,8210,comment_6,"HDFS-3148, which this jira tracks the common side of, just pertains to outgoing client interfaces, so doesn't pertain to datanode IPs. However, I think your comment is relevant to HADOOP-8198. I think HADOOP-7510 is complimentary, if we don't use hostnames then we'd have to have a separate service token for each interface on the datanode. Could you take a look at the design doc and see if it jives with what you're thinking?",non_debt,-
hadoop,8210,comment_7,Very nice doc. I need to study it a bit more and see if/how it meshes with a few ideas I've been pondering with regards to tokens and multiple hostnames and ips. I started thinking of alternatives after seeing the token splitting that HA is currently doing.,non_debt,-
hadoop,8210,comment_8,"I think it's worth changing the return type of this function to LinkedHashSet, so it's clear that the ordering here is on purpose. Perhaps also add a comment here saying something like: - Nits: please un-abbreviate ""first"" for better readability. Also, ""e.g."" instead of ""Eg."" -- or just say ""For example"" - I think it's more idiomatic to just put the postincrement inside the []s - - there's a small spurious whitespace change in NetUtils.java - looks like the pom change is still in this patch (redundant with HADOOP-8211)",code_debt,low_quality_code
hadoop,8210,comment_9,Thanks Todd. Updated patch attached.,non_debt,-
hadoop,8210,comment_11,1,non_debt,-
hadoop,8210,comment_12,+1 this part doesn't look like it will impact tokens,non_debt,-
hadoop,8210,comment_15,"Thank you for the reviews, Todd and Daryn! I've committed this and merged to branch-2.",non_debt,-
hadoop,8222,summary,bin/hadoop should allow callers to set jsvc pidfile even when not-detached,non_debt,-
hadoop,8222,description,"it would be nice if the jsvc pid file were properly namespaced in /var/run to avoid collisions with other jsvc instances jsvc uses /var/run/jsvc.pid for the datanode pid file. If that's configurable, it should be configured: /var/run/jsvc.pid is sorely not namespaced. if [ = ""true"" ]; then -errfile &1 -outfile else # Even though we are trying to run a non-detached datanode, # jsvc will not write to stdout/stderr, so we have to pipe # it and tail the logfile. -errfile &1 -outfile $log_path"" echo Non-detached jsvc output piping to: $log_path touch $log_path tail -f $log_path & fi And the relevant argument is '-pidfile'",code_debt,low_quality_code
hadoop,8222,comment_0,This doesn't apply to branch-1 or branch-2.,non_debt,-
hadoop,8226,summary,a few native libraries are missing from a full build and binary tarball,non_debt,-
hadoop,8226,description,"Hadoop 1.0.X ships the following native libraries in its tarball: While Hadoop 0.23.1 ships a subset: The question is: do we have any reason not to support/publish the following libraries: libhadooppipes.a, libhadooputils.a ? And we also used to have librecordio that now seems to be missing from both. If this is not intentional I can provide a patch to re-enable these bits.",non_debt,-
hadoop,8226,comment_0,Here's the first cut at the patch. It makes hadooppipes and utils bits appear under include and lib subdirectories and it also makes C++ examples appear under examples. Please let me know if you'd rather have me integrate the native parts of the build with make-maven-plugin.,non_debt,-
hadoop,8226,comment_2,Doesn't seem to be a problem on trunk anymore. From a built tarball extracted:,non_debt,-
hadoop,8236,summary,haadmin should have configurable timeouts for failover commands,non_debt,-
hadoop,8236,description,"The HAAdmin failover could should time out reasonably aggressively and go onto the fencing strategies if it's dealing with a mostly dead active namenode. Currently it uses what's probably the default, which is to say no timeout whatsoever.",non_debt,-
hadoop,8236,comment_0,"Attached patch adds the following timeouts: for asking the new active to become active| for asking the old active to become standby, before fencing| for CLI commands like -getServiceState, -monitorHealth| Philip, do you think these seem reasonable? I tested this manually by kill -STOPping my namenode and then initiating a failover. The RPC timed out after 5s and then fenced the stopped NN before doing the failover.",non_debt,-
hadoop,8236,comment_2,"Todd, These timeouts look reasonable to me. Worth noting that new-active is also the timeout for the active pre-check, ie the check that the new active is alive and well before we ask the current active to go standby. This is important because we don't want to impatiently wait 5s before fencing then wait a minute to make the new active active. In practice since we already contacted the new active we probably won't have to wait 60s to transition it to active unless something happened in between the pre-check and the transition to active, which is why 60s timeout here is reasonable. Nit: can remove the ""TODO"" before transitionToActive since this is now configurable. Otherwise patch looks great.",requirement_debt,requirement_partially_implemented
hadoop,8236,comment_3,New patch removes the TODO. I'll commit this version momentarily.,requirement_debt,requirement_partially_implemented
hadoop,8236,comment_5,Committed to branch-2 and trunk. Thanks for reviewing.,non_debt,-
hadoop,8245,summary,Fix flakiness in,test_debt,flaky_test
hadoop,8245,description,When I loop I occasionally see two types of failures: 1) the ZK JMXEnv issue (ZOOKEEPER-1438) 2) fails with a timeout This JIRA is for fixes for these issues.,non_debt,-
hadoop,8245,comment_0,"For problem #1, the solution is the same as is already done in some other test cases. We just need to add a workaround to clear the ZK MBeans before running the tearDown method. It's a hack, but in the absense of a fix for ZOOKEEPER-1438, it's about all we can do. I spent some time investigating problem #2. The bug is as follows: - these test cases create a new and call on it before running the main body of the tests. Although they don't call {{joinElection()}}, the creation of the elector does create a {{zkClient}} object with an associated Watcher. - in the test case, we shut down and restart ZK. This causes the above Watcher instance to fire its Disconnected and then Connected events. There was a bug in the handling of the Connected event that would cause it to re-monitor the lock znode regardless of whether it was previously in the election. - So, when ZK comes back up, there was not two but *three* electors racing for the lock. However, two of the electors actually corresponded to the same dummy service. In some cases this race would be resolved in such a way that the test timed out. I don't think this is a problem in practice, since the ""formatZK"" call runs in its own JVM in the current code. However, it's worth fixing to get the tests to not be flaky, and to have a more reasonable behavior. There are several fixes to be done: - Add extra asserts for to catch cases where we might accidentally re-join the election when we weren't supposed to be in it. - Fix the handling of the ""Connected"" event to only re-join if the elector wants to be in the election - Cause exceptions thrown by watcher callbacks to be propagated back as fatal errors Will post a patch momentarily.",architecture_debt,using_obsolete_technology
hadoop,8245,comment_1,Attached patch fixes the flaky behavior for me. I looped for 30+ minutes and didn't see failures after applying this.,test_debt,flaky_test
hadoop,8245,comment_2,"+1, the patch looks good to me. Good finds/fixes, Todd.",non_debt,-
hadoop,8245,comment_3,"Committed to branch, thanks for reviewing. I verified all the HA tests passed on branch before committing.",non_debt,-
hadoop,8246,summary,Auto-HA: automatically scope znode by nameservice ID,non_debt,-
hadoop,8246,description,"Talking to some folks who work on they pointed out that it would make sense to automatically include the nameservice ID in the base znode used for automatic failover. For example, even though the ""root znode"" is ""/hadoop-ha"", we should put the znodes for a nameservice ""my-ha-cluster"" within This allows federated setups to work with no additional configuration.",non_debt,-
hadoop,8246,comment_0,"Attached patch scopes the znodes by the configured nameservice ID. So, if the configured parent znode is /hadoop-ha, then it will work inside without having to manually configure the parent znode per-quorum.",non_debt,-
hadoop,8246,comment_1,"+1, the patch looks good to me. Good on you, Todd.",non_debt,-
hadoop,8246,comment_2,Thanks for the quick review. I ran all the ZKFC tests and committed this to the branch.,non_debt,-
hadoop,8256,summary,Compilation error in ViewFileSystem.java,non_debt,-
hadoop,8256,description,ViewFileSystem.java fails to compile in branch-2 with these errors:,non_debt,-
hadoop,8256,comment_0,"My Bad, I missed a file on checkin, but it is in now.",non_debt,-
hadoop,8283,summary,Allow tests to control token service value,non_debt,-
hadoop,8283,description,Tests in projects other than common need to be able to change whether the token service uses an ip or a host.,code_debt,low_quality_code
hadoop,8283,comment_0,Expose,non_debt,-
hadoop,8283,comment_2,The patch looks good to me. +1,non_debt,-
hadoop,8283,comment_3,"Thanks Daryn. I put this into trunk, branch-2, and branch-0.23",non_debt,-
hadoop,8288,summary,Remove references of mapred.child.ulimit etc. since they are not being used any more,code_debt,dead_code
hadoop,8288,description,"Courtesy Philip Su, we found that were not being used at all. The configuration exists but is never used. Its also mentioned in mapred-default.xml and . Also the method in Shell.java is now useless and can be removed.",architecture_debt,using_obsolete_technology
hadoop,8288,comment_0,Linking the JIRA which makes the changes in MapReduce,non_debt,-
hadoop,8288,comment_1,The patch removes references to ulimit.,non_debt,-
hadoop,8288,comment_3,Kicking Jenkins,non_debt,-
hadoop,8288,comment_5,Hey Bobby! I have no idea what patch is complaining about! Could it be because I am removing two files in hadoop-tools ? The patch applies just fine. I have to svn rm TestUlimit.java and UlimitApp.java but it works :(,non_debt,-
hadoop,8288,comment_6,This patch is failing because it makes some modifications to the tools project as well. I ran the tests myself and they pass so +1.,non_debt,-
hadoop,8288,comment_7,"Thanks Ravi, I just put this into trunk, branch-2, and branch-0.23",non_debt,-
hadoop,8289,summary,Hadoop Common project build failure in cygwin,non_debt,-
hadoop,8289,description,None,non_debt,-
hadoop,8289,comment_0,Merge the patch of MAPREDUCE-3540,non_debt,-
hadoop,8289,comment_2,"closing as cannot reproduce as everything is building happily on windows on trunk/branch-2 these days, without a cygwin environment",non_debt,-
hadoop,8303,summary,Clean up hadoop-streaming,code_debt,low_quality_code
hadoop,8303,description,Clean up a bunch of existing javac warnings in hadoop-streaming module.,code_debt,low_quality_code
hadoop,8303,comment_0,Patch that cleans all warnings of hadoop-streaming sub-module save for usage of MiniMRCluster and DistributedCache and one instance of StreamJob's non usual constructor cause a job reference is still required there.,code_debt,low_quality_code
hadoop,8303,comment_2,"A few things: # Looks like even Hadoop-Common QA does not pick up hadoop-tools changes. # The eclipse's auto unused import fixer removed RecRecord imports which caused TestIO not to compile. I fixed those manually in this new patch. # TestStreaming had a change in original patch that caused it to lose info about the running jobs and such. I tweaked the change to use a non ToolRunner approach, and now work nicely again - minus deprecation warnings. Since Hadoop QA bot can't run the tests, here is the test result manually run, with patch applied: {{cd mvn clean test}}",code_debt,low_quality_code
hadoop,8303,comment_4,"Patches were too broad and have gone stale. Will address these forms of issue over separate, smaller and more divided JIRAs in future. Closing out parent JIRA MAPREDUCE-4172, and hence closing out this.",non_debt,-
hadoop,8304,summary,DNSToSwitchMapping should add interface to resolve individual host besides a list of host,non_debt,-
hadoop,8304,description,DNSToSwitchMapping now has only one API to resolve a host list: public List<String> names). But the two major caller: and are taking single host name but have to wrapper it to an single entry ArrayList. This is not necessary especially the host has been cached before.,design_debt,non-optimal_design
hadoop,8304,comment_0,"The patch to add resolve(String host) interface to DNSToSwitchMapping, and some related unit tests. Also, identify a potential bug that a hostname start with number may not been resolved properly.",non_debt,-
hadoop,8304,comment_2,"Sounds reasonable to me Junping. test-patch doesn't run on cross-project patches. For this patch just introduce the new interface in common, and then file a separate HDFS and MR jira to use the new API. Thanks, Eli",non_debt,-
hadoop,8304,comment_3,"Hi Eli, Thanks for suggestion. Will do that soon. Best, Junping",non_debt,-
hadoop,8304,comment_4,"Update patch to include hadoop-common project only, and put rest code into other (HDFS/MAPREDUCE) jira issue",non_debt,-
hadoop,8304,comment_5,"Hello Eli, I already updated the code to include common part only. The rest code (HDFS/MAPREDUCE) are going to MAPREDUCE-4198 and HDFS-3324. All are marked patch available for review. Cheers, Junping",non_debt,-
hadoop,8304,comment_7,I update patches in HDFS-3324 and MAPREDUCE-4198 as previous patch is wrongly including common project code (caused by switching between local branch). I should be more careful next time. Sorry for the trouble.,non_debt,-
hadoop,8304,comment_8,Grant license to ASF.,non_debt,-
hadoop,8304,comment_10,"This adds a method to an interface which is an incompatible change, and will break implementers. Is the overhead of wrapping a string in a list causing performance problems?",design_debt,non-optimal_design
hadoop,8304,comment_11,"Good point, forgot that people override DNSToSwitchMapping. Technically DNSToSwitchMapping is evolving (and should stay that way), but yea no sense in breaking people if we don't have to. Junping, how about introducing the fix in a compatible way with the List interface?",non_debt,-
hadoop,8304,comment_12,"That's good comments on compatibility of this change. I saw evolving tag there but not consider people extends DNSToSwitchMapping as well (just thought ScriptBased, TableMapping and cached are good enough). I won't say original interface cause some performance headache as the time of resolving rack info can be overwhelmed comparing with the whole flow (replica placement or task scheduling). However, it is more easy to use for major consumers of original interface which are expecting to resolve individual host. Do you see any scenario to resolve a list of host? (not counting the unit test) Eli, I don't understand the question of last comment there as I just want to fix the interface here. :)",code_debt,slow_algorithm
hadoop,8304,comment_13,"DatanodeManager does that today with a list obtained from the hosts files. You mentioned earlier ""identify a potential bug that a hostname start with number may not been resolved properly"" - doesn't that issue still need to be addressed?",non_debt,-
hadoop,8304,comment_14,"That's for caching only. If you think list interface here is good, I agree that it may be better to leave it there which will not take complexity of incompatibility. About the bug that wrongly resolve the hostname starting with number, yes. I will go ahead to file a jira and it could be easy to fix it.",non_debt,-
hadoop,8304,comment_15,"Cool, closing this one as won't fix. Thanks for filing a separate jira, please link it in here.",non_debt,-
hadoop,8304,comment_16,"Hi Eli, The jira is filed as I uploaded a patch to fix this, but that will cause some side-effect. Thanks, Junping",non_debt,-
hadoop,8316,summary,Audit logging should be disabled by default,non_debt,-
hadoop,8316,description,"HADOOP-7633 made hdfs, mr and security audit logging on by default (INFO level) in log4j.properties used for the packages, this then got copied over to the non-packaging log4j.properties in HADOOP-8216 (which made them consistent). Seems like we should keep with the v1.x setting which is disabled (WARNING level) by default. There's a performance overhead to audit logging, and HADOOP-7633 provided not rationale (just ""We should add the audit logs as part of default confs"") as to why they were enabled for the packages.",design_debt,non-optimal_design
hadoop,8316,comment_0,"Patch attached. - update to NullAppender in log4j.properties, it is set explicitly by default in the bin and env scripts, so this is mostly a nop - and now default to the NullAppender in log4j.properties. Update hdfs.audit.logger in hadoop-env.sh to match. This is being made configurable in HADOOP-8224. mapred.audit.logger is not set in the bin or env scripts and is dead code, filed HADOOP-8392 for that (and to hookup RM/NM). Testing, verified the hdfs audit log is no longer automatically created and logged to when run from a tarball install.",code_debt,dead_code
hadoop,8316,comment_2,"Reviewed the patch, looks good to me.",non_debt,-
hadoop,8316,comment_3,1,non_debt,-
hadoop,8316,comment_4,I've committed this and merged to branch-2.,non_debt,-
hadoop,8327,summary,distcpv2 and distcpv1 jars should not coexist,non_debt,-
hadoop,8327,description,"Distcp v2 and Distcp v1 are currently both built, and the resulting hadoop-distcp-x.jar and hadoop-extras-x.jar end up in the same class path directory. This causes some undeterministic problems, where v1 is launched when v2 is intended, or even v2 is launched, but may later fail on various nodes because of mismatch with v1. According to (""Understanding class path wildcards"") ""The order in which the JAR files in a directory are enumerated in the expanded class path is not specified and may vary from platform to platform and even from moment to moment on the same machine."" Suggest distcpv1 be deprecated at this point, possibly by discontinuing build of distcpv1.",non_debt,-
hadoop,8327,comment_0,"Logalyzer and TestCopyFiles are two utilities that depend on distcp V1, and further use an incompatible constructor. I suggest renaming the DistCp (v1) class to DistCPV1 for now, which will prevent random distcp failures from the above problem, and not affect those utilities that still depend on DistCpV1. Further any external utilities that use this class will be flushed out, but the class will still be accessible (though now called DistCpV1). DistCp (v2) will still remain (untouched) as DistCp.",non_debt,-
hadoop,8327,comment_1,"The attached patch renames the v1 DistCp class to DistCpV1. Further, the utility dependencies that use it Logalyzer and TestCopyFiles are adjusted to use the renamed DistCpV1.",non_debt,-
hadoop,8327,comment_3,I don't seem to have any visibility as to why the above auto patching isn't succeeding. Patch looks good to me.,non_debt,-
hadoop,8327,comment_4,"Though the intent is for the fix is branch 0.23.2, I'm attaching a trunk patch now as it has been speculated that the auto patch is testing to trunk rather than 0.23.2 as the attached patch name specifies.",non_debt,-
hadoop,8327,comment_6,"Same trunk patch, just uploaded with grant license toggle flipped.",non_debt,-
hadoop,8327,comment_8,The patch looks good to me +1.,non_debt,-
hadoop,8327,comment_9,"Thanks Dave, I have put this into trunk, branch-2, and branch-0.23",non_debt,-
hadoop,8334,summary,HttpServer sometimes returns incorrect port,non_debt,-
hadoop,8334,description,{{HttpServer}} is not always returning the correct listening port.,non_debt,-
hadoop,8334,comment_0,"During some pending yarn work, {{HttpServer}} was returning -1 for the listening port on some of the tests. I found that -1 is only returned if the socket is not bound. This simplifies the logic of ensuring the port is bound.",non_debt,-
hadoop,8334,comment_2,"The changes look good, and a lot simpler then they were before. I like the new tests, and if everything passes I think it is OK. Have you tested this on a cluster?",non_debt,-
hadoop,8334,comment_3,"I haven't brought this up on a full cluster, but yarn tests appear ok. This is only about successfully binding, so there aren't host issues to worry about.",non_debt,-
hadoop,8334,comment_4,"Yes, you are right once it is bound your really don't need to do any more then that. +1 (looks like there is already a JIRA for TestSequenceFile)",non_debt,-
hadoop,8334,comment_5,"Thanks Daryn, I put this into trunk, branch-2, and branch-0.23",non_debt,-
hadoop,8341,summary,Fix or filter findbugs issues in hadoop-tools,non_debt,-
hadoop,8341,description,Now that the precommit build can test hadoop-tools we need to fix or filter the many findbugs warnings that are popping up in there.,code_debt,low_quality_code
hadoop,8341,comment_0,This is the breakdown of the existing findbugs issues. * Streaming - 7 (mostly returning internal data) * distcp - 2 (threading issues) * archives - 1 (casting Configuration to JobConf) * Rumen - 8 (returning internal data and serialization) * extras - 2 (some things should be marked final),non_debt,-
hadoop,8341,comment_1,"This patch fixes most of the findbugs issues, but also filters out a few of them where changes looked like they may impact the overall functionality of the code.",non_debt,-
hadoop,8341,comment_3,"Addressing the missing license statement in findbugs exclude file. The test passes for me locally. I think it is caused by running with an older version of an HDFS jar, so I am going to upload the new patch and see if that fixes it.",documentation_debt,low_quality_documentation
hadoop,8341,comment_5,"The javadoc issues are unrelated, and are being covered by a separate JIRA. No tests were included because no functionality should have changed, just cleared up some findbugs issues for defensive programming.",non_debt,-
hadoop,8341,comment_6,1,non_debt,-
hadoop,8358,summary,Config-related WARN for dfs.web.ugi can be avoided.,non_debt,-
hadoop,8358,description,"Looks easy to fix, and we should avoid using old config params that we ourselves deprecated.",code_debt,low_quality_code
hadoop,8358,comment_0,"This patch is in HADOOP cause its slightly wide/crossproject. Lemme know if I should split it though. Changes summary: * The NodeManager log (from where I noticed this first) showed this cause there was a hdfs-default.xml config set for this deprecated prop, and its web filter loaded that up. Cleaned up hdfs-default.xml. * The old property lookup existed in JspHelper in HDFS. Changed that to use the new property. * Cleaned up the usage of constants for this property, via classes instead of its own constant refs. * Added new prop and default to core-default.xml",code_debt,low_quality_code
hadoop,8358,comment_2,"Findbugs test-patch run seems to have issues with cross-module patches perhaps. I'm able to get perfect results from it via manual per-module exec: (Is the build supposed to fail? Per the test-patch, that message is given when it returns non-zero results.) I ran {{mvn javadoc:javadoc}} manually on common, where I did include one javadoc change and the results indicate that the warning is from some other change: Probably HADOOP-8172 was the one that introduced these. Hence, the -1s are both unrelated to my moderately trivial patch.",non_debt,-
hadoop,8358,comment_3,Filed HADOOP-8359 to address the javadocs.,non_debt,-
hadoop,8358,comment_4,Any further comments on the patch? Its quite trivial a change and helps remove unnecessary WARN noise.,code_debt,dead_code
hadoop,8358,comment_5,"Given this patch's trivialness (simple, non-breaking, change of old prop reliance across projects) I'll commit this in by Monday EOD if no one has any further objections.",non_debt,-
hadoop,8358,comment_6,Resubmitting patch pre-commit just to be sure about the Findbugs initialization issues.,non_debt,-
hadoop,8358,comment_8,"Rebased the core-site.xml changes that caused the patch application to fail. The Auto-HA merge to trunk changed the core-site.xml leading to this. Re-submmitting for another QA round before committing (for re-checking findbugs initialization fail, which doesn't occur locally).",non_debt,-
hadoop,8358,comment_10,"Failing test is unrelated to this change. Findbugs succeeded this time, so the previous issue was something else on trunk at the time or was a flaky result.",test_debt,flaky_test
hadoop,8358,comment_11,Committed revision 1343294 to branch-2 (i.e. merge -c of 1343290 also committed to trunk).,non_debt,-
hadoop,8360,summary,fails xml validation,non_debt,-
hadoop,8360,comment_1,+1. Also did an mvn rat:check to check that it passed license detection after this change. It does pass it. Committing to trunk.,non_debt,-
hadoop,8360,comment_2,Committed to trunk. Thanks for the contribution Radim!,non_debt,-
hadoop,8360,comment_8,"Harsh, given that this is a trivial change, can you please merge this to branch-2 and branch-2.1.0-beta. That way the delta between trunk and 2.1.0-beta is small.",non_debt,-
hadoop,8364,summary,Rationalize the way sub-components are built with ant in branch-1,non_debt,-
hadoop,8364,description,"Three different compile flags, compile.native, compile.c++, and compile.libhdfs, turn on or off different subcomponent builds, but they are generally all off or all on and there's no evident need for three different ways to do things. Also, in build.xml, jsvc and task-controller are included in targets ""package"" and ""bin-package"" as sub-ant tasks, while librecordio is included as a simple dependency. We should work through these and get them done in one understandable way. This is a matter of maintainability and understandability, and therefore robustness under future changes in build.xml. No substantial change in functionality is proposed.",build_debt,build_others
hadoop,8364,comment_0,"While we're at it, the result of doing a 64-bit and 32-bit build in either order should be made the same; ie order-independent, for the cumulative aspects of build. See further, HADOOP-8307 suggested patch.",non_debt,-
hadoop,8364,comment_1,Moved to 1.2.0 upon release of 1.1.0.,non_debt,-
hadoop,8364,comment_2,Changed Target Version to 1.3.0 upon release of 1.2.0. Please change to 1.2.1 if you intend to submit a fix for branch-1.2.,non_debt,-
hadoop,8364,comment_3,Closing as Won't Fix given development on branch-1 has effectively ceased.,non_debt,-
hadoop,8367,summary,Improve documentation of in rpc headers,documentation_debt,low_quality_documentation
hadoop,8367,description,None,non_debt,-
hadoop,8367,comment_0,The declaring-class name was added to the writable rpc engine's header to deal with the VersionedProtocol. Not needed for PB since version number is handled differently.,non_debt,-
hadoop,8367,comment_1,"I was wrong here. Since the connection is reused, the is needed. Will better document this in the code; will change the jira title to do this.",documentation_debt,outdated_documentation
hadoop,8367,comment_3,Minor comments: # #* Typo differnt #* The newly added comment is not very clear. Can you please add more information about what you mean by metaProtocols. Also the sentense does not read right. It might make sense to capture the same comments from hadoop_rpc.proto in here. # hadoop_rpc.proto some lines are going beyond 80 characters. Also the last sentence in the newly added comment does not read right.,documentation_debt,low_quality_documentation
hadoop,8367,comment_5,New tests not needed - javadoc improvements. Committed.,non_debt,-
hadoop,8395,summary,Text shell command unnecessarily demands that a SequenceFile's key class be WritableComparable,design_debt,non-optimal_design
hadoop,8395,description,"Text from Display set of Shell commands (hadoop fs -text), has a strict subclass check for a loaded key class to be a subclass of WritableComparable. The sequence file writer itself has no such checks (one can create sequence files with just plain writable keys, comparable is needed for sequence file's sorter alone, which not all of them use always), and hence its not reasonable for Text command to carry it either. We should relax the check and simply just check for ""Writable"", not",design_debt,non-optimal_design
hadoop,8395,comment_1,Seems reasonable. +1 assuming you tested this manually.,non_debt,-
hadoop,8395,comment_2,"Thanks for checking Todd. Yes, I manually tested the Text command with these two types of files (one used a WritableComparable set, the other uses Writable sets), just to make sure the subclass checks don't break anything. As WritableComparable subclasses Writable, this does not break anything, only relaxes the false requirement. Committing to trunk shortly.",non_debt,-
hadoop,8395,comment_5,Committed to trunk.,non_debt,-
hadoop,8403,summary,bump up POMs version to 2.0.1-SNAPSHOT,non_debt,-
hadoop,8403,description,None,non_debt,-
hadoop,8403,comment_0,* run 'mvn versions:set * verified no refs to 2.0.0-SNAPSHOT through out the POMs. * built TARBALL and verified all versions referenced are 2.0.1-SNAPSHOT,non_debt,-
hadoop,8403,comment_1,I think you need to update and also,non_debt,-
hadoop,8403,comment_3,"thx todd, missed the ant/ivy stuff, update patch takes care of it (hopefully mavenized gridmix goes in soon and ant/ivy will go away)",non_debt,-
hadoop,8403,comment_5,1,non_debt,-
hadoop,8403,comment_6,committed to branch-2,non_debt,-
hadoop,8405,summary,ZKFC tests leak ZK instances,code_debt,low_quality_code
hadoop,8405,description,"The ZKFC code wasn't previously terminating the ZK connection in all cases where it should (eg after a failed startup or after formatting ZK). This didn't cause a problem for CLI usage, since the process exited afterwards, but caused the test results to get clouded with a lot of ""Reconecting to ZK"" messages, which make the logs hard to read.",code_debt,low_quality_code
hadoop,8405,comment_0,+1 looks god,non_debt,-
hadoop,8405,comment_1,"Committed to branch, thanks Eli.",non_debt,-
hadoop,8421,summary,Verify and fix build of c++ targets in Hadoop on Windows,non_debt,-
hadoop,8421,description,There are a bunch of c++ files that are not compiled by default for legacy reasons. They represent important functionality. We need to make sure they build on Windows. There is some dependency on HADOOP-8368 ideas could be used in here.,non_debt,-
hadoop,8421,comment_0,"At this point, I don't think there is any portion of the native build that doesn't have an equivalent implementation on Windows, or at least a separate jira tracking a specific enhancement. I'm resolving this.",non_debt,-
hadoop,8431,summary,Running distcp wo args throws,non_debt,-
hadoop,8431,description,Running distcp w/o args results in the following:,non_debt,-
hadoop,8431,comment_0,What was the expected behaviour? A usage() statement?,non_debt,-
hadoop,8431,comment_1,"I guess this has been fixed by MAPREDUCE-2765. If args are null, method would be called in DistCp.java.",non_debt,-
hadoop,8431,comment_2,I'll close this issue.,non_debt,-
hadoop,8431,comment_3,closing this JIRA. I've checked with Daisuke this issue is already fixed in MAPREDUCE-2765. Thanks for checking!,non_debt,-
hadoop,8431,comment_4,I just tried this and confirmed it still fails with the latest build. In the future please try to reproduce the issue before you close it. $ ./bin/hadoop distcp 12/07/23 19:21:48 ERROR tools.DistCp: Invalid arguments: Target path not specified,non_debt,-
hadoop,8431,comment_5,"I reproduced it as well. A usage() statement is printed in addition to the error - is the change to be made just removing the ""Invalid arguments: Target path not specified"" printed to System.err? Or the ERROR log statement as well?",non_debt,-
hadoop,8431,comment_6,"It should just print the usage. Eg in the following I'd remove the ERROR log, the backtrace and the ""Invalid arguments"" log.",code_debt,low_quality_code
hadoop,8431,comment_8,"+1 looks great, verified the fix on a trunk tarball.",non_debt,-
hadoop,8431,comment_9,I've committed this and merged to branch-2. Thanks Sandy!,non_debt,-
hadoop,8431,comment_13,"Hi Eli and Sandy, I apologize for not having checked this issue carefully. Thanks for taking this and writing a patch!",non_debt,-
hadoop,8432,summary,SH script syntax errors,non_debt,-
hadoop,8432,description,"Hi, Everyone I just can't start with new binary release of hadoop with following CLI command: ... 78: ... Syntax error: word unexpected (expecting "")"") Inside the script start-dfs.sh there are multiple syntax wrongs. Could you fix it? Regards",non_debt,-
hadoop,8432,comment_0,"That failure would be expected if /bin/sh is not bash. For example, by default Debian uses dash for /bin/sh. If this is the cause of your problem, you can fix it by simply running the start-dfs.sh program directly, or by explicitly running it under bash with ""bash or by changing your system's /bin/sh back to bash with ""sudo dpkg-reconfigure dash"" and answer ""no"" when asked ""use dash as system default shell"".",non_debt,-
hadoop,8432,comment_1,"That failure would be expected if /bin/sh is not bash. For example, by default Debian uses dash for /bin/sh. If this is the cause of your problem, you can fix it by simply running the start-dfs.sh program directly, or by explicitly running it under bash with ""bash or by changing your system's /bin/sh back to bash with ""sudo dpkg-reconfigure dash"" and answer ""no"" when asked ""use dash as system default shell"".",non_debt,-
hadoop,8432,comment_2,"Hi, Everyone I have installed hadoop0.23.10 latest version Syntax error: word unexpected (expecting "")"") Could you please let us know how to resolve the above issue. Thanks, Madhavi",non_debt,-
hadoop,8432,comment_3,"Hello Everyone, Hadoop Version : 2.4.1v Am also getting same issue,can anyone feel free guide me 82: Syntax error: word unexpected (expecting "")"") Thanks, Govardhan.",non_debt,-
hadoop,8432,comment_4,Dropping fix-version from 'non-fixed' (didn't have code-fixes) JIRAs.,non_debt,-
hadoop,8438,summary,refers to examples jar file which doesn't exist,non_debt,-
hadoop,8438,description,is trying to find the file with the name and it is failing to find because the examples jar is renamed to,non_debt,-
hadoop,8438,comment_0,I have updated with the patch to fix the issue.,non_debt,-
hadoop,8438,comment_2,{code:xml} -1 tests included. The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. This test failure is not related to the patch.,non_debt,-
hadoop,8438,comment_3,"+1, change looks good.",non_debt,-
hadoop,8438,comment_5,I have just committed this to trunk and branch-2. Thanks a lot Devaraj K.,non_debt,-
hadoop,8439,summary,Update to support yarn configurations,non_debt,-
hadoop,8439,description,At present refers the classic mapred properties. It can be updated to support yarn configurations.,non_debt,-
hadoop,8439,comment_0,Closing as won't fix. This no longer is part of the Hadoop codebase.,non_debt,-
hadoop,8465,summary,hadoop-auth should support ephemeral authentication,non_debt,-
hadoop,8465,description,"Currently, once a client is authenticated the generated (& cookie) are valid for a given (service configurable) lifespan. Once the (& cookie) is issued, the authentication logic will not be triggered until the expires. This behavior does not work well with delegation tokens expected behavior where delegation tokens can be canceled at any time. Having ephemeral authentication (which is check on every request) would address this issue.",non_debt,-
hadoop,8465,comment_0,"This can be addressed by allowing an to set the expiration of the authentication token to ZERO (note that only ZERO would be supported, the cannot change to an arbitrary expiration interval). When the expiration is set to ZERO, the would let the request continue to the target resource but it will not issue an HTTP Cookie. This means that subsequent requests will be forced through the This will work with webhdfs delegation tokens where the delegationtoken must be part of the querystring of the request.",non_debt,-
hadoop,8465,comment_2,updated patch to apply after formatting changes in HADOOP-8458,non_debt,-
hadoop,8465,comment_4,"+1, the patch looks good to me.",non_debt,-
hadoop,8465,comment_5,committed to trunk and branch-2,non_debt,-
hadoop,8478,summary,hadoop-auth AuthenticatedURL does not work within an UGI doAs() call,non_debt,-
hadoop,8478,description,hadoop-auth is failing within a doAs block. It seems that the Kerberos credentials are not avail within the doAs block.,non_debt,-
hadoop,8478,comment_0,"It works just fine, by mistake I was kinit-ing instead login via the UGI. Once I've done the later it just works",non_debt,-
hadoop,8548,summary,test-patch.sh shows an incorrect link in Jekins builds,non_debt,-
hadoop,8548,description,Precommit builds show an incorrect link for javac warnings. Note that 'trunk' appears twice. Do we need $(basename $BASEDIR) in the following? Other places don't have it.,code_debt,low_quality_code
hadoop,8548,comment_0,"Thanks for the patch Kihwal, and thanks for fixing my bug :). +1 I put this into trunk.",non_debt,-
hadoop,8566,summary,throws a NPE if the class has no package (primitive types and arrays),non_debt,-
hadoop,8566,description,the accept() method should consider the case where the class getPackage() returns NULL.,non_debt,-
hadoop,8566,comment_0,+1 pending Jenkins (which I just kicked off).,non_debt,-
hadoop,8566,comment_3,Run the failed tests locally without and with the patch and they pass. The failures seem unrelated. Reattaching patch to force a rerun.,non_debt,-
hadoop,8566,comment_5,committed to trunk and branch-2,non_debt,-
hadoop,8597,summary,FsShell's Text command should be able to read avro data files,non_debt,-
hadoop,8597,description,"Similar to SequenceFiles are Apache Avro's DataFiles. Since these are getting popular as a data format, perhaps it would be useful if {{fs -text}} were to add some support for reading it, like it reads SequenceFiles. Should be easy since Avro is already a dependency and provides the required classes. Of discussion is the output we ought to emit. Avro DataFiles aren't simple as text, nor have they the singular Key-Value pair structure of SequenceFiles. They usually contain a set of fields defined as a record, and the usual text emit, as available from avro-tools via is in proper JSON format. I think we should use the JSON format as the output, rather than a delimited form, for there are many complex structures in Avro and JSON is the easiest and least-work-to-do way to display it (Avro supports json dumping by itself).",non_debt,-
hadoop,8597,comment_0,"The proposed patch adds the logic to output the content of Avro data files in JSON format. The implementation does not use the DataFileReadTool class since, as it turned out, the package is not currently part of the project's dependencies. As a consequence this allowed a more memory efficient implementation, which keeps only a constant number of Avro records in memory.",non_debt,-
hadoop,8597,comment_1,This looks like a useful addition. Can you please add a unit test for it?,test_debt,lack_of_tests
hadoop,8597,comment_2,Done - a unit test is added.,non_debt,-
hadoop,8597,comment_3,"Not sure why, but your patch file didn't apply cleanly for me. Here's the same patch, but a version that applies cleanly.",non_debt,-
hadoop,8597,comment_4,+1 Patch looks good to me. Let's see what Jenkins says.,non_debt,-
hadoop,8597,comment_6,Jenkins says that should be static.,code_debt,low_quality_code
hadoop,8597,comment_7,New version with AvroFileInputStream made static.,non_debt,-
hadoop,8597,comment_9,"Sorry for the inconvenience that applying my patch caused. Since I am new to the project I was unsure against which version (or branch) to create the patch - so I chose It seemed to most closely match the ""Affects Version/s"" field. In retrospect the choice was probably a mistake. To avoid such problems in the future, I would like to ask the following question - Should patches be created against the first branch with a version number greater or equal to that in the ""Affects Version/s"" field in the current case) or if the version is new enough to directly use the trunk. Thank you for taking the time to review my patch. I hope that it will be useful and would be very happy if it gets committed.",non_debt,-
hadoop,8597,comment_10,"Ivan, patches are normally against trunk. After they're committed to trunk they may be backported to a branch. This patch should probably be committed to trunk and to branch-2 with fix-version 2.0.3-alpha.",non_debt,-
hadoop,8597,comment_12,"I just committed this. Thanks, Ivan!",non_debt,-
hadoop,8633,summary,Interrupted FsShell copies may leave tmp files,non_debt,-
hadoop,8633,description,"Interrupting a copy, ex. via SIGINT, may cause tmp files to not be removed. If the user is copying large files then the remnants will eat into the user's quota.",non_debt,-
hadoop,8633,comment_0,{{deleteOnExit}} issues contribute and/or are related to the problem,non_debt,-
hadoop,8633,comment_1,Will submit after deps are checked in.,non_debt,-
hadoop,8633,comment_2,Looks good to me. I like the use of,non_debt,-
hadoop,8633,comment_4,The changes look OK. I'm not sure I really like TargetFileSystem. I just don't see a lot of benefit from making TargetFileSystem. I think it would be cleaner to just have your own list of Paths to delete and then delete them in the finally block. Having it be a FileSystem just seems confusing to me. Because TargetFileSystem is never added to the FileSystem cache I assume that if a System.exit is called somewhere while processing the command the TargetFileSystem will not be closed and the files will not actually be deleted on exit. Is this important?,code_debt,low_quality_code
hadoop,8633,comment_5,"The is just a shim over a real filesystem that is registering and canceling temp paths for deletion. The shim simplifies all the code performing the copy and helps ensure the temp files are cancelled and/or deleted immediately. I originally did what you suggest and I wound up with multiple nested try blocks and conditions that made the code (imho) harder to read, understand, and difficult to test. It's true that a call to {{System.exit}} won't cleanup the filesystem, but it's only called as the last line in {{main}}. Calling it in other places would break functionality and be a bug. (I did manually run a copy with 100 iterations and pounded on control-c and no remnants where left)",non_debt,-
hadoop,8633,comment_6,Ok +1,non_debt,-
hadoop,8633,comment_7,"I went ahead and committed this. Thanks Daryn, Bobby, and Kihwal!",non_debt,-
hadoop,8637,summary,is broken,non_debt,-
hadoop,8637,description,is being passed through as Example of impact is checksums cannot be disabled for LFS if a filter fs (like is applied.,non_debt,-
hadoop,8637,comment_1,Change looks good +1 assuming Jenkins comes back OK.,non_debt,-
hadoop,8637,comment_3,"Thanks Daryn, I put this into trunk, branch-2, and branch-0.23",non_debt,-
hadoop,8688,summary,Hadoop in Pseudo-Distributed mode on Mac OS X 10.8,non_debt,-
hadoop,8688,description,"When running hadoop on pseudo-distributed mode, the map seems to work, but it cannot compute the reduce. 12/08/13 08:58:12 INFO mapred.JobClient: Running job: 12/08/13 08:58:13 INFO mapred.JobClient: map 0% reduce 0% 12/08/13 08:58:27 INFO mapred.JobClient: map 20% reduce 0% 12/08/13 08:58:33 INFO mapred.JobClient: map 30% reduce 0% 12/08/13 08:58:36 INFO mapred.JobClient: map 40% reduce 0% 12/08/13 08:58:39 INFO mapred.JobClient: map 50% reduce 0% 12/08/13 08:58:42 INFO mapred.JobClient: map 60% reduce 0% 12/08/13 08:58:45 INFO mapred.JobClient: map 70% reduce 0% 12/08/13 08:58:48 INFO mapred.JobClient: map 80% reduce 0% 12/08/13 08:58:51 INFO mapred.JobClient: map 90% reduce 0% 12/08/13 08:58:54 INFO mapred.JobClient: map 100% reduce 0% 12/08/13 08:59:14 INFO mapred.JobClient: Task Id : Status : FAILED Too many fetch-failures 12/08/13 08:59:14 WARN mapred.JobClient: Error reading task outputServer returned HTTP response code: 403 for URL: 12/08/13 08:59:14 WARN mapred.JobClient: Error reading task outputServer returned HTTP response code: 403 for URL: 12/08/13 08:59:18 INFO mapred.JobClient: map 89% reduce 0% 12/08/13 08:59:21 INFO mapred.JobClient: map 100% reduce 0% 12/08/13 09:00:14 INFO mapred.JobClient: Task Id : Status : FAILED Too many fetch-failures Here is what I get when I try to see the tasklog using the links given in the output 2012-08-13 08:58:39.189 java[74092:1203] Unable to load realm info from SCDynamicStore  I have changed my hadoop-env.sh acoording to Mathew Buckett in Also this error of Unable to load realm info from SCDynamicStore does not show up when I do 'hadoop namenode -format' or 'start-all.sh'",non_debt,-
hadoop,8688,comment_0,"Hi Subho, Please redirect your question to the lists. The JIRA is for identified bugs on the project, not user support. P.s. ""Unable to load realm info from SCDynamicStore"" warnings shouldn't block out functionality, and is more of an annoyance than anything else. 403 suggests a permission issue of some sort, and I'd look at the TaskTracker logs for any logs from org.mortbay package classes. In any case, please do send your issue to and am sure the community will help you out.",non_debt,-
hadoop,8688,comment_1,Ok... I will send it there! Thanks.,non_debt,-
hadoop,8688,comment_2,Thanks!,non_debt,-
hadoop,8688,comment_3,"""Unable to load realm info from SCDynamicStore"" is OSX whining about kerberos stuff -happens a lot, meaningless, no way to disable it (it's coming in at the java.util.logging stream, too)",non_debt,-
hadoop,8718,summary,may throw,non_debt,-
hadoop,8718,description,"may throw as may return Empty Array ""[]"". this happened when run the test case of viewfs on my jenkins server. In my situation, ""/"" is passed into as its param ""src"". Here is the Message given by Jenkins: 1 Method) Method) Standard Output 2012-08-22 10:31:40,487 INFO mortbay.log - Home dir base /",non_debt,-
hadoop,8718,comment_0,"this issue is the same as HADOOP-8299, so I closed it.",non_debt,-
hadoop,8741,summary,"Broken links from ""Cluster setup"" to *-default.html",documentation_debt,low_quality_documentation
hadoop,8741,description,"Hi, The links from the cluster setup pages to the configuration files are broken. Read-only default configuration should be The same holds for the three configuration : core, hdfs and mapred.",documentation_debt,low_quality_documentation
hadoop,8741,comment_0,Yeah instead of /current we need to point to /stable. The /current points to the latest release now.,non_debt,-
hadoop,8741,comment_1,"Harsh, currently we configure the links for the default configuration in Forrest's site.xml as external. Is this on purpose ? Or can we refer to the configuration html files generated as part of the doc build. That way, I suppose the cluster_setup can directly refer without going external. Since configuration changes per release, we should probably link relative, right ? Also, I see lots of other files files referred this way, including streaming, distcp, HAR, etc.",non_debt,-
hadoop,8741,comment_2,"One reason for the current state is probably because of the first project split, when the links might have become external. Do we want to merge this back now ? (I see there's a general discussion going on about splitting to TLPs again, so waiting on this bug could be another option).",non_debt,-
hadoop,8741,comment_3,On version 2.x and above this is no longer an issue. The affected documentations are 1.0.4 and 1.2.1. These versions are EOL.,defect_debt,uncorrected_known_defects
hadoop,8766,summary,should randomize the root dir,non_debt,-
hadoop,8766,description,should randomize the name of the root directory it creates. It currently hardcodes LOCAL_FS_ROOT_URI to {{/tmp/test}}. This causes the job to fail if it clashes with another jobs that also uses that path. Eg,non_debt,-
hadoop,8766,comment_0,It's easier just to use (that's the approach I took here),non_debt,-
hadoop,8766,comment_2,"+1, the patch looks good to me. I'm going to commit this momentarily.",non_debt,-
hadoop,8766,comment_3,"I've just committed this to trunk and branch-2. Thanks a lot for the contribution, Colin.",non_debt,-
hadoop,8819,summary,Should use && instead of & in a few places in,code_debt,low_quality_code
hadoop,8819,description,Should use && instead of & in a few places in,code_debt,low_quality_code
hadoop,8819,comment_1,+1 for the change.,non_debt,-
hadoop,8819,comment_2,I will commit after SVN maintenance is done to trunk and 2.0. Is there a similar problem in 1.x as well?,non_debt,-
hadoop,8819,comment_3,I committed the patch trunk and branch-2. Keeping it open to see if this patch is also needed for 1.x.,non_debt,-
hadoop,8819,comment_6,"Note that the change in FTPFileSystem actually is a behavior change, and perhaps an incompatible one. All of the rest of these changes seem harmless, but that one seems a little suspect.",design_debt,non-optimal_design
hadoop,8819,comment_8,"In this code if {{created}} is true it enters the if condition and then executes If {{created}} is false that part of the code is not entered. Aaron, can you add details on why this would change the behavior?",code_debt,low_quality_code
hadoop,8819,comment_9,"Aha! I misread it. You're right - it won't change the behavior at all. Sorry about that. That would imply, then, that we could completely get rid of the ""created &&"", right?",non_debt,-
hadoop,8819,comment_10,On the second reading of the code I thought of it - too late :-),non_debt,-
hadoop,8819,comment_13,"The same problem exists in branch-1 (except viewfs). Uploaded a branch-1 patch. Removed ""created &&"" this time. :-)",non_debt,-
hadoop,8819,comment_15,+1 for the the branch-1 patch. Thank you Brandon!,non_debt,-
hadoop,8819,comment_16,I just pulled this into branch-0.23,non_debt,-
hadoop,8843,summary,Old trash directories are never deleted on upgrade from 1.x,non_debt,-
hadoop,8843,description,"The older format of the trash checkpoint for 1.x is yyMMddHHmm the new format is yyMMddHHmmss(-\d+)? so if you upgrade from an old cluster to a new one, all of the entires in .trash will never be deleted because they currently are always ignored on deletion. We should support deleting the older format as well.",non_debt,-
hadoop,8843,comment_0,Marking the bug as critical.,non_debt,-
hadoop,8843,comment_1,Moved to HADOOP since this is an issue in TrashPolicyDefault. It needs to look for the old format when parsing checkpoints for timestamps.,non_debt,-
hadoop,8843,comment_2,Patch to update TrashPolicyDefault so it checks for the old checkpoint format if parsing the new format fails.,non_debt,-
hadoop,8843,comment_4,"Looks good to me, except for one small nit: it would be worth a javadoc comment on OLD_CHECKPOINT saying something like: /** Format of checkpoint directories used prior to Hadoop 0.23. */ and then a small comment at the point where it's used, like // Check for old-style checkpoint directories left over after an upgrade from Hadoop 1.x Otherwise I think people may be confused what ""old"" refers to, a few years down the line.",code_debt,low_quality_code
hadoop,8843,comment_5,"Thanks for the review, Todd. Updated the patch accordingly.",non_debt,-
hadoop,8843,comment_6,"+1, I'll let you have the honor of committing it yourself :)",non_debt,-
hadoop,8843,comment_8,"Thanks, Todd. Pushing this in.",non_debt,-
hadoop,8843,comment_9,"I committed this to trunk, branch-2, and branch-0.23.",non_debt,-
hadoop,8866,summary,is O(N^2) instead of O(N),non_debt,-
hadoop,8866,description,"does O(N) calls LinkedList#get() in a loop, rather than using an iterator. This makes query O(N^2), rather than O(N).",code_debt,slow_algorithm
hadoop,8866,comment_0,"Small patch. TestMutableMetrics still passes, so I think correctness is fine. I can run a real performance test if people want to see that too, but it seems strictly better to me.",non_debt,-
hadoop,8866,comment_1,"While you're at it, why not get rid of size() and numeric iteration too? eg:",code_debt,low_quality_code
hadoop,8866,comment_2,"I suppose, though I think we need to keep a counter to pass to Would you still prefer to see the foreach-style syntax?",non_debt,-
hadoop,8866,comment_3,"I guess what you're doing now makes more sense then, sorry I missed that. +1 pending Jenkins",non_debt,-
hadoop,8866,comment_5,The patch looks good to me as well. Committing momentarily.,non_debt,-
hadoop,8866,comment_6,"I've just committed this to trunk and branch-2. Thanks a lot for the contribution, Andrew.",non_debt,-
hadoop,8895,summary,"TokenRenewer should be an interface, it is currently a fully abstract class",design_debt,non-optimal_design
hadoop,8895,description,TokenRenewer is a fully abstract class. Making it an interface will allow classes extending other classes to implement the interface.,design_debt,non-optimal_design
hadoop,8895,comment_0,"Just curious, do you have a specific case in mind where this would be useful? Keep in mind that the service loader is going to instantiate the registered classes with their default ctor, which probably won't work well for heavier weight classes intended for other purposes.",non_debt,-
hadoop,8895,comment_1,"Daryn, Thanks for your input. Though it is not absolutely necessary for {{TokenRenewer}} to be an interface, I felt it would make things simpler when working on HDFS-4009. For instance, if a could implement {{TokenRenewer}}, we might not need Now that we are removing completely - {{HADOOP-8891}} - it won't be immediately applicable. However, I was not sure why it should be an abstract class. Do you think we should leave it as is? Is there an advantage of abstract class over interface? I am uploading a patch with my proposed changes.",code_debt,low_quality_code
hadoop,8895,comment_3,"Abstract classes are easier to evolve than interfaces, since you can add a method with a default implementation without breaking implementors. TokenRenewer is only for use internally so it doesn't matter so much, but I would still leave it as it is since it's always possible to create a new implementation class that extends the abstract class.",non_debt,-
hadoop,8895,comment_4,"Correct to the second half. The interface only existed for the code being removed, so it too should be removed. I actually intended for {{TokenRenewer}} to be implemented as an interface for classes implementing tokens. This idea was rejected in favor of using lower level renewers that bypassed the token issuing class. I believe part of the idea as a class is to force simple lightweight renewer objects since the service loader will instantiate all of them. Also, the filesystems currently cannot directly implement {{TokenRenewer}} because the service loader instantiation won't be usable. It will either need to instantiate another copy of itself or its lower level client with the token's address, or will need to initialize itself - but filesystems are not designed to be initialized multiple times. This is why I think it's not needed.",non_debt,-
hadoop,8895,comment_5,Fair enough. Thanks Tom and Daryn. I ll close this JIRA as won't fix. We can may be re-open should a need arise. Thanks again.,non_debt,-
hadoop,8912,summary,adding .gitattributes file to prevent CRLF and LF mismatches for source and text files,non_debt,-
hadoop,8912,description,Source code in hadoop-common repo has a bunch of files that have CRLF endings. With more development happening on windows there is a higher chance of more CRLF files getting into the source tree. I would like to avoid that by creating .gitattributes file which prevents sources from having CRLF entries in text files. I am adding a couple of links here to give more primer on what exactly is the issue and how we are trying to fix it. # # This issue for adding .gitattributes file to the tree.,design_debt,non-optimal_design
hadoop,8912,comment_0,"Please do not submit this, until HADOOP-8911 is submitted.",non_debt,-
hadoop,8912,comment_1,Let's do this for trunk and branch-2 as well.,non_debt,-
hadoop,8912,comment_2,I have patches ready for trunk also. I am just waiting for this to be accepted and will submit the other patches.,non_debt,-
hadoop,8912,comment_5,"Maybe I'm missing something, but I don't see how this will actually prevent CRLF and LF files from getting checked in to the repo, since actual commits/merges are done via svn, not git. The svn repo is mirrored to a git repo, but that repo is read only.",non_debt,-
hadoop,8912,comment_6,"I am not sure how many people pull from git, but this would help only a subset of developers. This will only prevent CR/LF from developers, who clone from a git repo, make changes and submit a patch.",non_debt,-
hadoop,8912,comment_7,"Ah, got it. Thanks for the explanation. It will help for those developers who clone the git repo and `git add ...' their files before generating a patch. In that case, it will help somewhat to commit this, but it won't solve the problem 100% of the time. Raja, do you happen to know if there's a semantically equivalent thing we could do for svn, to ensure that committers don't check in bad line endings?",design_debt,non-optimal_design
hadoop,8912,comment_8,"It can definitely be achieved with a pre-commit hook with svn. A small script that disallows CRLF for known file types and text files would do the trick. It should be pretty straight forward to do this. I am not quite sure with apache infrastructure, if they let you run these triggers on their servers etc.,",non_debt,-
hadoop,8912,comment_9,+1 for the patch. We should figure out a way to do this for svn and do it in a separate jira.,non_debt,-
hadoop,8912,comment_10,"I committed the patch to trunk, branch-2 and 1-win. Thank you for the patch Raja.",non_debt,-
hadoop,8921,summary,ant build.xml in branch-1 ignores -Dcompile.native,non_debt,-
hadoop,8921,description,"ant still runs autoconf and libtoolize According to ant 1.8 manual, any <target if Fixing it by moving the if condition up into the ""compile-native"" target and changing it to a param substitution instead of being evaluated as a condition.",non_debt,-
hadoop,8921,comment_0,Patch to build.xml,non_debt,-
hadoop,8921,comment_1,After patch,non_debt,-
hadoop,8921,comment_2,Expand patch to cover the compile-core target as well,non_debt,-
hadoop,8921,comment_3,"Re-update patch to run native builds by default, but disable when is provided.",non_debt,-
hadoop,8921,comment_4,Enable native libraries only after checking for x86_64/x86 availability.,non_debt,-
hadoop,8921,comment_6,"The patch looks good. Even though is redundant in the command ""ant compile-native compile-native"" can do the job itself), it's still good to be consistent. One minor thing is the os-arch check. Original code doesn't check the x86_64/x86 availability, native code compilation can fail on certain platforms(such as MacOS). At lease the developer will notice the failure. With the os-arch check in the patch, the ""ant compile-native"" command will report success but actually didn't do the work. The original behavior seems more intuitive to me.",code_debt,complex_code
hadoop,8921,comment_7,"I've personally wasted over an hour to figure out that option has to be explicitly disabled before it skips running autoconf/automake (in create-configure part). The older approach was bad for someone who did a git checkout and ran a build without paying attention to the docs about native library compatibility. I'd say the optimizations (i.e non-core features) can be skipped over for a clean build on platforms where the code won't compile / isn't supported. Every single attempt to build on a Mac would result in a failed compilation, until someone discovers the -Dcompile.native option. Clean builds on ""ant compile"" should be encouraged (on any platform) without RTFMing for a -D option. Of course on platforms where the native code is indeed supported, it would error out if say, JNI headers can't be found.",build_debt,build_others
hadoop,8921,comment_8,"Thanks for providing this patch, . I've found that in the current branch-1 codebase, I also get a failure on Mac due to the native compile of task controller. I'm uploading The only difference from the version 4 patch is that I have added the if native check to the task-controller target:",non_debt,-
hadoop,8921,comment_9,"I agree with Brandon. compile-native target was made available for the purpose of building libhadoop without having to set the compile.native flag. Yes, I agree, when calling compile-core-native without setting compile.native flag autoreconf should get triggered. Here is what I propose, How about moving the depends into antcalls inside the by doing so the autoreconf won't get triggered if compile.native is not set. Finally if we think task-controller target should be controlled with compile.native flag then we should mark this jira ""incompatible"" At least for me I do ant task-controller and expect it to build. here is my v6 patch with my thoughts above.",non_debt,-
hadoop,8921,comment_10,"Thanks, Giri. This makes sense. For my part, I was really just looking for a way to run ""ant clean package"" on Mac without triggering autoreconf and the other native build steps. Moving these from depends to antcalls inside compile-core-native accomplishes the same goal, and like you said, it preserves the behavior of not needing to set the compile.native property when you're already calling the targets related to native build. Regarding task-controller, the difficulty is that task-controller is called as a subant of package, which makes it difficult to test the package target on Mac, where task-controller fails to compile. Could we set this up in a way similar to compile-native, which calls and explicitly turns on the compile.native parameters? Then, inclusion of task-controller would be controlled by the compile.native flag when running ""ant package"", but users who run ""ant task-controller"" would still get the existing behavior without needing to pass the extra parameter. (Then, there would be no need to mark this ""incompatible"".)",non_debt,-
hadoop,8921,comment_11,"Actually, I take back my last comment about task-controller. It would still be incompatible in the sense that people running ""ant package"" may currently expect it to bundle task-controller, and the change I described would require them to start passing the compile.native parameter. I am +1 for version 6 of the patch after removal of if=""compile.native"" from the task-controller target. This is enough to get ""ant compile"" working on Mac. We'd still need a Linux VM to run ""ant package"". Does anyone think that perhaps the right thing to do is to get task-controller compiling on Mac? This would be handled in a separate jira. I've pasted the compilation failures below. PATH_MAX undeclared is trivially fixed by adding #include <limits.h",non_debt,-
hadoop,8921,comment_12,"Also linked to HADOOP-7927, which is a similar bug report.",non_debt,-
hadoop,8921,comment_13,If anyone has comments pls do so before 02/05. I'm planning to commit version 6 (removing if compile.native) by tomorrow.,non_debt,-
hadoop,8921,comment_14,"FYI, I filed MAPREDUCE-4995 for task-controller compilation failure on Mac.",non_debt,-
hadoop,8921,comment_15,"+1 for the minimal change that gets ""ant compile"" working on OS X. This change can be left out. {code:xml} <!-- taskcontroller targets  <target depends=""init""+ <target depends=""init"" if=""compile.native"" + task-controller if compile.native is set""",non_debt,-
hadoop,8921,comment_16,I edited Gopal's patch to keep the bare minimal changes to fix the OS X build. Verified build on OS X and native build on Linux (branch-1).,non_debt,-
hadoop,8921,comment_17,"Hi Giri, does the latest patch looks good to you? Could you commit it?",non_debt,-
hadoop,8923,summary,WEBUI shows an intermediatory page when the cookie expires.,non_debt,-
hadoop,8923,description,"The WEBUI does Authentication (SPNEGO/Custom) and then drops a cookie. Once the cookie expires, the webui displays a page saying that ""authentication token expired"". The user has to refresh the page to get authenticated again. This page can be avoided and the user can authenticated without showing such a page to the user. Also the when the cookie expires, a warning is logged. But there is no need to log this as this is not of any significance.",non_debt,-
hadoop,8923,comment_10,This patch works for me. +1,non_debt,-
hadoop,8923,comment_11,"Benoy, the patch does not apply to branch-1. Can you please rebase it?",non_debt,-
hadoop,8923,comment_12,Seems like this is an important patch. Can you-all please get it in to branch-1? Thanks!,non_debt,-
hadoop,8923,comment_13,This patch applies on branch 1.1 . Let me know if any changes are required.,non_debt,-
hadoop,8923,comment_14,Changed Target Version to 1.3.0 upon release of 1.2.0. Please change to 1.2.1 if you intend to submit a fix for branch-1.2.,non_debt,-
hadoop,8923,comment_15,Cleaning up jiras which is not relevant anymore.,non_debt,-
hadoop,8929,summary,"Add toString, other improvements for SampleQuantiles",non_debt,-
hadoop,8929,description,"The new SampleQuantiles class is useful in the context of benchmarks, but currently there is no way to print it out outside the context of a metrics sink. It would be nice to have a convenient way to stringify it for logging, etc. Also: - made it Comparable and changed the HashMap to TreeMap so that the printout is in ascending percentile order. Given that this map is always very small, and snapshot() is only called once a minute or so, the runtime/memory differences between treemap and hashmap should be negligible. - changed the behavior to return null instead of throw, because all the catching, etc, got pretty ugly. In implementing toString, I figured I'd clean up the other behavior along the way.",code_debt,low_quality_code
hadoop,8929,comment_0,Attached patch implements toString. Sample output from,non_debt,-
hadoop,8929,comment_1,"In addition to toString, the patch also makes Quantile a Comparable and changes the contract of SampleQuantiles to return null instead of throwing IOException if no samples were added. Is that intentional, or was the goal to just do toString right now?",non_debt,-
hadoop,8929,comment_2,"Sorry, should have explained the patch in more detail: - made it Comparable and changed the HashMap to TreeMap so that the printout is in ascending percentile order. Given that this map is always very small, and snapshot() is only called once a minute or so, the runtime/memory differences between treemap and hashmap should be negligible. - changed the behavior to return null instead of throw, because all the catching, etc, got pretty ugly. In implementing toString, I figured I'd clean up the other behavior along the way.",code_debt,low_quality_code
hadoop,8929,comment_4,"+1 Thanks for the additional details. I asked about the throw change because I wondered if it changed API contract for any callers. I took a deeper look though, and now I can see that this is internal to these metrics classes, so it looks like that's not an issue.",non_debt,-
hadoop,8929,comment_5,"+1, patch looks great.",non_debt,-
hadoop,8929,comment_6,Summary and the bug description needs update based on this [comment |,documentation_debt,outdated_documentation
hadoop,8929,comment_7,"Committed to branch-2 and trunk, thanks for the reviews.",non_debt,-
hadoop,8985,summary,Add namespace declarations in .proto files for languages other than java,non_debt,-
hadoop,8985,description,"Currently .proto files use java_package to specify java packages in proto files, but namespace are not specified for other languages such as cpp, this causes name collision in cpp. we can add namespace declarations to avoid this. In Java, the package specifier is used as the Java package, unless you explicitly provide a option java_package in your .proto file. So the original java package will not be affected. About namespace name, how about in cpp) for all common sub-project proto files, and in cpp) for all hdfs sub-project proto files?",code_debt,low_quality_code
hadoop,8985,comment_0,Please create a separate jira for HDFS. Lets do the ones in this.,non_debt,-
hadoop,8985,comment_2,+1 for the patch. I committed it to trunk and branch-2. Thank you Binglin.,non_debt,-
hadoop,9056,summary,Build native library on Windows,non_debt,-
hadoop,9056,description,The native library (hadoop.dll) must be compiled on Windows.,non_debt,-
hadoop,9056,comment_0,Attached a patch to port Chuan Liu's changes to branch-trunk-win. Some additional changes to account for branch divergence.,non_debt,-
hadoop,9056,comment_1,"Looks good in general! I have the following questions and comments. # It seems this patch only contains changes that made compiling hadoop.dll available on Windows. Some functionality was missing. For example, POSIX.chmod was not ported over from branch-1-win; so were some other IO methods. I also notice some new functions were introduced in trunk compared with branch-1. For example, and What is the plan for those missing functions? Do we plan to port them over in other JIRAs? # file is not needed. # Snappy does not work on Windows right now. It may be better to exclude snappy related files from Windows build. # I think it is better to use WINDOWS instead of  _WIN32 macro in some places because we explicitly defined the UNIX and WINDOWS macros in the beginning of It will make it easier in the future if we want to change the definition of the macros. # Some changes in SecureIOUtils and Datanode are not ported over. Again I think this is related to point 1 above.",build_debt,build_others
hadoop,9056,comment_2,"Thanks for the feedback Chuan. I have addressed most of your comments. Most of the SecureIOUtils changes don't seem to be applicable in trunk. There is no equivalent to posix_fadvise in Win32 but we may be able to achieve a similar effect by passing flags to CreateFile, we can address that in a separate patch.",non_debt,-
hadoop,9056,comment_3,", why the SecureIOUtils and Datanode changes not apply to trunk? Could you explain a little bit? Because I still see the old methods presenting in the trunk code base.",non_debt,-
hadoop,9056,comment_4,", I had missed porting one change to so thanks for pointing that out. Fixed that and updated the patch to merge HADOOP-7115 which was pulled down from trunk on 9/30. Also added NativeCrc32 into hadoop.dll (this is new to trunk). The function appears to have been deprecated in trunk. Please let me know if you see anything else missing. Thanks! Arpit",non_debt,-
hadoop,9056,comment_5,"Arpit, thanks for the new patch! All look good! I just want to know if we still need the datanode change in HADOOP-8564. I still see moveBlockFiles() method in FsDatasetImpl.java, however, I am not sure if its usage will cause the same problem we have seen in branch-1.",non_debt,-
hadoop,9056,comment_6,"Hi Chuan, Thanks for bringing HADOOP-8564 to my attention. I have opened HDFS-4297 to investigate and port it since it looks like a rather large patch in itself. I am a little hesitant to increase the scope of HADOOP-9056 any further. Let me know if that sounds reasonable. Thanks!",non_debt,-
hadoop,9056,comment_7,+1 Thanks for all the work and investigation! The change looks good for me!,non_debt,-
hadoop,9056,comment_8,+1 for the patch. I committed the patch to branch-trunk-win. Thank you Chuan for the original branch-1-win patch and reviewing this patch. Thank you Arpit for porting all the related changes to branch-trunk-win.,non_debt,-
hadoop,9056,comment_9,Thanks Suresh!,non_debt,-
hadoop,9165,summary,Dynamic Resource/Slot Configuration on NM/TT,non_debt,-
hadoop,9165,description,"The current Hadoop MRV1/YARN resource management logic assumes per node (TT/NM) resource is static during the lifetime of the TT/NM process. Allowing run-time configuration on per node resource will give us finer granularity of resource elasticity. This allows Hadoop workloads to coexist with other workloads on the same hardware efficiently, whether or not the environment is virtualized.",non_debt,-
hadoop,9165,comment_0,Attached is a draft proposal v0.1. I will create two separated JIRAs to address effort on YARN and MRV1.,non_debt,-
hadoop,9165,comment_1,"Junping, I'd suggest splitting the YARN and MAPREDUCE docs. I am in favor of YARN-291 instead of TT changes mainly because it flows so much smoothly in YARN and also how complicated the TT changes could turn out to be. Please close this as invalid as you already have YARN and MAPREDUCE specific tickets. Tx.",documentation_debt,low_quality_documentation
hadoop,9165,comment_2,Ok. Thanks Vinod for good suggestions here. I will close this issue and tracking in separated JIRAs for YARN and MRV1.,non_debt,-
hadoop,9165,comment_3,Track this issue in separated JIRA for YARN and MRV1.,non_debt,-
hadoop,9165,comment_4,"Hi Vinod, I don't think TT changes is complicated, in fact TT can be unchanged, and only some small changes in JT can do the trick. I post a initial patch in MAPREDUCE-4900 to demonstrate this.",non_debt,-
hadoop,9218,summary,Document the Rpc-wrappers used internally,non_debt,-
hadoop,9218,description,None,non_debt,-
hadoop,9218,comment_0,Several methods on ipc.Server and RPC use type Writable *internally* as a wrapper so that they work across multiple RpcEngine kinds. The request on the wire is in Protobuf for the Protocbuf Rpc Engine. Also the basic rpc headers are also in protobuf. Patch adds javadoc and also changes the names of wrappers.,documentation_debt,low_quality_documentation
hadoop,9218,comment_2,+1 for the change.,non_debt,-
hadoop,9218,comment_7,Merged into branch2,non_debt,-
hadoop,9223,summary,support specify config items through system property,non_debt,-
hadoop,9223,description,"The current hadoop config items are mainly interpolated from the *-site.xml files. In our production environment, we need a mechanism that can specify config items through system properties, which is something like the gflags in system built with C++, it's really very handy. The main purpose of this patch is to improve the convenience of hadoop systems, especially when people do testing or perf tuning, which always need to modify the *-site.xml files If this patch is applied, then people can start hadoop programs in this way: java -cp $class_path $program",design_debt,non-optimal_design
hadoop,9223,comment_0,This is currently possible via substitution: Does that alone not suffice?,non_debt,-
hadoop,9223,comment_1,"Substitution does suffice the basic function, but it needs *-site.xml as a template, this isn't very handy sometime.",design_debt,non-optimal_design
hadoop,9223,comment_2,"For example, we implemented a deploy and manage toolkit for our hadoop clusters, most of the operations are done in shell command line, we do not want to specify several config files for each cluster, we use the toolkit to generate the config at runtime, and pass it by it's really handy:)",non_debt,-
hadoop,9223,comment_3,Thanks Zesheng. The Tool and ToolRunner interface also support parsing -D app-level options to add to the base loaded configs; but am not sure if you can use that as well as a more elegant fix for this.,non_debt,-
hadoop,9223,comment_4,"Thanks Harsh. It's certain that the Tool and ToolRunner can fix the above senario, but not all programs are suitable to run with Tool or ToolRunner interfaces. For example, in our deploy system, we deploy hadoop programs using our own deploy scripts(start/stop, etc), suppose that if we want to start namenode using our own start.sh, we pass config items by -D options, in this senario, the Tool/ToolRunner interface is not so suitalbe. I wish I expressed myself clearly.",non_debt,-
hadoop,9223,comment_5,"Got it and what you propose to add is good. The only worry of mine is that this may unintentionally bloat up the config object size which inadvertently affects MR and other systems that do not really require this feature. Perhaps we can add it with a switch that has it turned off by default and only enabled if an advanced user needs it? Also, some tests would be good to have in the added patch, that also demonstrate order of preference (I assume it is to be most preferred if the value is from a sys-prop rather than a file), etc.. P.s. Fix Version is to be set only after it has been committed into a branch. Please use the Target Version alone to track the version where you want it to be committed.",design_debt,non-optimal_design
hadoop,9223,comment_6,"Thanks for your quick reply! 1. In the patch, I added a prefix 'hadoop.property.' for config items, this intends to differ the config items passed by from the current normal options passed by -D'$name=$value', I do understand your worry, which is also mine, considering this, I added the prefix, which just acts like a switch:) 2. About the order of preference, it's just the same as what you said, a system property is more preferred than a file. I will add some tests, and submit it later 3. Sorry about the my mistake of error specifying of the version:(",test_debt,lack_of_tests
hadoop,9223,comment_7,I missed that prefix part; that should help address my worry. Thanks for pointing it out!,non_debt,-
hadoop,9223,comment_8,"I've added tests and passed my local testing, resubmitted.",non_debt,-
hadoop,9223,comment_9,ping Harsh,non_debt,-
hadoop,9223,comment_10,Anyone who can make sure of this?,non_debt,-
hadoop,9223,comment_12,update the relative path,non_debt,-
hadoop,9223,comment_15,"Please use ""Target Version"" for your intention. Dropping fix-version as it is only supposed to be set at patch commit time.",non_debt,-
hadoop,9245,summary,mvn clean without running mvn install before fails,non_debt,-
hadoop,9245,description,HADOOP-8924 introduces plugin dependency on in hadoop-common and hadoop-yarn-common. Calling mvn clean on a fresh m2/repository (missing fails due to this dependency.,non_debt,-
hadoop,9245,comment_0,Setting the phase of the plugin execution to compile.,non_debt,-
hadoop,9245,comment_1,"+1 for the patch This patch also applies cleanly to trunk and branch-trunk-win, so I've added those in the target versions. I tested the builds on both of those branches, and they looked good. Thank you!",non_debt,-
hadoop,9245,comment_2,Changing the Affects Versions to 3.0.0 and trunk-win. HADOOP-8924 is not in branch-2 yet.,non_debt,-
hadoop,9245,comment_3,+1 for the patch,non_debt,-
hadoop,9245,comment_5,we should bind the version-info plugin to the prepare-resources phase instead of compile.,non_debt,-
hadoop,9245,comment_6,I committed the patch to trunk and branch-trunk-win. Thank you Karthik!,non_debt,-
hadoop,9245,comment_7,I committed the patch. Can you open another jira to make this change?,non_debt,-
hadoop,9245,comment_8,"Thanks Chris, Suresh and Alejandro. Created HADOOP-9246 for that change - will upload a patch shortly.",non_debt,-
hadoop,9245,comment_12,Note that this change broke web service tests in YARN. See YARN-361. I'm guessing this is related to the issue Alejandro brought up earlier about the version-info plugin? Was there any followup?,non_debt,-
hadoop,9245,comment_13,Filed HADOOP-9246 - I believe it is ready to reviewed/committed. Thanks Jason.,non_debt,-
hadoop,9245,comment_14,I beleive im still seeing this issue. To reproduce: 1) Clone hadoop-common 2) run mvn clean package And you'll get something like this: Are we sure this patch fixes it in all cases?,non_debt,-
hadoop,9245,comment_15,"apologies, ... (i cant reproduce the above error on afresh clone of hadoop-common)... so i guess this bug is fixed :).",non_debt,-
hadoop,9245,comment_16,"Just as an FYI for those hitting this same issue. I think you still need to do: In order to build However, I cant say if this is really still a bug or not.",non_debt,-
hadoop,9254,summary,Cover packages,non_debt,-
hadoop,9254,description,None,non_debt,-
hadoop,9254,comment_3,Thanks  for the patch. +1 (non-binding) lgtm,non_debt,-
hadoop,9254,comment_4,"+1, lgtm. One minor nit: has an extra license comment in the middle of the imports which I'll cleanup on checkin.",code_debt,low_quality_code
hadoop,9254,comment_5,Thanks to Vadim for the contribution and Robert for the review! I committed this to trunk and branch-2.,non_debt,-
hadoop,9259,summary,should be less brittle in teardown,design_debt,non-optimal_design
hadoop,9259,description,the teardown code in assumes that {{fs!=null}} and that it's OK to throw an exception if the delete operation fails. Better to check the {{fs}} value and catch and convert an exception in the {{fs.delete()}} operation to a {{LOG.error()}} instead. This will stop failures in teardown becoming a distraction from the root causes of the problem (that your FileSystem is broken),design_debt,non-optimal_design
hadoop,9259,comment_0,straightforward hardening of teardown logic,design_debt,non-optimal_design
hadoop,9259,comment_2,"+1, patch looks good to me.",non_debt,-
hadoop,9267,summary,"hadoop -help, -h, --help should show usage instructions",non_debt,-
hadoop,9267,description,It's not friendly for new users when the command line scripts don't show usage instructions when passed the defacto Unix usage flags. Imagine this sequence of commands: Same applies for the `hdfs` script.,documentation_debt,low_quality_documentation
hadoop,9267,comment_0,"Patch for {{hadoop}} and {{hdfs}}. Ran TestHDFSCLI and TestCLI successfully. No unit tests since this is kind of trivial, but I could wrangle it if desired.",test_debt,lack_of_tests
hadoop,9267,comment_1,"Hi, Andrew. The changes look good. Would you want to do the same thing for",non_debt,-
hadoop,9267,comment_2,"Sure, new patch with same changes to the yarn script.",non_debt,-
hadoop,9267,comment_3,"+1 I applied the patch locally, built the distro, and ran namenode, datanode, resourcemanager, and nodemanager. I tested the various ways to get help (--help, -help, -h) through the 3 different scripts. I also tested a few other HDFS interactions and a MapReduce job. Everything looked good. Thank you!",non_debt,-
hadoop,9267,comment_6,"No tests because it's a pretty small change. I can add some if desired though, wouldn't want it to block a commit.",non_debt,-
hadoop,9267,comment_7,"+1, I'm going to commit this momentarily.",non_debt,-
hadoop,9267,comment_8,"I've just committed this to trunk and branch-2. Thanks a lot for the contribution, Andrew.",non_debt,-
hadoop,9278,summary,HarFileSystem may leak file handle,code_debt,low_quality_code
hadoop,9278,description,fails on Windows due to invalid HAR URI and file handle leak. We need to change the tests to use valid HAR URIs and fix the file handle leak.,code_debt,low_quality_code
hadoop,9278,comment_0,"The invalid HAR URI is caused here in On Windows, this creates a path that includes the drive specifier. Later, this gets used to construct a HAR URI for testing, which isn't valid, because it doesn't adhere to the the protocol-host format used by HAR URIs. The file handle leak happens in This method opens the _masterindex file, but not all code paths guarantee that the file will be closed. For example, parsing an invalid version throws an exception before the _masterindex file gets closed. There is a test that covers this case, so it leaks a file handle when that test runs. On Windows, this ultimately causes tests to fail during their post-test cleanup, because Windows file locking behavior causes the delete to fail.",code_debt,low_quality_code
hadoop,9278,comment_1,"The attached patch strips the drive specifier from the test working directory path on Windows and ensures that we close file handles that were opened in The patch looks bigger than it really is at first glance because of indentation changes. The actual parsing logic is unchanged. With this patch, I ran on Mac and Windows, and it passed on both platforms.",non_debt,-
hadoop,9278,comment_2,I also meant to mention that this patch can apply to trunk and then merge to branch-trunk-win.,non_debt,-
hadoop,9278,comment_4,+1 patch looks good.,non_debt,-
hadoop,9278,comment_5,"I have committed this. Thanks, Chris!",non_debt,-
hadoop,9292,summary,in httpfs tests,non_debt,-
hadoop,9292,description,problem in method,non_debt,-
hadoop,9292,comment_0,"Thanks a lot for filing this bug, Vadim. Not a big deal, but in the future when reporting issues it would be great if you could please set the ""affects"" and ""targets"" version fields appropriately.",non_debt,-
hadoop,9292,comment_1,It seems that all the tests are in httpfs. Revised summary.,non_debt,-
hadoop,9292,comment_2,Stale.,non_debt,-
hadoop,9305,summary,Add support for running the Hadoop client on 64-bit AIX,non_debt,-
hadoop,9305,description,"HADOOP-9283 added support for running the Hadoop client on AIX, but only with 32-bit JREs. This JIRA is to add support for 64-bit JREs as well.",non_debt,-
hadoop,9305,comment_0,"Here's a simple patch which addresses the issue. The only behavior changes are to conditionally load the AIX64LoginModule and the UsernamePrincipal classes if we're on a 64-bit AIX box, instead of the AIXLoginModule and AIXPrincipal classes. This patch also refactors the getOsPrincipalClass a little bit to reduce some code repetition. No tests are included since to test this properly would require an AIX box. I tested this manually by running with both 32-bit and 64-bit AIX clients and confirming that it works as expected, both with and without Kerberos enabled. Without the patch only 32-bit clients will work. I also ensured there are no regressions by testing the Hadoop client with both IBM Java and Sun Java on Linux both with and without Kerberos enabled. Everything worked as expected.",code_debt,duplicated_code
hadoop,9305,comment_1,"+1 pending jenkins. ATM, what about opening a JIRA to clean this spaghetti of conditionals replacing it with a MAP and a simple struct having the needed settings?",code_debt,low_quality_code
hadoop,9305,comment_2,"Thanks a lot for the review. Good idea, Tucu. Filed: HADOOP-9306",non_debt,-
hadoop,9305,comment_4,"I've just committed this to trunk and branch-2. Thanks again for the review, Tucu.",non_debt,-
hadoop,9309,summary,test failures on Windows due to in,non_debt,-
hadoop,9309,description,"Checking for Snappy support calls native method This method has not been implemented for Windows in hadoop.dll, so it throws",requirement_debt,requirement_partially_implemented
hadoop,9309,comment_0,"Linking to HADOOP-8756, which introduced the change in The failing tests on Windows are and",non_debt,-
hadoop,9309,comment_1,Fix link errors by building LZ4 library and Snappy stub routine on Windows.,non_debt,-
hadoop,9309,comment_2,"Thanks, Arpit. I tested this on Windows and Ubuntu with native build, and it worked great. Here are a couple of comments. I am +1 for the patch after removal of some unneeded #includes unless those #includes are there for some reason that I missed. (See below for details.) I believe this setting would disable the warning across the whole project, right? Another option could be to upgrade our lz4.c. It looks like there have been some recent changes to address warnings seen when compiling on Windows: I'm sure that would be a much larger scope though, so it's probably best to treat it as a separate jira. Is it necessary to add these #includes to NativeCodeLoader.c? I tried removing them, and I was still able to compile.",code_debt,dead_code
hadoop,9309,comment_3,Thanks for the review! org_apache_hadoop.h defines UNIX/WINDOWS so it is necessary. I removed the second include.,non_debt,-
hadoop,9309,comment_4,+1 Sounds good!,non_debt,-
hadoop,9309,comment_5,+1. I will commit this patch shortly.,non_debt,-
hadoop,9309,comment_6,I committed the patch to branch-trunk-win. Thank you Aprit! Thank you Chris for the review.,non_debt,-
hadoop,9309,comment_7,Thanks Suresh.,non_debt,-
hadoop,9336,summary,Allow UGI of current connection to be queried,non_debt,-
hadoop,9336,description,"Querying is synch'ed and inefficient for short-lived RPC requests. Since the connection already contains the UGI, there should be a means to query it directly and avoid a call to",design_debt,non-optimal_design
hadoop,9336,comment_0,"Daryn, how (if) will this affect RPC calls done within the context of a doAs()?",non_debt,-
hadoop,9336,comment_1,"The semantics will be returning the UGI of the connection, so it will always report the UGI of the original user making the connection, not of any subsequent {{UGI.doAs}} calls. However, this jira will not universally affect anything that doesn't explicitly use it. I intend for only the {{FSNamesystem}} audit calls to currently use it to reduce the significant performance bottlenecks we are encountering. I'm contemplating filing another jira for {{UGI.doAs}} to cache a stack of UGIs. That would greatly accelerate in general.",code_debt,slow_algorithm
hadoop,9336,comment_2,"Testing of this jira revealed a 3.0/2.x bug, not present in 23, where auth type is set incorrectly.",non_debt,-
hadoop,9336,comment_3,"Add a to return the current RPC connection's UGI. Update tests to ensure that matches - note: as mentioned before, another {{doAs}} on the server-side intentionally won't be reflected in",non_debt,-
hadoop,9336,comment_5,"I only modified existing test cases, but was dinged for no timeout which I thought only applied to new tests, so I added timeouts to the tests I modified.",non_debt,-
hadoop,9336,comment_7,"+1 on the code change, but more detailed comment on getRemoteUser() will be nice, in order to avoid potential misuse.",documentation_debt,low_quality_documentation
hadoop,9336,comment_8,Updated comments per Kihwal,non_debt,-
hadoop,9336,comment_10,I've committed the patch to branch-2 and branch-0.23.,non_debt,-
hadoop,9350,summary,Hadoop not building against Java7 on OSX,non_debt,-
hadoop,9350,description,Maven stack-traces out in the {{jspc}} compilation as the JSPC plugin doesn't work against the new JDK7 JAR layout. Needs a symlink set up to fix,non_debt,-
hadoop,9350,comment_0,"On java7 on OSX, the fails as it cannot find {{classes.jar}} where it expects.",non_debt,-
hadoop,9350,comment_1,"This is triggered by # Oracle moving things around # The JSPC plugin having hard-coded expectations of where things live. There is a known (currently open) plugin JIRA The workaround is to create the {{Home/Classes}} dir, then symlink to {{classes.jar}} closing as WONTFIX as it's an upstream problem; this JIRA is here to note the problem and the fix",non_debt,-
hadoop,9350,comment_2,"Steve, how about adding a note in the BUILDING.txt file with the workaround until it is fixed upstream?",non_debt,-
hadoop,9350,comment_3,Just hit this problem earlier and agree please put this in BUILDING.txt as it will save a lot of time for new devs.,non_debt,-
hadoop,9350,comment_4,will patch BUILDING.TXT,non_debt,-
hadoop,9350,comment_5,I should add that this patch isn't enough; hadoop-annotations won't compile; imports of {{ com.sun.javadoc}}: It looks like someone needs to patch up maven's POM * *,non_debt,-
hadoop,9350,comment_6,I've patched BUILDING.TXT for this; here's the lines changed. Leaving the JIRA open as there is a POM fixup to add too.,non_debt,-
hadoop,9350,comment_11,I created a patch that includes the two fixes suggested by # BUILDING.txt gives directions on creating the symlink (instead of just saying it doesn't work) # Hadoop-Annotations pom has a Java 7 profile Verified that it compiles on my Mac against Java 7 and Java 6. Verifies that it also compiles on Linux against Java 7 and Java 6 (thanks !),non_debt,-
hadoop,9350,comment_12,"+1, after creating the symlink according to the direction, 'mvn install -DskipTest' succeeded with the patch on my Mac against Java7.",non_debt,-
hadoop,9350,comment_13,"OK -we'll need people on other platforms (linux, windows, power) to make sure that this doesn't break.",non_debt,-
hadoop,9350,comment_14,I verified it on Linux. +1.,non_debt,-
hadoop,9350,comment_15,Patch also verified on Windows & that it doesn't break the build on OS/X + Java6 Fixed in trunk and branch-2. Thanks!,non_debt,-
hadoop,9353,summary,Activate native-win profile by default on Windows,non_debt,-
hadoop,9353,description,Hadoop on Windows requires native components to be available so the native-win profile should be activated by default. The fix should update BUILDING.txt appropriately.,non_debt,-
hadoop,9353,comment_0,"Fairly trivial patch to activate native-win by default on Windows. Patch tested on Windows, Linux and OS X.",non_debt,-
hadoop,9353,comment_2,No tests needed since no functionality is added or changed. Purely a build profile fix.,non_debt,-
hadoop,9353,comment_3,"Hi Arpit, do we need to update BUILDING.txt as well?",non_debt,-
hadoop,9353,comment_4,"Yes, I will update the patch. Thanks Nicholas.",non_debt,-
hadoop,9353,comment_5,Updated BUILDING.txt.,non_debt,-
hadoop,9353,comment_7,"+1 for the patch. Thanks, Arpit! I applied the patch locally and built and tested full distros on Mac and Windows. The BUILDING.txt changes look good too.",non_debt,-
hadoop,9353,comment_8,"I have committed this. Thanks, Arpit! Also thanks Chris for reviewing the patch.",non_debt,-
hadoop,9353,comment_13,I merged the patch to branch-2.,non_debt,-
hadoop,9369,summary,DNS#reverseDns() can return hostname with . appended at the end,non_debt,-
hadoop,9369,description,DNS#reverseDns uses to do a reverse DNS lookup. This can sometimes return hostnames with a . at the end. Saw this happen on hadoop-1: two nodes with set to eth0,non_debt,-
hadoop,9369,comment_0,The fix is fairly straight-forward as documented here - Check if the hostname has a '.' at the end,non_debt,-
hadoop,9369,comment_2,"Ran a 2-node cluster, one with the patch and one without. Logged the hostname: the one with the patch doesn't have a '.' while the other one has. If one were to write a unit test for this, we should either (1) mock InitialDirContext's getAttributes method (it is not static either) or (2) write a singleton wrapper for InitialDirContext and modify the method in the context of the test. Manual verification validates the fix.",non_debt,-
hadoop,9369,comment_4,"Hi Karthik, the patch seems fine to me, and I agree that the patch is so simple and writing a test sufficiently difficult that it seems unnecessary to write a test for this. One question - can you comment on the ramifications of this issue? How does it manifest itself? And what triggers it?",test_debt,lack_of_tests
hadoop,9369,comment_5,"Thanks Aaron for taking a look at the patch. Consider a cluster setup where is used to list nodes that can be included, say nodes are 'sample1', 'sample2'. Now, if they also choose to fix the interface to be used (say eth0), the TTs during initialization call themselves 'sample1.' and 'sample2.', note the '.' at the end of the hostnames. Now, as this doesn't match any of the nodes listed in the file, the two TTs are disallowed to join. Ideally, the TTs should identify themselves as correct hostnames without '.' at the end. The code path that the patch modifies is invoked only when we specify a particular interface, otherwise the default/canonical hostnames are picked. As for when returns a hostname with '.' at the end, I am not quite certain - it didn't show up on a pseudo-distributed test.",non_debt,-
hadoop,9369,comment_6,"Thanks a lot for the explanation, Karthik. Makes sense, and the patch seems quite straightforward. +1, I'm going to commit this momentarily.",non_debt,-
hadoop,9369,comment_7,"I've just committed this to trunk, branch-2, and branch-1. Thanks a lot for the contribution, Karthik.",non_debt,-
hadoop,9369,comment_12,Closed upon release of Hadoop 1.2.0.,non_debt,-
hadoop,9379,summary,capture the ulimit info after printing the log to the console,non_debt,-
hadoop,9379,description,Based on the discussions in HADOOP-9253 people prefer if we dont print the ulimit info to the console but still have it in the logs. Just need to move the head statement to before the capture of ulimit code.,non_debt,-
hadoop,9379,comment_0,Patch for trunk,non_debt,-
hadoop,9379,comment_2,No tests added as this is a change to the shell scripts. Manually verified that the ulimit info is only in the logs.,non_debt,-
hadoop,9379,comment_3,Verified it works well on my environment. Thanks Arpit.,non_debt,-
hadoop,9379,comment_4,"I committed the patch to trunk, branch-2, branch-1 and branch-1.2. Thank you Arpit! Thanks to Junping for doing quick validation.",non_debt,-
hadoop,9379,comment_8,merged into branch-0.23,non_debt,-
hadoop,9379,comment_11,"It is marked for 2.0.4-alpha, but never made it in. Committing.",non_debt,-
hadoop,9419,summary,CodecPool should avoid OOMs with buggy codecs,non_debt,-
hadoop,9419,description,"I recently found a bug in the gpl compression libraries that was causing map tasks for a particular job to OOM. Now granted it does not make a lot of sense for a job to use the LzopCodec for map output compression over the LzoCodec, but arguably other codecs could be doing similar things and causing the same sort of memory leaks. I propose that we do a sanity check when creating a new If the codec newly created object does not match the value from getType... it should turn off caching for that Codec.",code_debt,low_quality_code
hadoop,9419,comment_0,"Never mind. I created a patch, and it is completely useless in fixing this problem. The tasks still OOM because the codec itself is so small and the MergeManager creates new codecs so quickly that on a job with lots of reduces it literally uses up all of the address space with direct byte buffers. Some of the processes get killed by the NM for going over the virtual address space before they OOM. We could try and have the CodecPool detect that the codec is doing the wrong thing and ""correct"" it for the codec, but that is too heavy handed in my opinion.",design_debt,non-optimal_design
hadoop,9440,summary,Unit Test: hadoop-common2.0.3 TestIPC fails on protobuf2.5.0,non_debt,-
hadoop,9440,description,"TestIPC runs normally if use protobuf2.4.1 or below version. But if using protobuf2.5.0, & will fail. Failed on local exception: 500 millis timeout while waiting for channel to be ready for read. ch : Host Details : local host is: destination host is: 2009 sec <<< ERROR! Failed on local exception: 2000 millis timeout while waiting for channel to be ready for read. ch : Host Details : local host is: destination host is: & fails because it catches the not",non_debt,-
hadoop,9440,comment_0,In order to ensure the minimal change of the original code and keep the add additional IOException catch statement to catch other IOException except the,non_debt,-
hadoop,9440,comment_2,"Any comments, Suresh?",non_debt,-
hadoop,9440,comment_3,Why do we get a protobuf exception in the first place? We shouldn't be modifying the test to capture an IOException as we don't expect that to happen. Please correct my understanding if am wrong.,design_debt,non-optimal_design
hadoop,9440,comment_4,"Harsh, you are right that we don't expect this exception. But it is really thrown rather than If we use hadoop2.4.0a, it runs well. So can we think it as a protobuf2.5.0+ problem?",non_debt,-
hadoop,9440,comment_6,"I've tried to compile hadoop-commons' trunk with protobuf 2.5.0 and succeeded to pass TestIPC though I needed to run ""mvn clean"" command explicitly under directory. Therefore, the test failure seems to be caused the mixture of old jar file compiled with protobuf 2.4.x and new jar file compiled with protobuf 2.5.0. If this assumption is correct, this ticket should be marked as ""Not a problem"". Did you ""mvn clean"" before recompile?",non_debt,-
hadoop,9440,comment_7,"And, let me know if I am wrong. Thanks!",non_debt,-
hadoop,9440,comment_8,"Thanks Tsuyoshi OZAWA for your comment. I agree with what you said. So close it as ""Not a problem"".",non_debt,-
hadoop,9440,comment_9,"I have the same issue even after I run ""mvn clean"".",non_debt,-
hadoop,9458,summary,"In branch-1, RPC.getProxy(..) may call without retry",non_debt,-
hadoop,9458,description,RPC.getProxy(..) may call without retry even when client has specified retry in the conf.,non_debt,-
hadoop,9458,comment_0,"Actually, the bug is not in rpc.Client.call(..) since it is not supposed to handle retry. The bug is in one of the RPC.getProxy(..) methods which calls The retry handler is not yet set up at that point. Revised summary and description.",non_debt,-
hadoop,9458,comment_1,adds a checkVersion parameter to RPC.getProxy(..).,non_debt,-
hadoop,9458,comment_2,I applied this patch to my cluster and ran through the pig tests where within 5 seconds of the pig submission the job tracker was restarted. With this patch the tests passed consistently.,non_debt,-
hadoop,9458,comment_3,"+1 I just committed this, thanks Nic! Also, thanks to  for verifying this!",non_debt,-
hadoop,9458,comment_4,Closed upon release of Hadoop 1.2.0.,non_debt,-
hadoop,9479,summary,Ability to plugin custom authentication mechanisms based on Jaas and Sasl,non_debt,-
hadoop,9479,description,"Currently, it is not possible to hookup new/modified authentication mechanism to Hadoop. The task is to create an extension in hadoop to plugin new Authentication mechanism. The new authentication mechanism should have both Jaas and Sasl implementations.",non_debt,-
hadoop,9479,comment_0,Attached is the first version of the patch to get guidance and feedback on the general approach. The document briefly explains the design and test.,non_debt,-
hadoop,9479,comment_1,"I like the overall goal, but feel it's a bit rigid in only providing support for only one additional authentication method. This change dovetails with the stalled SASL work I've been doing in the subtasks for HADOOP-8779. I keep meaning to get back to it. Many of the changes were nudging the authentication scheme towards a pluggable design - you've even taken advantage of some of those changes! The new hadoop SASL related interfaces shouldn't be necessary. The javax SASL framework uses a factory pattern to create clients and servers via SecurityProviders. SaslPlainServer does this, although there's probably a cleaner way to do it. The good news is the patch should be significantly smaller if leveraging the javax framework.",design_debt,non-optimal_design
hadoop,9479,comment_2,"Hi Benoy, Very interesting patch! It provides custom and configurable by wrapping SASL mechanism and JAAS module. Would it be better to generalize the so that it can also fit the existing simple and Kerberos authentication method, and then refactor related code based on it? In my view it would be great to have such authentication provider that wraps client authentication and server authentication JAAS modules, the client-server SASL mechanism and related configurations together as you do, and then have concrete authentication implementations like and HADOOP-9296. In this way code and configuration related to one mechanism can be localized to the provider implementation. Another benefit would be it allows introducing additional methods in the future without the need to enumerate all of them in",non_debt,-
hadoop,9479,comment_3,"I've only skimmed the patch, but I'm unclear what benefit this custom layer provides that standard java security providers and factories do not already provide? I already plan to abolish this, perhaps with the RPCv9 sasl changes.",non_debt,-
hadoop,9479,comment_4,"The SaslClient and Sasl server implementations accept parameters including Callbackhandlers. These parameters could be independent of Hadoop or the Sasl Implementation. The custom layer allows the user to specify these parameters. I was studying the related SASL jiras. With those changes, the mechanism of plugging in is going to change. I'll contribute to those jiras.",non_debt,-
hadoop,9479,comment_5,Any new update about this issue? This feature is very useful to make a authentication plug-in for company with own single sign on service.,non_debt,-
hadoop,9479,comment_6,", Thanks for the interest in this jira. We now have a requirement to integrate with different authentication mechanism than kerberos. So I will be spending some time on this feature. It will be great to know about your requirements too.",non_debt,-
hadoop,9479,comment_7,Is there any updates to this issue?,non_debt,-
hadoop,9484,summary,Genetic Algorithm Library for Hadoop,non_debt,-
hadoop,9484,description,"Developed a Genetic Algorithm Library for Hadoop. Using this library, problems using Genetic Algrorithm can be solved based on Hadoop MapReduce.",non_debt,-
hadoop,9484,comment_0,"Hi Vaibhav, Have you checked out Mahout? bq. The Apache Mahout machine learning library's goal is to build scalable machine learning libraries.",non_debt,-
hadoop,9484,comment_1,"Hi Vaibhav, While this is potentially very valuable work, this probably isn't something we'd want to include in core Hadoop. Per Colin's comment, I would take a look at Mahout and see if they would be interested in your contribution. Best, Aaron",non_debt,-
hadoop,9508,summary,chmod -R behaves unexpectedly on Windows,non_debt,-
hadoop,9508,description,"FileUtil.chmod behaves unexpectedly on Windows (it uses ""winutils chmod -R"" under the covers. The problem can be manually reproduced with winutils or with the attached test case",non_debt,-
hadoop,9508,comment_0,"I was repeatedly misreading the code, this is not a bug. Resolving as such.",non_debt,-
hadoop,9508,comment_1,"If the code itself is confusing, perhaps you could use this jira to add comments/fix comments.",code_debt,low_quality_code
hadoop,9508,comment_2,I think it is fine. It was user error on my part. Also access checks while traversing nested directories seem to work a little differently on Windows and Unix. I'll post the details on HDFS-4741 when I understand it better.,non_debt,-
hadoop,9544,summary,backport UTF8 encoding fixes to branch-1,non_debt,-
hadoop,9544,description,"The trunk code has received numerous bug fixes related to UTF8 encoding. I recently observed a branch-1-based cluster fail to load its fsimage due to these bugs. I've confirmed that the bug fixes existing on trunk will resolve this, so I'd like to backport to branch-1.",non_debt,-
hadoop,9544,comment_0,"I'm attaching the backport patch. The prior jiras included in this backport are: HADOOP-6522, HADOOP-7547 (partially), HADOOP-9103, HDFS-3844 (partially), and HDFS-4282. Porting in the new tests also required including several dependency classes that previously didn't exist in branch-1: {{Time}}, and One of the changes in trunk had been in which doesn't exist in branch-1. I made the equivalent change in {{FSImage}}.",non_debt,-
hadoop,9544,comment_1,"I'm uploading version 2 of the patch. I realized that the Guava dependency caused by could be problematic. This version of the patch does not introduce a Guava dependency, and instead uses a stripped down version of that contains just enough to support the new tests in {{TestUTF8}}.",non_debt,-
hadoop,9544,comment_2,+1 patch looks good. Please run all the unit tests and post the results.,non_debt,-
hadoop,9544,comment_3,"Thanks, Nicholas. I'll report back when I have the test results. Here is the test-patch output. Regarding findbugs, it appears that the filterBugs command in test-patch.sh is not actually filtering out anything. I've seen mention of other people having this problem on the mailing lists. It's unrelated to this patch.",non_debt,-
hadoop,9544,comment_4,I also have run test-patch. I consistently see 19 Findbugs warnings in my machine. So that I believe they are not related to the patch.,non_debt,-
hadoop,9544,comment_5,"I have committed this. Thanks, Chris!",non_debt,-
hadoop,9544,comment_6,"Thanks very much for the review and commit. Just to document it, my full test run came back successful. There was just 1 failure in which is known to be a flaky test.",test_debt,flaky_test
hadoop,9544,comment_7,Closed upon release of Hadoop 1.2.0.,non_debt,-
hadoop,9614,summary,smart-test-patch.sh hangs for new version of patch (2.7.1),non_debt,-
hadoop,9614,description,"patch -p0 -E --dry-run prints ""checking file "" for the new version of patch(2.7.1) rather than ""patching file"" as it did for older versions. This causes TMP2 to become empty, which causes the script to hang on this command forever: -d '/' -f 1 | sort | uniq)",non_debt,-
hadoop,9614,comment_0,Simple enough fix,non_debt,-
hadoop,9614,comment_1,Shoot! Missed the beginning anchor. Ok. This is good to go for all branches. Can someone please commit?,non_debt,-
hadoop,9614,comment_3,"+1. Looks good to me based on change listed in patch's git repository commit Date: Tue Apr 17 22:37:17 2012 +0200 Improve messages when in --dry-run mode * src/patch.c (main): Say that we are checking a file and not that we are patching it in --dry-run mode. Don't say ""saving rejects to file"" when we don't create reject files. * Add rejects with --dry-run test case. * tests/fifo, Update.",non_debt,-
hadoop,9617,summary,HA HDFS client is too strict with validating URI authorities,non_debt,-
hadoop,9617,description,"HADOOP-9150 changed the way FS URIs are handled to prevent attempted DNS resolution of logical URIs. This has the side effect of changing the way Paths are verified when passed to a FileSystem instance created with an authority that differs from the authority of the Path. Previous to HADOOP-9150, a default port would be added to either authority in the event that either URI did not have a port. Post HADOOP-9150, no default port is added. This means that a FileSystem instance created using the URI will no longer process paths containing just the authority and will throw an error like the following: Though this is not necessarily incorrect behavior, it is a change that at least breaks certain clients' ability to connect to an HA HDFS cluster.",design_debt,non-optimal_design
hadoop,9617,comment_0,"The attached patch addresses the issue by changing to check if the passed-in path does not contain a port but this FS URI does contain a port. In this case, the default port is added to the provided Path's authority before checking for equality of the authorities.",non_debt,-
hadoop,9617,comment_2,I think something else is wrong because I specifically made port/no-port work last I time I was in that code. Canonicalizing the uri is supposed to handle it. I believe it's incorrect behavior. Users should not have to fully qualify path authorities with ports. It will break customers.,non_debt,-
hadoop,9617,comment_3,-- maybe the reason why your code isn't taking effect is that we explicitly _don't_ call for the case of logical authorities?,non_debt,-
hadoop,9617,comment_4,"Sorry, not quite sure I follow. What else do you think is wrong? I think this pattern of using FileSystem with Paths with different authorities is itself incorrect. One should not create a FileSystem object using one authority and then pass Paths to that FileSystem which use another authority. Instead, users should be calling Path#getFileSystem to get the proper FS associated with that Path. Regardless, this issue is moot because the change was an inadvertent backward incompatible one, which I think we all agree should be fixed.",non_debt,-
hadoop,9617,comment_5,"It looks like Todd is right. DFS isn't canonicalizing as ""expected"", ie. adding a default port, after the change to prevent attempts to resolve logical names. I think it may be better to change to add the default port to a logical uri lacking a port so it conforms to the behavior expected by {{checkPath}}, instead of modifying {{checkPath}} itself?",non_debt,-
hadoop,9617,comment_6,"Wouldn't we have to add that in other places as well, like viewfs, etc? Here's another thought: what would happen if returned 0 if it's a logical URI? Would that fix the issue?",non_debt,-
hadoop,9617,comment_7,"Viewfs appears ok. It falls back to the default impl of {{canonicalizeUri}} which uses {{if (uri.getPort() == -1 && getDefaultPort() HA logical URIs are an interesting twist, where the port can be argued to be irrelevant. However just as context for my concern, there's a distinct possibility that our HA logical uris will be the former primary NN's host address. If so, it's critical that HA logical uris must have identical behavior to non-HA uris.",non_debt,-
hadoop,9617,comment_8,"Thanks a lot for the comments, Daryn and Todd. I took Daryn's suggestion of changing to call in the event of a logical URI, which I think should cover all the bases. I also created a new test case to suit, which is restricted to just Please have a look.",test_debt,low_coverage
hadoop,9617,comment_10,"The new javac warning is because I'm using a Sun-proprietary API in the test. The test uses an Assume check so that this test will not be run on a non-Sun JDK. Daryn, Todd - does this look OK to you?",non_debt,-
hadoop,9655,summary,Connection object in IPC Client can not run concurrently during connection time out,non_debt,-
hadoop,9655,description,"When one machine power off during running a job ,MRAppMaster find tasks timed out on that host and then call stop container for each container concurrently. But the IPC layer did it serially, for each call,the connection time out exception toke a few minutes to raise after 45 times reties. And AM hang for many hours to wait for stopContainer to finish. The jstack output file shows that most threads stuck at Connection.addCall waiting for a lock object hold by (The setupIOstreams method run slowlly becauseof connection time out during setupconnection.)",code_debt,slow_algorithm
hadoop,9655,comment_0,"This patch use a different object for wait and notify ,so one thread invoking addCall method won't be blocked by another thread calling setupConnection method.",non_debt,-
hadoop,9655,comment_2,This patch has been tested on my cluster and has solved the problem.,non_debt,-
hadoop,9669,summary,Reduce the number of byte array creations and copies in XDR data manipulation,code_debt,low_quality_code
hadoop,9669,description,"XDR.writeXxx(..) methods ultimately use the static XDR.append(..) for writing each data type. The static append creates a new array and copy data. Therefore, for a singe reply such as there are multiple array creations and array copies. For example, there are at least 6 array creations and array copies for",non_debt,-
hadoop,9669,comment_0,Here is a more version that utilizes Java's ByteBuffer. It should be more efficient. The APIs are compatible with the previous version. The APIs might not be ideal towards an efficient implementation of XDR serialization / deserialization. I'm tracking my proposals to the APIs in a separate JIRA.,design_debt,non-optimal_design
hadoop,9669,comment_3,"Thanks, Haohui. Some comments: 1. please try to keep the original javadoc for the same named methods 2. can you make ""State state"" as final? 3. please fix the javadoc /** check if the rest of data has more than <len""len"" is not visible in generated javadoc 4. readFixedOpaque still has a copy not sure if it's possible to generat a read-only bytebuffer from another bytebuffer 5. it would be nice to remove the extra copy for writeFixedOpaque For 4 and 5, I am ok if you think it's out of scope of this JIRA.",code_debt,low_quality_code
hadoop,9669,comment_4,A patch that addresses Brandon's comments. I'll address 4/5 in HADOOP-9966.,non_debt,-
hadoop,9669,comment_6,"+1. New patch looks good. Nit: I noticed that most NFS related RPC call response size is less than 256 bytes, so we can have as 256 instead of 512.",code_debt,low_quality_code
hadoop,9669,comment_7,A new patch that addresses the problems found by . Thanks very much for .,non_debt,-
hadoop,9669,comment_10,"I've committed the patch. Thank you, Haohui.",non_debt,-
hadoop,9672,summary,Upgrade Avro dependency to 1.7.4,non_debt,-
hadoop,9672,description,"Hadoop still depends on Avro-1.5.3, when the latest release is 1.7.4. I've observed this cause problems when using Hadoop 2 with Crunch, which uses a more recent version of Avro.",non_debt,-
hadoop,9672,comment_1,+1 We've seen OOMs due to AVRO-1169 and AVRO-1175. We should definitely upgrade. We need to gather more opinions on this.,non_debt,-
hadoop,9672,comment_2,", I assume you have tested the new version. Would you share your experience? We could first commit this to trunk and branch-2, then later to branch-2.1 if there is no surprises.",non_debt,-
hadoop,9672,comment_3,"I ran a few jobs on a pseudo-distributed cluster. I also verified some of the JobHistoryServer functionality, as it's the primary consumer of Avro within Hadoop. It also fixed a Crunch pipeline that had previously been failing for me.",non_debt,-
hadoop,9672,comment_4,"I did a quick check of avro jiras, and it seems there has been no regression specific to Avro 1.7.4 (released in Feb) found so far. The new patch specifies the version as 1.7.4 instead of 1.7.0.",non_debt,-
hadoop,9672,comment_5,+1 to 1.7.4,non_debt,-
hadoop,9672,comment_7,"I've committed this to trunk, branch-2 and branch-2.1-beta. Thanks for working on this, Sandy.",non_debt,-
hadoop,9672,comment_9,Thanks Kihwal.,non_debt,-
hadoop,9674,summary,RPC#Server#start does not block until server is fully initialized and listening,non_debt,-
hadoop,9674,description,"This problem was originally mentioned in discussion on HADOOP-8980. When calling initialization of the server's internal {{Listener}} and {{Reader}} threads happens in the background. This initialization is not guaranteed to complete by the time the caller returns from This may be misleading to a caller that expects the server has been fully initialized. This problem sometimes manifests as a test failure in This test looks at the stack frames of all running threads, expecting to find the {{Listener}} and {{Reader}} threads, but sometimes it doesn't find them.",design_debt,non-optimal_design
hadoop,9674,comment_0,"After further code review, I don't think this is really a problem in practice. While it's true that the {{Listener}} and {{Reader}} threads are not guaranteed to be fully initialized after return from the important thing is that the server socket is listening and accepting connections, via the code in the {{Listener}} constructor: The server socket bind is guaranteed to be done before returns, so a caller can start a server and be guaranteed that an immediate connection attempt will succeed. It might experience some extra latency on the response if it needs to wait for the {{Listener}} and {{Reader}} threads to finish initialization, but it won't fail. I don't think this is really a bug. We probably just need to change the logic of the failing test, and this is already covered in HADOOP-8980. I'm planning on resolving this as Not a Problem. I'll leave this open a few more days in case anyone else wants to comment.",non_debt,-
hadoop,9674,comment_1,"As per prior comments, this doesn't appear to be a real problem in practice, so I'm resolving it as not a problem.",non_debt,-
hadoop,9731,summary,Uncaught in MiniDFSCluster.java,non_debt,-
hadoop,9731,description,Ted Yu suggests that this error is actually an HDFS issue.,non_debt,-
hadoop,9731,comment_0,Resolving as incomplete. Need more info than 'suggestion'.,non_debt,-
hadoop,9748,summary,Reduce blocking on,non_debt,-
hadoop,9748,description,"EnsureInitialized is always sync'ed on the class, when it should only sync if it actually has to initialize.",design_debt,non-optimal_design
hadoop,9748,comment_0,Minor change. This helps reduce unnecessary class lock contention between such common methods as and etc.,design_debt,non-optimal_design
hadoop,9748,comment_2,first patch was made 1-level deep,non_debt,-
hadoop,9748,comment_4,"ensureInitialized() forced many frequently called methods to unconditionally acquire the class lock. This patch certainly reduces the lock contention, but the highly contended getCurrentUser() can still block many threads. I understand this is being addressed in HADOOP-9749. There may be something else we can do to improve this for namenode. According to the ""worst"" jstack of a busy namenode I took, there were 29 BLOCKED handler threads. All of them were blocked on the UGI class lock. Here is the breakdown: - 2 ensureInitialized() - from non static synchronized methods. This Jira will unblock these. - 27 getCurrentUser() Among the 27 threads that were blocked at getCurrentUser(), - 18 - from in most namenode RPC methods - 8 - getBlockLocations() - 1 I think FSPermissionChecker can be modified to be created with a passed in UGI. FSNamesystem can the one already stored in RPC server by calling getRemoteUser(). This will eliminate a bulk of getCurrentUser() calls from namenode RPC handlers. A similar change can be made to mkdirs. Block token generation is not as straightforward. Even without it we can eliminate majority of the calls. We could potentially do the same for other RPC servers. I will file a HDFS jira for this. HADOOP-9749 is still needed since getCurrentUser() is used everywhere beyond namenode RPC server. +1 for this patch.",design_debt,non-optimal_design
hadoop,9748,comment_5,Slightly modified patch for 23.,non_debt,-
hadoop,9748,comment_7,"In the 0.23 patch, I think you can simply take a similar approach as trunk.",non_debt,-
hadoop,9748,comment_8,"Yes, looking at it again, I just need to remove the boolean that was removed on newer branches. Then the patch is equivalent.",non_debt,-
hadoop,9748,comment_9,+1 The new patch looks good. It is more in line with trunk.,non_debt,-
hadoop,9748,comment_10,Thanks Kihwal!,non_debt,-
hadoop,9748,comment_11,"It's probably really really hard to trigger, but I think this double-checked locking isn't quite correct -- the initialize() function is free to make the conf member visible to other threads before the object is fully initialized. I think to make it safe you need to do a write to a volatile variable in initialize to ensure memory ordering. Thoughts?",non_debt,-
hadoop,9748,comment_12,"A volatile would probably be safest but I'm not sure there's harm in the race. If the UGI isn't initialized, it does so with a default conf . I avoided a volatile when I made the patch to avoid a memory barrier. Barriers are pretty cheap on modern processors, and the barrier preventing instruction reordering is probably inapplicable in this case due to the subsequent sync forcing an ordering already. Old habits die hard. I'll look a little deeper and see if there's actually an issue. The kerberos auth to local rule init might be but I thought I investigated that.",non_debt,-
hadoop,9762,summary,RetryCache utility for implementing RPC retries,non_debt,-
hadoop,9762,description,HDFS-4979 has the RetryCache implementation in uncommitted patches. This jira moves this utility to hadoop-common.,non_debt,-
hadoop,9762,comment_0,This patch still needs to use the mechanism being added in HDFS-5017.,non_debt,-
hadoop,9762,comment_2,The patch looks pretty good to me. Maybe we can also add a Thread.sleep(random time) in for each worker thread? +1 with or without this change.,non_debt,-
hadoop,9762,comment_3,Good idea. Updated patch to address this comment. I still need to wait for HDFS-5017 and use instead of LightWeightGSet.,non_debt,-
hadoop,9762,comment_5,Updated patch to use LightWeightCache in RetryCache. This depends on HADOOP-9763. I will submit this once that change gets committed.,non_debt,-
hadoop,9762,comment_6,Changed from byte[] for clientId to two longs to save memory related to the cost of Array.,non_debt,-
hadoop,9762,comment_8,+1 for the new patch.,non_debt,-
hadoop,9762,comment_9,"I've committed this to trunk, branch-2 and branch-2.1-beta.",non_debt,-
hadoop,9763,summary,Extends LightWeightGSet to support eviction of expired elements,non_debt,-
hadoop,9763,description,We extends LightWeightGSet to support eviction of expired elements so that namenode could use it as the RetryCache.,non_debt,-
hadoop,9763,comment_0,adds LightWeightCache. Still need to test it.,test_debt,lack_of_tests
hadoop,9763,comment_1,slightly changes some code and comments.,non_debt,-
hadoop,9763,comment_2,adds some new tests.,non_debt,-
hadoop,9763,comment_4,"Nicholas, this should be moved to hadoop-common, right?",architecture_debt,violation_of_modularity
hadoop,9763,comment_5,GSet and LightWeightGSet are currently in hdfs. I agree that we should move them to common. Let's do it in a separated issue?,architecture_debt,violation_of_modularity
hadoop,9763,comment_6,"The patch looks good to me. Some thoughts after discussing with : 1. Maybe we do not need to refresh a cache entry's access time and position in the priority queue when the entry is accessed, since the expected timeout on the client side is based on the time the first request is sent. 2. It would be better if we can add an upper limit for the size of the GSet. This can guarantee NN continues to function even when a large amount of retry requests come within a short period of time.",design_debt,non-optimal_design
hadoop,9763,comment_7,"Nicholas, HADOOP-9762 is almost ready. Waiting for this patch to be committed. No pressure! :)",non_debt,-
hadoop,9763,comment_8,Thanks Jing for the suggestions. supports creation-expiration period and size limit.,non_debt,-
hadoop,9763,comment_10,adds more tests.,test_debt,lack_of_tests
hadoop,9763,comment_12,"The patch looks very good to me. Only some minors: 1. it would be better if more javadoc/comments can be added to such as the general steps for each check process, and what is tested for each step. 2. Maybe we can assign an initial random value for",documentation_debt,low_quality_documentation
hadoop,9763,comment_13,- adds more cases to the short test; - disables the longer test; - randomizes initial value of currentTestTime and the increments; - adds slightly more comments in,code_debt,low_quality_code
hadoop,9763,comment_14,+1 pending Jenkins for the new patch.,non_debt,-
hadoop,9763,comment_16,"I've committed this to trunk, branch-2 and branch-2.1-beta.",non_debt,-
hadoop,9764,summary,MapReduce output issue,non_debt,-
hadoop,9764,description,"Hi, I am new to Hadoop concepts. While practicing with one custom MapReduce program, I found the result is not as expected after executing the code on HDFS based file. Please note that when I execute the same program using Unix based file,getting expected result. Below are the details of my code. MapReduce in java ================== import import java.util.*; import import import import import import public class WordCount1 { public static class Map extends MapReduceBase implements Mapper { private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(LongWritable key, Text value, OutputCollector output, Reporter reporter) throws IOException { String line = value.toString(); String tokenedZone=null; StringTokenizer tokenizer = new while { one); } } } public static class Reduce extends MapReduceBase implements Reducer { public void reduce(Text key, Iterator values, OutputCollector output, Reporter reporter) throws IOException { int sum = 0; int val = 0; while (values.hasNext()) { val = sum += val; } if(sum&gt;1) output.collect(key, new IntWritable(sum)); } } public static void main(String[] args) throws Exception { JobConf conf = new JobConf(); Path inPath = new Path(args[0]); Path outPath = new Path(args[0]); ); outPath); } } input File =========== test my program during test and my hadoop your during get program hadoop generated output file on HDFS file system during 2 my 2 test 2 hadoop generated output file on local file system during 2 my 2 program 2 test 2 Please help me on this issue",non_debt,-
hadoop,9764,comment_0,Please use hadoop user mailing list to ask these type of questions. Jira is for reporting bugs.,non_debt,-
hadoop,9773,summary,fails,non_debt,-
hadoop,9773,description,"It fails on some size limit tests when the random seed is 1374774736885L,",non_debt,-
hadoop,9773,comment_0,"After added some debug messages, it shows that the test may set size limit to 0 and try to check it. However, size limit <= 0 mean no limit. So the bug is in the test but not in the main code.",non_debt,-
hadoop,9773,comment_1,avoids size limit = 0 in the test.,non_debt,-
hadoop,9773,comment_2,+1 for the patch.,non_debt,-
hadoop,9773,comment_4,Thanks Suresh for reviewing the patch. I have committed this.,non_debt,-
hadoop,9791,summary,Add a test case covering long paths for new FileUtil access check methods,non_debt,-
hadoop,9791,description,We've seen historically that paths longer than 260 chars can cause things not to work on Windows if not properly handled. Filing a tracking Jira to add a native io test case with long paths for new FileUtil access check methods added with HADOOP-9413.,test_debt,low_coverage
hadoop,9791,comment_0,Attaching the branch-1-win compatible patch. Will attach trunk patch as well.,non_debt,-
hadoop,9791,comment_1,That's MAX_PATH for non-UNICODED paths in NTFS; paths that don't begin {{\\?\}} see,non_debt,-
hadoop,9791,comment_2,"Thanks Steve for taking a look. Yes, MAX_PATH is the limit you are referring to. In this specific case, we already properly handle this case in native code by adding the long path {{\\?\}} prefix. Just adding a unittest to make sure this remains true.",non_debt,-
hadoop,9791,comment_3,Attaching the trunk patch.,non_debt,-
hadoop,9791,comment_5,Findbugs warnings are not related to the patch.,non_debt,-
hadoop,9791,comment_6,"+1 for the patch. Thank you, Ivan!",non_debt,-
hadoop,9791,comment_7,"Thanks Chris for the review, will commit this later today.",non_debt,-
hadoop,9791,comment_8,"Fix committed to trunk, branch-2 and branch-1-win. Thanks again Chris for the review!",non_debt,-
hadoop,9791,comment_13,Closing old tickets that are already part of a release.,non_debt,-
hadoop,9883,summary,Local mode File does not exist,non_debt,-
hadoop,9883,description,"Hive jobs in local mode fail with the error posted below. The jar file that's not being found exists and has the following access: rw-rw-r-- 1 ashahab ashahab 3914 Dec 18 2012 Steps to reproduce ~]$ hive Logging initialized using configuration in Hive history hiveOK Time taken: 1.675 seconds hiveOK Time taken: 0.029 seconds hiveOK Time taken: 0.301 seconds hiveTotal MapReduce jobs = 1 Launching Job 1 out of 1 Number of reduce tasks determined at compile time: 1 In order to change the average load for a reducer (in bytes): set order to limit the maximum number of reducers: set order to set a constant number of reducers: set Job = Tracking URL = Kill Command = job -kill Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1 2013-07-16 21:20:25,617 Stage-1 map = 0%, reduce = 0% 2013-07-16 21:20:30,026 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.13 sec 2013-07-16 21:20:31,110 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.13 sec 2013-07-16 21:20:32,188 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.13 sec 2013-07-16 21:20:33,270 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.13 sec 2013-07-16 21:20:34,356 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.13 sec 2013-07-16 21:20:35,455 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 3.4 sec MapReduce Total cumulative CPU time: 3 seconds 400 msec Ended Job = MapReduce Jobs Launched: Job 0: Map: 1 Reduce: 1 Cumulative CPU: 3.4 sec HDFS Read: 246 HDFS Write: 2 SUCCESS Total MapReduce CPU Time Spent: 3 seconds 400 msec OK 0 Time taken: 20.627 seconds hiveAutomatically selecting local only mode for query Total MapReduce jobs = 1 Launching Job 1 out of 1 Number of reduce tasks determined at compile time: 1 In order to change the average load for a reducer (in bytes): set order to limit the maximum number of reducers: set order to set a constant number of reducers: set 21:20:49 WARN conf.Configuration: attempt to override final parameter: Ignoring. 13/07/16 21:20:49 WARN conf.Configuration: attempt to override final parameter: Ignoring. WARNING: is deprecated. Please use in all the log4j.properties files. Execution log at: File does not exist: Method) Method) Method) Job Submission failed with exception does not exist: Execution failed with exit status: 1 Obtaining error information Task failed! Task ID: Stage-1 Logs:",non_debt,-
hadoop,9883,comment_0,Changes JobSubmitter code so that it submits the full classpath of jars to DistributedCache. This allows DistributedCache to resolve the classpath in local mode.,non_debt,-
hadoop,9883,comment_1,Changes the JobSubmitter. It now submits the full classpath to the DistributedCache. This allows the DistributedCache to resolve the classpath in local mode.,non_debt,-
hadoop,9883,comment_3,The Jobsubmitter was passing classpaths to the DistributedCache without the protocol. This caused issues in local mode. This changes the JobSubmitter so that it passes the whole path to the DistributedCache.,non_debt,-
hadoop,9883,comment_5,No change was made to or its dependencies for this patch.,non_debt,-
hadoop,9883,comment_8,"Hi Abin! Thanks for your contribution! Please refer to Matt's comment (from when I first started :-)) Also, please consider reading . The two things I'd recommend are using 2 spaces for indentation and following the 80 char limit per line. I noticed that the change was made as part of MAPREDUCE-181 . Before that it used to be what you are proposing. Do you understand why that change might / might not be relevant any more? I can review the test code once we finalize the src/main code.",code_debt,dead_code
hadoop,9883,comment_11,Attached test case TestMRDistributed is passing on trunk code without applying the fix. DistributedCache is supporting files in local mode. This issue doesn't seem to be valid now. May be it can be closed.,non_debt,-
hadoop,9883,comment_13,"Closing with cannot reproduce based upon previous comments. If this is still an issue, please reopen. Thanks.",non_debt,-
hadoop,9896,summary,TestIPC fail on trunk with error VM crash or System.exit,non_debt,-
hadoop,9896,description,"I'm running hadoop unit tests on a Ubuntu 12.04 64 bit virtual machine, every time I try to run all unit tests with command ""mvn test"", the TestIPC unit test will fail, the console will show ""The forked VM terminated without saying properly goodbye. VM crash or System.exit called?"" To reproduce: $cd $mvn clean install -Pdist -DskipTests $mvn test -Pdist -Dtest=TestIPC",non_debt,-
hadoop,9896,comment_0,surefire report output attached,non_debt,-
hadoop,9896,comment_1,"More information. I was using Hyper-V on Windows 8 running Ubuntu 12.04 VM. I found out that it was due to got stuck. However, if I just run that one test case, it will pass for me. But if I run all TestIPC unit tests, it will get stuck.",non_debt,-
hadoop,9896,comment_2,"The freeze is actually caused by test case If I just run this one test case, it was passing, but running all TestIPC tests results in main thread getting stuck in call.wait() (Client.java, line 1390). If I exclude this test case, or set a timeout, then TestIPC can pass. This is caused by timing issue. Sometimes randomly the tests pass for me, but they fail most of the time. And if I attach a debugger to the test process and set a break point at this test case (testRetryProxy), the tests will pass then.",non_debt,-
hadoop,9896,comment_3,I investigated this problem a little bit. I think there is some socket leak in the test code. Attach a patch that fix the leaking issue in the test by explicitly closing client and server at the end of each test case.,design_debt,non-optimal_design
hadoop,9896,comment_5,"I think we may also want to add timeout for each unit test in TestIPC, as proposed in HDFS-9916.",non_debt,-
hadoop,9896,comment_6,"This failure does not seem related to the fix at all. , I will add timeout as suggested.",non_debt,-
hadoop,9896,comment_7,"+1 Thanks Chuan! I tried the patch and it works fine! Not sure when lingering client thread is causing this problem though. Looks like sometimes the Server response was probably consumed by other client thus the testRetryProxy()'s client is waiting forever. But anyway, this patch fixed the TestIPC test case and I think they are good fix.",design_debt,non-optimal_design
hadoop,9896,comment_8,Attach a new patch. A 30 sec timeout is added to each test case. The unit test can pass consistently on my single core Ubuntu box. So I assume the timeout is enough for the test cases.,design_debt,non-optimal_design
hadoop,9896,comment_10,"Hi , I think this is not the root cause. I run restRetryProxy alone and also get the timeout, this probably has nothing to do with socket leak. Here is the error message. You can see client retried twice and got stuck",code_debt,low_quality_code
hadoop,9896,comment_11,"Another run timeout, this time with a bit more log I suspect there is race condition in Client or Server causing this.",design_debt,non-optimal_design
hadoop,9896,comment_12,", I think the timeout is a separate issue. In our case, if we run testRetryProxy alone, it never fails. However when running all the test cases in the test class together, we will always get the JVM crash error due to testRetryProxy hangs. It could be a race condition. What is your OS and configuration?  suspects your guys never run into our problem because you have faster machines. So it could also be that our Linux VM is slower and we never run into this timeout issue.",design_debt,non-optimal_design
hadoop,9896,comment_13,"My test env is Ubuntu 11.10 Linux 3.0.0, VM running in Mac Fusion. If I run all the tests, it timeouts more often, but not always, if I run just testRetryProxy, the timeout probability is lower.",non_debt,-
hadoop,9896,comment_14,"I may found the race condition in ipc.Client and have a fix, please see the detail in [HADOOP-9916]",code_debt,low_quality_code
hadoop,9896,comment_15,I think we should mark it as duplicated?,non_debt,-
hadoop,9896,comment_16,Resolving this as HADOOP-9916 should fix the root cause in IPC Client implementation.,non_debt,-
hadoop,9914,summary,nodes overview should use FQDNs,non_debt,-
hadoop,9914,description,"I am running a hadoop cluster in a bunch of VMs on my local machine and I am using avahi/zeroconf to do local name resolution (this is to avoid having to fiddle with my /etc/hosts file). The resourcemanager has an overview page, with links to all the nodemanager web-interfaces. The links do not work with zeroconf, due to the fact that the links are not including the domain part. zeroconf domains look like this ""hadoop1.local"", but the web-interface uses ""hadoop1"", which will not resolve. In hadoop 1.x all web-interfaces were using FQDN, meaning using avahi/zeroconf for name resolution was no problem. The same should be possible in hadoop 2.x. I am still beginning to work with hadoop 2.x, so there might be other parts, having the same problem, but I am not yet aware of any. If I find more of these, I will update this bug.",non_debt,-
hadoop,9914,comment_0,"We do not intentionally use shortnames. Many OSes out there are getting their /etc/hosts wrong, and specify the format as ""IP SHORT FULL"" when it should be ""IP FULL SHORT"" per the standard. Please check if that may be your issue as well? Do your nodes use FQDN in and/or /etc/hostname?",non_debt,-
hadoop,9914,comment_1,"I spent a day to get the /etc/hosts and /etc/avahi/hosts right, when I created the setup for hadoop 1.1.2 and that works fine. The namenode is actually still working correctly: When you browse the file system, it uses the fully qualified names of the datanodes, meaning everything works as I expect it. The resourcemanager does not have the same same behaviour. Here are my hosts: $ cat /etc/hosts 127.0.0.1 localhost 192.168.7.10 master.local master 192.168.7.11 backup.local backup 192.168.7.12 hadoop1.local hadoop1 192.168.7.13 hadoop2.local hadoop2 192.168.7.14 hadoop3.local hadoop3 $ cat /etc/avahi/hosts 127.0.0.1 localhost 192.168.7.10 master.local master 192.168.7.11 backup.local backup 192.168.7.12 hadoop1.local hadoop1 192.168.7.13 hadoop2.local hadoop2 192.168.7.14 hadoop3.local hadoop3 The hostnames are also correct: $ for host in master hadoop1 hadoop2 hadoop3; do vagrant ssh $host --command hostname ; done master.local hadoop1.local hadoop2.local hadoop3.local and just to be sure: $ for host in master hadoop1 hadoop2 hadoop3; do vagrant ssh $host --command 'cat /etc/hostname' ; done master.local hadoop1.local hadoop2.local hadoop3.local I also attached a screenshot of the webinterface.",non_debt,-
hadoop,9914,comment_2,"Sorry, I hit submit, but did not intend to. I found the issue, that /etc/hostname was not exactly right. Funny enough, HDFS always worked fine. PEBKAC. Closing the issue.",non_debt,-
hadoop,9929,summary,Insufficient permissions for a path reported as file not found,non_debt,-
hadoop,9929,description,"Using ""hadoop fs -ls"" to list a path where the permissions of a parent directory are insufficient ends up reporting ""no such file or directory"" on the full path rather than reporting the permission issue. For example:",non_debt,-
hadoop,9929,comment_0,"I added 2.1.0-beta for affects version, since I have seen it there as well.",non_debt,-
hadoop,9929,comment_1,In a way the current behavior is more secure. Is it not? A person who does not have permission to cd into a directory should not be able to discover the presence or absence of a file in the directory. Am I missing something?,non_debt,-
hadoop,9929,comment_2,"This is a good catch, Jason. The solution to this might be a bit tricky. If we're listing a single path (like /a/b/c), we definitely want to throw if we can't get to c. If we're listing a glob like /a/*/c, I'm not sure if we want to abort the whole glob (by throwing an exception), just because one path was not accessible to us. Perhaps we should just return the things we can get to? For what it's worth, the old behavior in branch-1 is to abort the whole glob, I believe. Another aspect is that exceptions such as StandbyException, etc. should *always* just propagate to the top level. Basically any IOException that we don't specifically catch (like permission denied, etc.) we should propagate to the top level, aborting the globStatus.",non_debt,-
hadoop,9929,comment_3,I don't believe the globbing code I cleaned up for v23+ has these issues. You want want to use it for reference.,code_debt,low_quality_code
hadoop,9929,comment_4,"No, we're not suggesting that the user can probe for paths. Like *nix, you should receive the permission denied exception for _any_ path under a directory you can't access. The glob rewrite makes it claim the entire path doesn't exist which it didn't do in 23 and earlier.",non_debt,-
hadoop,9929,comment_5,"Here's a patch which lets all exceptions except FileNotFound propagate, effectively restoring the old behavior. This means that, as before, your glob will fail if you encounter permission issues. It's probably best to keep it this way for compatibility reasons. I also added a unit test for this.",non_debt,-
hadoop,9929,comment_6,Thanks Darryn for the clarification on my last quesiton: Question: In a way the current behavior is more secure...?,non_debt,-
hadoop,9929,comment_7,"Hi Dilli, Neither the current (buggy) nor the fixed behavior allows users to list directories that they shouldn't. The question is whether an exception should be thrown, or the offending paths should silently be left out of the glob. Users are allowed to know that the inaccessible directory exists, because it exists in a directory which they do have list access to. For example if you had The question is whether /* by an unprivileged user should throw an exception, or simply return ""/mundane"". The unprivileged user is allowed to know that /secret exists, since it is located in a directory which he has list permission for, e.g. the root directory.",non_debt,-
hadoop,9929,comment_8,"ok, that was a bad example since it would succeed. (Wish I could edit comments!) The point is that something like /*/foo would fail since it would try to recurse into the /secret subdirectory.",non_debt,-
hadoop,9929,comment_10,"Andre, I tried to reproduce this on branch-2.1-beta, and was unable to do so. I also looked at the code in branch-2.1-beta, and was unable to see why this bug would exist there. So I would like to remove branch-2.1-beta from the versions fields, unless someone else can reproduce this.",non_debt,-
hadoop,9929,comment_11,"Just to be clear, the bug exists in branch-2. Just not in branch-2.1-beta. Let's update the JIRA accordingly.",non_debt,-
hadoop,9929,comment_12,"I also fired up a pseudodistributed 2.1 cluster and can't reproduce, so I think it's related to the Globber rewrite which is only in branch-2. Setting affects version appropriately.",non_debt,-
hadoop,9929,comment_13,"Patch looks good to me, but needs to be rebased a bit since we reverted HADOOP-9877. +1 pending rebase and Jenkins.",non_debt,-
hadoop,9929,comment_14,fixed in 2.3,non_debt,-
hadoop,9959,summary,Shell completion is broken,non_debt,-
hadoop,9959,description,I get some error related to awk: warning: escape sequence... when I try to use hadoop tab completion. I will attach a patch later.,non_debt,-
hadoop,9959,comment_0,This patch fixes bash completion by correcting multiples wrong uses of escaping.,non_debt,-
hadoop,9974,summary,Maven OutOfMemory exception when building with protobuf 2.5.0,non_debt,-
hadoop,9974,description,"Recently Hadoop upgraded to use Protobuf 2.5.0. To build the trunk, I updated my installed Protobuf 2.5.0. With this upgrade, I didn't encounter the build failure due to protoc, but failed when building HDFS sub-project. Bellow is failure message. I'm using Mac OS X.",non_debt,-
hadoop,9974,comment_0,Moved to Hadoop since it appears to be a general build issue. I can reproduce it when building trunk with '-Pdist -Dtar'. The only workaround is to use protobuf 2.4 and pass,non_debt,-
hadoop,9974,comment_1,"sounds unrelated. Try increasing your heap size for maven"" {code} export -Xmx512m {code",non_debt,-
hadoop,9974,comment_2,"Hi Arpit, In my environment(OS X), I could compile HttpFs correctly with trunk. Before compiling after upgrading your protobuf, please issue mvn clean. If not, the jar binaries which is compiled with protobuf 2.4.1 and 2.5.0 mixes, and it can causes some regression.",non_debt,-
hadoop,9974,comment_3,SteveL's workaround solved it for me. Tsuyoshi - 'mvn compile' would work but building a distribution with '-Pdist -Dtar' would fail consistently and only with protobuf 2.5.0. Thanks!,non_debt,-
hadoop,9974,comment_4,"Resolving, please re-open if Steve's fix is insufficient. FWIW, I have not hit this in my Ubuntu dev environment.",non_debt,-
hadoop,9974,comment_5,Edited a few fields to make it easier to find in case someone else hits the same issue.,non_debt,-
hadoop,9974,comment_6,"Patched BUILDING.TXT in branch-trunk, 2.1 and 2.1-beta to discuss this issue and provide the example env (I dropped the -headless flag to avoid distractions, though it is useful in OS/X to stop surefire bringing up windows)",non_debt,-
hadoop,10049,summary,2.1.0 beta won't run under cygwin?,non_debt,-
hadoop,10049,description,"I recently tried out 2.1.0 beta in the hope of benefiting from numerous improvements made re: running on cygwin. I cannot even get a ""bin/hadoop version"" to work, it gets a for the common classes such as I've looked a similiar bug report and have made sure that HADOOP_PREFIX is set, and that HADOOP_HOME is not set. I've looked at the output of ""bin/hadoop classpath"" and it looks correct EXCEPT that 2.1.0 shows the classpath using unix syntax and paths, while my older version of hadoop (1.2.1) shows it using Windows syntax and paths.",non_debt,-
hadoop,10049,comment_0,cygwin support is effectively deprecated with the native batch files for Windows. Closing as won't fix.,non_debt,-
hadoop,10049,comment_1,"HADOOP-11464, targeted to Apache Hadoop 2.7.0, reinstates support for launching Hadoop processes on Windows using Cygwin.",non_debt,-
hadoop,10067,summary,Missing POM dependency on jsr305,build_debt,under-declared_dependencies
hadoop,10067,description,Compiling for Fedora revels a missing declaration for This is the result of a missing explicit dependency on jsr305.,build_debt,under-declared_dependencies
hadoop,10067,comment_0,Robert -this POM patches in Is it the right dependency?,non_debt,-
hadoop,10067,comment_1,Previous patch in correct and for a different issue. This new patch resolves the jsr305 dependency issue.,non_debt,-
hadoop,10067,comment_2,looks better. The version declaration needs to go into for unified versions across the entire project. HADOOP-9594 is an example of this,architecture_debt,violation_of_modularity
hadoop,10067,comment_3,Moved version declaration into,architecture_debt,violation_of_modularity
hadoop,10067,comment_4,"+1 for the change, though I note that the patch does't apply directly to trunk or branch-2 -can you make sure your branches are up to date for patches. I also made the scope for the JAR {{<compile>}} to stop yet another JAR going downstream.",non_debt,-
hadoop,10067,comment_5,Fixed in 2.3 -thanks!,non_debt,-
hadoop,10067,comment_11,Is there anything that needs to be done here or will HADOOP-10101 take care of any issues?,non_debt,-
hadoop,10067,comment_12,"-dont worry, this issue is closed",non_debt,-
hadoop,10093,summary,hadoop-env.cmd sets HADOOP_CLIENT_OPTS with a max heap size that is too small.,non_debt,-
hadoop,10093,description,HADOOP-9211 increased the default max heap size set by hadoop-env.sh to 512m. The same change needs to be applied to hadoop-env.cmd for Windows.,non_debt,-
hadoop,10093,comment_0,"Patch attached. Change the HADOOP_CLIENT_OPTS to use 512m as heap size, aligned with HADOOP-9211.",non_debt,-
hadoop,10093,comment_1,"Example stack trace: Exception in thread ""main"" Java heap space va:93) putStream.java:59) 78) ands.java:229) at 90)",non_debt,-
hadoop,10093,comment_2,"+1 for the patch, pending Jenkins test-patch run.",non_debt,-
hadoop,10093,comment_4,"I committed this to trunk and branch-2. Shanyu, thank you for the contribution.",non_debt,-
hadoop,10106,summary,Incorrect thread name in RPC log messages,code_debt,multi-thread_correctness
hadoop,10106,description,"INFO IPC Server listener on 8020: readAndProcess from client 10.115.201.46 threw exception Unknown out of band call #-2147483647 This is thrown by a reader thread, so the message should be like INFO Socket Reader #1 for port 8020: readAndProcess from client 10.115.201.46 threw exception Unknown out of band call #-2147483647 Another example is which can also be called by handler thread. When that happend, the thread name should be the handler thread, not the responder thread.",code_debt,multi-thread_correctness
hadoop,10106,comment_0,"The fix is to move doRead method from Listener to Reader as that is the better place from code abstraction point of view. In addition, change all Thread.getName() to",non_debt,-
hadoop,10106,comment_3,You seemed to have unnecessarily moved the doRead() function's location in the server.java file. Can you please leave it in its original place in the file and please resubmit the patch.,architecture_debt,violation_of_modularity
hadoop,10106,comment_4,"Thanks, Sanjay. Moving doRead from Listener class to Reader class is another way to fix the specific issue found. When I was about to revert doRead, I feel like semantically wise it is better to have doRead be a member of Reader. So I includes it as part of the fix. What is the reason to keep doRead method under Listener class?",non_debt,-
hadoop,10106,comment_5,"Refactoring of code generally need to be done in separate Jira. Isn't the fixing of the thread name possible with the old structure? If so, I suggest that you do the refactoring in a separate jira.",non_debt,-
hadoop,10106,comment_6,ok. I have opened a new jira to move doRead method. Here is the new patch that just includes the thead name fix.,non_debt,-
hadoop,10106,comment_8,1,non_debt,-
hadoop,10106,comment_10,"I've committed this to trunk and branch-2. , currently I could not add your name to the contributor list (got some error msg in the jira system) so I could not assign this jira to you. I will try it again and update the jira information later.",non_debt,-
hadoop,10106,comment_14,", I have added you as a contributor and assigned the jira to you.",non_debt,-
hadoop,10139,summary,Update and improve the Single Cluster Setup document,non_debt,-
hadoop,10139,description,"The document should be understandable to a newcomer because the first place he will go is ""setup a single node"".",documentation_debt,low_quality_documentation
hadoop,10139,comment_0,Attaching a patch. I used as a reference and applied MRv2 config.,non_debt,-
hadoop,10139,comment_2,The findbugs warnings are not related to the patch.,non_debt,-
hadoop,10139,comment_3,"Could you please rebase the patch to current trunk? I will review it. Thanks, Arpit.",non_debt,-
hadoop,10139,comment_4,"Thank you , rebased the patch.",non_debt,-
hadoop,10139,comment_6,"This is a really useful update and should take care of a common complaint on the mailing lists. My comments: # I think the pseudo-distributed instructions result in the job being executed locally instead of being submitted to yarn so its not a complete example. We need minimal yarn configuration to demonstrate job execution on yarn. # should be Same with {{start-yarn.sh}} # {{HADOOP_PREFIX}} seems to be necessary, else {{HADOOP_CONF_DIR}} gets initialized to /etc/hadoop and this breaks the DataNode, for one. We should document setting {{HADOOP_PREFIX}}. # Typo: _quickly perform single operations using Hadoop_; _single_ should be _simple_? # Windows setup is quite different from Linux and the steps given here would be confusing for someone attempting to install on Windows. Can you please add a link to the Windows setup wiki page and clarify that the instructions given here are for Linux only? Users tend to take the Linux instructions and attempt to run them in Cygwin which never works as expected. Nitpick comments, not that important: # Instead of ""Format a new perhaps we can just say Format the filesystem. # It may help readability if steps in the execution section were a numbered list. What do you think? # It would be nice if the bullet points after ""Now you are ready to start your Hadoop cluster in one of the three supported modes were links to subsequent sections, not a big deal.",documentation_debt,low_quality_documentation
hadoop,10139,comment_7,"Thanks, ! I think the v3 patch reflects all of your comments. Split the instructions to 'local' execution and 'yarn' execution, and added a configuration to execute job on yarn. Yes. Changed 'single' to 'simple'. A numbered list is better to me. Changed to a numbered list.",non_debt,-
hadoop,10139,comment_9,+1 the instructions look great. I will commit this shortly.,non_debt,-
hadoop,10139,comment_10,I committed this to trunk and branch-2. Thanks for the contribution .,non_debt,-
hadoop,10139,comment_12,"Thanks for reviewing and committing, !",non_debt,-
hadoop,10164,summary,Allow UGI to login with a known Subject,non_debt,-
hadoop,10164,description,For storm I would love to let Hadoop initialize based off of credentials that were already populated in a Subject. This is not currently possible because logging in a user always creates a new blank Subject. This is to allow a user to be logged in based off a pre-existing subject through a new method.,non_debt,-
hadoop,10164,comment_0,"is only for branch-0.23. login-from-subject applies to trunk, branch-2, and branch-2.2",non_debt,-
hadoop,10164,comment_2,"The test failure appears to be spurious. The test failed for me 1 out of 4 runs. It is a multi-threaded test case that failed too. I will look into it a bit more and will file a JIRA about the test failure. I did not include any new tests, because it is a really small re-factor and the existing tests should all pass.",non_debt,-
hadoop,10164,comment_3,Ahh it looks someone else found this already HADOOP-10062.,non_debt,-
hadoop,10164,comment_4,+1 Looks simple enough.,non_debt,-
hadoop,10164,comment_5,Great I'll merge it in.,non_debt,-
hadoop,10164,comment_6,"I checked this into trunk, branch-2, and branch-0.23",non_debt,-
hadoop,10165,summary,occasionally fails,non_debt,-
hadoop,10165,description,From :,non_debt,-
hadoop,10165,comment_0,Closing this issue as duplicate.,non_debt,-
hadoop,10169,summary,remove the unnecessary synchronized in JvmMetrics class,code_debt,dead_code
hadoop,10169,description,"When i looked into a HBase JvmMetric impl, just found this synchronized seems not essential.",code_debt,dead_code
hadoop,10169,comment_0,one line change,non_debt,-
hadoop,10169,comment_2,"Is it possible that multiple threads call boolean) around the same time, which causes getGcInfo to be called concurrently? In that case, we still need the synchronized here?",non_debt,-
hadoop,10169,comment_3,"The map gcInfoCache will be filled up once the first getMetrics() be invoked, seems then it'll be keep unchanged always. Then during almost of all the life cycle, the gcInfoCache will be read only without any write. Do we still need the synchronized? Am i missing sth ? thanks for taking a look at it:)",non_debt,-
hadoop,10169,comment_4,"As Jing pointed out, the function needs to be synchronized as written. It's true that once initialized the code will no longer modify the map, but there's a race during the initialization itself. If two or more threads call this function before it's initialized then they can both attempt to put() at the same time. Alternatively, a thread could be late to the party and be calling get() just as the initializing thread is calling put() which is also a thread safety violation. One possible way to avoid the synchronization bottleneck is to use a volatile boolean to indicate whether it's initialized so the vast majority of callers don't have to grab a lock. Something like this pseudo-code:",requirement_debt,non-functional_requirements_not_fully_satisfied
hadoop,10169,comment_5,", , thanks for your review, yes, seems my original thought has some flaw:) I just attached another small v2 patch, could you help to review again? thanks in advance! I think it's enough to use the ConcurrentHashMap to replace the big Synchronized, and it should be OK for all the read-only requests once initialization done. , thanks for your detailed pseudo-code, IMHO, it seems a bit tricky here to work correctly maybe, since the getGcInfo() is invoked by a loop in getGcUsage(), then if the first time in loop initialized successful, then the latter ones will be omitted:)",non_debt,-
hadoop,10169,comment_7,"Doh, in my haste I misread the code, so my pseudo-code isn't relevant. We're not initializing the map just once overall, we're initializing it once per key. As you point out, we can be initializing a new key as we're busy accessing an old key, and something like a ConcurrentMap is more appropriate. However in general we cannot just replace HashMap with ConcurrentHashMap, remove the synchronized keywords, and expect it to work properly in all cases. There's now a race in getGcInfo where thread A comes along, sees there isn't an entry in the map for key K, and starts creating an empty MetricsInfo for it. Meanwhile thread B comes along, also sees there isn't an entry for key K, creates an empty MetricsInfo, puts it in the map, updates the MetricsInfo with new metrics, and continues on. Thread A then wakes back up and pokes the empty MetricsInfo into the map for key K, causing data loss of the metrics computed by thread B. The gcInfoCache.put needs to be and if putIfAbsent returns a value then we need to return that instead of the empty metrics info. Couple of other nits on the patch: the import is no longer necessary, and the patch includes an unrelated whitespace change that pushes one of the modified lines over 80 columns.",non_debt,-
hadoop,10169,comment_8,"Thanks  for your nice reply, yes, you are correct! Attached v3 with an IDE formatting now:)",non_debt,-
hadoop,10169,comment_10,Thanks for updating the patch. One last nit is that this code: can be simplified to the following so there aren't so many return statements to track:,code_debt,low_quality_code
hadoop,10169,comment_11,"Yes, you are right, now attached v4 patch. Thanks for review, i really appreciate and enjoy it:) and have learned a lot from those detail code/styles.",non_debt,-
hadoop,10169,comment_13,"+1 for the latest patch. Thanks for cleaning the code, Liang Xie! And thanks to Jason for the review! I will commit it shortly.",non_debt,-
hadoop,10169,comment_14,I've committed this to trunk and branch-2.,non_debt,-
hadoop,10175,summary,Har files system authority should preserve userinfo,non_debt,-
hadoop,10175,description,"When Har file system parse the URI to get the authority at initialization, the userinfo is not preserved. This may lead to failures if the underlying file system relies on the userinfo to work properly. E.g. will be parsed to where user:passwd is lost in the processing.",non_debt,-
hadoop,10175,comment_0,Attaching a patch. A unit test is also added to cover the case.,non_debt,-
hadoop,10175,comment_2,"Hi, Chuan. The patch looks good. There is a very minor typo on the test name: I think this patch will be ready to commit after that's corrected.",documentation_debt,low_quality_documentation
hadoop,10175,comment_3,"Thanks for reviewing, Chris! Attaching a new patch that corrects the typo.",documentation_debt,low_quality_documentation
hadoop,10175,comment_4,+1 for the patch. Thanks for fixing that typo. I'll commit this.,documentation_debt,low_quality_documentation
hadoop,10175,comment_5,"I committed this to trunk, branch-2 and branch-2.3. Chuan, thank you for the patch.",non_debt,-
hadoop,10184,summary,Hadoop Common changes required to support HDFS ACLs.,non_debt,-
hadoop,10184,description,This is an umbrella issue for tracking all Hadoop Common changes required to support the HDFS ACLs project.,non_debt,-
hadoop,10184,comment_0,"I have merged the HDFS-4685 branch to trunk, as per the passing merge vote here:",non_debt,-
hadoop,10184,comment_5,I'm resolving this. All sub-tasks have been completed for inclusion of HDFS ACLs in release 2.4.0.,non_debt,-
hadoop,10214,summary,Fix multithreaded correctness warnings in,code_debt,low_quality_code
hadoop,10214,description,"When i worked at HADOOP-9420, found the unrelated findbugs warning:",code_debt,low_quality_code
hadoop,10214,comment_0,"A simple fix. I think it was introduced by: r1556103 | vinodkv | 2014-01-07 09:56:11 +0800 (Tue, 07 Jan 2014) | 2 lines YARN-1029. Added embedded leader election in the ResourceManager. Contributed by Karthik Kambatla. The above change made public, so the multi-threaded warning be observed.",code_debt,multi-thread_correctness
hadoop,10214,comment_1,"verified on my local box, after applied the attached patch, all 4 warnings gone away",non_debt,-
hadoop,10214,comment_3,My bad. The patch makes sense. Thanks . +1. Will commit this shortly.,non_debt,-
hadoop,10214,comment_4,"Thanks for the contribution, Liang Xie. Just committed this to trunk and branch-2.",non_debt,-
hadoop,10225,summary,Publish Maven javadoc and sources artifacts with Hadoop releases.,non_debt,-
hadoop,10225,description,Right now Maven javadoc and sources artifacts do not accompany Hadoop releases within Maven central. This means that one needs to checkout source code to DEBUG aspects of the codebase... this is not user friendly. The build script(s) should be amended to accommodate publication of javadoc and sources artifacts alongside pom and jar artifacts. Some history on this conversation can be seen below,design_debt,non-optimal_design
hadoop,10225,comment_0,Can someone please mark 'fix versions' box and i will produce patches for all branches this concerns? Thanks in advance Lewis,non_debt,-
hadoop,10225,comment_1,"I am developing a patch for common-trunk however I would like to obtain some advice on the following: The parent pom.xml [0] defines X profiles, namely *src*, *dist*, *sign* and *clover*. I am unsure which profile the Hadoop release manager uses for pushing releases. Can someone (a release manager or someone familiar with the Maven profile(s) used within the release process) please confirm what the process actually is? Once I know this I can edit the Maven workflow and add the correct config. Thank you [0]",non_debt,-
hadoop,10225,comment_2,"Patch for hadoop-common trunk codebase. Adds scm properties as well as a release profile to parent pom.xml. Using the like mvn release:clean release:prepare to check generated artifacts, then mvn release:perform to push the Maven artifacts to Nexus should do the trick. I've confirmed that sources and Javadoc artifacts are now generated locally. This works perfectly for us over on Apache Gora when we are doing the release procedure.",non_debt,-
hadoop,10225,comment_3,I would be happy to update documentation to accommodate the new *release* profile introduced here. I've asked for write permissions on the common-dev list,non_debt,-
hadoop,10225,comment_4,"I checked on Maven central, and it looks like the 2.x release pretty much have javadoc and source artifacts: We're missing javadoc for 3.0.0-alpha1 though, so let's update the affects/targets accordingly.",documentation_debt,outdated_documentation
hadoop,10225,comment_5,Hi  wow this is a blast from the past :) Are you planning on a putting a ptch together for 3.0.0-alpha1 or do you want me to do it?,non_debt,-
hadoop,10225,comment_6,"Hey Lewis, if you're game, I'd definitely welcome the help. trunk has a new release script at and instructions at . I'm fairly certain I followed these instructions accurately, but for whatever reason the ""mvn deploy"" step didn't upload the javadoc jars.",non_debt,-
hadoop,10225,comment_7,"cool, I'll have a crack tomorrow afternoon. Can you remind me of the committer workflow again.. it's been a while since I submitted a patch and I don't want to waste anyone's time. Thanks.",non_debt,-
hadoop,10225,comment_8,"cool, I'll have a crack tomorrow afternoon. Can you remind me of the committer workflow again.. it's been a while since I submitted a patch and I don't want to waste anyone's time. Thanks.",non_debt,-
hadoop,10225,comment_9,"I think these two pages should explain things in great detail: tl;dr though is that if you assign the JIRA to yourself, post a patch, and click the ""Submit patch"" button, it'll run precommit and I'll review it. As a little pre-patch review, I'm hoping we can do some minimal change so the ""mvn deploy"" also picks up the javadoc jar. I see your original patch uses the which is a bigger change.",non_debt,-
hadoop,10225,comment_13,"Hi Lewis, thanks for working on this. As I said in an earlier comment, I was hoping to get this working without pulling in the release plugin. Since the current release script and instructions deploy everything except the javadoc jar, I think some smaller patch will also get the javadoc deployed.",non_debt,-
hadoop,10225,comment_16,"Bulk update: moved all 3.2.0 non-blocker issues, please move back if it is a blocker.",non_debt,-
hadoop,10225,comment_17,The original patch was allowed to stagnate. I've closed the Github PR. Someone else can take this on if they deem javadoc and sources being useful for consumers of the software.,non_debt,-
hadoop,10235,summary,Hadoop tarball has 2 versions of stax-api JARs,non_debt,-
hadoop,10235,description,"They are: stax-api-1.0-2.jar stax-api-1.0.2.jar Maven cannot resolve them property because they are published under different groupIds, because of that for maven are different things. we need to exclude one of them explicitly",non_debt,-
hadoop,10235,comment_0,This should be a simple exclusion in Maven no?  are you going to provide a patch? If not then maybe I can look in to it.,non_debt,-
hadoop,10235,comment_1,"patch excluding stax 1.0.1 JAR, 1.0-2 surviving. Reference for the decision of which one:",non_debt,-
hadoop,10235,comment_2,1,non_debt,-
hadoop,10235,comment_4,Committed to trunk and branch-2.,non_debt,-
hadoop,10284,summary,Add metrics to the HistoryRpcScheduler,non_debt,-
hadoop,10284,description,None,non_debt,-
hadoop,10284,comment_0,Resolved in HADOOP-10281,non_debt,-
hadoop,10291,summary,fails,non_debt,-
hadoop,10291,description,fails with Assertion Error,non_debt,-
hadoop,10291,comment_0,The root cause is that s property used by some test cases has a side effect. The tests set The broken test case assumed that it will be set when other tests are invoked. The fix is to explicitly set the property instead of depending on the execution order.,design_debt,non-optimal_design
hadoop,10291,comment_2,"+1 for the patch. Committed to trunk, branch-2 and branch-2.3. Thanks for the contribution .",non_debt,-
hadoop,10295,summary,Allow distcp to automatically identify the checksum type of source files and use it for the target,non_debt,-
hadoop,10295,description,"Currently while doing distcp, users can use to specify the checksum type in the target FS. This works fine if all the source files are using the same checksum type. If files in the source cluster have mixed types of checksum, users have to either use ""-skipcrccheck"" or have checksum mismatching exception. Thus we may need to consider adding a new option to distcp so that it can automatically identify the original checksum type of each source file and use the same checksum type in the target FS.",non_debt,-
hadoop,10295,comment_0,"Initial patch for review. The patch adds a option to distcp. If the option is set, it simply checks the checksum type for each source file and uses it for creating the corresponding temp file in the target FS. Still need to add unit tests and do some system tests.",test_debt,lack_of_tests
hadoop,10295,comment_1,"Funny, I have been preparing a patch for this very same issue for a week. Some comments regarding your patch: * instead of a new commandline option, it may be better to extend FileAttribute enum * and are probably HDFS specific (although being available in hadoop-common). I opened HADOOP-10297 for having * Instead of doing two instanceof check, it is possible to use the super class * is not equivalent of setting overwrite argument to true. From it is * Having a test to check if the option actually works would be a nice to have (according to me) Since I also have a patch, I'll attach it to this ticket to, and let have a hadoop maintainer help us sorting them out :)",design_debt,non-optimal_design
hadoop,10295,comment_2,Alternative patch to implement the requested feature. It extends the current list of file attributes option with a checksum attribute. It requires two other patches to be applied first in order to compile and tests to pass: * HADOOP-10294 : FileChecksum should provide getChecksumOpt method * HDFS-5843: throws IOException if checksum is disabled,non_debt,-
hadoop,10295,comment_3,"It seems you are actually a hadoop commiter, so it's just great. Hope you'll find my patch helpful and you'll be able to add this feature soon!",non_debt,-
hadoop,10295,comment_4,"Thanks for the comment ! That's right. I also found this problem in my patch. I personally like your idea in HADOOP-10297. That can simplify the logic there. However, FileChecksum is a public API marked as stable, to add a new abstract method there may cause incompatibility (e.g., other ppl may have implemented their own FileChecksum). A workaround here can be adding getChecksumOpt() to FileChecksum and let it return null. Totally agree. Actually I've added a new unit test in my 001 patch, and the new unit test is very similar to yours :) I thought about this problem. To me checksum type may be a little bit different from other file attributes, since other file attributes are all metadata stored in NN. Thus in my first patch I just add a new option. But now I think to put the checksum type in the FileAttribute enum should be more clear. Currently I have a 001 patch which fixes the CreateFlag bug and adds a unit test. My original plan is to post it after I finish system test in my local cluster. But since you've worked on this issue for some time and already have a decent patch, I'd like to review your patch and commit it when it is ready.",code_debt,complex_code
hadoop,10295,comment_5,"Yes, my patch for HADOOP-10297 breaks source compatibility (but not binary compatibility). It may be okay for next Hadoop major version, but probably not for a minor version. Waiting for some guidance here (and it's really easy to change) From the user point of view, block size, replication and checksum option are seen as the same kind of metadata. Only from the FileSystem API, it is seen as different kind of metadata because the information is not stored in the same place. My patch is mostly ready I think, but it is blocked by the other tickets I mentioned. Hopefully they will be reviewed quickly.",non_debt,-
hadoop,10295,comment_6,"Besides the concern on FileChecksum, some other comments on the current patch: # We may want to change ""checksum"" to ""checksumtype"" in the changes of PRESERVE_STATUS and FileAttribute. # We actually do not need to pass a FileChecksum to In if we need to preserve the checksum type, we get the checksum type of the source file and we reuse this checksum in compareCheckSums(). In that case we only need to call once (note that getFileChecksum is very costly). # We should use in the following change (see boolean, int, short, long, Progressable)) # The new added unit test does not cover there scenario where source files have different REAL checksum types (CRC32 and CRC32C), in which case copy with preserving checksum type should succeed and the original checksum types should be preserved in the target FS. We should add unit tests for this. # There are some unnecessary whilespace and blank line changes.",code_debt,low_quality_code
hadoop,10295,comment_7,"For point 3, I was using because it was the previous behavior, and in once copy succeed, a call is made to which sets the owner, group, replication and permissions. Should it be refactored?",non_debt,-
hadoop,10295,comment_8,"Thanks for working on this, Jing. One thing to note is that the block size needs to be identical in addition to the checksum parameters in order for the checksums to match. So it might make more sense to introduce an option to preserve the two together.",design_debt,non-optimal_design
hadoop,10295,comment_9,Agree the option needs to mean that the checksum algorithm *and* the blocksize are preserved.,non_debt,-
hadoop,10295,comment_10,"Thanks for the comments, Kihwal and Sangjin! So this 002 patch is based on my 001 patch and Laurent's patch, and it also preserve the block size when processing the preserving checksum type option. I've tested in my local cluster with the patch. In my test I simply generate some files with different checksum types, and run distcp with/without ""-pc"". The distcp succeeded when -pc is enabled.",non_debt,-
hadoop,10295,comment_12,+1 patch looks good.,non_debt,-
hadoop,10295,comment_13,"The patch looks good. Just one question. I see now there is an explicit call to create the permission in copyToTmpFile(). What is the nature of this change? Was the same thing being done implicitly and it is just made explicit, or is there another reason?",non_debt,-
hadoop,10295,comment_14,"Thanks for the review, Nicholas and Sangjin! , that is originally implicitly contained in the FileSystem#create call (see boolean, int, short, long, Progressable)). I just pulled it out to make the code not too long.",code_debt,low_quality_code
hadoop,10295,comment_15,I will commit this patch later today if there is no more comment.,non_debt,-
hadoop,10295,comment_16,"I've committed this to trunk and branch-2. Thanks to  for the contribution! Thanks to Kihwal, Sangjin and Nicholas for the review!",non_debt,-
hadoop,10305,summary,Add and to core-default.xml,non_debt,-
hadoop,10305,description,"and were added in HADOOP-9420, but these two parameters are not written in core-default.xml.",non_debt,-
hadoop,10305,comment_0,Attaching a patch.,non_debt,-
hadoop,10305,comment_2,"The patch is to add parameters to config file, so new test are not needed.",non_debt,-
hadoop,10305,comment_3,"+1, nice find and nice patch. Will commit shortly.",non_debt,-
hadoop,10305,comment_4,"Merged to trunk, branch-2, branch-2.3. Thanks for the contribution Akira!",non_debt,-
hadoop,10327,summary,Trunk windows build broken after HDFS-5746,non_debt,-
hadoop,10327,description,Hadoop build broken with Native code errors in windows.,non_debt,-
hadoop,10327,comment_0,This seems to fix the issue. Please check,non_debt,-
hadoop,10327,comment_1,Build fails with following error,non_debt,-
hadoop,10327,comment_2,"I have not tested this with Windows, but a review of the patch looks good. +1.",non_debt,-
hadoop,10327,comment_3,"I can try this out on my Windows VM later. , thanks for picking this up. HDFS-5746 added and Do you know if any of those tests are failing on Windows? It looks like they're all set up to skip running on Windows, but I haven't confirmed yet. I'll confirm when I review this patch.",non_debt,-
hadoop,10327,comment_4,"Yes, these tests will be skipped in windows. I have verified in my eclipse on windows 7.",non_debt,-
hadoop,10327,comment_5,+1 for the patch. I confirmed on my own Windows machine. I plan to commit this after a Jenkins run.,non_debt,-
hadoop,10327,comment_7,"I committed this to trunk and branch-2. Vinay, thanks very much for providing a patch to fix this. Colin, thank you for help with code review. I confirmed by testing the Windows build manually.",non_debt,-
hadoop,10337,summary,from,non_debt,-
hadoop,10337,description,This stack trace came from our HBase 0.94.3 production env: ERROR getting attribute of threw an exception Caused by: ... 27 more,non_debt,-
hadoop,10337,comment_0,A tentitive quick fix,non_debt,-
hadoop,10337,comment_1,"From ConcurrentHashMap Javadoc: ""The view's iterator is a ""weakly consistent"" iterator that will never throw and guarantees to traverse elements as they existed upon construction of the iterator""",non_debt,-
hadoop,10337,comment_3,lgtm Will commit over next day or so unless objection.,non_debt,-
hadoop,10337,comment_4,ping ?,non_debt,-
hadoop,10337,comment_5,"+1, looks good to me as well.",non_debt,-
hadoop,10337,comment_6,"Committed to branch-2.4, branch-2 and to trunk. Thank you for the patch  and the review Mr",non_debt,-
hadoop,10343,summary,Change info to debug log in,non_debt,-
hadoop,10343,description,in we print logs at info level when we drop responses. This causes lot of noise on the console.,code_debt,low_quality_code
hadoop,10343,comment_0,+1 pending Jenkins.,non_debt,-
hadoop,10343,comment_2,no tests added as this is just a change in log level.,non_debt,-
hadoop,10343,comment_3,Merged to branch-2 Thanks for the review Jing,non_debt,-
hadoop,10352,summary,Recursive setfacl erroneously attempts to apply default ACL to files.,non_debt,-
hadoop,10352,description,"When calling setfacl -R with an ACL spec containing default ACL entries, the command can fail if there is a mix of directories and files underneath the specified path. It attempts to set the default ACL entries on the files, but only directories can have a default ACL.",non_debt,-
hadoop,10352,comment_0,"The attached patch splits off a separate list of just the access ACL entries. Then, as the command recurses through the tree, files get just the access ACL entries and directories get the full list of access and default ACL entries. This matches the behavior of Linux setfacl. I also added tests covering each of the 3 APIs in {{TestAclCLI}}.",non_debt,-
hadoop,10352,comment_1,"+1 for the patch. Chris, will the #LF# checks fail on Windows due to the extra CR?",non_debt,-
hadoop,10352,comment_2,"Thanks, Arpit. I'll commit this shortly. No, actually this is a bit of indirection implemented in {{TestAclCLI}} so that we have a platform-agnostic token for the line separator inside the XML file. At runtime, this gets replaced with the platform-specific line separator:",non_debt,-
hadoop,10352,comment_3,Thanks.,non_debt,-
hadoop,10352,comment_4,Clarification: I'm going to wait on a Jenkins +1 before committing.,non_debt,-
hadoop,10352,comment_6,"I committed this to trunk. Thank you for the code review, Arpit. The failure in {{TestHASafeMode}} was unrelated. I couldn't reproduce it locally.",non_debt,-
hadoop,10352,comment_11,Closing old tickets that are already part of a release.,non_debt,-
hadoop,10353,summary,is not thread safe,requirement_debt,non-functional_requirements_not_fully_satisfied
hadoop,10353,description,"The class uses a plain {{HashMap}} for caching. When the number of inserted values exceeds the the map's load threshold, it triggers a rehash. During this time, a different thread that performs a get operation on a previously inserted key can obtain a null value instead of the actual value associated with that key. The result is a NPE potentially being thrown when calling protocol)}} concurrently.",code_debt,multi-thread_correctness
hadoop,10353,comment_0,"+1 for the patch, pending Jenkins run. I've submitted it for a Jenkins run.",non_debt,-
hadoop,10353,comment_2,Added patch with license header.,non_debt,-
hadoop,10353,comment_4,+1. Thanks for taking care of the license header. I'll commit this now.,non_debt,-
hadoop,10353,comment_6,"I committed this to trunk, branch-2 and branch-2.4. Tudor, thank you for reporting the bug and contributing a patch.",non_debt,-
hadoop,10353,comment_7,Thank you too!,non_debt,-
hadoop,10374,summary,InterfaceAudience annotations should have,non_debt,-
hadoop,10374,description,"There are valid use cases for accessing the InterfaceAudience annotations programatically. In HBase, we are writing a unit test to check whether every class in the client packages are annotated with one of the annotations. Plus, we are also thinking about a golden file to containing all public method sigs, so that we can ensure public facing API-compatibility from a unit test. Related: HBASE-8546, HBASE-10462, HBASE-8275",test_debt,lack_of_tests
hadoop,10374,comment_0,Straightforward patch. It would be good if we can have this in 2.4 line as well.,non_debt,-
hadoop,10374,comment_1,"+1 for the patch, I will commit it shortly.",non_debt,-
hadoop,10374,comment_4,I committed this to trunk through branch-2.4. Thanks for the contribution .,non_debt,-
hadoop,10423,summary,Clarify compatibility policy document for combination of new client and old server.,documentation_debt,low_quality_documentation
hadoop,10423,description,"As discussed on the dev mailing lists and MAPREDUCE-4052, we need to update the text of the compatibility policy to discuss a new client combined with an old server.",documentation_debt,outdated_documentation
hadoop,10423,comment_0,Here is a patch to update the documentation.  and both of you have expressed interest in the content of this change. Do you want to code review?,non_debt,-
hadoop,10423,comment_2,"Looks good. +1. Will commit this later today. Thanks for drafting it, Chris.",non_debt,-
hadoop,10423,comment_3,Great! Thanks for the review Karthik and for volunteering to do the commit.,non_debt,-
hadoop,10423,comment_4,"Thanks Chris. Just committed this to trunk, branch-2, and branch-2.4.",non_debt,-
hadoop,10427,summary,KeyProvider implementations should be thread safe,requirement_debt,non-functional_requirements_not_fully_satisfied
hadoop,10427,description,The {{KeyProvider}} API should be thread-safe so it can be used safely in server apps.,requirement_debt,non-functional_requirements_not_fully_satisfied
hadoop,10427,comment_0,Patch that states in the {{KeyProvider}} javadocs that implementations must be thread-safe and makes and {{UserProvider}} (the {{KeyProvider}} impls in Hadoop) thread-safe. uses a read-write lock while UserProvider synchronizes its methods.,requirement_debt,non-functional_requirements_not_fully_satisfied
hadoop,10427,comment_2,"patch adds documentation, synchronized keywords and synchronization via a read write lock. it is obvious and not that easy to write a testcase",test_debt,lack_of_tests
hadoop,10427,comment_3,rebasing to trunk,non_debt,-
hadoop,10427,comment_4,The latest patch looks good to me. It was quite trivial to review once I applied the diff and then generated a new diff using `git diff -b'. +1 pending Jenkins.,non_debt,-
hadoop,10427,comment_6,I start getting a little concerned when we are talking about thread safety of these. Mainly because the should not be used as a database. Initial implementation of the KeyProvider API assumes a rather basic and controlled access to key management. We do need to ensure thread safety for these implementations and equally as important for protection against corrupted keystores. See: HADOOP-10224 I will add that jira as related. I would like to consider a more appropriate provider type for access from a KMS as well.,requirement_debt,non-functional_requirements_not_fully_satisfied
hadoop,10427,comment_7,"Thread safety is needed regardless of KMS server, in case a client is doing multithreaded stuff. Agree that the is not the ideal backend for the KMS server. Currently is the only thing we have in Hadoop. The KMS server simply uses the KeyProvider API, so you could plugin any impl there, potentially one talking with a KMIP server. The KMS server does not attempt to provide a reliable store, just a client/server implementation deferring to another implementation. Sounds reasonable?",requirement_debt,non-functional_requirements_not_fully_satisfied
hadoop,10427,comment_8,Looks good. +1,non_debt,-
hadoop,10427,comment_9,", have you concerns been addressed by my last comment?",non_debt,-
hadoop,10427,comment_10,"i assume we are good, i will commit this tomorrow unless i hear otherwise.",non_debt,-
hadoop,10427,comment_11,committed to trunk.,non_debt,-
hadoop,10432,summary,Refactor SSLFactory to expose static method to determine HostnameVerifier,non_debt,-
hadoop,10432,description,"The method is private and takes a configuration to fetch a hardcoded property. Having a public method to resolve a verifier based on the provided value will enable getting a verifier based on the verifier constant (DEFAULT, STRICT, STRICT_IE6, ALLOW_ALL).",code_debt,low_quality_code
hadoop,10432,comment_1,The patch is straight forward refactoring of code that is already tested.,non_debt,-
hadoop,10432,comment_2,"+1, the patch looks good to me. I agree there is no need for a test case for this change.",non_debt,-
hadoop,10432,comment_3,committed to trunk.,non_debt,-
hadoop,10485,summary,Remove dead classes in hadoop-streaming,code_debt,dead_code
hadoop,10485,description,Hadoop-streaming no longer requires many classes in o.a.h.record. This jira removes the dead code.,code_debt,dead_code
hadoop,10485,comment_1,The v0 patch introduces no functionality changes. It only removes the unused classes.,code_debt,dead_code
hadoop,10485,comment_2,"+1. Thank you, Haohui, for the code cleaning up.",code_debt,low_quality_code
hadoop,10485,comment_3,I've committed the patch to trunk and branch-2. Thanks  for the review.,non_debt,-
hadoop,10485,comment_8,I reverted this from branch-2 per the discussion in HADOOP-10474. These classes weren't deprecated in 1.x and normally we should allow a full major release after deprecating for users to migrate.,non_debt,-
hadoop,10496,summary,Metrics system FileSink can leak file descriptor.,code_debt,low_quality_code
hadoop,10496,description,"{{FileSink}} opens a file. If the {{MetricsSystem}} is shutdown, then the sink is discarded and the file is never closed, causing a file descriptor leak.",code_debt,low_quality_code
hadoop,10496,comment_0,"I found this bug during runs of {{TestFileSink}} on Windows. The test concludes by trying to delete the output file. On Windows, the delete fails, because the file is still held open and locked. Here is a patch that fixes it. {{MetricsSink}} is public, and there can be custom implementations in the wild. I didn't want to introduce a by defining a {{close}} method right on the {{MetricsSink}} interface. Instead, this patch adds JavaDocs to {{MetricsSink}} stating that implementations also may optionally implement {{Closeable}}, and then does an instanceof check and calls close. I verified on Mac and Windows.",non_debt,-
hadoop,10496,comment_2,+1 the patch looks good to me.,non_debt,-
hadoop,10496,comment_3,"Thank you for the review, Jing. I committed this to trunk and branch-2.",non_debt,-
hadoop,10498,summary,Add support for proxy server,non_debt,-
hadoop,10498,description,HDFS-6218 & HDFS-6219 require support for configurable proxy servers.,non_debt,-
hadoop,10498,comment_0,"Hi Daryn - I'm interested in what is going on here since it may affect Knox. If you could provide a high level description of what the overall requirements for a proxy server integration would be that would be a good way to frame the impact on existing proxies. Of course, I would prefer that changes here not break proxy server (Knox) contracts that are currently working.",non_debt,-
hadoop,10498,comment_2,"I don't think these changes will conflict with Knox. What we found is: # webhdfs doesn't support proxy superusers, ex. oozie, via a http proxy when superuser privs are restricted to a specific set of hosts. The reason is the NN sees the remote ip as that of the http proxy instead of the oozie server. # the audit log for webhdfs via a http proxy always logs the proxy which is not useful... This trio of jiras enables optional configuration of proxy servers that will allow the NN to ""see"" the actual remote client IP for the purposes of superuser proxy checks and for audit logging. Essentially the http proxy becomes invisible to the NN.",non_debt,-
hadoop,10498,comment_3,I see. I presume this invisibility is accomplished by presenting the Referer as the ip downstream?,non_debt,-
hadoop,10498,comment_4,"Hi Darryn, Reviewed the trio of Jiras. Piecing together my understanding: You could now specify a list of IPs as allowed http proxies as property: If the http request comes from one of those listed IPs, you would read ""X-Forwarded-For"" header value as end client IP. Otherwise, the end client IP is, the client IP as implied by HttpServletRequest. None of the other processing rules change. Is this right understanding?",non_debt,-
hadoop,10498,comment_5,Exactly correct.,non_debt,-
hadoop,10498,comment_6,Very good - thanks for the clarifications!,non_debt,-
hadoop,10498,comment_7,"Thanks Daryn for the confirmation. A nice to have. I do not have strong argument for this. The header name could be configurable instead of hard coding to ""X-Forwarded-For"". The default value being ""X-Forwarded-For"".",code_debt,low_quality_code
hadoop,10498,comment_8,"According to the IETF draft, , the header format may change to include information about multiple proxies involved in forwarding. Even if this becomes the standard, proxy servers will most likely still put the defacto ""X-Forwarded-For"" header for compatibility reasons. For this reason, I think it is reasonable to assume ""X-Forwarded-For"" for now. If the proposal becomes a standard, it will be more than just header name change, so making it configurable doesn't seem to add much value. +1 for the patch.",non_debt,-
hadoop,10498,comment_9,Thanks all. I've committed to branch-2 and trunk.,non_debt,-
hadoop,10499,summary,Remove unused parameter from,code_debt,dead_code
hadoop,10499,description,The Configuration parameter is not used in the authorize() function. It can be removed and callers can be updated. Attaching the simple patch which removes the unused _conf_ parameter and updates the callers. The ProxyUsers is defined as a private audience and so there shouldn't be any external callers.,code_debt,dead_code
hadoop,10499,comment_0,Attaching the patch which removes conf from authorize(),non_debt,-
hadoop,10499,comment_1,"+1 for the patch pending Jenkins run. I've clicked the Submit Patch button. I'm going to wait a day before committing in case anyone has further comments about this. Thanks, Benoy!",non_debt,-
hadoop,10499,comment_3,"Although it makes sense to remove an unused parameter, I wonder if the parameter should be honored? It would allow different rpc services to have different proxy superuser configurations as opposed to the current static configuration... It probably made sense when daemons generally had only one rpc service. I don't have a strong opinion because I suppose it could be by another jira.",code_debt,low_quality_code
hadoop,10499,comment_4,", would you please investigate the compilation failure in HDFS {{JspHelper}} and post a new patch? I wonder if part of HADOOP-10448 is intended to support this requirement in a better way. Benoy, can you respond to Daryn's question too? Thanks!",non_debt,-
hadoop,10499,comment_5,Attaching the patch which corrects all the calls to _authorize_ . I did not run a mvn clean and hence the classes which were not changed did not get recompiled. Sorry for that.,non_debt,-
hadoop,10499,comment_6,"I believe , the static nature of the ProxyUser capability should be addressed separately, if there is a requirement for different services to have different ProxyUser capabilities. HADOOP-10448 would make it easier since it will move the state out of ProxyUsers. I am trying to clean up the interface exposed by _ProxyUsers_ in order to work on HADOOP-10448 and that's why this jira.",code_debt,low_quality_code
hadoop,10499,comment_7,"+1 for the new patch, pending Jenkins run. After this passes Jenkins, I'll split this into separate issues/separate patches for HADOOP, HDFS and MAPREDUCE. (We're supposed to track the commits to the different sub-projects separately.)",non_debt,-
hadoop,10499,comment_9,I don't think that the test case failure is related to the changes.,non_debt,-
hadoop,10499,comment_10,I'm attaching a rebased patch to get a Jenkins run.,non_debt,-
hadoop,10499,comment_12,"I committed this to trunk and branch-2. Thank you for contributing the patch, Benoy.",non_debt,-
hadoop,10499,comment_17,"Thank you, Chris.",non_debt,-
hadoop,10499,comment_18,"After this change, HBase has to resort to using reflection so that the build against hadoop-2 passes. Is this change necessary for branch-2 ?",non_debt,-
hadoop,10499,comment_19,"Hi, Sorry for the inconvenience this caused. Is it possible to patch HBase to stop calling the 3-arg form of that was removed? The third argument was unused anyway, so this won't change any functionality in HBase. Additionally, this class is currently annotated {{Private}}, so I expected we didn't need to check for downstream impacts before I committed. That means one of two additional changes needs to happen: # Change HBase to stop calling altogether. I'm not really sure what alternative you would have though, so maybe this isn't feasible. # Change the annotation to {{LimitedPrivate}} for HBase, so devs know to check for downstream impacts on interface-breaking changes in the future. What are your thoughts on this?",code_debt,low_quality_code
hadoop,10499,comment_20,"Option #1 doesn't seem feasible. Option #2 is a good one. That way, developer would check relevant reference in HBase code before finalizing the change.",non_debt,-
hadoop,10499,comment_21,"Since the class is marked as , I assumed that it is not used by other components. I have opened HADOOP-10539 to fix it.",non_debt,-
hadoop,10501,summary,accesses handlers without synchronization,code_debt,multi-thread_correctness
hadoop,10501,description,All the other methods accessing handlers are synchronized methods.,code_debt,multi-thread_correctness
hadoop,10501,comment_0,It's currently only meant for unit tests. Do you have any other use case in mind?,non_debt,-
hadoop,10501,comment_1,I did see VisibleForTesting annotation. Is it expected that tests may see inconsistent Handler array ?,non_debt,-
hadoop,10501,comment_2,"It should be okay as long as it is called after Server#start(). Adding {{synchronized}} won't do much good, but won't hurt either.",design_debt,non-optimal_design
hadoop,10508,summary,RefreshCallQueue fails when authorization is enabled,non_debt,-
hadoop,10508,description,When the callqueue cannot be refreshed with hadoop dfsadmin -refreshCallQueue (protocol not found).,non_debt,-
hadoop,10508,comment_0,Patch registers in the HDFSPolicyProvider so that it can be called when authorization is enabled. Access control is done by setting in the policy.xml file,non_debt,-
hadoop,10508,comment_2,The patch looks good. Can you please provide a unit test for this functionality?,test_debt,lack_of_tests
hadoop,10508,comment_3,"Hi , I uploaded a new patch with a test.",non_debt,-
hadoop,10508,comment_5,+1. I'll commit it shortly.,non_debt,-
hadoop,10508,comment_6,"Actually, , can you move the test from to +1 once it is addressed.",architecture_debt,violation_of_modularity
hadoop,10508,comment_7,is this what you mean? The test is now under the package org.apache.hadoop instead of,non_debt,-
hadoop,10508,comment_8,Yes. It looks good to me. I'll commit it shortly.,non_debt,-
hadoop,10508,comment_9,I've committed the patch to trunk and branch-2. Thanks  for the contribution.,non_debt,-
hadoop,10526,summary,Chance for Stream leakage in CompressorStream,design_debt,non-optimal_design
hadoop,10526,description,"In , finish() can throw IOException . But out will not be closed in that situation since it is not in finally",non_debt,-
hadoop,10526,comment_1,"The patch looks good except a typo: ""Oveerriding"". I changed the target to be 2.5 and trunk. Although this bug is present in other branches, This is where you intend to fix this bug. ""Fix version"" is filled in after check-in.",documentation_debt,low_quality_documentation
hadoop,10526,comment_2,Thanks  for your comments. Submitting a new patch incorporating those comments.,non_debt,-
hadoop,10526,comment_4,I moved it from hdfs to common. +1 The patch looks good.,non_debt,-
hadoop,10526,comment_5,"I've committed this to trunk and branch-2. Thanks for working on the patch, Rushabh.",non_debt,-
hadoop,10528,summary,A TokenKeyProvider for a Centralized Key Manager Server (BEE: bee-key-manager),non_debt,-
hadoop,10528,description,"This is a key provider based on HADOOP-9331. HADOOP-9331 has designed a complete Hadoop crypto codec framework, but the key can only be retrieved from a local Java KeyStore file. To the convenience, we design a Centralized Key Manager Server (BEE: bee-key-manager) and user can use this TokenKeyProvider to retrieve keys from the Centralized Key Manager Server. By the way, to secure the key exchange, we leverage HTTPS + SPNego/SASL to protect the key exchange. To the detail design and usage, please refer to Moreover, there are still much more requests about Hadoop Data Encryption (such as provide standalone module, support KMIP...etc.), if anyone has interested in those features, pleas let us know. Ps. Because this patch based on HADOOP-9331, please use patch HADOOP-9333, and HADOOP-9332 and before use our patch HADOOP-10528.patch.",non_debt,-
hadoop,10528,comment_0,HADOOP-10528.patch,non_debt,-
hadoop,10528,comment_2,"While this seems like interesting work, it also duplicates a number of jiras - all of which are already committed and being collaboratively worked on. I will add them as jiras that this duplicates but here are the ones that come to mind: duplicates KeyProvider API, KeyShell and Alejandro's KMS. KMS KeyShell KeyProvider API Another thing that I noticed is that Key.deriveKeys doesn't seem to be using a salt of any kind in its creation of a key from a password. This is going to end up creating the same key each time - no? I could also use a bit of description about the Token aspect of this provider - this will be good in determining how to fit it into the existing KeyProvider API.",non_debt,-
hadoop,10528,comment_3,central key management service,non_debt,-
hadoop,10528,comment_4,KeyShell is a CLI for key management commands for the KeyProvider API,non_debt,-
hadoop,10528,comment_5,KeyProvider API,non_debt,-
hadoop,10528,comment_6,Correction: the KMS patch is not quite committed yet but it is on its way in. That is HADOOP-10433.,non_debt,-
hadoop,10528,comment_7,"Certainly in the sense that parts of HADOOP-9331 were peeled off in JIRAs that duplicate some of its scope. That seems to have forked work in this area rather than foster healthy community collaboration between those with and without the commit bit. It's disappointing to see this pattern continuing here with this issue dismissed as ""duplicate"".",design_debt,non-optimal_design
hadoop,10528,comment_8,"I regret that you read my comments as dismissive and disappointing. I certainly don't want to come across as dismissive of what looks like a good amount of work. What I was hoping to do is get a sense for what the Token aspect of this provider is and help determine how it fits into the existing KeyProvider API. As for your characterization of that work, it seems to me that a common need was identified across multiple projects. It was started and continues to evolve to meet the needs of its consumers. It would be perfectly reasonable for the needs represented in this jira to inform further evolution in the KeyProvider API and KMS work. Cross cutting concerns such as these types of security efforts are difficult and I can fully appreciate the frustration there.",non_debt,-
hadoop,10547,summary,Give public scope,non_debt,-
hadoop,10547,description,Trying to use in Hive project but the method has protected scope. Please make this a public method if appropriate.,non_debt,-
hadoop,10547,comment_0,"Hey Arpit, according to HIVE-6741 it sounds like folks are just planning to remove Hive's use of the Sasl properties, since Hive only fetches it to log it, this property is overridden anyway. Change it if it will be of use to other callers, but it sounds like Hive will not be dependent on this one. Thanks.",non_debt,-
hadoop,10547,comment_1,Attaching the patch which changes the visibility,non_debt,-
hadoop,10547,comment_3,+1 for the patch. The test failure looks unrelated. I will commit this later today.,non_debt,-
hadoop,10547,comment_4,Thanks for the fix . I committed this to trunk and branch-2. Thanks for the pointer to the Hive Jira. I saw no harm in fixing this anyway.,non_debt,-
hadoop,10602,summary,"Documentation has broken ""Go Back"" hyperlinks.",documentation_debt,low_quality_documentation
hadoop,10602,description,"Multiple pages of our documentation have ""Go Back"" links that are broken, because they point to an incorrect relative path.",documentation_debt,low_quality_documentation
hadoop,10602,comment_0,"I found occurrences of this in docs for multiple sub-projects, at least HDFS and MapReduce, so I'm filing the jira in Common for now.",non_debt,-
hadoop,10602,comment_1,Attaching a patch.,non_debt,-
hadoop,10602,comment_2,"Thanks for taking this, Akira. It looks like you're taking the approach of deleting the links. I think this is fine, because we have all the direct links in the left nav, and the user always has the browser back button too. It streamlines the pages a bit too. Does anyone else out there object to removing the Go Back links? If not, let's go ahead with this approach. To make this change comprehensive, we'll need to update some additional files. Here is what I turned up in a grep of 'Go Back' across the whole repo:",documentation_debt,low_quality_documentation
hadoop,10602,comment_4,"Thanks  for the comment. The approach of the v1 patch was to delete the only ""dead"" links. Now I agree with you. I'll update the patch to remove all the ""Go Back"" links. By the way, removing all the ""Go Back"" links makes not linked from anywhere. I think we can remove the document because is a superset of it.",documentation_debt,low_quality_documentation
hadoop,10602,comment_5,"Attached a patch to remove all the ""Go Back"" links.",non_debt,-
hadoop,10602,comment_6,Updated the patch to remove 'Go Back' link from XAttr document.,non_debt,-
hadoop,10602,comment_7,"+1, pending Jenkins run for the latest patch. I built the site, and it looked good. Thanks, Akira. I'm going to hold off committing until Tuesday, 5/27, just in case anyone objects to removal of the Go Back links.",non_debt,-
hadoop,10602,comment_10,"I committed this to trunk and branch-2. Akira, thank you for the contribution.",non_debt,-
hadoop,10673,summary,Update rpc metrics when the call throws an exception,non_debt,-
hadoop,10673,description,Currently RPC metrics isn't updated when the call throws an exception. We can either update the existing metrics or have a new set of metrics in the case of exception.,non_debt,-
hadoop,10673,comment_0,The patch updates the existing rpc metrics in the case of exception. HDFS unit tests will be updated to cover this.,test_debt,lack_of_tests
hadoop,10673,comment_2,", I think your suggestion of maintaining dedicated metrics for exceptions is useful to measure ""wasted"" calls.",non_debt,-
hadoop,10673,comment_3,"Thanks, Gera. Here is the updated patch. For RpcMetrics, it doesn't distinguish good call from waste call. For RpcDetailedMetrics, if the call throws exception, it will use as the metrics name.",non_debt,-
hadoop,10673,comment_5,Updates with unit test.,non_debt,-
hadoop,10673,comment_7,", thank you for the suggestion. I think this improvement is useful, too. One minor comment: both {{rpcMetrics}} and are not final field, so how about adding null check before accessing them?",code_debt,low_quality_code
hadoop,10673,comment_8,"Thanks, Tsuyoshi. Here is the updated patch that makes the variables final.",non_debt,-
hadoop,10673,comment_10,"Thanks for the updating, Ming. It looks better approach to me. One additional point: if we can make the variables final, we can remove null check in {{Server#stop()}}.",code_debt,low_quality_code
hadoop,10673,comment_11,"Thanks, Tsuyoshi.  asked a good question how the increase of the number of metrics will impact metrics subsystem. The most common one might be StandbyException, which can be thrown regardless of methods. Other exceptions are likely to be specific to certain methods. Based on the current number of metrics, JMX size; number of methods, a very rough estimate can put the increase to around 10-15% for NN. So to be safe, we can aggregate by exception class name instead; that will still provide useful information (from the exception name, we can find the possible methods). Here is the updated patch.",non_debt,-
hadoop,10673,comment_13,The patch looks good to me. +1.,non_debt,-
hadoop,10673,comment_15,"I've committed this to trunk and branch-2. Thanks for the contribution, ! Thanks for the review, !",non_debt,-
hadoop,10681,summary,Remove synchronized blocks from SnappyCodec and ZlibCodec buffering inner loop,non_debt,-
hadoop,10681,description,"The current implementation of SnappyCompressor spends more time within the java loop of copying from the user buffer into the direct buffer allocated to the compressor impl, than the time it takes to compress the buffers. The bottleneck was found to be java monitor code inside SnappyCompressor. The methods are neatly inlined by the JIT into the parent caller which unfortunately does not flatten out the synchronized blocks. The loop does a write of small byte[] buffers (each IFile key+value). I counted approximately 6 monitor enter/exit blocks per k-v pair written.",code_debt,slow_algorithm
hadoop,10681,comment_0,"With sed -i ~ ""s/synchronized //"" snappy/*.java, the cpu fractions look more sane",non_debt,-
hadoop,10681,comment_1,"Added a SnappyCode impl into hive, which should come before in the classpath for hive+tez. Tested out TPC-H Query5, which has a spill-merge on the JOIN. Query times went from 539.8 seconds to 464.89 seconds, mostly from speedup to a single reducer stage.",code_debt,slow_algorithm
hadoop,10681,comment_3,"The patch removes unsafe synchronized blocks from the individual methods in Snappy and Zlib codecs. This synchronization is slow and when used in the most common pattern for CompressionCodec is still thread-unsafe for sharing streams with loops running across multiple threads and would ideally require explicit code { while { 0, buffer.length); } } to get correct stateful behaviour. As code exists today it is not thread-safe and does slow lock-prefix x86_64 instructions. The JNI library below in SnappyCodec.c actually does its own locking mutexes for the actual critical sections within.",design_debt,non-optimal_design
hadoop,10681,comment_4,"Since this bug has caused some alarm among those who looked at it, I will pessimize this a little. The core open loop which needs per-stream sync is (the correct code version) I will make sure only this incorrect loop follows the required fast-path.",non_debt,-
hadoop,10681,comment_5,Add test-case to test synchronization characteristics of the codec pool. Only remove synchronization of functions which require external synchronization anyway - also remove synchronization flags for empty methods.,non_debt,-
hadoop,10681,comment_7,Address findbugs warnings,code_debt,low_quality_code
hadoop,10681,comment_9,"Go back to the initial approach of unsynchronized streams, but retain the multi-threaded test-case. The JNI locks remain in-place.",non_debt,-
hadoop,10681,comment_10,"The synchronized blocks would've made a lot of sense if setInput() or was atomic. Since it only reads part of the data (64kb or so) in for an invocation, the user has never been able to use this with multiple threads safely. To make sure this was never used with threading in something like HBase, I cross-checked & HBase has an unsynchronized improved version of gzip which writes its own header/trailer chunks without synchronization.",requirement_debt,non-functional_requirements_not_fully_satisfied
hadoop,10681,comment_13,"The test failure are unrelated, verified. I just committed this, sorry it took me so long. Thanks !",non_debt,-
hadoop,10681,comment_17,Can we do the same for {{Lz4Compressor}} and I noticed a significant performance overhead using {{Lz4Compressor}} for compared to due to the same problem.,code_debt,slow_algorithm
hadoop,10729,summary,Add tests for PB RPC in case version mismatch of client and server,non_debt,-
hadoop,10729,description,"We have ProtocolInfo specified in protocol interface with version info, but we don't have unit test to verify if/how it works. We should have tests to track this annotation work as expectation.",test_debt,lack_of_tests
hadoop,10729,comment_0,Upload a patch.,non_debt,-
hadoop,10729,comment_5,Moving bugs out of previously closed releases into the next minor release 2.8.0.,non_debt,-
hadoop,10729,comment_7,Hi  can you please rebase the patch? Thanks.,non_debt,-
hadoop,10729,comment_8,Ok. Will do. Thanks Haohui.,non_debt,-
hadoop,10729,comment_9,Update patch to sync to latest trunk.,non_debt,-
hadoop,10729,comment_11,+1. Committing.,non_debt,-
hadoop,10729,comment_12,"I've committed the patch to trunk, branch-2 and branch-2.8. Thanks  for the contribution.",non_debt,-
hadoop,10733,summary,Potential null dereference in,non_debt,-
hadoop,10733,description,"newPassword1 might be null, leading to in Arrays.fill() call. Similar issue for the following call on line 381:",non_debt,-
hadoop,10733,comment_0,"Hi  - good catch - will you provide a patch for this with a test? Otherwise, I can.",non_debt,-
hadoop,10733,comment_1,How about this patch ? TestCredShell passes.,non_debt,-
hadoop,10733,comment_3,LGTM - thanks Ted! +1,non_debt,-
hadoop,10733,comment_4,+1. Patch looks good. Will commit it shortly.,non_debt,-
hadoop,10733,comment_6,"I just committed this to trunk and branch-2. Thanks, Ted!",non_debt,-
hadoop,10743,summary,Problem building hadoop -2.4.0 on FreeBSD 10 (without -Pnative),non_debt,-
hadoop,10743,description,fails to compile with java 1.6 on FreeBSD 10.,non_debt,-
hadoop,10743,comment_0,"I think this should be closed, Hadoop 2.7.1 is in ports and seems to work.",non_debt,-
hadoop,10743,comment_1,2.4 is no longer supported.,non_debt,-
hadoop,10748,summary,HttpServer2 should not load JspServlet,non_debt,-
hadoop,10748,description,Currently HttpServer2 loads the JspServlet by default. It should be removed as JSP support is no longer required.,code_debt,dead_code
hadoop,10748,comment_1,Compare with the webdefault.xml in jetty 6. The patch looks good to me. +1.,non_debt,-
hadoop,10748,comment_2,I've committed the patch to trunk. Thanks  for the review.,non_debt,-
hadoop,10748,comment_3,I've merge the patch to branch-2,non_debt,-
hadoop,10752,summary,Add support for hardware crc on ARM aarch64 architecture,non_debt,-
hadoop,10752,description,"This patch adds support for hardware crc for ARM's new 64 bit architecture. The patch is completely conditionalized on __arch64__ For the moment I have only done the non pipelined version as the hw I have only has 1 crc execute unit. Some initial benchmarks on terasort give sw crc: 107 sec hw crc: 103 sec The performance improvement is quite small, but this is limited by the fact that I am using early stage hw which is not performant. I have also built it on x86 and I think the change is fairly safe for other architectures because post conditionalization the src is identical on other architectures. This is the first patch I have submitted for Hadoop so I would welcome any feedback and help.",code_debt,slow_algorithm
hadoop,10752,comment_2,"Minor nit: I'd really like for this to say that the ifdef is for ARM. At least, I'd never guess that this was ARM without a for a while.",code_debt,low_quality_code
hadoop,10752,comment_3,See,non_debt,-
hadoop,10757,summary,KeyProvider KeyVersion should provide the key name,non_debt,-
hadoop,10757,description,Currently the {{KeyVersion}} does not provide a way to get the key name to do a reverse lookup to get the metadata of the key. For the and the {{UserProvider}} this is not an issue because the key name is encoded in the key version name. This encoding of the key name in the key version name cannot be expected in all KeyProvider implementations. It is common for key management systems to use UUID to refer to specific key materials (KeyVersions in Hadoop parlance).,non_debt,-
hadoop,10757,comment_0,"We could add a {{String getKeyName()}} method to the {{KeyVersion}}. For the implementation we don't need to store the key name, we simply decode it from the key version name on read.",non_debt,-
hadoop,10757,comment_1,The and the {{KMS}} REST API must handle the name explicitly because the {{KeyProvider}} configured in the {{KMS}} may handle it explicitly as well.,non_debt,-
hadoop,10757,comment_2,"There is already a utility function to parse the keyversions. As we've discussed in each of the other jiras where you've brought this up, changing the keyversion to a UID is an anti-goal.",non_debt,-
hadoop,10757,comment_3,"Owen, I not proposing changing the key version name to UUID, but to enable the key version name to be a UUID. This is to enable integration with external key management systems and HSMs.",non_debt,-
hadoop,10757,comment_4,"The use of UUIDs are version name was discussed at length in HADOOP-10433. Also, if you are using KMIP, key materials are identified by a UUID.",non_debt,-
hadoop,10757,comment_5,Uploading initial patch to modify KeyVersion to contain the keyName as well..,non_debt,-
hadoop,10757,comment_7,1,non_debt,-
hadoop,10757,comment_8,Thanks Arun. Committed to trunk.,non_debt,-
hadoop,10760,summary,Helper scirpt: looping tests until it fails,non_debt,-
hadoop,10760,description,"Some tests can fail intermittently because of timing bugs. To reproduce the test failure, it's useful to add script which launches specified test until it fails.",non_debt,-
hadoop,10770,summary,KMS add delegation token support,non_debt,-
hadoop,10770,description,This is a follow up on HADOOP-10769 for KMS itself.,non_debt,-
hadoop,10770,comment_0,This is belongs to a string of patches built on top of each other: * HADOOP-10817 proxyuser logic supporting custom prefix properties * HADOOP-10799 HTTP delegation token built in client/server authentication classes * HADOOP-10800 HttpFS using HADOOP-10799 and removing custom code * HADOOP-10835 HTTP proxyuser logic built in client/server authentication classes * HADOOP-10836 HttpFS using HADOOP-10835 and removing custom code * HADOOP-10770 KMS delegation support using HADOOP-10799 * HADOOP-10698 KMS proxyuser support using HADOOP-10835,code_debt,dead_code
hadoop,10770,comment_1,rebasing patch because of KMS changes.,non_debt,-
hadoop,10770,comment_2,rebasing to trunk,non_debt,-
hadoop,10770,comment_3,rebasing to trunk.,non_debt,-
hadoop,10770,comment_5,"Patch looks pretty good to me, Tucu. Only one little nit - seems like you've got some excessive indentation in +1 otherwise.",code_debt,low_quality_code
hadoop,10770,comment_6,Thanks atm. Uploading patch with corrected indentation. committing momentarily.,non_debt,-
hadoop,10770,comment_7,committed to trunk.,non_debt,-
hadoop,10819,summary,build hadoop on 64 bit linux failure ..,non_debt,-
hadoop,10819,description,"I have insall all pre dependency software for building the hadoop 64bit native lib. like: cmake lzo-devel zlib-devel gcc autoconf automake libtool ncurses-devel openssl-deve and ant. then run : mvn clean package -DskipTests -Pdist,native -Dtar -e get Exception : [INFO] [INFO]  (default) @ hadoop-common   12, 2014 3:08:13  warn : Error injecting: at Method) Method) Caused by: ... 58 more [INFO] [INFO] Reactor Summary: [INFO] [INFO] Apache Hadoop Main SUCCESS [1.126s] [INFO] Apache Hadoop Project POM SUCCESS [0.755s] [INFO] Apache Hadoop Annotations SUCCESS [2.958s] [INFO] Apache Hadoop Assemblies SUCCESS [0.251s] [INFO] Apache Hadoop Project Dist POM SUCCESS [1.844s] [INFO] Apache Hadoop Maven Plugins SUCCESS [2.539s] [INFO] Apache Hadoop MiniKDC SUCCESS [2.329s] [INFO] Apache Hadoop Auth SUCCESS [3.178s] [INFO] Apache Hadoop Auth Examples SUCCESS [1.842s] [INFO] Apache Hadoop Common FAILURE [8.757s] [INFO] Apache Hadoop NFS SKIPPED [INFO] Apache Hadoop Common Project SKIPPED [INFO] Apache Hadoop HDFS SKIPPED [INFO] Apache Hadoop HttpFS SKIPPED [INFO] Apache Hadoop HDFS BookKeeper Journal ............. SKIPPED [INFO] Apache Hadoop HDFS-NFS SKIPPED [INFO] Apache Hadoop HDFS Project SKIPPED [INFO] hadoop-yarn SKIPPED [INFO] hadoop-yarn-api SKIPPED [INFO] hadoop-yarn-common SKIPPED [INFO] hadoop-yarn-server SKIPPED [INFO] SKIPPED [INFO] SKIPPED [INFO] SKIPPED [INFO] ...... SKIPPED [INFO] ................ SKIPPED [INFO] SKIPPED [INFO] hadoop-yarn-client SKIPPED [INFO] SKIPPED [INFO] ......... SKIPPED [INFO] .... SKIPPED [INFO] hadoop-yarn-site SKIPPED [INFO] hadoop-yarn-project SKIPPED [INFO] SKIPPED [INFO] SKIPPED [INFO] SKIPPED [INFO] ................... SKIPPED [INFO] SKIPPED [INFO] SKIPPED [INFO] ................. SKIPPED [INFO] ................ SKIPPED [INFO] Apache Hadoop MapReduce Examples .................. SKIPPED [INFO] hadoop-mapreduce SKIPPED [INFO] Apache Hadoop MapReduce Streaming ................. SKIPPED [INFO] Apache Hadoop Distributed Copy SKIPPED [INFO] Apache Hadoop Archives SKIPPED [INFO] Apache Hadoop Rumen SKIPPED [INFO] Apache Hadoop Gridmix SKIPPED [INFO] Apache Hadoop Data Join SKIPPED [INFO] Apache Hadoop Extras SKIPPED [INFO] Apache Hadoop Pipes SKIPPED [INFO] Apache Hadoop OpenStack support ................... SKIPPED [INFO] Apache Hadoop Client SKIPPED [INFO] Apache Hadoop Mini-Cluster SKIPPED [INFO] Apache Hadoop Scheduler Load Simulator ............ SKIPPED [INFO] Apache Hadoop Tools Dist SKIPPED [INFO] Apache Hadoop Tools SKIPPED [INFO] Apache Hadoop Distribution SKIPPED [INFO] [INFO] BUILD FAILURE [INFO] [INFO] Total time: 27.249s [INFO] Finished at: Sat Jul 12 15:08:13 CST 2014 [INFO] Final Memory: 54M/435M [INFO] [ERROR] Failed to execute goal (default) on project hadoop-common: Execution default of goal failed: A required class was missing while executing [ERROR] [ERROR] realm = plugin[ERROR] strategy = [ERROR] urls[0] = [ERROR] urls[1] = [ERROR] urls[2] = [ERROR] urls[3] = [ERROR] urls[4] = [ERROR] urls[5] = [ERROR] urls[6] = [ERROR] urls[7] = [ERROR] urls[8] = [ERROR] urls[9] = [ERROR] urls[10] = [ERROR] urls[11] = [ERROR] urls[12] = [ERROR] urls[13] = [ERROR] Number of foreign imports: 1 [ERROR] import: Entry[import from realm [ERROR] [ERROR] Failed to execute goal (default) on project hadoop-common: Execution default of goal failed: A required class was missing while executing realm = pluginstrategy = urls[0] = urls[1] = urls[2] = urls[3] = urls[4] = urls[5] = urls[6] = urls[7] = urls[8] = urls[9] = urls[10] = urls[11] = urls[12] = urls[13] = Number of foreign imports: 1 import: Entry[import from realm ClassRealm[project Method) Caused by: Execution default of goal failed: A required class was missing while executing realm = pluginstrategy = urls[0] = urls[1] = urls[2] = urls[3] = urls[4] = urls[5] = urls[6] = urls[7] = urls[8] = urls[9] = urls[10] = urls[11] = urls[12] = urls[13] = Number of foreign imports: 1 import: Entry[import from realm ClassRealm[project ... 19 more Caused by: A required class was missing while executing realm = pluginstrategy = urls[0] = urls[1] = urls[2] = urls[3] = urls[4] = urls[5] = urls[6] = urls[7] = urls[8] = urls[9] = urls[10] = urls[11] = urls[12] = urls[13] = Number of foreign imports: 1 import: Entry[import from realm ClassRealm[project at ... 20 more Caused by: at Method) ... 20 more Caused by: ... 58 more [ERROR] [ERROR] Re-run Maven using the -X switch to enable full debug logging. [ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles: [ERROR] [Help 1] [ERROR] [ERROR] After correcting the problems, you can resume the build with the command [ERROR] mvn <goals",non_debt,-
hadoop,10819,comment_0,it's a maven version problem by the look of things.Try Apache Maven 3.0.5,non_debt,-
hadoop,10819,comment_1,"[suan@localhost conf]$ mvn -version Apache Maven 3.0.5 2013-02-19 21:51:28+0800) Maven home: Java version: 1.7.0_51, vendor: Oracle Corporation Java home: Default locale: zh_CN, platform encoding: UTF-8 OS name: ""linux"", version: arch: ""amd64"", family: ""unix"" [suan@localhost conf]$",non_debt,-
hadoop,10819,comment_2,"Looks like it could maybe be this error, in which case it is a missing build-time dependency in hadoop",build_debt,under-declared_dependencies
hadoop,10819,comment_3,Apache Hadoop can be built from 64bit Linux. Closing this.,non_debt,-
hadoop,10822,summary,Refactor HTTP proxyuser support out of HttpFS into common,non_debt,-
hadoop,10822,description,"HttpFS implements HTTP proxyuser support inline in httpfs code. For HADOOP-10698 we need similar functionality for KMS. Not to duplicate code, we should refactor existing code to common. We should also leverage HADOOP-10817.",code_debt,low_quality_code
hadoop,10822,comment_0,converted HADOOP-10835 and HADOOP-10836 to issues as HADOOP-10836 should be an HDFS issue and subtasks cannot belong to different projects.,non_debt,-
hadoop,10904,summary,Provide Alt to Clear Text Passwords through Cred Provider API,non_debt,-
hadoop,10904,description,This is an umbrella jira to track various child tasks to uptake the credential provider API to enable deployments without storing in clear text.,non_debt,-
hadoop,10904,comment_0,WebAppUtils use of conf.getPassword for SSL passwords,non_debt,-
hadoop,10904,comment_1,HDFS-6790 uptakes credential provider api for ssl passwords through cond.getPassword.,non_debt,-
hadoop,10904,comment_2,", how about following a common pattern such as... All components that load a password (i.e. for SSL stuff) would add a new property (the old one + "".ref"", ie: that points to the credential ID. If the *REF* the property is defined, it takes precedence over the old property, if the *REF* property is not present, we print a deprecation warning and use the old property.",non_debt,-
hadoop,10904,comment_3,Hi  - that was something that was discussed early on in development of the credential provider api in HADOOP-10607 and it was decided that it would be better to just add the getPassword method that basically does the same thing without adding additional properties. If you look at the getPassword method you will see that: 1. if there is no provider.path defined for credential providers then it fallsback to config 2. if there is alias of the provided name then it fallsback to config 3. it is possible to turn off the fallback for deployments where it isn't acceptable to have passwords in config I think the deprecation warning may be interesting as a separate jira. I haven't actually proposed that we deprecate the existing mechanisms - that may need some additional discussion. What do you think?,non_debt,-
hadoop,10904,comment_4,"#2 above was supposed to say ""if there is NO alias of the provided name then it fallsback to config""",non_debt,-
hadoop,10904,comment_5,"got it, sounds good. BTW, the method has a typo.",documentation_debt,low_quality_documentation
hadoop,10904,comment_6,"Ugh - it sure does. Thanks, .",non_debt,-
hadoop,10904,comment_7,"Please fix in this jira or subtask, to not violate URI syntax. Specifically: The ""@"" is RFC defined to separate userinfo and host:port in an authority. I've been meaning to take advantage of the userinfo for a legit purpose (well beyond the scope of this discussion) and an ""abuse"" like this will likely conflict. A more URI-friendly approach is using subschemes:",code_debt,low_quality_code
hadoop,10904,comment_8,"We should fix this for KeyProvider too, since it currently does the same thing with repurposing the authority. Googling for ""subscheme"" didn't turn up much, but there is some precedent for using ""+"" like this, e.g. ""svn+ssh"".",non_debt,-
hadoop,10904,comment_9,Seems reasonable to me. I can't get to this till next week - if that's okay then we can assign a new jira to me.,non_debt,-
hadoop,10904,comment_10,"Daryn, This is a very different use case from ""svn+ssh"" and this reads really badly. furthermore, it doesn't nest right: is a complete mess. What are you trying to accomplish that this makes difficult?",non_debt,-
hadoop,10904,comment_11,"Why not do and where nesting simply adds a path prefix, and the un-nesting bring it back as scheme?",non_debt,-
hadoop,10904,comment_12,"Can this be marked as closed now, given that all sub-tasks are resolved?",non_debt,-
hadoop,10904,comment_13,Hi  - You are right - I've closed it. Thanks!,non_debt,-
hadoop,10915,summary,chukwa-0.5.0 dead links on the main page,documentation_debt,low_quality_documentation
hadoop,10915,description,"I've found 4 dead links on the main page of chukwa component. I think a good idea is generate the html with last build , than all links will be updated automatically with packages",documentation_debt,low_quality_documentation
hadoop,10915,comment_0,"Looks like is the correct URL, and Apache Hadoop site does not seem to have the link to the URL you specified.",non_debt,-
hadoop,10915,comment_1,"Closing this issue as invalid. , please feel feel to reopen this or file a jira in Chukwa project if you disagree.",non_debt,-
hadoop,10930,summary,HarFsInputStream should implement PositionedReadable with thead-safe.,code_debt,multi-thread_correctness
hadoop,10930,description,"definition requires the implementations for its interfaces should be thread-safe. HarFsInputStream doesn't implement these interfaces with tread-safe, this JIRA is to fix this.",requirement_debt,non-functional_requirements_not_fully_satisfied
hadoop,10930,comment_1,"This relates to HDFS-6803, where we are defining the thread safety of the DFS operations. Can you catch up with the discussion there and see what other sync options are needed? Also, if you are going to work on the HarFS, can you look at the new contract tests added in HADOOP-9361 and have a go at subclassing those tests with a set for HarFS? That will catch other issues (e.g it looks like it is only throwing {{IOException}} on out-of-range operations, not the preferred {{EOFException}}. Doing that would get you past the ""no tests"" warning on jenkins, and ensure that HarFS is in sync with current expectations",non_debt,-
hadoop,10930,comment_2,"Thanks I'm catching up the discussion :) The suggestion is very good, after we have a conclusion, I will go back to handle the tests as you said.",non_debt,-
hadoop,10930,comment_4,Moving bugs out of previously closed releases into the next minor release 2.8.0.,non_debt,-
hadoop,10931,summary,"compile error on project ""Apache Hadoop OpenStack support""",non_debt,-
hadoop,10931,description,"use command mvn package -Pdist,native -DskipTests -Dtar to compile hadoop, it have errors as below: [ERROR] cannot access class file for not found",non_debt,-
hadoop,10931,comment_1,this patch is ok?,non_debt,-
hadoop,10931,comment_2,"-1. I'd actually applied this and was about to fix the issue when I realised that adding the dependency to the {{<compile Can you just do this as a {{<test>}} dependency? As nobody but you is seeing it (currently), we can't verify this is valid",non_debt,-
hadoop,10979,summary,Auto-entries in hadoop_usage,non_debt,-
hadoop,10979,description,It would make adding common options to hadoop_usage output easier if some entries were auto-populated. This is similar to what happens in FsShell and other parts of the Java code.,design_debt,non-optimal_design
hadoop,10979,comment_0,In particular: * --config * --daemon options are standardized across all of the subsystems * help * version?,non_debt,-
hadoop,10979,comment_1,"-00: * two new functions, one to generate usage and the other to add content to it * hadoop, mapred, hdfs, and yarn modified to use new functions subcommands options are now automatically sorted. All text is placed into appropriately sized columns based using spaces, not tabs, with a screen with of 80 chars.",non_debt,-
hadoop,10979,comment_2,The new output: hadoop: hdfs: mapred: yarn:,non_debt,-
hadoop,10979,comment_3,-01: * fix new shellcheck errors,non_debt,-
hadoop,10979,comment_5,-02: * correct patch file?,non_debt,-
hadoop,10979,comment_6,+1 lgtm,non_debt,-
hadoop,10979,comment_7,Thanks! Committed to trunk.,non_debt,-
hadoop,11013,summary,"CLASSPATH handling should be consolidated, debuggable",code_debt,low_quality_code
hadoop,11013,description,"As part of HADOOP-9902, java execution across many different shell bits were consolidated down to (effectively) two routines. Prior to calling those two routines, the CLASSPATH is exported. This export should really be getting handled in the exec function and not in the individual shell bits. Additionally, it would be good if there was: so that bash -x would show the content of the classpath or even a '--debug classpath' option that would echo the classpath to the screen prior to java exec to help with debugging.",code_debt,low_quality_code
hadoop,11013,comment_0,I'm in the midst of rewriting slsrun.sh and it's very clear that this needs to happen first. It's only going to get worse with KMS and HTTPFS.,non_debt,-
hadoop,11013,comment_1,a) adds --debug option b) reworks CLASSPATH export to be consistent/safer,code_debt,low_quality_code
hadoop,11013,comment_3,Some sample output:,non_debt,-
hadoop,11013,comment_4,This could have saved me hours of debugging classpath issues.,non_debt,-
hadoop,11013,comment_5,"+1 LGTM, thanks Allen",non_debt,-
hadoop,11013,comment_6,"-01: * Replaced all of the if's with a function * Added more messages that fill in some of the blanks (e.g., when does get added?) * Because I'm never happy, even with my own code",code_debt,low_quality_code
hadoop,11013,comment_7,Some examples: Namenode: ResourceManager: distcp,non_debt,-
hadoop,11013,comment_8,"What's interesting to me in that output is the prevention of the dupe'ing of the HADOOP_CONF_DIR in the CLASSPATH. It could be (easily) argued that HADOOP_CONF_DIR should only be added once, but I'm leaning towards leaving it in there like it currently is as a ""safety"" feature.",non_debt,-
hadoop,11013,comment_10,"Committing this to trunk, given the differences between the patches is minor.",non_debt,-
hadoop,11014,summary,Potential resource leak in due to unclosed stream,code_debt,low_quality_code
hadoop,11014,description,"From : IOException is not among the catch blocks. According to IOException may be thrown from the store() call. In that case, out would be left unclosed. In loadFromPath(): The InputStream should be closed upon return from load()",non_debt,-
hadoop,11014,comment_0,"Good catch, . The close calls should actually be done inside of a finally block. This will likely also be an issue in the credential provider as well.",non_debt,-
hadoop,11014,comment_1,Fixed to use in finally blocks.,non_debt,-
hadoop,11014,comment_2,What about the following code inside loadFromPath() ?,non_debt,-
hadoop,11014,comment_3,"As  mentioned, also has the same issue. Revised a patch to fix it.",non_debt,-
hadoop,11014,comment_4,"Good catch, . I'll update it soon.",non_debt,-
hadoop,11014,comment_5,Updated to close streams in password).,non_debt,-
hadoop,11014,comment_8,"Hi , thank you for the patch. Would you mind import in the beginning of this file and replace with",non_debt,-
hadoop,11014,comment_9,"Hi , thanks for your review. it's fully qualified because both of import",non_debt,-
hadoop,11014,comment_10,", could you take a look?",non_debt,-
hadoop,11014,comment_11,Attaching a new patch to use try-with-resources statement instead of closeStream.,non_debt,-
hadoop,11014,comment_13,"The test failure looks not related. , could you take a look?",non_debt,-
hadoop,11014,comment_14,"+1, the changes look good to me , please commit!",non_debt,-
hadoop,11014,comment_15,"Thank you, Harsh! Sure, committing this shortly.",non_debt,-
hadoop,11014,comment_16,"Committed this to trunk, branch-2, and branch-2.7. Thanks , , and  for your reviews!",non_debt,-
hadoop,11047,summary,tomcat.download.url is mostly ignored by kms,non_debt,-
hadoop,11047,description,"Specifing tomcat.download.url as a maven property only works if the tomcat.version matches the one in the URL. This is different than how it previously worked, resulting in unexpected build breaks.",non_debt,-
hadoop,11047,comment_0,"On my Mac, I build with: I get the following build error: ... which, of course, is true because I'm not giving it 6.0.41.",non_debt,-
hadoop,11047,comment_1,"Looks like it was always this way, but because I was using the same version, it worked.",non_debt,-
hadoop,11063,summary,"KMS cannot deploy on Windows, because class names are too long.",code_debt,low_quality_code
hadoop,11063,description,"Windows has a maximum path length of 260 characters. KMS includes several long class file names. During packaging and creation of the distro, these paths get even longer because of prepending the standard war directory structure and our share/hadoop/etc. structure. The end result is that the final paths are longer than 260 characters, making it impossible to deploy a distro on Windows.",code_debt,low_quality_code
hadoop,11063,comment_0,"I have a patch in progress that splits the build into a hadoop-kms-lib module for the main class files and hadoop-kms for the final built war. With this in place, the long class file names get packaged into inside the war so that we avoid the long path problem without needing to change a lot of class names and do arbitrary refactorings just to satisfy the path length limitation.",non_debt,-
hadoop,11063,comment_1,", no need to split KMS in 2 modules, adding to the configuration of the war plugin in the kms pom would create a KMS JAR and use it within the WAR.",non_debt,-
hadoop,11063,comment_2,", thanks for the tip.",non_debt,-
hadoop,11063,comment_3,"Here is the patch. It turns out what we really want is {{attachClasses}} helps publish the jar as an artifact for use in other projects, but we don't need to expose these classes beyond KMS.",non_debt,-
hadoop,11063,comment_4,"LGTM, +1",non_debt,-
hadoop,11063,comment_6,"No tests are required, because this is a change in packaging only. I committed this to trunk and branch-2. Alejandro, thank you for the code review.",non_debt,-
hadoop,11103,summary,Clean up RemoteException,code_debt,low_quality_code
hadoop,11103,description,"RemoteException has a number of undocumented behaviors * has no javadocs on getClassName. Reading the source, the String returned is the classname of the wrapped remote exception. * String) is equivalent to calling String, null) * Constructors allow null for all arguments * Some of the test code doesn't check for correct error codes to correspond with the wrapped exception type * methods don't document when they might return null",documentation_debt,outdated_documentation
hadoop,11103,comment_0,+1 (non-binding),non_debt,-
hadoop,11103,comment_3,TestFileTruncate passes locally.,non_debt,-
hadoop,11103,comment_4,"updated patch to ensure it works on current trunk, added tests for statements in the javadocs about how unspecified error codes are handled and how unwrapping might fail.",non_debt,-
hadoop,11103,comment_6,"+1, Will commit shortly.",non_debt,-
hadoop,11103,comment_7,Committed to trunk and branch-2 Thanks,non_debt,-
hadoop,11117,summary,UGI HadoopLoginModule doesn't catch & wrap all kerberos-related exceptions,non_debt,-
hadoop,11117,description,"If something is failing with kerberos login, should fail with useful information. But not all exceptions from the inner code are caught and converted to LoginException. Those exceptions that aren't wrapped have their text and stack trace lost somewhere in the javax code, leaving on the text ""login failed"" and a stack trace of no value whatsoever.",code_debt,low_quality_code
hadoop,11117,comment_0,seen during YARN-913 dev,non_debt,-
hadoop,11117,comment_1,stack/message of little or no value,code_debt,low_quality_code
hadoop,11117,comment_2,With better catch and forward,non_debt,-
hadoop,11117,comment_3,another message that gets lost,non_debt,-
hadoop,11117,comment_4,"first pass, ensures that exception messages get retained, and are upconverted.",non_debt,-
hadoop,11117,comment_5,"I really wish we could catch these and not throw a stack trace to figure out why/what/where it broke. From an end user perspective, these stacks are completely unfriendly.",code_debt,low_quality_code
hadoop,11117,comment_7,I agree ... but this is something minimal which can go in with little/no work though it looks like a couple of tests are not picking up the changed text. What we could do long term is replicate some of the stuff we did for the networking errors: point to wiki pages. But how is anyone going to make sense of a ? I don't want to write the wiki page for that.,test_debt,low_coverage
hadoop,11117,comment_8,"Some of the test failures are spurious, the only regression appears to be These tests are failing because the test code is doing an {{assertEquals}} on the expected string; the propagation of the underlying exception message is breaking this comparison. Fix is what could have been done in the first place: use as the probe",code_debt,low_quality_code
hadoop,11117,comment_9,patch -002 which patches the test to make it less brittle to exception strings,code_debt,low_quality_code
hadoop,11117,comment_11,+1 lgtm.,non_debt,-
hadoop,11201,summary,Hadoop Archives should support globs resolving to files,non_debt,-
hadoop,11201,description,Consider the following scenario: Archive 'dir3/file3*': Archiving dir3 (directory) which is equivalent to the above works.,non_debt,-
hadoop,11201,comment_0,v01 with unit tests demonstrating the problem and a proposed fix.,non_debt,-
hadoop,11201,comment_2,"Hi, . This is a good find. Thank you for posting a patch. From reading this code, it's unclear to me why it ever bothered splitting directory paths into the separate {{justDirs}} list. The subsequent logic operates on the parents of the paths. A parent is by definition always going to be a directory. I think we can remove {{justDirs}} entirely and change the inner loop to iterate directly over {{paths}} instead. Do you agree? If any of the other watchers can think of a reason for the {{justDirs}} logic, please comment. I couldn't find anything relevant in revision history.",non_debt,-
hadoop,11201,comment_3,", thank you for taking a look. {{justDirs}} seems to be sort of a misnomer at this point that may have led to this bug, to begin with. It should be {{justPaths}}. The main thing going on there is extracting paths from, potentially absolute, input URI's. I can make this change if we have a consensus about the bug per se.",non_debt,-
hadoop,11201,comment_4,"Ah, now it makes sense. In that case, I think renaming to {{justPaths}} would help clarify the code.",code_debt,low_quality_code
hadoop,11201,comment_5,v02 patch renaming justDirs to justPaths,non_debt,-
hadoop,11201,comment_7,+1 for patch v2. I plan to commit this later today.,non_debt,-
hadoop,11201,comment_8,"I have committed this to trunk and branch-2. Gera, thank you for contributing this patch.",non_debt,-
hadoop,11219,summary,Upgrade to netty 4,non_debt,-
hadoop,11219,description,This is an umbrella jira to track the effort of upgrading to Netty 4.,non_debt,-
hadoop,11219,comment_0,"Please note that the upgrade can be done incrementally which reduces the risks. Currently Hadoop uses netty 3, and netty 3 and netty 4 can coexist. The package of netty 3 and netty 4 are different in netty 3 vs {{io.netty}} in netty 4). They have different artifact ids in the poms. Therefore the upgrade can be done incrementally.",non_debt,-
hadoop,11219,comment_1,"In Tez, we provide a Tez Shuffle Handler that is similar to MR Shuffle Handler with some added feature support. There may be an interaction here that needs to be tested and verified. I'd be happy to verify once there is a patch available.",test_debt,lack_of_tests
hadoop,11219,comment_2,Closing this as duplicate.,non_debt,-
hadoop,11219,comment_3,"Sorry, some modules are still using Netty 3. Reopening.",non_debt,-
hadoop,11219,comment_4,"Netty 3 is EOL and the last Netty 3 was released on 6/2016, more than 3 years old. We can't afford if Netty is found to have security vulnerability one day. Time to prioritize this work.",architecture_debt,using_obsolete_technology
hadoop,11263,summary,NativeS3FileSystem doesn't work with hadoop-client unless jets3t added to classpath,non_debt,-
hadoop,11263,description,"When you start using the NativeS3FileSystem (which is in hadoop-common) based on the hadoop-client set of jars, it fails with a NativeS3FileSystem depends on a library called jets3t, which is not found in the hadoop-client build. It turns out that this library was specifically excluded in the hadoop-client pom.xml: This strikes me as an issue, as a component that's part of hadoop-common cannot run with a hadoop-client build.",non_debt,-
hadoop,11263,comment_0,"It's not in there to keep the dependency graph down; add jets3t and will work All S3 support moves in 2.6+ to the hadoop-aws JAR, along with s3a:; these do explicitly add the dependencies, on the basis that if you pull in that JAR, you want the dependencies, I'm closing this as a wontfix. Sorry",non_debt,-
hadoop,11263,comment_1,"Thanks for the explanation. I missed that things look different on the trunk. Yes, I'm fine with wontfix.",non_debt,-
hadoop,11265,summary,Credential and Key Shell Commands not available on Windows,non_debt,-
hadoop,11265,description,Must add the credential and key commands to the hadoop.cmd file for windows environments.,non_debt,-
hadoop,11265,comment_0,Adds the integration of the KeyShell and CredentialShell classes to hadoop.cmd.,non_debt,-
hadoop,11265,comment_1,Adding initial patch,non_debt,-
hadoop,11265,comment_3,"Thank you for the patch, Larry. I'm uploading the same code you provided, but with line endings converted to CRLF. Let's see if that appeases Jenkins.",non_debt,-
hadoop,11265,comment_5,"Hmmmm,  - I don't get it. Patch seems fine to me.",non_debt,-
hadoop,11265,comment_6,Attempting a 3rd patch. I can't seem to apply it with patch but with git apply -p0 it applies. Has something changed here?,non_debt,-
hadoop,11265,comment_8,Hi  - can you try and apply the patch with: git apply -p0 That works for me - I can't get patch to work the way that I used to do it. Maybe jenkins is using patch?,non_debt,-
hadoop,11265,comment_9,"+1 for the patch. I built a distro locally with the patch and confirmed that I can run the commands on Windows. I'll commit this now. Converting to CRLF has worked for me in the past, so I'm not sure what's wrong. There have been some changes in the Jenkins automation, so I'll have to review that another time.",non_debt,-
hadoop,11265,comment_10,"I have committed this to trunk, branch-2 and branch-2.6. Larry, thank you for the contribution.",non_debt,-
hadoop,11275,summary,TestSSLFactory fails on Java 8,non_debt,-
hadoop,11275,description,Below are a few of the exceptions I got running this test against Java 8:,non_debt,-
hadoop,11275,comment_0,What version did you run with? I think this is fixed in latest trunk with: HADOOP-10847 HADOOP-11230,non_debt,-
hadoop,11275,comment_1,Refreshed workspace and this test passes.,non_debt,-
hadoop,11289,summary,Fix typo in RpcUtil log message,documentation_debt,low_quality_documentation
hadoop,11289,description,"From RpcUtil.java: LOG.info(""Malfromed RPC request from "" +",non_debt,-
hadoop,11289,comment_0,Patch fixes the typo.,documentation_debt,low_quality_documentation
hadoop,11289,comment_2,+1. I'll commit it shortly.,non_debt,-
hadoop,11289,comment_3,I've committed the patch to trunk and branch-2. Thanks  for the contribution.,non_debt,-
hadoop,11289,comment_4,Thank you  for the quick review and commit. I should mention FTR that the patch doesn't need any tests since it is a log message typo fix.,documentation_debt,low_quality_documentation
hadoop,11309,summary,"System class pattern package.Foo should match package.Foo$Bar, too",non_debt,-
hadoop,11309,description,"Currently when job classloader is enabled and the user specifies {{package.Foo}} as a system class explicitly, nested classes are not considered system classes.",non_debt,-
hadoop,11309,comment_0,v01 with proposed fix.,non_debt,-
hadoop,11309,comment_2,This JIRA is a follow-up for MAPREDUCE-6128.,non_debt,-
hadoop,11309,comment_3,"Failures seem unrelated. , please add this to your review queue!",non_debt,-
hadoop,11309,comment_4,", thanks for the patch. It looks good to me. One minor nit: it might be good to enclose the last condition (the one that checks for the nested class) in parentheses for clarity:",code_debt,low_quality_code
hadoop,11309,comment_5,"Thanks, , for review! Clarity is a subjective matter. I find redundant parentheses distracting especially when indentation has already been used to emphasize the grouping of operators as in this case. I leave it to committers if they have a strong preference one way or another.",code_debt,low_quality_code
hadoop,11309,comment_6,+1 lgtm. Committing this.,non_debt,-
hadoop,11309,comment_7,Thanks to Gera for the contribution and to Sangjin for additional review! I committed this to trunk and branch-2.,non_debt,-
hadoop,11313,summary,Adding a document about,non_debt,-
hadoop,11313,description,"is a good tool to check whether native libraries are loaded correctly. We don't have any docs about this, so we should add it to",documentation_debt,low_quality_documentation
hadoop,11313,comment_0,Attaching first patch.,non_debt,-
hadoop,11313,comment_2,"Hi, Tsuyoshi. This is a good idea. Thank you for posting a patch. I have a few recommendations: # Instead of running it through {{hadoop jar}} with the full class name, the shorter way to run this command is {{hadoop checknative}}. I recommend using the shorter form in the documentation. # Related to the above, since it's part of our CLI, I recommend adding documentation to too. Let's include mention of the {{-a}} option. Without {{-a}}, the command only checks for With {{-a}}, the command checks for all the libraries that hadoop-common dynamically links to. # I think it would be more informative for the sample here to show what the output looks like when the native code is found. In particular, it displays the absolute path to each library it finds, which can be very helpful. Here is a sample from one of my dev environments:",documentation_debt,low_quality_documentation
hadoop,11313,comment_3,"Hi Chris, thanks for your comment. Agreeing with your points. I'll update a patch.",non_debt,-
hadoop,11313,comment_4,Updated a patch based on 's suggestion.,non_debt,-
hadoop,11313,comment_6,Fixed the build.,non_debt,-
hadoop,11313,comment_8,"+1 for the patch. I committed this to trunk and branch-2. Tsuyoshi, thank you for contributing this documentation improvement.",documentation_debt,low_quality_documentation
hadoop,11317,summary,Increment SLF4J version to 1.7.10,non_debt,-
hadoop,11317,description,YARN-2875 highllights some problems with SLF4J 1.7.5; needs to be pulled up to 1.7.7;,non_debt,-
hadoop,11317,comment_0,"When this issue was created 1.7.7 was the latest 1.7 branch, but looking at I see no reason not to bump to 1.7.10. Attaching a patch to do just",non_debt,-
hadoop,11317,comment_1,Submitting a patch.,non_debt,-
hadoop,11317,comment_3,Justification: No unit tests were added as this was a version bump to bring in methods missing the log4j SLF4J bridge. It is only to ease the use for those folks shipping log4j through SLF. Manual steps taken were only to ensure that it compiled - unit tests for HDFS would not run on my environment (with or without this patch) for reasons currently unknown - each test run is 8hr+ though so slow progress...,non_debt,-
hadoop,11317,comment_4,1,non_debt,-
hadoop,11317,comment_5,"The current assignee is Steve, so I'll wait for his comment before committing.",non_debt,-
hadoop,11317,comment_6,+1 from me,non_debt,-
hadoop,11317,comment_7,Committed this to trunk and branch-2. Thanks Tim for your contribution and thanks Steve for your review and report.,non_debt,-
hadoop,11355,summary,"When accessing data in HDFS and the key has been deleted, a Null Pointer Exception is shown.",non_debt,-
hadoop,11355,description,When using the KMS with the file based keystore we can see this error when trying to access an encryption zone that got his key deleted:,non_debt,-
hadoop,11355,comment_0,Attaching trivial patch to fix this..,non_debt,-
hadoop,11355,comment_2,"+1 LGTM, small nit for the future, you can use when checking exception text. Will commit shortly, thanks Arun.",code_debt,low_quality_code
hadoop,11355,comment_3,"Committed, thanks again Arun!",non_debt,-
hadoop,11360,summary,GraphiteSink reports data with wrong timestamp,non_debt,-
hadoop,11360,description,"I've tried to use GraphiteSink with metrics2 system, but it looks that timestamp sent to Graphite is refreshed rarely (about every 2 minutes approx.) no mather how small period is set. Here is my configuration: *.period=10 And here is dumped network traffic to graphite-relay.host (only selected lines, every line appears in 10 seconds as period suggests): 0 1418041472 0 1418041472 0 1418041472 0 1418041600 3 1418041600 4 1418041600 2 1418041600 3 1418041600 2 1418041600 2 1418041600 1 1418041600 1 1418041600 0 1418041600 0 1418041600 0 1418041600 0 1418041600 0 1418041600 0 1418041728 0 1418041728 As you can see, AllocatedContainers value is refreshed every 10 seconds, but timestamp is not. It looks that the problem is level above (in classes providing MetricsRecord - because timestamp value is taken from MetricsRecord object provided in argument to putMetrics method in Sink implementation) which implies that every sink will have the same problem. Maybe I misconfigured something?",non_debt,-
hadoop,11360,comment_0,"Thanks for the reporting this JIRA Kamil! This looks like a duplicate of which was fixed in Hadoop 2.6.0. If not, please re-open this JIRA",non_debt,-
hadoop,11379,summary,Fix new findbugs warnings in hadoop-auth*,code_debt,low_quality_code
hadoop,11379,description,"When locally run findbugs 3.0, there are new warnings generated. This Jira aims to address the new warnings in hadoop-auth and",code_debt,low_quality_code
hadoop,11379,comment_0,In this patch I fixed warnings raised by findbugs 3 against hadoop-auth and They're all encoding related warnings.,code_debt,low_quality_code
hadoop,11379,comment_2,1,non_debt,-
hadoop,11379,comment_3,I've committed the patch to trunk and branch-2. Thanks  for the contribution.,non_debt,-
hadoop,11379,comment_5,", feel free to fix findbugs in a single jira. It is not necessary to open individual bugs per module.",non_debt,-
hadoop,11379,comment_6,", sure, I'll consolidate the rest of them into one Jira.",non_debt,-
hadoop,11384,summary,Fix new findbugs warnings in hadoop-openstack,code_debt,low_quality_code
hadoop,11384,description,"When locally run findbugs 3.0, there are new warnings generated. This Jira aims to address the new warnings in hadoop-openstack.",code_debt,low_quality_code
hadoop,11384,comment_0,Duplicate to HADOOP-11381,non_debt,-
hadoop,11409,summary,can stack overflow if default fs misconfigured,non_debt,-
hadoop,11409,description,If the default filesystem is misconfigured such that it doesn't have a scheme then Configuration) will call which in turn calls the former and we loop until the stack explodes.,non_debt,-
hadoop,11409,comment_0,I believe this was a side-effect of the MAPREDUCE-5960 change.  would you mind taking a look?,non_debt,-
hadoop,11409,comment_1,"Jason, I added a regression test and a fix.",non_debt,-
hadoop,11409,comment_3,findbugs report appears unrelated.,non_debt,-
hadoop,11409,comment_4,"Thanks for the patch, Gera! When we throw the exception due to lacking a scheme, it would be very helpful to users to mention the property where the bad URI originated to show them what needs to be changed. The unit test should have an Assert.fail call or something similar after the getFileContext call, otherwise the lack of throwing an exception for a bad fs URI will still pass the test. ""Excpected"" s/b ""Expected""",test_debt,low_coverage
hadoop,11409,comment_5,"Thanks for review, Jason! 002 to accommodate for your comments. In addition I made the UNFsE message in AbstractFileSystem more descriptive.",non_debt,-
hadoop,11409,comment_7,+1 lgtm. Findbug warnings and test failure is unrelated. Committing this.,non_debt,-
hadoop,11409,comment_8,"Thanks, Gera! I committed this to trunk and branch-2.",non_debt,-
hadoop,11416,summary,Move ChunkedArrayList into hadoop-common,non_debt,-
hadoop,11416,description,"Move ChunkedArrayList into hadoop-common so that it can be used by classes in hadoop-common, not just hdfs",non_debt,-
hadoop,11416,comment_0,"No code changes, purely a rename of the file and its test",non_debt,-
hadoop,11416,comment_1,"+1 pending, TY colin",non_debt,-
hadoop,11416,comment_3,"committed, thanks. findbugs warnings are for unrelated code",non_debt,-
hadoop,11421,summary,Add,non_debt,-
hadoop,11421,description,"We should have a drop-in replacement for File#listDir that doesn't hide IOExceptions, and which returns a ChunkedArrayList rather than a single large array.",code_debt,low_quality_code
hadoop,11421,comment_0,"Thanks for working on this Colin. It'll be nice to swap this in where we can, JDK7 does a much better job at exposing filesystem APIs. I wonder if we should really return a ChunkedArrayList here. It only implements a subset of the AbstractList interface, and this is a pretty general-purpose method. For huge dirs, we should probably just be using the DirectoryStream iterator directly. I do see the use of these helper functions for quick-and-dirty listings though. I'd be okay providing variants of these functions that return a ChunkedArrayList, but it seems like the default should just be a normal ArrayList. Couple other things: * Need {{<p/* I read the docs at and it'd be nice to do like the example and unwrap the into an IOException.",code_debt,low_quality_code
hadoop,11421,comment_2,"I think maybe later will become more general-purpose. But you're right; for now, we better use {{ArrayList}}. ok Yeah, that's important... io errors should result in io exceptions. Looks like is a probably in order to conform to the {{Iterator}} interface. I removed the variant that returns a list of File, since I found that the JDK6 file listing interfaces actually returned an array of String, so returning a list of String is compatible-ish.",code_debt,low_quality_code
hadoop,11421,comment_3,"LGTM +1 pending Jenkins, thanks Colin",non_debt,-
hadoop,11421,comment_5,"thanks, Andrew",non_debt,-
hadoop,11437,summary,Remove the version and author information from distcp's README file,non_debt,-
hadoop,11437,description,That text shouldn't be there.,non_debt,-
hadoop,11437,comment_0,"Hello  I did not find author tag, ""DistCp Version 2"" is pressent ,here Version 2 can be removed..Please correct me if I am wrong..",documentation_debt,low_quality_documentation
hadoop,11437,comment_1,Look at the very bottom of the readme: That should be removed.,documentation_debt,outdated_documentation
hadoop,11437,comment_2,"We should also remove all the ""DistCp V2"" stuff and just make it distcp since there is no distcp v1 anymore...",non_debt,-
hadoop,11437,comment_3,Hi  Removed from readme and documentation guide..,documentation_debt,outdated_documentation
hadoop,11437,comment_5,+1 committed to trunk. Thanks!,non_debt,-
hadoop,11487,summary,FileNotFound on distcp to s3n/s3a due to creation inconsistency,non_debt,-
hadoop,11487,description,"I'm trying to copy a large amount of files from HDFS to S3 via distcp and I'm getting the following exception: However, when I try hadoop fs -ls the file is there. So probably due to Amazon's S3 eventual consistency the job failure. In my opinion, in order to fix this problem must use fs.s3.maxRetries property in order to avoid failures like this.",non_debt,-
hadoop,11487,comment_0,"# Which version of hadoop? # Which S3 zone? Only US-east lacks create consistency Blobstores are the bane of our lives. They aren't real code around it needs to recognise this and act on it, though as they all have standard expectations of files and their metadata, that's not easy It's not enough to retry on FS status as there are other inconsistencies: directory renames and deletes, blob updates, etc. There's a new FS client, s3a, in hadoop 2.6 which is where all future fs/s3 work is going on. Try it to see if it is any better, though I doubt it. If we were to fix it, the route would be to go with something derived off NetFlix S3mper. Retrying on a 404 is not sufficient.",non_debt,-
hadoop,11487,comment_1,"Looking at this stack trace a bit more, it's actually the logic where the mapper is trying to propagate use & group, ACL & extended permissions. None of which s3 has. once HADOOP-9565 is in, this bit of distcp could be checking to see if there is create/update consistency at the destination, and if not, spinning a bit on failure. This would have it handle the problem of no-entry-to-update against s3 or other FS, without having to hide polling & waiting in the s3 clients themselves.",non_debt,-
hadoop,11487,comment_2,"Nice find (unfortunately for you). I think the most likely culprit is which uses rename(): another ""commiting by renaming"" operation identified which we might be able to resolve in HADOOP-9565.",non_debt,-
hadoop,11487,comment_3,"marking as a dependency on HADOOP-9565 and changing JIRA to problem, not (initial) proposed solution",non_debt,-
hadoop,11487,comment_4,Is it possible DRILL-3546 is similar?,non_debt,-
hadoop,11487,comment_5,"no, that looks like creation inconsistency, which is an AWS architecture issue bq. Amazon S3 buckets in the US Standard Region only provide read-after-write consistency when accessed through the Northern Virginia endpoint",non_debt,-
hadoop,11487,comment_6,Are you sure? It's a read only op from the same region (eu-central),non_debt,-
hadoop,11487,comment_7,"eu-central is consistent, so its not create inconsistency. But s3 everywhere lags in listing consistency: if you GET a file it should be there, but if you list the pattern, it may not",non_debt,-
hadoop,11487,comment_8,"I just attached a patch to HADOOP-13145 to prevent this {{getFileInfo}} call when DistCp is run without the {{-p}} option. I suspect that patch can help us get past this problem with eventual consistency on DistCp to a destination on S3A, at least when DistCp is run without the {{-p}} option. I filed that patch on a new JIRA instead of here, because the discussion here indicates that it may expand to a larger scope for addressing a wider set of eventual consistency concerns. HADOOP-13145 is more of a spot performance enhancement.",non_debt,-
hadoop,11487,comment_9,"Hi, We are processing data on US west and still seeing consistency issue. As per forums US west should not be having consistency issue but we are doing update of a table. Not sure if 'read-after-write' consistency will take care of 'read-after-update' consistency also. Will 9565 help us here. Below is the back trace of the issue we are seeing when we write some tables in parquet format from Apache Spark to S3n.",non_debt,-
hadoop,11487,comment_10,"Hello . The stack trace indicates a problem during a call. The listing calls against S3 are subject to eventual consistency. The goals of the S3Guard project, tracked in issue HADOOP-13345, would help address this scenario. However, please note that this effort is targeted to the S3A file system, which is where our ongoing development effort on Hadoop S3 integration is happening. (Your stack trace indicates you are currently using S3N.)",non_debt,-
hadoop,11487,comment_11,"Hi Chris, Thanks for replying. As per AWS forum all of the S3 regions now support read-after-write consistency for new objects added to Amazon s3. Does listStatus falls outside above consistency ? For Hadoop 2.7 we started using s3a as per spark recommendations but but after moving to s3a we started using 3x degradation, hence moved backed to s3n. When will be the patch available for general use ?",non_debt,-
hadoop,11487,comment_12,"Yes, it does. maps to an operation listing the keys in an S3 bucket. For that listing operation, the consistency model you quoted does not apply. Instead, it follows an eventual consistency model. There may be propagation delays between creating a key and that key becoming visible in listings. There are more details on this behavior in the AWS S3 consistency model doc:",non_debt,-
hadoop,11487,comment_13,"Cool thanks, will take a look. Is patch for HADOOP-13345 available for general use ?",non_debt,-
hadoop,11487,comment_14,"No, that's just a prototype right now. It's still under development. I can't recommend running it.",non_debt,-
hadoop,11487,comment_15,"Np, will start watching it. Thanks for the help.",non_debt,-
hadoop,11487,comment_16,"Linking to HADOOP-13950, as it may be a lower cost solution. AWS caches negative GETs on a path, and FS.create() does an existence check as part of its getFileStatus call, which is always done to look for the dest being a directory. if overwrite=true, we don't care if a dir exists, so only need the 2 directory probes, not the direct path HEAD, so will skip the possibility of a -ve HEAD result being cached. This *may* be all that is needed",non_debt,-
hadoop,11487,comment_17,"The specific codepath here is addressed by HADOOP-13145; other inconsistency issues are different and addresssed elsewhere (HADOOP-13345, HADOOP-13786). Closing as fixed",non_debt,-
hadoop,11523,summary,"StorageException complaining "" no lease ID"" when updating in WASB",non_debt,-
hadoop,11523,description,"In current WASB (Windows Azure Storage - Blob) implementation, when rename operation succeeds, WASB will update the parent folder's ""last modified time"" property. By default we do not acquire lease on this folder when updating its property and simply pass ""null"" to it. In HBase scenario, when doing distributed log splitting, there might be a case that multiple processes from different region servers will access the same folder, and randomly we will see this exception in regionserver's log, which makes log splitting fail. So we should acquire the lease when updating the folder property rather than pass ""null"" to it.",design_debt,non-optimal_design
hadoop,11523,comment_0,Could you take a look?,non_debt,-
hadoop,11523,comment_1,Good catch Duo. Thanks for fixing this.,non_debt,-
hadoop,11523,comment_3,"Hi Duo. Thank you for the patch. The current patch would acquire the lease for all {{rename}} operations, covering both block blobs and page blobs. During initial development of atomic rename, we were careful to limit the scope to only page blobs (typically used by HBase logs). Existing applications using block blobs might not be expecting the leases, so I think we need to make sure the lease acquisition only happens after checking In the error handling, if an exception is thrown from the try block, and then another exception is also thrown from freeing the lease, then this would drop the original exception and throw the exception from freeing the lease. I suspect in general the main exception from the try block is going to be more interesting for root cause analysis, so I recommend throwing that one and just logging the one from freeing the lease, like so: In addition to the unit tests, I ran the test suite against a live Azure storage account and confirmed that everything passed. The release audit warning from Jenkins is unrelated to this patch.",design_debt,non-optimal_design
hadoop,11523,comment_4,"Hi,  Current HBase clusters in Azure, our configuration is In the exception above, WASB tried to update folder/blob This is apparently not a page blob. So lease acquisition is needed in block blob, at least for HBase. For the error handling, agree and I have attached a new patch.",non_debt,-
hadoop,11523,comment_5,"Sorry, I mixed up 2 different concepts: page blobs and atomic rename. If a path is configured for atomic rename, that doesn't necessarily imply that it's a page blob. What is your setting for I suspect it covers the path that you mentioned. This is why I suggested a check on",non_debt,-
hadoop,11523,comment_7,"I looked at it will check if the key is in the set of atomicRenameDirs. And from I saw it will add hbase root dir into it. This means all HBase folders are eligible for atomic rename, right? For in our PROD clusters, it is not set. So atomicRenameDirs only contains ""/hbase"". In this way, I am OK to add it, so that lease acquisition e will only apply to folders under /hbase. I have updated the patch.",non_debt,-
hadoop,11523,comment_9,"Thanks, . I have just 2 more minor nitpicks. I suggest changing this: to this: This way, we'll log the full stack trace on lease free errors and have more information for troubleshooting. This is just a minor style point, but for the following, the keyword should be inline with the closing brace. Instead of this: We do this: Instead of this: We do this:",design_debt,non-optimal_design
hadoop,11523,comment_10,Thanks for pointing these out. I have updated the patch.,non_debt,-
hadoop,11523,comment_11,+1 for patch v4 pending a fresh Jenkins run.,non_debt,-
hadoop,11523,comment_13,"I have committed this to trunk and branch-2. Duo, thank you for the contribution.",non_debt,-
hadoop,11524,summary,throws a shellcheck warning,non_debt,-
hadoop,11524,description,"We should probably use a local var here and return it or something, even though CLASS is technically a global.",non_debt,-
hadoop,11524,comment_0,This got by me in my last code review. I'll take this and clean it up.,non_debt,-
hadoop,11524,comment_1,Not a big deal. I should spend some time trying to figure out how to add shellcheck to the test-patch bits.,non_debt,-
hadoop,11524,comment_2,"This patch avoids the warning by asking the caller to pass in the output variable name to receive the class and then eval'ing it. I tested it through the {{hadoop}}, {{hdfs}}, {{yarn}} and {{mapred}} entry points. , how does this look?",non_debt,-
hadoop,11524,comment_3,+1 lgtm. Probably a better API this way too.,non_debt,-
hadoop,11524,comment_5,"I committed this to trunk. The test warnings are unrelated to any changes in the shell code. Allen, thank you for the code review.",non_debt,-
hadoop,11544,summary,Remove unused configuration keys for tracing,code_debt,low_quality_code
hadoop,11544,description,are no longer used.,code_debt,low_quality_code
hadoop,11544,comment_1,"The patch looks good to me. One comment: looks noop, so could you remove the parameter from core-site.xml and update the document for tracing also?",documentation_debt,outdated_documentation
hadoop,11544,comment_2,Thanks for the comment . I removed the entry of from core-default.xml because the default (NeverSampler) is defined in I would like to left the description in Tracing.apt.vm because users still can use for setting trace sampler though the config key should be specified as + rather than in java code.,code_debt,low_quality_code
hadoop,11544,comment_4,"Thanks  for updating the patch. Make sense, +1.",non_debt,-
hadoop,11544,comment_5,Committed this to trunk and branch-2. Thanks Masatake for the contribution!,non_debt,-
hadoop,11544,comment_9,"Thanks, !",non_debt,-
hadoop,11585,summary,Fix formatting in Tracing.md,documentation_debt,low_quality_documentation
hadoop,11585,description,Sampler subsection needs newline.,non_debt,-
hadoop,11585,comment_0,"* fixed broken ""Samplers"" section title * added newlines for ease of editing.",documentation_debt,low_quality_documentation
hadoop,11585,comment_2,+1 committed to trunk. Thanks!,non_debt,-
hadoop,11607,summary,Reduce log spew in S3AFileSystem,code_debt,low_quality_code
hadoop,11607,description,"{{S3AFileSystem}} generates INFO level logs in {{open}} and {{rename}}, which are not necessary.",code_debt,low_quality_code
hadoop,11607,comment_0,Change several INFO level log messages to DEBUG. No test is added since the change is trivial.,non_debt,-
hadoop,11607,comment_1,"+1 Note that as s3a uses SLF4J for its log API, it can switch to {{log.info(""item {}"", value)}} for terser/more efficient logging logic (we still recommend wrapping debug statements though, just to skip string creation. If you switch to that mode, I'd be even happier. But I'm OK with what you've done so far",code_debt,low_quality_code
hadoop,11607,comment_2,"Thanks much for reviewing. Sure, I'd love to change it. I will post a new patch tomorrow.",non_debt,-
hadoop,11607,comment_3,Updated the patch to use SLF4J formating. Would you mind to take another look?,non_debt,-
hadoop,11607,comment_5,1,non_debt,-
hadoop,11607,comment_6,Thank you so much for the review and +1. Could you help us to commit this as well?,non_debt,-
hadoop,11607,comment_7,"-sorry, thought you were a committer. Done!",non_debt,-
hadoop,11607,comment_9,Thanks a lot for committing this. Hopefully I can commit sometime soon :),non_debt,-
hadoop,11658,summary,Externalize property,non_debt,-
hadoop,11658,description,"A minor code refactoring, externalizing as configuration key.",non_debt,-
hadoop,11658,comment_0,Uploaded a minor patch for the refactoring.,non_debt,-
hadoop,11658,comment_2,Thanks  for the report and the patch! Two comments: * Can we use in * Some inserted lines are longer than 80 characters. Would you render these?,code_debt,low_quality_code
hadoop,11658,comment_3,Updated the patch according to review.,non_debt,-
hadoop,11658,comment_4,Thanks  for your review and good comments. The patch is updated accordingly.,non_debt,-
hadoop,11658,comment_6,"LGTM, +1.",non_debt,-
hadoop,11658,comment_7,Committed this to trunk and branch-2. Thanks  for the contribution!,non_debt,-
hadoop,11677,summary,Add cookie flags for logs and static contexts,non_debt,-
hadoop,11677,description,"In HTTPServer2.java for the default context the secure attributes are set. But when the contexts are created for /logs and /static, new contexts are created and the session handler is assigned as null. Here also the secure attributes needs to be set. Is it not done intentionally ? please give your thought Background trying to add login action for HTTP pages. After this when security test tool is used, it reports error for these 2 urls (/logs and /static).",non_debt,-
hadoop,11677,comment_0,attaching the patch with the change. Please review if the change make sense.,non_debt,-
hadoop,11677,comment_1,Please review the patch,non_debt,-
hadoop,11677,comment_3,Added missing import. Could not find any test class for HTTPServer2. I will try to add tests for this class,test_debt,lack_of_tests
hadoop,11677,comment_5,I think it is a bug as the spnego filter might set cookies for both the logs and static context. +1. Committing.,non_debt,-
hadoop,11677,comment_6,I've committed the patch to trunk and branch-2. Thanks  for the contribution.,non_debt,-
hadoop,11690,summary,Groups vs.,non_debt,-
hadoop,11690,description,There appears to be an opportunity of code reduction by re-implementing Groups to use the,code_debt,duplicated_code
hadoop,11690,comment_0,shall I take up this jira and what exactly you are lokking for..? thanks,non_debt,-
hadoop,11720,summary,[JDK8] Fix javadoc errors caused by incorrect or illegal tags in hadoop-tools,documentation_debt,low_quality_documentation
hadoop,11720,description,"""mvn package -Pdist -DskipTests"" fails with JDK8, caused by incorrect or illegal tags in doc comments.",non_debt,-
hadoop,11720,comment_0,Attaching a patch.,non_debt,-
hadoop,11720,comment_2,"+1, warnings by findbugs is not related to the patch since it's just includes fixes about javadoc. Committing this shortly.",non_debt,-
hadoop,11720,comment_3,"Committed this to trunk, branch-2, branch-2.7. Thanks Akira for your contribution.",non_debt,-
hadoop,11720,comment_6,"Thank you, Tsuyoshi!",non_debt,-
hadoop,11730,summary,Regression: s3n read failure recovery broken,non_debt,-
hadoop,11730,description,"s3n attempts to read again when it encounters IOException during read. But the current logic does not reopen the connection, thus, it ends up with no-op, and committing the wrong(truncated) output. Here's a stack trace as an example. It seems this is a regression, which was introduced by the following optimizations. Also, test cases should be reviewed so that it covers this scenario.",test_debt,low_coverage
hadoop,11730,comment_0,"The first proposal without the test case. 2015-03-20 12:05:08,473 [TezChild] INFO - Received IOException while reading attempting to reopen. 2015-03-20 12:05:08,473 [TezChild] DEBUG - Retrieving All information for bucket shared and object Verified manually that it reopens a new connection after IOException.",non_debt,-
hadoop,11730,comment_1,The first patch with the updated test case.,non_debt,-
hadoop,11730,comment_3,"+1 committing Here's the patch in sync with trunk; it also incorporates HADOOP-11851 in the close logic, as they go hand in hand. We can't have the recovery process damaged by ConnectionReset exceptions being picked up while it closes the old stream.",non_debt,-
hadoop,11730,comment_4,"patch applied; tested against s3 EU. Given the nature of these problems, it may be good to start thinking about whether we can do things with better simulate failures; the test here is a good start, though we may want more complex policies...mockito might be the tool to reach for.",test_debt,low_coverage
hadoop,11730,comment_13,Pulled this into 2.6.1 after  verified that the patch applies cleanly. Ran compilation and before the push.,non_debt,-
hadoop,11740,summary,Combine erasure encoder and decoder interfaces,non_debt,-
hadoop,11740,description,Rationale [discussed | under HDFS-7337.,non_debt,-
hadoop,11740,comment_0,"This initial patch simply removes {{ErasureEncoder}} and {{ErasureDecoder}}. I think the following further simplifications are possible: # We can get rid of {{ErasureCoder}} since it has a single subclass now # Similarly, maybe we can get rid of since provides enough abstraction anyway # If {{ECBlockGroup}} can provide erased indices, we can further combine encoding and decoding classes",code_debt,low_quality_code
hadoop,11740,comment_1,"Thanks  for the good thoughts. Yes it's often a good question to think about either interface or abstract class in Java. My feeling is that if it's in a framework, subject to be pluggable and implemented by customers, an interface would be good to have. So I guess we could keep {{ErasureCoder}} interface, and convert interface to a class. I'm not sure, as erased indices have to be be computed according to input blocks and output blocks just for decoders, and encoders don't have the related logics. Currently in RS coder the decoding is rather simple but I believe it will be much complicated for codes like LRC and Hitchhiker, so separating encoding class and decoding class is desired.",code_debt,low_quality_code
hadoop,11740,comment_2,", How about letting this issue just do what it tells to do ? For other considerations we can further discuss offline and then handle them separately if necessary.",non_debt,-
hadoop,11740,comment_3,I agree. The current patch does this actually.,non_debt,-
hadoop,11740,comment_4,"Great. I will review it late today, if looks ok I will commit it. Thanks.",non_debt,-
hadoop,11740,comment_5,"I had reviewed the codes and had discussed with  offline. Zhe, would you update the patch ? Or please let me know if anything I can help.",non_debt,-
hadoop,11740,comment_6,Thanks Kai for the review! The updated patch addresses the issue in the test code. I also made a pass of the and removed unnecessary Javadoc,documentation_debt,outdated_documentation
hadoop,11740,comment_7,"Thanks for the update. I looked the new patch, just two minor comments: 1. In the test codes, may be better to use {{ErasureCoder}} instead of or since the interface type is good enough, which is why we're here. With this refining, from caller's point of view, nothing different from between encoder and decoder, so it should use the common interface. 2. Those unnecessary Javadoc are there to conform Javadoc conventions and format. In future someone may fill them. I suggest we don't remove them, you can find so many in the project.",code_debt,low_quality_code
hadoop,11740,comment_8,"Thanks Kai for the review! I read the test classes again and figured out more about the structure. Let me know if it looks OK now. Regarding Javadoc I believe we shouldn't have empty statements when merging to trunk. If a parameter or return value is self-descriptory, I think it's better not to add a Javadoc than adding an empty one. But let's discuss that separately since it's not in the scope of this JIRA.",documentation_debt,low_quality_documentation
hadoop,11740,comment_9,"Thanks Zhe for the update. The new patch looks great. +1 For the Javadoc question, let's find chances to discuss it separately.",non_debt,-
hadoop,11740,comment_10,Thanks Kai! I just committed the patch.,non_debt,-
hadoop,11750,summary,distcp fails if we copy data from swift to secure HDFS,non_debt,-
hadoop,11750,description,ERROR tools.DistCp: Exception encountered babynames.main Caused by: babynames.main ... 17 more,non_debt,-
hadoop,11750,comment_0,DistCp is for hdfs: to hdfs: copies only; you will have to use {{dfs -cp}} instead. Closing as invalid,non_debt,-
hadoop,11750,comment_1,"Hi thank you for reviewing this issue. If we use ""dfs -cp"", it is a single ""JVM"" serial copy, right? What if users want to copy 10TB data from swift to HDFS? Serial copy is impractical. IMHO, DistCp is a tool that can help people copy data across different filesystems in parallel but not limited to HDFS. Our team is working on resolving this problem, please at least leave one or two days for further discussion before close it directly.",non_debt,-
hadoop,11786,summary,Fix Javadoc typos in,documentation_debt,low_quality_documentation
hadoop,11786,description,"/** * Resets all statistics to 0. * * In order to reset, we add up all the thread-local statistics data, and * set rootData to the negative of that. * * This may seem like a counterintuitive way to reset the statsitics. Why * can't we just zero out all the thread-local data? Well, thread-local * data can only be modified by the thread that owns it. If we tried to * modify the thread-local data from this thread, our modification might get * interleaved with a read-modify-write operation done by the thread that * owns the data. That would result in our update getting lost. * * The approach used here avoids this problem because it only ever reads * (not writes) the thread-local data. Both reads and writes to rootData * are done under the lock, so we're free to modify rootData from any thread * that holds the lock. */ etc.",non_debt,-
hadoop,11786,comment_0,I have a patch for this class. Could you please check it? (Most of the supplied tag descriptions was copied from subclasses),non_debt,-
hadoop,11786,comment_1,"Thank you for the patch, . I will review it this week.",non_debt,-
hadoop,11786,comment_3,Could you please review my patch? Thanks in advance.,non_debt,-
hadoop,11786,comment_4,"+1, I will commit this shortly.",non_debt,-
hadoop,11786,comment_5,Thank you for the contribution. I have committed this to trunk and branch-2.,non_debt,-
hadoop,11786,comment_6,"Thank you for the work,  and",non_debt,-
hadoop,11809,summary,"Building hadoop on windows 64 bit, windows 7.1 SDK : does not exist",non_debt,-
hadoop,11809,description,"I am trying to build hadoop 2.6.0 on Windows 7 64 bit, Windows 7.1 SDK. I have gone through Build.txt file and have did follow all the pre-requisites for build on windows. Still when I try to build, I am getting following error: Maven command: mvn package -X -Pdist -Pdocs -Psrc -Dtar -DskipTests -Pnative-win findbugs:findbugs [INFO] BUILD FAILURE [INFO] [INFO] Total time: 04:35 min [INFO] Finished at: [INFO] Final Memory: 123M/1435M [INFO] [ERROR] Failed to execute goal run (site) on project hadoop-common: An Ant BuildException has occured: input fi le xml does not exist [ERROR] around Ant part ...<xslt xsl"" [ERROR] Failed to execute goal o (site) on project hadoop-com mon: An Ant BuildException has occured: input file does not exist around Ant part ...<xslt out at .java:216) at .java:153) at .java:145) eStarter.java:128) Method) at java:62) at sorImpl.java:43) cher.java:289) at a:229) at uncher.java:415) at 356) Caused by: An Ant BuildException has occured: input file does not exist around Ant part ...<xslt out at ) .java:208) ... 20 more Caused by: input file does not exist at a:1243) at 5) Source) at sorImpl.java:43) a:106) ) ... 22 more [ERROR] [ERROR] [ERROR] For more information about the errors and possible solutions, please rea d the following articles: [ERROR] [Help 1] xception [ERROR] [ERROR] After correcting the problems, you can resume the build with the command [ERROR] mvn <goals> -rf :hadoop-common",non_debt,-
hadoop,11809,comment_0,Hi please use the dev mailing list for questions. Resolving as Invalid.,non_debt,-
hadoop,11835,summary,"hadoop version command outputs ""Subversion Unknown -r Unknown""",non_debt,-
hadoop,11835,description,"Built Hadoop 2.7.0-RC0 from source and executed ""hadoop version"". Subversion is no longer used for Hadoop, so the line should be removed or replaced by Git.",non_debt,-
hadoop,11835,comment_0,"This patch is for branch-2, branch-2.7.0 and branch-2.7 Trunk already has solution for this issue in JIRA HADOOP-11041 Solution is same "" + getUrl() + "" -r "" + getRevision()); is changed to: code repository: "" + getUrl() + "" -r "" + getRevision());",non_debt,-
hadoop,11835,comment_1,"Thanks  for pointing HADOOP-11041. Looking into the issue, it's not safe to change the output format of the hadoop command. Therefore I'd like to close this issue as invalid. Sorry for confusion.",non_debt,-
hadoop,11837,summary,should destroy in Tomcat deployments,non_debt,-
hadoop,11837,description,creates if the filter is initialized through Tomcat. The needs to be properly destroyed.,non_debt,-
hadoop,11837,comment_1,Looks good. The code needs to add a flag to indicate that the provider has created the filter so that the provider won't be destroyed by both the filter and httpserver2.,non_debt,-
hadoop,11837,comment_2,Thanks  for the quick review. Not sure I understand your comment. HttpServer2 creates a new instance of the signer provider using and will destroy that object. instance is not exposed to others. Why would you need a flag to protect the private instance in the destroy method.,non_debt,-
hadoop,11837,comment_3,There are two use cases here: * HttpServer2 creates and owns the meaning that the will be destroyed by the HttpServer2. should not call {{destroy()}}. * The Tomcat server initializes through This is the use case of Oozie. In this case the should call the {{destroy()}} method of the when its life cycle ends.,non_debt,-
hadoop,11837,comment_4,LGTM. Minor nits: Change to +1 once addressed.,code_debt,low_quality_code
hadoop,11837,comment_7,I'll commit it shortly.,non_debt,-
hadoop,11837,comment_8,"I've committed the patch to trunk, branch-2 and branch-2.7. Thanks  for the contribution.",non_debt,-
hadoop,11844,summary,SLS docs point to invalid rumen link,documentation_debt,low_quality_documentation
hadoop,11844,description,at least on 2.6.0 points to an invalid link to rumen. Need to verify and potentially fix this link in newer releases.,documentation_debt,low_quality_documentation
hadoop,11844,comment_0,Thanks  for raising this issue. Will soon update the patch.,non_debt,-
hadoop,11844,comment_1,Attached an initial patch to fix the rumen link. Please review.,non_debt,-
hadoop,11844,comment_3,Patch has to be applied on Hadoop-2.6.0 Test report failed because it is applied on trunk.,non_debt,-
hadoop,11844,comment_4,"Name the patch appropriately and it will switch to branch-2. Also, is this broken in trunk as well? We need to check.",non_debt,-
hadoop,11844,comment_5,"Thanks  for clarifying. I have updated the patch name . Verified in trunk ,it is not been broken. Please review.",non_debt,-
hadoop,11844,comment_7,Updated the patch file,non_debt,-
hadoop,11844,comment_9,", I think you are creating the patch on the branch-2.6. due to which its not applying on branch-2. In the later versions all .apt files are converted to markdown (.md) files. And the Exactly this issue has been fixed in 2.7.0 through HADOOP-11558. , Do you need this in branch-2.6 as well ? in case any more releases from branch-2.6",non_debt,-
hadoop,11844,comment_10,"Hi , would you please update the patch as Vinayakumar suggested?",non_debt,-
hadoop,11844,comment_11,Please ignore the before comment.,non_debt,-
hadoop,11844,comment_12,"Hi , thank you for taking this issue. 1. Would you use instead of ? The latter style is not supported by apt format. 2. When you upload the patch, please name it appropriately (e.g.",non_debt,-
hadoop,11844,comment_13,Thanks  and  for reviewing. I have updated the patch. Please review.,non_debt,-
hadoop,11844,comment_14,Thanks  for updating the patch. You can use relative link in the document such as,non_debt,-
hadoop,11844,comment_15,Thanks  for correcting me. I have attached the updated patch. Please review.,non_debt,-
hadoop,11844,comment_17,", again the patch is been applied on trunk instead on hadoop2.6 branch. Let me know if i have done any mistake in naming the patch file.",non_debt,-
hadoop,11844,comment_18,"Nah. If this has been fixed in trunk, I'm good. Unfortunately, test-patch doesn't know about minor releases. :( But let's go ahead and close this as won't fix since it is correct in newer branches. Thanks for the work on this!",non_debt,-
hadoop,11844,comment_19,"Already fixed in 2.7+, so closing as won't fix.",non_debt,-
hadoop,11862,summary,Add support key replicas mechanism for KMS HA,non_debt,-
hadoop,11862,description,"The patch only supports specification of multiple hostnames in the kms key provider uri. it means that it support config as: but HA is still not available, if one of KMS instances goes down, Encrypted files, which encrypted by the keys in the KMS, can not be read.",non_debt,-
hadoop,11862,comment_0,", technically KMS is a proxy for an actual key provider, that, in addition to the keys, generates EDEKs for keys and decrypts them to corresponding DEKs. It also caches the keys in memory. It delegates the Key operations to a backing keyprovider specified by the specified in the *kms-site.xml* conf file. The only concrete implementation (shipped with hadoop) of a KeyProvider is currently the Consider the following deployment scenario : # *KMS1* configured with as ""jcek://file@..."". and thus will delegate to a # *KMS2* configured with as ""kms://http@KMS1.."" and thus will delegate Key operations to KMS1 but will provide generate/decrypt operations. It also caches the Keys for faster access # *KMS3* ALSO configured with as ""kms://http@KMS1.."" and thus will, like KMS2 delegate Key operations to KMS1 but will provide generate/decrypt operations. Now if we set the to the special loadbalancing url : then all requests will be loadbalanced across KMS2 and KMS3 and keys will be shared.",design_debt,non-optimal_design
hadoop,11862,comment_1,"hi , thank you for your reply. yes, the scenario as you said, actually will be loadbalanced and will shared accross KMS instances. But, it's not High Available(HA), there are 2 senarios: 1. if the KMS1 goes down, KMS2 and KMS3 will not available. 2. if the kms.keystore file was delete, the files encrypted by the keys in kms.keystore won't be read. So, I think if keys have several replicas, like HDFS replicas mechanism, it will be really HA. ps. maybe I should modify the title more clearly.",design_debt,non-optimal_design
hadoop,11862,comment_2,", Hmmm.. it is not HA for some operations (create / rollover / delete) most other operations including get / encrypt / decrypt, they should be (since the keys are actually cached by KMS.. and EDEKs generated by 1 KMS can be decrypted by the other.. if the EZ key version is in cache). But yes, i agree, there is no replica stored anywhere.. so for catastrophic failures where the backing KMS does not come up, you lose data. I would expect an enterprise deployment to use a more robust production quality key store for which you can easily write a KeyProvider and use it as a backing key store. But yes, we dont ship one with hadoop.",design_debt,non-optimal_design
hadoop,11862,comment_3,"Resolving, since a full HA story for the KMS also requires a HA backing key provider. Thanks for the nice responses Arun!",non_debt,-
hadoop,11880,summary,Update test-patch to leverage rewrite in Hadoop,non_debt,-
hadoop,11880,description,None,non_debt,-
hadoop,11880,comment_1,"Looking at the tests, turns out some tests do expect hard-coded paths for minidfs cluster, so the cluster comes back up in the same run this is going to be fun. Either revert with a hard-coded path and expect parallel tests to fail, or extend the dfs builder to add an operation to set the subdir, which would be retained over a test case/test suite. The revert is the short-term option, but mini dfs will need to be fixed for reliable hdfs test runs",code_debt,low_quality_code
hadoop,11880,comment_2,"PIng , who will be most interested in this comment. Although it should probably be copied over to HDFS-9263. :) I'm a bit hesitant to enable Yetus for HDFS, etc, until some of this gets cleaned up. :(",non_debt,-
hadoop,11880,comment_3,Hi and . Can you please clarify what you're asking me to do? Are you asking me to code review HDFS-9263? Are you asking me to pick up work on a {{MiniDFSCluster}} fix to be done in a JIRA separate from HDFS-9263? Thanks!,non_debt,-
hadoop,11880,comment_4,"None of the above. I thought you might have an opinion on what the next step is. Frankly, I'm half-leaning towards setting a different hard-coded path and disabling parallel tests in HDFS again since it's pretty clear that the unit tests are pretty screwed up based upon investigation. :( We still need to fix it, though.",test_debt,low_coverage
hadoop,11880,comment_5,"Got it. Thanks! I just commented in HDFS-9263 that the Maven build, when running tests in parallel mode, automatically creates a separate test directory for each concurrent test process to use. Thus, the isolation is achieved at the Maven build level, and there is no requirement for {{MiniDFSCluster}} itself to generate a unique workspace. Of course, that only holds true if all tests are playing nice and using the properties passed down by Maven to determine their test path. I'd like to explore fixing this by backtracing from the bad paths shown in the RAT report to try to find the offending tests. Given that I think is correct already, I suspect we'll find the problems are in tests that don't use {{MiniDFSCluster}}, or in tests that explicitly configure their own data directories instead of relying on I'm reluctant to revert the test parallelization. If it's more urgent to complete this cutover first though, then I understand.",test_debt,expensive_tests
hadoop,11880,comment_6,"OK, I didn't know about the unique temp dir...that reduces the risk of problems",non_debt,-
hadoop,11880,comment_7,"The naughty test is The problem is specific to the HDFS-9263 patch, so I'll add more details over there.",test_debt,flaky_test
hadoop,11953,summary,Binary flags for NativeIO incorrect on Solaris,non_debt,-
hadoop,11953,description,"NativeIO.c has defines for standard input output, (O_CREAT, etc). These are different on Solaris (similar to FreeBSD).",non_debt,-
hadoop,11953,comment_0,"Duplicate of HADOOP-7824. Also, please don't set the fix version, as committers use that field at commit time. Thanks.",non_debt,-
hadoop,11966,summary,Variable cygwin is undefined in hadoop-config.sh when executed through hadoop-daemon.sh.,non_debt,-
hadoop,11966,description,"HADOOP-11464 reinstated support for running the bash scripts through Cygwin. The logic involves setting a {{cygwin}} flag variable to indicate if the script is executing through Cygwin. The flag is set in all of the interactive scripts: {{hadoop}}, {{hdfs}}, {{yarn}} and {{mapred}}. The flag is not set through hadoop-daemon.sh though. This can cause an erroneous overwrite of {{HADOOP_HOME}} and inside hadoop-config.sh.",code_debt,low_quality_code
hadoop,11966,comment_0,"The attached patch refactors setting the {{cygwin}} variable into hadoop-config.sh, where it can take effect for all the interactive scripts as well as hadoop-daemon.sh. This is more like how the branch-1 code handled it. Unfortunately, I missed it the first time. I have retested Cygwin/Windows with this change. I also retested hadoop-daemon.sh on Linux.",non_debt,-
hadoop,11966,comment_1,+1 pending jenkins,non_debt,-
hadoop,11966,comment_2,"You comment says that you tested hadoop-daemon.sh. Have you tested the hadoop, yarn and mapred commands (your change affects all of them and yes it should work since they all source hadoop-config.sh",non_debt,-
hadoop,11966,comment_3,", yes, I tested both hadoop-daemon.sh and all 4 of the interactive commands. Thanks!",non_debt,-
hadoop,11966,comment_5,"The HDFS unit test failures are unrelated, because this patch changes shell script code only. I committed this to branch-2 and branch-2.7. Haohui and Sanjay, thank you for reviewing.",non_debt,-
hadoop,12002,summary,test-patch.sh needs to verify all of the findbugs tools exist,non_debt,-
hadoop,12002,description,"{{test-patch.sh}} was used with {{FINDBUGS_HOME}} set. See below for an example - there were 4 findbugs warnings generated - however, {{test-patch.sh}} doesn't seem to realize that there are missing findbugs tools and +1s the finbugs check. Findbugs check reported as successful :",non_debt,-
hadoop,12002,comment_0,Attaching a patch.,non_debt,-
hadoop,12002,comment_3,"It'd be better to loop over all the executables and print out all of them that aren't found, then give the -1 and return. Right now, if someone is missing several of them they'll probably have to run through multiple times.",code_debt,low_quality_code
hadoop,12002,comment_4,"Thanks , I revised the patch. -02 * cover Sean's feedback * make temporary variables in for loops local * examine the executable paths with -f, not only -x",code_debt,low_quality_code
hadoop,12002,comment_7,"* Generate a -1 jira table for every missing executable. This simplifies the code and gets rid of a lot of excess variables. * We can drop the extra line feed, so just just use echo instead of the more complex printf * This isn't java, we don't need camelCase or really long names. ;) (e.g., findbugsExecutable) [Yes, i recognize I didn't purge them out the first time, but we should make the effort to get rid of them when we can.] * Remove the extra check: -x should fail if the file doesn't exist.",code_debt,complex_code
hadoop,12002,comment_8,"Thanks , I'd like to discuss about the last point. Without -f condition, the expression returns true also if the path is a directory, not only a real executable. On the other hand, I feel checking with -f is too strict because it is very rare case. Should I remove this condition?",non_debt,-
hadoop,12002,comment_9,"I always forget that because as you said: Frankly, I would have put this entire JIRA as a rare case. ( I mean, why would you only install only one of the findbugs binaries when we are clearly requesting the findbugs home and not the location of the findbugs exec???) So I guess leave it in since people get confused.",non_debt,-
hadoop,12002,comment_10,"The mistake I made was to set {{FINDBUGS_HOME}} to instead of as specified below. So this issue is indeed not as common as I thought it would be. That said, adding a +1 due to an invalid {{FINDBUGS_HOME}} could cause unnecessary patch iterations on the corresponding JIRA (as it did in my case).",non_debt,-
hadoop,12002,comment_11,"after HADOOP-12030 those would be -1s with some kind of ""I couldn't run findbugs"" message, I think. this'll need a rebase for the change in HADOOP-12030, and I think we might use more executables now.",non_debt,-
hadoop,12002,comment_12,Closing as won't fix. test-patch has been replaced by Yetus. This bug is already fixed there.,non_debt,-
hadoop,12021,summary,Augmenting Configuration to accomodate <description>,non_debt,-
hadoop,12021,description,Over on the ML I explained a use case which requires me to obtain the value of the Configuration advised me to raise the issue to Jira for discussion. I am happy to provide a patch so that the <description I wanted to find out what people think about this one and whether I should check out Hadoop source and submit a patch. If you guys could provide some advice it would be appreciated.,non_debt,-
hadoop,12021,comment_0,"If you are providing a patch, you'll probably will want to make it optional to turn on. Configs get pushed out to the cluster for each job. Already configs can get pretty large. Adding text will only make them larger. Each node that launches containers for a job needs to receive these configs. Memory and network pressure will certainly mount. When doing work for hRaven and YARN-2928 we do find that configs can be large and costly at times. I suspect that changing a simple key- Btw. during debugging issues on our clusters we often see unzip taking considerable time. That usually indicated that a Configuration object is getting created (which often reads jars from the classpath). Making that even more expensive will probably not be great. Perhaps you can provide a special constructor to create an extensively loaded Configuration so that you're not burdening everybody else, but I don't know if you need the full description to be passed around on the cluster, or if this a client-side only use case.",non_debt,-
hadoop,12021,comment_1,"Hey Joep, I agree and thank you for the context and detail. This was already mentioned in part so it's great to see consensus on it generally being a 'bad' idea. I wonder if I can even override the function which parses out the XML. I wonder f anyone who wrote the original code or who can locate the parsing code can poitb me at it please? I'll then know whether I Can implement this on the Nutch side. Thanks for the prompt feedback it makes a huge difference. -- *Lewis*",non_debt,-
hadoop,12021,comment_2,"See private Resource properties, Resource wrapper, boolean quiet) in",non_debt,-
hadoop,12021,comment_3,"Lewis, could you give a little more detail of your Nutch usecase? It's also worth noting that while we provide the description in core-default.xml / hdfs-default.xml / etc for documentation, but probably not in user-provided config files. The -default.xml files are already included in our JARs, so it shouldn't increase dependency size. Loading them in will, however, increase in-memory size, which is probably a concern for some user app.",documentation_debt,low_quality_documentation
hadoop,12021,comment_4,"Hi , please see attached screenshot which illustrates exactly what I am trying to do. You will see on the right hand side that the property description is not available with the current values being duplicates of the <propertyThis is due to the code currently not parsing out the values for the property descriptions. I wanted to see if I could @override properties, Resource wrapper, boolean quiet) but I can't.",non_debt,-
hadoop,12021,comment_5,"Gotcha, thanks Lewis. One more q, do you think loading a Configuration is easier than doing this yourself? I wrote up a little snippet which might be a nice starting point.",non_debt,-
hadoop,12021,comment_6,Hi Folks. The suggestions here have convinced me that the place to do this is over in Nutch as it is certainly application specific and just causing too much overhead in Hadoop. The text is not required in-memory within the Hadoop Configuration object.,documentation_debt,low_quality_documentation
hadoop,12021,comment_7,Thanks to everyone who commented here. Greatly appreciated.,non_debt,-
hadoop,12021,comment_8,"Thanks for the code snippet Andrew, and Lewis, +1 let's do this in Nutch.",non_debt,-
hadoop,12068,summary,Remove @author tags,non_debt,-
hadoop,12068,description,"Apache generally frowns on @author tags (as they imply ownership of the code by a single individual, see [best asf link is not working, annoyingly] and as an example)",non_debt,-
hadoop,12068,comment_0,Wrong project.,non_debt,-
hadoop,12095,summary,fails,non_debt,-
hadoop,12095,description,"*Error Message* Argument(s) are different! Wanted: "" DISK_QUOTA REM_DISK_QUOTA SSD_QUOTA REM_SSD_QUOTA ARCHIVE_QUOTA REM_ARCHIVE_QUOTA PATHNAME"" ); -Actual invocation has different arguments: "" SSD_QUOTA REM_SSD_QUOTA DISK_QUOTA REM_DISK_QUOTA ARCHIVE_QUOTA REM_ARCHIVE_QUOTA PATHNAME"" ); - Check the following report for same..",non_debt,-
hadoop,12095,comment_0,"Changing output format of shell command is an incompatible change, however, shell command support for reporting quota per storage type (HDFS-7701) is not released now. I'm thinking we should fix the test case and that's okay.",non_debt,-
hadoop,12095,comment_1,"thanks for looking into this issue.. Yes,It should ok. Attached the patch to fix the testcase..kindly review..",non_debt,-
hadoop,12095,comment_2,+1 pending Jenkins. Thanks  for the quick fix.,non_debt,-
hadoop,12095,comment_4,The test failure is not related to the patch. Committing this.,non_debt,-
hadoop,12095,comment_5,I've committed the patch to trunk and branch-2. Thanks  for the contribution!,non_debt,-
hadoop,12095,comment_7,Thanks  for review and commit,non_debt,-
hadoop,12135,summary,cleanup releasedocmaker,code_debt,dead_code
hadoop,12135,description,"For Yetus, releasedocmaker needs some work: * de-hadoop-ify it * still allow it to work w/4+ different projects simultaneously * just running it w/out any options should provide more help I think probably other things too.",non_debt,-
hadoop,12135,comment_0,Most important would be to fix my inability to write python. ;),non_debt,-
hadoop,12135,comment_1,"-00: * removed hadoop hard-codes * added a flag to turn on/off asf license * supports multiple projects, but only as a single file * support for ranges of versions * more of my inability to write python",code_debt,low_quality_code
hadoop,12135,comment_2,"FYI, I'm now using this patch to generate all of (well, at least, the files updated at the time of this patch.)",non_debt,-
hadoop,12135,comment_5,+1 we can make things more pythonic in follow-ons.,code_debt,low_quality_code
hadoop,12135,comment_6,Thanks! Committed.,non_debt,-
hadoop,12135,comment_7,"I pulled this down to trunk/branch-2, thanks all.",non_debt,-
hadoop,12135,comment_15,Removing incompat and release notes since this was committed elsewhere prior to release.,non_debt,-
hadoop,12153,summary,ByteBufferReadable doesn't declare @InterfaceAudience and @InterfaceStability,non_debt,-
hadoop,12153,description,"doesn't set any attributes. Is it intended for public consumption? If so, it should declare it.",non_debt,-
hadoop,12153,comment_0,AFAIK it should be public...Attached the patch kindly review..,non_debt,-
hadoop,12153,comment_1,should we also mark the interface as,non_debt,-
hadoop,12153,comment_2,"thanks for taking a look into this issue..I think, we can add to this class..",non_debt,-
hadoop,12153,comment_4,"+1, committing this shortly.",non_debt,-
hadoop,12153,comment_5,Committed this to branch-2 and trunk. Thanks  for your contribution and thanks for the reporting.,non_debt,-
hadoop,12155,summary,to handle SSL exceptions,non_debt,-
hadoop,12155,description,"downgrades SSL exceptions & subclasses to IOEs, which surfaces when using it in REST APIs. We can look for them specifically and retain the type when wrapping",non_debt,-
hadoop,12155,comment_0,Attaching first patch for the same.,non_debt,-
hadoop,12155,comment_2,Hi have add patch for the issue .Please provide your review comments .,non_debt,-
hadoop,12155,comment_3,Please review the patch uploaded,non_debt,-
hadoop,12155,comment_4,"Thanks  for the patch. It looks good to me except these, * Please add a test case for the code change. * Can we also add the wiki page for the SSLException similar the other exceptions and update the message accordingly?",test_debt,lack_of_tests
hadoop,12155,comment_5,Thanks  for review comments # Testcase i will add to the same soon. # Wiki i will create account and will update on the same. In case someone is already having wiki on SSLException that can be added please do share. :),documentation_debt,outdated_documentation
hadoop,12155,comment_6,Create the a/c then email common-dev to get write access to the wiki,non_debt,-
hadoop,12155,comment_7,Please review patch attached. Have created wiki page for the same.,non_debt,-
hadoop,12155,comment_9,Hi could you please review the patch attached.,non_debt,-
hadoop,12155,comment_10,Any update required??,non_debt,-
hadoop,12155,comment_11,Hi  Could you please review patch attached 1.Have updated wiki page 2.Testcase added for the same.,non_debt,-
hadoop,12155,comment_13,Testcase failure not related to patch attached.,non_debt,-
hadoop,12202,summary,releasedocmaker drops missing component and assignee entries,non_debt,-
hadoop,12202,description,"After HADOOP-11807, releasedocmaker is dropping missing component and assignee entries. It shouldn't drop entries, even if they are errors that lint mode will flag.",non_debt,-
hadoop,12202,comment_0,-00: * fix lint mode to work with multiple versions correctly * re-order how lint mode gathers errors * fix the missing data problems,non_debt,-
hadoop,12202,comment_1,"+1 for the patch. Thanks, Allen!",non_debt,-
hadoop,12202,comment_2,"OK, trunk requires a different patch because it is missing some fixes from yetus.",non_debt,-
hadoop,12202,comment_3,"Anyway, committed to yetus. Thanks!",non_debt,-
hadoop,12202,comment_4,"There's a bug in this that I missed. Not my day, today.",non_debt,-
hadoop,12202,comment_5,I've reverted .,non_debt,-
hadoop,12202,comment_7,"-01: * fix a bug where files that failed lint got deleted, even when running in non-lint mode",non_debt,-
hadoop,12202,comment_8,I blame the code reviewer who +1'd it earlier. :-) +1 for v01. Thanks for fixing this. I just noticed that releasedocmaker.py is using a mix of 2-space and 4-space indentation. I filed HADOOP-12204 as a minor follow-up jira to clean it up.,non_debt,-
hadoop,12202,comment_9,"Thanks for the quick review. Committed to branch. I guess we need to figure out what to do about hadoop trunk, but I'm inclined to just revert the last patch to it without the lint mode.",non_debt,-
hadoop,12202,comment_12,"I've reverted HADOOP-11807 from trunk, so I'm going to close this as resolved now.",non_debt,-
hadoop,12202,comment_13,"I pulled this down to trunk/branch-2, thanks all.",non_debt,-
hadoop,12212,summary,"Hi, I am trying to start the namenode but it keeps showing: Failed to start namenode. Address already in use",non_debt,-
hadoop,12212,description,"Hi, I am trying to start the namenode but it keeps showing: Failed to start namenode. Address already in use;.",non_debt,-
hadoop,12212,comment_0,"Hi Joel, I am closing this. You probably want to send your setup questions to Thanks.",non_debt,-
hadoop,12268,summary,misses rename operation.,non_debt,-
hadoop,12268,description,misses rename operation. Also can pass the original test after fix the issue at,non_debt,-
hadoop,12268,comment_1,The failure test is not related to my change. it was already reported at HDFS-8785.,non_debt,-
hadoop,12268,comment_2,"Nice catch, ! The fix looks good to me. Two comments: 1. Would you remove unused imports in and 2. Would you please fix the following javadoc comment? ""concat"" should be ""append"".",code_debt,low_quality_code
hadoop,12268,comment_3,Thanks for the review ! I uploaded a new patch which addressed all your comments. Please review it.,non_debt,-
hadoop,12268,comment_8,"+1, thanks . These test failures look unrelated to the patch. They were reported by HDFS-8772 and HDFS-7553.",non_debt,-
hadoop,12268,comment_9,thanks for the review ! Will commit the patch shortly.,non_debt,-
hadoop,12268,comment_10,has change in hdfs project. Split the patch to two changes: for HADOOP-12268 and for HDFS-8847.,non_debt,-
hadoop,12268,comment_11,I committed the patch to trunk and branch-2.,non_debt,-
hadoop,12288,summary,test-patch.sh does not give any detail on its -1 findbugs warning report.,non_debt,-
hadoop,12288,description,test-patch.sh does not give any detail on its -1 warning report. This has been seen in Jenkins run of HDFS-8830.,non_debt,-
hadoop,12288,comment_0,This is fixed in yetus/branch HADOOP-12111.,non_debt,-
hadoop,12302,summary,Fix native compilation on Windows after HADOOP-7824,non_debt,-
hadoop,12302,description,"HADOOP-7824 introduced a way to set the java static values for POSIX flags, this resulted in compilation error in Windows",non_debt,-
hadoop,12302,comment_0,Attaching the patch to fix the same.,non_debt,-
hadoop,12302,comment_3,+1. Thanks,non_debt,-
hadoop,12318,summary,Expose underlying LDAP exceptions in SaslPlainServer,non_debt,-
hadoop,12318,description,"In the code of class the underlying exception is not included in the {{SaslException}}, which leads to below error message in HiveServer2: Make COEs very hard to understand what the real error is. Can we change that line as:",non_debt,-
hadoop,12318,comment_0,That seems reasonable to me.,non_debt,-
hadoop,12318,comment_2,"+1, the patch looks good to me. I don't think we need a test for this since it's just adding a new cause for an exception. I ran both of the failed tests and they passed locally for me, so I think the failures are unrelated. I'm going to commit this momentarily.",non_debt,-
hadoop,12318,comment_3,"I've just committed this to trunk and branch-2. Thanks very much for the contribution, Mike.",non_debt,-
hadoop,12318,comment_5,"-1. It's critical to use and not .getMessage(), as some exceptions (NPE) don't have messages. please roll back and add one which reports exceptions fully.",non_debt,-
hadoop,12318,comment_6,"Looking @ the patch more, the patch itself isn't introducing the getmessage bug, merely retaining it. Even so, this is the time to fix as its only going to lead to messages like ""auth failed cause: Null""",non_debt,-
hadoop,12318,comment_13,"Hey Steve, thanks a lot for taking a look at this patch, and I think your point is a good one. Mind if we file a new JIRA to make this change? Seems like that'd make the history a bit cleaner than reverting this patch and applying a new one with the revision you're suggesting.",non_debt,-
hadoop,12368,summary,Mark and ViewFsBaseTest as abstract,non_debt,-
hadoop,12368,description,"These are test base classes that need to be subclassed to run, can mark as abstract.",code_debt,low_quality_code
hadoop,12368,comment_0,Trivial patch attached,non_debt,-
hadoop,12368,comment_1,"+1, pending Jenkins. Thanks Andrew.",non_debt,-
hadoop,12368,comment_3,"Thanks Yi for reviewing, committed to trunk and branch-2.",non_debt,-
hadoop,12371,summary,Add a force option for hdfs dfs -expunge to remove all check points,non_debt,-
hadoop,12371,description,Hadoop document has that expunge Usage: hdfs dfs -expunge Empty the Trash. However when I run that command. It returns following message from shell output : The configured checkpoint interval is 0 minutes. Using an interval of 360 minutes that is used for deletion instead. Trash is not emptied.,non_debt,-
hadoop,12371,comment_0,"Run {panel} hadoop fs -expunge 15/09/01 01:02:34 INFO Namenode trash configuration: Deletion interval = 360 minutes, Emptier interval = 0 minutes. {panel} Shell output doesn't indicate expunge is in progress or not, we need to improve the messages to be more informative. And this command seems not work, trash files are not removed, check NN log {panel} 2015-07-27 22:32:14,837 INFO - Namenode trash configuration: Deletion interval = 360 minutes, Emptier interval = 0 minutes. 2015-07-27 22:32:14,837 INFO So it looks like the Emptier thinks this is a client configuration and ignore it, using server side 360 minutes instead. By running this command, I expect to remove trash however it doesn't honor the intention.",non_debt,-
hadoop,12371,comment_1,"@Weiwei Yang, the can't be expunged. The expunge command only deletes *checkpoints* (if exist) that are older than fs.trash.interval. Can you post ""hdfs dfs -ls -R /user/foo/.Trash"" output on the JIRA? It will tell us whether you have trash checkpoints that should be deleted by expunge or not.",non_debt,-
hadoop,12371,comment_2,"thanks.. however .. what if I simply want to wipe out trash ? On my testing cluster, with rarely small storage, I am running jobs generating some data and deleting them over and over again, the trash easily blows up the storage and my HDFS becomes unavailable. Does it make sense to provide an force option to clean up trash ? E.g hadoop fs -expunge -f",design_debt,non-optimal_design
hadoop,12371,comment_3,"Why not using existing cmd to achieve that, e.g., 'hdfs dfs -rm -r /user/foo/.Trash' ? Or using '-skipTrash' in your jobs to avoid putting it in trash at all.",non_debt,-
hadoop,12371,comment_4,"Hi  For people who knows HDFS well, of cause they can just use commands you mentioned. I am suggesting from a normal user angle ... they would not know anything about /user/foo/.Trash at all. Expunge command sets up the expectation that HDFS provides utilities to manage trash, then it is pretty straightforward to ask for a command removing all trashes.",non_debt,-
hadoop,12371,comment_5,"Hi  If you think this is necessary I can work on it. If you still think it is not, feel free to close it. Thanks for watching this.",non_debt,-
hadoop,12408,summary,java8 build failing in javadocs at,non_debt,-
hadoop,12408,description,Jenkins is failing in javadocs;,non_debt,-
hadoop,12408,comment_1,Hi would you check HADOOP-12087? I reported the same issue there before and created a patch to fix java8 build.,non_debt,-
hadoop,12408,comment_2,"You're right you have covered it. I think we should start treating anything that stops the java8 builds as blockers, because we need that build working 100% of the time. If things like this arise again, let's push for immediate fixes.",non_debt,-
hadoop,12431,summary,NameNode should bind on both IPv6 and IPv4 if running on dual-stack machine and IPv6 enabled,non_debt,-
hadoop,12431,description,"NameNode works properly on IPv4 or IPv6 single stack (assuming in the latter case that scripts have been changed to disable preferIPv4Stack, and dependent on the client/data node fix in HDFS-8078). On dual-stack machines, NameNode listens only on IPv4 (even ignoring preferIPv6Addresses being set.) Our initial use case for IPv6 is IPv6-only clusters, but ideally we'd support binding to both the IPv4 and IPv6 machine addresses so that we can support heterogenous clusters (some dual-stack and some IPv6-only machines.)",non_debt,-
hadoop,12431,comment_0,"Nate -why not create an ber-JIRA to cover the overall problem of ""Hadoop to support IPv6"" .. all these things could be grouped underneath. HADOOP-11574 is mainly about ""recognise network problems and provide diagnostics"", rather than actual IPv6 support",non_debt,-
hadoop,12431,comment_1,"An uber jira is a good idea. Also, let's not change default behavior by binding on ipv6 by default. It will create problems for sure.",non_debt,-
hadoop,12431,comment_2,"is working on this, reassigning.",non_debt,-
hadoop,12431,comment_3,Going to resolve this one as won't fix. We don't want to bind to ipv6 by default. Instead I'm going to open a documentation jira about how to set up a cluster with dual stack.,non_debt,-
hadoop,12452,summary,Fix tracing documention reflecting the update to htrace-4,documentation_debt,outdated_documentation
hadoop,12452,description,The sample code in Tracing.md have some problems. * compilation error by not importing Tracer. * generic options are not reflected because Tracer is initialized before ToolRunner#run. * it may be confusing to use FsShell in example because it has embedded Tracer now.,documentation_debt,outdated_documentation
hadoop,12452,comment_0,I replaced sample code using FsShell in it. I also added subsection explaining starting tracing by FileSystem Shell configuration. HADOOP-12167 will be duplicated by the patch.,non_debt,-
hadoop,12452,comment_2,"+1. Thanks, .",non_debt,-
hadoop,12458,summary,Retries is typoed to spell Retires in parts of hadoop-yarn and hadoop-common,documentation_debt,low_quality_documentation
hadoop,12458,description,Spotted this typo in the code while working on a separate YARN issue. E.g Checked in the whole project. Found a few occurrences of the typo in code/comment. The JIRA is meant to help fix those typos.,documentation_debt,low_quality_documentation
hadoop,12458,comment_1,"Failed tests aren't related. Thanks for the changes! +1, committing shortly. Quick notes: - Please do not set a Fix Version. Use Target Version field instead. The Fix Version must indicate only the branches where it has *already* been committed to. The former is to indicate requests of branches it must go to, so is more appropriate. - For more typo corrections in future, please also feel free to roll up multiple corrections into the same patch.",documentation_debt,low_quality_documentation
hadoop,12458,comment_3,Committed to branch-2 and trunk. Thank you Neelesh! Keep the improvements coming :),non_debt,-
hadoop,12458,comment_11,"Thank you for the review, notes and commit.",non_debt,-
hadoop,12460,summary,Add overwrite option for 'get' shell command,non_debt,-
hadoop,12460,description,I think it'd be good to add an argument to specify that the local file be overwritten if it exists when doing a DFS Get operation.,non_debt,-
hadoop,12460,comment_0,"Attached the patch , please review",non_debt,-
hadoop,12460,comment_2,"Corrected the checkstyle errors and white spaces & Attached the patch, Please review",code_debt,low_quality_code
hadoop,12460,comment_4,"Thanks  Few comments, 1. Identation can be same as other lines and \n not required at the end. 2. Document need to be updated for the {{get}} command instead of {{copyToLocal}} 3. {{TestCLI}} failure should be fixed.",documentation_debt,outdated_documentation
hadoop,12460,comment_5,"Thanks  for your review, I have updated the patch based on your comments, Please review",non_debt,-
hadoop,12460,comment_7,"Fixed the Test Failures , Attaching the patch , Please review",non_debt,-
hadoop,12460,comment_9,"The Test Case failures are not related to Patch,  ,please review the same",non_debt,-
hadoop,12460,comment_10,1,non_debt,-
hadoop,12460,comment_11,Committed to trunk and branch-2. Thanks,non_debt,-
hadoop,12484,summary,Single File Rename Throws Incorrectly In Potential Race Condition Scenarios,non_debt,-
hadoop,12484,description,function - in the case where src and dst both exist gets a lease on src blob to block write access and then deletes the file. However in the time between checking existence of source file and acquiring lease the file may be deleted by another process (race condition). Presently the function simply throws in this scenario. In this case the function should treat this as a case where rename is complete; i.e. catch the exception and if this is the cause exit gracefully with the result that the rename is complete,non_debt,-
hadoop,12484,comment_1,"Hello . If {{acquireLease}} succeeds, but then the {{delete}} fails, then this will leak the lease. Can you please use a {{finally}} block to guarantee that the lease gets released in all code paths? You can look at other points in the class that acquire and free a lease for an existing example. Could you please review the checkstyle and whitespace warnings from the pre-commit run and fix them? If it's not feasible to write a unit test to simulate the race condition, then can you please describe any manual testing that you've done to verify the change? Thank you!",design_debt,non-optimal_design
hadoop,12484,comment_3,", it looks like patch v02 was in UTF-16 encoding. Patch files need to use ASCII encoding. I'm attaching patch v03, which is the same thing in ASCII encoding.",non_debt,-
hadoop,12484,comment_4,", there is one more problem here: At this point, it's possible that {{lease}} is still {{null}} if the earlier {{acquireLease}} call threw an exception. I recommend checking this for null before calling {{free}}. Otherwise, it will cause a",code_debt,low_quality_code
hadoop,12484,comment_5,Sorry for that ; yes you are right. Im continuing to work on this for now and also explore the testing options.,non_debt,-
hadoop,12484,comment_7,"Addressed your comments above. With respect to tests: In this particular case the patch seems extremely straightforward but hard to test with unit tests or custom patch tests as this issue is exposed by hDP 2.3 integration tests that inject random failures. It would be good to have a thorough code review instead. ,  for reference",non_debt,-
hadoop,12484,comment_9,Addressed your comments above but I seem to be facing a mvninstall error. Please check,non_debt,-
hadoop,12484,comment_11,"This part is triggering a Checkstyle warning: We can clean that up by changing to: I realize there are existing instances of empty statements like this in the class, but let's avoid introducing new instances. The mvninstall failure appears to be a side effect of something in the bats testing of the bash scripts. It's unrelated to this patch. Since you need to upload one more patch revision to address the Checkstyle warning, let's do one more test run and see if it happens again. If it does, then I'll follow up. The license check warning is caused by a test copying a file to a location that it shouldn't, which is then covered by the license check. I'll follow up separately on that.",code_debt,low_quality_code
hadoop,12484,comment_13,"This looks good overall, aside from the remaining whitespace and Checkstyle problems. I'm attaching patch v06, which is the same thing with the whitespace and Checkstyle warnings fixed.",code_debt,low_quality_code
hadoop,12484,comment_14,Thanks a lot Chris. Apologise for the whitespace and checkstyle issues; I was about to put in a new patch myself but since you have already done it I will let Jenkins run again. What are the next steps here after this? Is any further action required from my end?,code_debt,low_quality_code
hadoop,12484,comment_15,", thank you. There is no further action required right now. We'll just wait for Jenkins.",non_debt,-
hadoop,12484,comment_17,"+1 for patch v06 now that the style problems have been fixed. I have committed this to trunk and branch-2. , thank you for contributing the patch.",non_debt,-
hadoop,12485,summary,Don't prefer IPV4 stack on tests.,non_debt,-
hadoop,12485,description,Don't prefer ipv4 stack and instead use ipv6 if it's there. This should mean that tests actually bind on ipv6 if it's there. It should mean that tests running against this branch are more likely to be representative of what would be running in production.,test_debt,lack_of_tests
hadoop,12485,comment_1,"Is this something we'd want to make an overrideable property, so you can mvn test -Dforce.ipv4=true?",non_debt,-
hadoop,12485,comment_2,Yeah probably. Let me get that in.,non_debt,-
hadoop,12485,comment_3,It's probably best to wait for all patches from HADOOP-12122 to get committed as there are two more places in YARN that need to be updated for those tests to work.,non_debt,-
hadoop,12485,comment_5,Got the non-binding +1 from  Pushed to branch,non_debt,-
hadoop,12496,summary,Update AWS SDK version (1.7.4),non_debt,-
hadoop,12496,description,"hadoop-aws jar still depends on the very old 1.7.4 version of aws-java-sdk. In newer versions of SDK, there is incompatible API changes that leads to the following error when trying to use the S3A class and newer versions of sdk presents. This is because S3A is calling the method with ""int"" as the parameter type while the new SDK is expecting ""long"". This makes it impossible to use kinesis + s3a in the same process. It would be very helpful to upgrade hadoop-awas's aws-sdk version. $iwC$$iwC.<init at $iwC.<init at <init at .<init at .<clinit at .<init at .<clinit at $print(<console at Method)",design_debt,non-optimal_design
hadoop,12496,comment_0,"Hi Yongjia, HADOOP-12269 upgraded aws-sdk-s3 to 1.10.6",non_debt,-
hadoop,12496,comment_1,"Linking to duplicate issues. Yonjia, please search JIRA first as you'll save us all time.",non_debt,-
hadoop,12496,comment_2,sorry: Yongjia.,non_debt,-
hadoop,12505,summary,should support group names with space,non_debt,-
hadoop,12505,description,"In a typical configuration, group name is obtained from AD through SSSD/LDAP. AD permits group names with space (e.g. ""Domain Users""). Unfortunately, the present implementation of parses the output of shell command ""id -Gn"", and assumes group names are separated by space. This could be achieved by using a combination of shell scripts, for example, bash -c 'id -G weichiu | tr "" "" ""\n"" | xargs -I % getent group ""%"" | cut -d"":"" -f1' But I am still looking for a more compact form, and potentially more efficient one.",non_debt,-
hadoop,12505,comment_0,I will submit a patch when HADOOP-12468 is done.,non_debt,-
hadoop,12505,comment_1,"Copying my comment: Yes, but that doesn't mean they are POSIX compliant, which much match this regex: . So a definite -1 on this.",non_debt,-
hadoop,12505,comment_2,"Thanks for your comment, Allen. I am OK with that if HDFS must be POSIX-compliant.",non_debt,-
hadoop,12505,comment_3,"yeah, it'll break the universe in really awful ways if we accept groups with spaces. Think about folks parsing hadoop fs -ls for example. or the REST interfaces. or ACLs. or ... ugh!",design_debt,non-optimal_design
hadoop,12505,comment_4,"FWIW, the JNI based groups plugin can grab the space-carrying groups accurately.",non_debt,-
hadoop,12505,comment_5,"The Hadoop code itself does not consistently enforce POSIX compliance for user names or group names. It's more a function of the hosting OS. For example, Harsh has pointed out that for operators using the JNI-based implementation instead of shell-based, Hadoop users already can show up with membership in groups that don't have a POSIX-compliant name. I've seen that for Windows deployments, and it sounds like the use case here was a Linux deployment connected to Active Directory. The group mapping alone is not sufficient to enforce POSIX compliance either. This is only used to populate the user's set of groups. It does not control other input paths for group names. For example, WebHDFS and Java API calls can accept names with spaces. Here is an example, tested on Mac. This kind of data of course complicates parsing of shell output, and I imagine many operators would prefer to enforce POSIX-compliant names by policy. However, I don't believe Hadoop has taken responsibility for that enforcement. I don't think the existing implementation of really provides any POSIX compliance benefits, at least not intentionally. In the example given, it would split the group ""Domain Users"" on spaces and decide to put the user into 2 groups: ""Domain"" and ""Users"". While those split names don't have spaces, they're also still not POSIX compliant because of the capital letters, and more importantly, they're completely erroneous. Hopefully the split isn't putting anyone into a real group where they don't really belong. I see this as a bug rather than a POSIX compliance feature. I would prefer to see the -1 lifted and have the bug fixed. That said, I also see it as low priority, since the majority of deployments I see use the JNI-based implementation now, which does not have the bug.",code_debt,low_quality_code
hadoop,12505,comment_6,"I just want to add that Linux also allows numerical group names (i.e. group names that starts with digits), so POSX-compliant is not strictly enforced. I wonder if there's documentation for this kind of thing on Linux or *NIX.",non_debt,-
hadoop,12505,comment_7,"Let's be careful here. An admin can add whatever they want to the group file; that doesn't mean it's *legal*. Rarely to do folks run group check utilities (grpck on Linux) to verify that their definitions are actually valid. Yes, definitely. See section ""3.190 Group Name"": . The ""portable filename character set"" is defined later on in the same manual. (... and, for those unclear, that above is the *official* definition for anything that carries the UNIX trademark.)",non_debt,-
hadoop,12505,comment_8,"I'd argue by calling something ""ShellBased *Unix* GroupMapping"", one is agreeing to the terms and conditions set forth by UNIX(tm). If one doesn't want to abide by those rules, then someone should create the :)",non_debt,-
hadoop,12505,comment_9,"I'm curious then about what is your stance on Do you see it as a bug that it works correctly with non-Unix-compliant names? Something else for us to consider is the severity to which tools break in the presence of non-compliant names. For a local file system, it can break parsing in scripts, but it can't harm permission checks, which are based on numeric UID/GID. In Hadoop, we don't have access to a canonical UID/GID, so we rely on the string names. (Arguably, we'd be better off with a real UID/GID, but that would be a significant design change.) Since the bug places users into incorrect groups (i.e. ""Domain"" and ""Users"" in the example), there is a risk of influencing permission checks. I see this as a more severe problem and something worthy of a bug fix.",non_debt,-
hadoop,12505,comment_10,"Yes, because it means unpredictable behavior. Unpredictable behavior almost always turns into a security hole. It's trivial to construct a group that turns into ../.. (or whatever) in the path structure if I'm interpreting the output of hadoop fs -ls. That's very very bad. (that said: it'd be an awesome crack. Change the default user's group and watch everyone nuke their own files...) The NFS folks added some code to do it, but didn't really integrate it correctly. Expedience always trumps correctness. :(",code_debt,low_quality_code
hadoop,12520,summary,Use XInclude in hadoop-azure test configuration to isolate Azure Storage account keys for service integration tests.,non_debt,-
hadoop,12520,description,"The hadoop-azure tests support execution against the live Azure Storage service if the developer specifies the key to an Azure Storage account. The configuration works by overwriting the file. This can be an error-prone process. The azure-test.xml file is checked into revision control to show an example. There is a risk that the tester could overwrite azure-test.xml containing the keys and then accidentally commit the keys to revision control. This would leak the keys to the world for potential use by an attacker. This issue proposes to use XInclude to isolate the keys into a separate file, ignored by git, which will never be committed to revision control. This is very similar to the setup already used by hadoop-aws for integration testing.",design_debt,non-optimal_design
hadoop,12520,comment_0,I'm attaching the proposed patch.,non_debt,-
hadoop,12520,comment_1,+1 pending jenkins.,non_debt,-
hadoop,12520,comment_3,"Thanks for the review, Haohui. I have committed this to trunk and branch-2.",non_debt,-
hadoop,12534,summary,User document for SFTP File System,non_debt,-
hadoop,12534,description,We should have a document for using SFTP File System.,documentation_debt,outdated_documentation
hadoop,12534,comment_1,# Should this be # The patch contains a smartquote It may also be worth adding some package documentation as part of this patch,documentation_debt,outdated_documentation
hadoop,12534,comment_2,Thanks for your comments.,non_debt,-
hadoop,12562,summary,Make hadoop dockerfile usable by Yetus,non_debt,-
hadoop,12562,description,It would be better if Hadoop's dockerfile could be used by Yetus so that external dependencies are owned by the project.,non_debt,-
hadoop,12562,comment_0,"00: * re-arrange, add some content, and add a 'YETUS CUT HERE' line",non_debt,-
hadoop,12562,comment_1,I've manually tested start-build-env.sh script and it appears to work as expected.,non_debt,-
hadoop,12562,comment_3,-01: * contains the additions for the HDFS-8707 branch,non_debt,-
hadoop,12562,comment_5,+1. I'll commit it shortly.,non_debt,-
hadoop,12562,comment_6,I've committed the patch to trunk and branch-2. Thanks  for the contribution.,non_debt,-
hadoop,12562,comment_12,", looks like the Jenkins has not picked up the changes yet: Are there many changes that need to be implemented so that Jenkins can work with the HDFS-8707 branch?",non_debt,-
hadoop,12562,comment_14,"Of course not. As documented in the JIRA links, you'll see this change is blocked on another change...",non_debt,-
hadoop,12598,summary,add XML namespace declarations for some hadoop/tools modules,non_debt,-
hadoop,12598,description,"Add missing project namespaces for pom.xml of hadoop-pipes, hadoop-datajoin and hadoop-gridmix modules",non_debt,-
hadoop,12598,comment_0,Thanks for working on this. Issue description will be helpful for other to understand what's going on and review.,non_debt,-
hadoop,12598,comment_1,"Yeah, I'm doing this. Delay because of the bad network.",non_debt,-
hadoop,12598,comment_2,Thanks.,non_debt,-
hadoop,12598,comment_4,"Good, the patch makes sense to me. Thanks.",non_debt,-
hadoop,12598,comment_5,"Make sense to me. Hi , would you fix pom.xml of hadoop-datajoin and hadoop-gridmix modules as well?",non_debt,-
hadoop,12598,comment_6,"OK, just wait a moment. I will create a new patch.",non_debt,-
hadoop,12598,comment_7,"fix pom.xml of hadoop-pipes, hadoop-datajoin and hadoop-gridmix modules",non_debt,-
hadoop,12598,comment_8,Done.,non_debt,-
hadoop,12598,comment_10,"+1, thanks Xin.",non_debt,-
hadoop,12598,comment_11,Committed this to trunk and branch-2. Thanks Xin for the contribution and thanks Steve for improving the title of this issue.,non_debt,-
hadoop,12598,comment_12,and thanks Mingliang for the review!,non_debt,-
hadoop,12600,summary,FileContext and AbstractFileSystem should be annotated as a Stable interface.,non_debt,-
hadoop,12600,description,"The {{FileContext}} class currently is annotated as {{Evolving}}. However, at this point we really need to treat it as a {{Stable}} interface.",non_debt,-
hadoop,12600,comment_0,Note the comment below. The class has been annotated {{Evolving}} for a long time now. I expect we simply forgot to graduate to {{Stable}}.,non_debt,-
hadoop,12600,comment_1,"I'm tentatively making this a 2.8.0 blocker. It's easy to get done, and we should commit to stability for this interface.",non_debt,-
hadoop,12600,comment_2,I'm attaching a one-liner patch. No tests are necessary since this is metadata only.,non_debt,-
hadoop,12600,comment_3,pre-emptive +1 if jenkins is happy,non_debt,-
hadoop,12600,comment_5,Patch LGTM. +1 The findbugs issue has just been fixed by HDFS-9451.,non_debt,-
hadoop,12600,comment_6,"I just noticed that has the same problem, so here is patch v002.",non_debt,-
hadoop,12600,comment_8,1,non_debt,-
hadoop,12600,comment_9,"I have committed this to trunk, branch-2 and branch-2.8. Thank you for the reviews, everyone.",non_debt,-
hadoop,12677,summary,DecompressorStream throws when calling skip(long),non_debt,-
hadoop,12677,description,"throws an when using a long bigger than Integer.MAX_VALUE This is because of this cast from long to int: The fix is probably to do the cast after applying Math.min: in that case, it should not be an issue since it should not be bigger than the buffer size (512)",non_debt,-
hadoop,12677,comment_0,"Thanks  for reporting this issue! I was able to reproduce the exception and then created a patch and a regression test case. Attaching my rev01 patch: * In declare both skipped and len as long to avoid overflowing the number. * Create a test case in inherits DecompressorStream, and DecompressorStream does not have its own test file.",non_debt,-
hadoop,12677,comment_2,Test failures are unrelated. ASF license warning is false positive.,non_debt,-
hadoop,12677,comment_3,"The unit test is incorrect. According to it doesn't throw EOFException and should return how many bytes were skipped. So the catch clause should be removed, and instead the return value should be checked (it should be 4). It looks also that the following check {code:java} if (bufLen > 0)",non_debt,-
hadoop,12677,comment_4,"Thank you for the comments and reviews! I have posted patch #2 to address comment#2. Regarding comment#1, even though DecompressorStream inherits InputStream, its skip() implementation does not call InputStream.skip(). Instead, its internal implementation uses InputStream.read(), which returns -1 if end of stream. If InputStream.end() returns -1, an EOFException is thrown.",non_debt,-
hadoop,12677,comment_5,"It's not about invoking InputStream.skip() but following the same behavior, as part of the API contract. Before your patch, DecompressorStream would also return the number of bytes skipped until EOF, changing it would mean introducing an incompatible change.",non_debt,-
hadoop,12701,summary,Run checkstyle on test source files,non_debt,-
hadoop,12701,description,Test source files are not checked by checkstyle because Maven checkstyle plugin parameter is *false* by default. Propose to enable checkstyle on test source files in order to improve the quality of unit tests.,code_debt,low_quality_code
hadoop,12701,comment_0,"After the fix, mvn for entire Hadoop tree took 01:27 min on my Macbook Pro. In comparison, it took 38.677 s before.",code_debt,slow_algorithm
hadoop,12701,comment_1,Let me know if the proposed change has any unintended side affect since it does have global impact on all Hadoop components.,non_debt,-
hadoop,12701,comment_2,"As an alternative, we can change Maven checkstyle plugin parameter to *true* by default, but that will even have a much wider impact beyond Hadoop project.",non_debt,-
hadoop,12701,comment_3,"Another alternative: test-patch uses a custom configuration that sets to true. Not savory IMHO. What do you think, ?",non_debt,-
hadoop,12701,comment_5,It doesn't make sense to only enable it for test-patch if it is meant to be enforced at all. So this is an all or nothing type of change.,non_debt,-
hadoop,12701,comment_6,"Thanks, .",non_debt,-
hadoop,12701,comment_7,"Could you review this fix, to prevent problem like [HADOOP-12700] from happening? It will help improve the quality of test code while doubling the time to run checkstyle. Thanks, John.",non_debt,-
hadoop,12701,comment_8,"Test the patch on Ubuntu VirtualBox VM under MacBook. Before the patch, it took 1:10.947 s to run {{mvn clean after the patch, the same command took 1:26.765 s. Test the patch on MacBook. Before the patch, it took 41.425 s to run {{mvn clean after the patch, the same command took 50.394 s.",non_debt,-
hadoop,12701,comment_11,"This change seems fine, I'm not too worried about adding 10s of seconds to checkstyle runs. However, why aren't we seeing checkstyle run by precommit? Do we need to add a space to a java file somewhere to trigger change detection?",non_debt,-
hadoop,12701,comment_12,@andrew wang Maven checkstyle plugin does not run on test source files by default.,non_debt,-
hadoop,12701,comment_13,Maven checkstyle plugin does not run on test source files by default.,non_debt,-
hadoop,12701,comment_14,"LGTM then, let's get this in. Thanks John!",non_debt,-
hadoop,12701,comment_15,"Committed, thanks John!",non_debt,-
hadoop,12701,comment_16,Thanks !,non_debt,-
hadoop,12721,summary,Hadoop-tools jars should be included in the classpath of hadoop command,non_debt,-
hadoop,12721,description,"Currently, jars under Hadoop-tools dir are not be included in the classpath of hadoop command. So we will fail to execute cmds about wasb or s3 file systems. A simple solution is to add those jars into the classpath of the cmds. Suggestions are welcomed~",non_debt,-
hadoop,12721,comment_0,Add Hadoop-tools jars to the classpath of hadoop commands,non_debt,-
hadoop,12721,comment_1,Could you help to review this patch? Thanks~,non_debt,-
hadoop,12721,comment_2,"-1 1) Adding hadoop-tools to the default path is going to break all sorts of user-level things due to the amount of transitive dependencies. 2) This will *greatly* impact the startup of time of commands by including a bunch of class files that will never get used. Instead of slamming in everything, users should be using shell profiles to include the jars they actually need. Another thing that would be good to do is to break up the hadoop tools dir to be per component.",code_debt,low_quality_code
hadoop,12721,comment_3,I'm marking this as an incompatible change.,non_debt,-
hadoop,12721,comment_4,"When I code reviewed HADOOP-11485, I tested it by writing my own shell profile to add the Azure jars to the default classpath. This was only ~5 lines of code. Perhaps we could address this by shipping the distro with a library of commonly useful shell profiles (azure, s3a, etc.). They would be inactive by default for but users could activate them quickly and easily by copying or symlinking.",non_debt,-
hadoop,12721,comment_5,"I think including some common shellprofiles is a great idea. At the same time, it's increasingly evident that hadoop-tools needs to get broken up. e.g., HADOOP-12556.",architecture_debt,violation_of_modularity
hadoop,12721,comment_6,"HADOOP-12857 should have effectively fixed this issue for trunk in a fairly safe, generic way. Closing as won't fix.",non_debt,-
hadoop,12733,summary,Remove references to obsolete io.seqfile configuration variables,non_debt,-
hadoop,12733,description,The following variables appear to no longer be used.,code_debt,dead_code
hadoop,12733,comment_0,Initial version.,non_debt,-
hadoop,12733,comment_2,"RE: ASF license warnings This looks like a bunch of generated files, not checked in files.",code_debt,low_quality_code
hadoop,12733,comment_4,Rebased against trunk,non_debt,-
hadoop,12733,comment_6,"+1 for removing the references. Hi , would you rebase the patch for the latest trunk?",non_debt,-
hadoop,12733,comment_8,Thanks ! Here's an updated patch for trunk.,non_debt,-
hadoop,12733,comment_10,"+1, checking this in.",non_debt,-
hadoop,12733,comment_11,"Committed this to trunk, branch-2, and branch-2.8. Thanks  for the contribution!",non_debt,-
hadoop,12733,comment_13,Thanks  for the review and the commit!,non_debt,-
hadoop,12792,summary,fails in chroot,non_debt,-
hadoop,12792,description,Bug fixed by [HADOOP-7811] broken by [HADOOP-8562]. Need to re-introduce the fix.,non_debt,-
hadoop,12792,comment_0,Re-introduce the fix from [HADOOP-7811].,non_debt,-
hadoop,12792,comment_2,The JUnit failure is related to and is completely separate from the patch for this JIRA. The patch for this JIRA fixes the issue.,non_debt,-
hadoop,12792,comment_3,+1 lgtm. Committing this.,non_debt,-
hadoop,12792,comment_4,"Thanks, Eric! I committed this to trunk, branch-2, branch-2.8, and branch-2.7.",non_debt,-
hadoop,12792,comment_6,Closing the JIRA as part of 2.7.3 release.,non_debt,-
hadoop,12806,summary,Hadoop fs s3a lib not working with temporary credentials in AWS Lambda,non_debt,-
hadoop,12806,description,"Trying to use the hadoop fs s3a library in AWS lambda with temporary credentials but it's not possible because of the way the is defined under Specifically the following code is used to initialise the credentials chain The above works fine when the EC2 metadata endpoint is available (i.e. running on an EC2 instance) however it doesn't work properly when the environment variables are used to define credentials as it happens in AWS Lambda. Amazon suggests to use the in AWS Lambda. To summarise and suggest an alternative I think that the could be used instead of the and that would cover the following cases: {panel} * Environment Variables - AWS_ACCESS_KEY_ID and (RECOMMENDED since they are recognized by all the AWS SDKs and CLI except for .NET), or AWS_ACCESS_KEY and AWS_SECRET_KEY (only recognized by Java SDK) * Java System Properties - aws.accessKeyId and aws.secretKey * Credential profiles file at the default location shared by all AWS SDKs and the AWS CLI * Instance profile credentials delivered through the Amazon EC2 metadata service {panel} If you think that the above change would be useful I could investigate more about what the required changes would be and submit a patch.",design_debt,non-optimal_design
hadoop,12806,comment_0,", improving support with AWS Lambda sound great, but I don't think we'll be able to jump right to due to concerns. We definitely need to be able to keep supporting If you want to suggest another way to modify our chain to support AWS Lambda, then we could go in that direction. We'd also likely need help with testing the various credential sources in different deployments.",non_debt,-
hadoop,12806,comment_1,"Given Hadoop 2.8 lets you declare whatever provider you want, it should be possible to wire things up, even if we aren't doing it out the box in a way which works with AWS Lambda. Nikolaos: could you build the latest 2.8 branch and see if it works now?",non_debt,-
hadoop,12806,comment_2,"Hadoop 2.8.0 is shipping. Can you try it now to see if the problem goes away...and if not, is it possible to configure the credential provider chain so it does work?",non_debt,-
hadoop,12806,comment_3,"Given the auth chain is configurable, I'm, closing this as fixed against Hadoop 2.8",non_debt,-
hadoop,12811,summary,Change kms server port number which conflicts with HMaster port number,non_debt,-
hadoop,12811,description,The HBase's HMaster port number conflicts with Hadoop kms port number. Both uses 16000. There might be use cases user need kms and HBase present on the same cluster. The HBase is able to encrypt its HFiles but user might need KMS to encrypt other HDFS directories. Users would have to manually override the default port of either application on their cluster. It would be nice to have different default ports so kms and HBase could naturally coexist.,design_debt,non-optimal_design
hadoop,12811,comment_0,"Stating from HBase 0.99, HMaster's port number was set to 16000 to avoid the conflict with linux ephemeral port. At roughly same time, hadoop kms was released. As pointed out by jmhsieh@ ""Timeline wise, the changes were released at roughly same time  This change was in a released by hbase in 30/oct/2014 [1] and in hadoop 18/nov/2014 [2]. [1] [2] """,non_debt,-
hadoop,12811,comment_1,I'll suggest moving this into the 9xxx range where the rest of the hadoop services are being moved to by HDFS-9427.,non_debt,-
hadoop,12811,comment_2,"Thanks Yufeng for creating this. I've linked HDFS-9427 which will change HDFS default ports. I'd like to work on this, but I'm thinking to wait for HDFS-9427 to be agreed and committed to avoid conflicts. Then I plan to change the default kms port to be in the same range (e,g, 9170 if HDFS-9427 goes with 9070). The only problem I can think of this change is backwards compatibility. Added a label to this jira.",non_debt,-
hadoop,12811,comment_3,"HDFS-9427 seems to be close to commit. I'm attaching a patch for review. KMS port is changed from 16000 to 9600. Thanks. I picked 9600 because it's in the same range as HDFS-9427. I also searched apache github, 9600 seems to not being used by other applications. (except which I feel should be Okay.) Also searched across projects for 16000, and found impala should update [some due to this change. But that should be implicit by the 'incompatible' flag anyway. :)",non_debt,-
hadoop,12811,comment_5,"LGTM +1, will commit shortly.  do you mind adding a release note? Good to have for incompatible changes.",non_debt,-
hadoop,12811,comment_6,"Committed to trunk, thanks for working on this Xiao!",non_debt,-
hadoop,12811,comment_8,"Thanks a lot ! I updated the release note, please let me know if you have any suggestions.",non_debt,-
hadoop,12829,summary,swallows interrupt exceptions,code_debt,low_quality_code
hadoop,12829,description,"The implemented in HADOOP-12107 swallows interrupt exceptions. Over in Solr/Sentry land, we run thread leak checkers on our test code, which passed before this change and fails after it. Here's a sample report: And here's an indication that the interrupt is being ignored: This is inconsistent with how other long-running threads in hadoop, i.e. PeerCache respond to being interrupted. The argument for doing this in HADOOP-12107 is given as I'm unclear on what ""spurious wakeup"" means and it is not mentioned in So, I believe this thread should respect interruption.",code_debt,low_quality_code
hadoop,12829,comment_0,"Attached a patch. Doesn't include any tests -- not sure exactly what to test. I internally tested by changing STATS_DATA_CLEANER to package-private and writing the following test: which passes with the change and hangs without it. I'm unclear on if hadoop even wants something like this, since I'm not up to speed on how hadoop handles JVM reuse for unit tests.",test_debt,lack_of_tests
hadoop,12829,comment_1,"Thank you, . I can't think of any reason why this thread should swallow without a trace. It is not performing any operations that should inherently generate INE, as far as I can see, and if someone else sends an INE we ought to... interrupt the thread. Can you include the stack trace so that this exception has a better chance of getting noticed? Also, capitalize the error? +1 pending those changes",code_debt,low_quality_code
hadoop,12829,comment_2,"I am +1 with the change proposed here. That said, I'd like to add a little more context to this. I agree that as a rule one should always restore the interrupt upon catching the in order to make the thread interruptible. However, in this particular case the issue becomes bit academic. This thread is private to the {{FileSystem}} class, meaning that one cannot easily obtain a reference to this thread and interrupt it explicitly. This thread is also a daemon thread, and as such it will not hold up the process when the process is terminating. Those two facts combined make it acceptable to have an uninterruptible daemon thread. In a non-test scenario, interrupting this thread should *not* happen. So in that sense, I don't think this is a major issue one way or another (in that sense I would recommend lowering the priority of the issue to minor). The most important goal here is to ensure that this thread does *NOT* terminate under any other condition (exceptions or errors) as that would mean a catastrophic consequence for memory, which we're still doing.",non_debt,-
hadoop,12829,comment_4,"Thanks for taking a look , will update the patch. Also agree with sjlee, lowering the priority to minor.",non_debt,-
hadoop,12829,comment_5,Updated patch for Colin's comments.,non_debt,-
hadoop,12829,comment_7,"+1. Thanks, . Good improvement. One small thing: in the future, please use different names for different patches (many people give them numbers).",non_debt,-
hadoop,12829,comment_8,"Sure thing. Every project is different (solr prefers patches to have the same name, except if it is substantially different, e.g. for a different branch).",non_debt,-
hadoop,12829,comment_9,"Committed to 2.9, thanks!",non_debt,-
hadoop,12829,comment_11,"Hello guys, I hit the same thread leak issue in FLINK-15239. In our use case, a Flink cluster is started w/o Hadoop dependencies, and accepts jobs submitted from various clients. If a job needs to access Hadoop, client will include Hadoop jars as its dependencies. On Flink side, we create a class loader to load all the dependencies of each job. As a result, we start a thread each time we load the Hadoop classes. While the change here made the thread respond to interrupt exception, it seems there's no one to interrupt the thread. So I guess the thread only dies when the JVM exits. If that's the case, is it possible to provide a way to explicitly interrupt the thread?",design_debt,non-optimal_design
hadoop,12837,summary,returns 0 for directories on S3n & S3a,non_debt,-
hadoop,12837,description,"Hi Team, We have observed an issue with the API on S3 filesystem. The method always returns 0. I googled for this however couldn't find any solution as such which would fit in my scheme of things. S3FileStatus seems to be an option however I would be using this API on HDFS as well as S3 both so can't go for it. I tried to run the job on: * Release label:emr-4.2.0 * Hadoop distribution:Amazon 2.6.0 * Hadoop Common jar: Please advise if any patch or fix available for this. Thanks, Jagdish",non_debt,-
hadoop,12837,comment_0,"If this is amazon EMR, then it's their own code talking to s3 and so can't be handled here. You'll need to take it with the EMR team via support channels. If it is pure ASF EMR, then is this an s3:// s3n:// or s3a:// URL",non_debt,-
hadoop,12837,comment_1,Thanks Steve for responding. Yes its s3n:// URL however the same code works on hdfs so the code is not EMR specific.,non_debt,-
hadoop,12837,comment_2,"Hello . Is this referring to a directory or a file? If it's a directory, then s3n always returns 0 for mtime. This is also true of s3a. I don't believe there are currently any plans in progress to change this behavior. The expected atomicity semantics of implementing directory mtime are more challenging to implement against a blob store compared to a traditional file system or HDFS. If a new file or sub-directory gets created under a directory, then users have an expectation that the corresponding update to mtime at the parent folder is atomic with respect to the file/directory creation operation. On HDFS, we can take a central lock at the NameNode to do all of the metadata manipulations as a transaction. For a blob store, this is multiple HTTP operations on different blob keys, and those multiple operations do not execute as an atomic transaction. The Azure file system does provide mtime on directories, but it does not provide atomicity of the mtime updates. (I just mention this to demonstrate that the behavior is not always consistent across different file system implementations.)",non_debt,-
hadoop,12837,comment_3,"Thanks  for sharing the details. Yes, it is referring to a directory. Would it be possible for you to suggest some workaround since there are no plans to fix it as you mentioned.",defect_debt,uncorrected_known_defects
hadoop,12837,comment_4,", can you provide more details about what exactly is not working for you due to the lack of directory mtime? The description mentions something about a job. Is that a MapReduce job? If so, how does it fail? Is there an error message or a stack trace? If I see those details, I might be able to suggest a workaround.",non_debt,-
hadoop,12837,comment_5,"Hi , I have a path filter utility which takes Path as input and returns true if the modification time of the give path is less than a specified time. Here's a method snippet for reference. The actual job takes all the paths for whom this returns true. Since the modification time for S3 based paths is returned as 0 this method returns true for all the paths specified. This results in processing unwanted data. This job doesn't fail. It just produces undesired output. Besides I have a use case where we create a backup of the directories by renaming them with the timestamp of the modification time. Also here the *filesystem* could be S3 or HDFS so need to find a generic solution. A probably workaround I can think of is writing some dummy file like _SUCCESS in each of these directories and then look for modification time of the file, however, that would be an added effort. Thanks, Jagdish",non_debt,-
hadoop,12837,comment_6,"Afraid this isn't going to work for an object store, they're not real filesystems. HADOOP-9545 proposes exposing that and adding some object store specific stuff, but it's not in yet. I would avoid doing a rename() against object store files too, that is often implemented client side...",non_debt,-
hadoop,12837,comment_7,"Yeah, I'm afraid there really isn't a feasible workaround at the file system layer right now. I think you're on the right track in trying to explore different filtering techniques on your data set so that it doesn't rely on mtime.",non_debt,-
hadoop,12837,comment_8,"Well, the above mentioned workaround of the _SUCCESS file works in my case since the content of the directory in question isn't expected to change after it is created. However in case of frequently updating directory contents that won't work. For that case one needs to dig deeper in the directory / sub-directories and determine mtime of each file and finally return the max value as mtime of the directory, however, that would be an expensive operation, particularly in case of huge directories / subdirectories. For now the workaround seems to be working for me. You may want to keep this ticket in backlog if this happens to find priority. Feel free to close if otherwise. Thanks guys for sharing your inputs. Regards, Jagdish",defect_debt,uncorrected_known_defects
hadoop,12837,comment_9,revisiting this. It's a wontfix I'm afraid. Sorry,non_debt,-
hadoop,12864,summary,Remove bin/rcc script,non_debt,-
hadoop,12864,description,"When o.a.h.record was moved, bin/rcc was never updated to pull those classes from the streaming jar.",non_debt,-
hadoop,12864,comment_0,rcc needs to have the streaming jar added to the classpath.,non_debt,-
hadoop,12864,comment_1,"Actually, it looks like bin/rcc should have been removed with HADOOP-10474.",code_debt,dead_code
hadoop,12864,comment_2,-00: * just remove rcc,non_debt,-
hadoop,12864,comment_4,Ping . We need this in for 3.x. Thanks.,non_debt,-
hadoop,12864,comment_5,"The Rcc code disappeared in HADOOP-10485, looks like this was left behind. Nice find Allen, LGTM, will commit shortly.",non_debt,-
hadoop,12864,comment_6,"Committed to trunk, thanks Allen!",non_debt,-
hadoop,12864,comment_8,Thanks Andrew!,non_debt,-
hadoop,12888,summary,Shell to disable bash and setsid support when running under JVM security manager,non_debt,-
hadoop,12888,description,"HDFS _client_ requires dangerous permission, in particular _execute_ on _all files_ despite only trying to connect to an HDFS cluster. A full list (for both Hadoop 1 and 2) is available here along with the place in code where they occur. While it is understandable for some permissions to be used, requiring {{FilePermission <<ALL FILES To make matters worse, the code is executed to initialize a field so in case the permissions is not granted, the VM fails with which is unrecoverable. Ironically enough, on Windows this problem does not appear since the code simply bypasses it and initializes the field with a fall back value ({{false}}). A quick fix would be to simply take into account that the JVM {{SecurityManager}} might be active and the permission not granted or that the external process fails and use a fall back value. A proper and long-term fix would be to minimize the use of permissions for hdfs client since it is simply not required. A client should be as light as possible and not have the server requirements leaked onto.",code_debt,low_quality_code
hadoop,12888,comment_0,"Hadoop is notoriously bad for security manager support (See HADOOP-5731) ... though being able to go securely client-side would be good. As you note: server-side requirements shouldn't impact client side. # what happens if you try to use webhdfs rather than hdfs:// ? # show us the stack trace? Shell is used client-side to detect OS, there is a languishing patch to isolate OS checks...someone needs to refresh that patch and we can get it into trunk. It's also critical on windows. Which field is causing the problem?",code_debt,low_quality_code
hadoop,12888,comment_1,"1. I haven't tried {webhdfs}} simply because it is not an option; for stronger guarantees, the code is based on {{FileContext}} and {{hdfs://}} instead of {{webhdfs://}}, even more so since it might not be always enabled. 2. It's posted in the description link: The current problematic field is",non_debt,-
hadoop,12888,comment_2,"OK. I expect the same flaw to exist elsewhere. Why not add a patch which catches the security exception and downgrades it to a {{false}}. After all, setsid isn't available, is it ?",non_debt,-
hadoop,12888,comment_3,"Sure. I've forked the 2.8 branch on github; change is available as a and Thanks,",non_debt,-
hadoop,12888,comment_4,"Not sure if it helps, but I've opened a against the Hadoop clone on Github as well. Cheers,",non_debt,-
hadoop,12888,comment_5,"attach the patch to this JIRA, use a name of the form hit the the ""submit patch"" button and it'll be trigger an automatic review. Yetus will complain about the lack of tests, but we'll have to go with that. Did it work for you in any manual tests Moving the issue from HDFS to Hadoop as its in the common module",test_debt,lack_of_tests
hadoop,12888,comment_6,Thanks. Looks like my previous comment was lost; I've improved the patch to take care of {{isBashSupported}} field as well (through Cheers,non_debt,-
hadoop,12888,comment_8,...afraid you'll have to look at the checkstyle issues. Don't worry about the test and whitespace complaints,code_debt,low_quality_code
hadoop,12888,comment_9,Fix checkstyle errors.,code_debt,low_quality_code
hadoop,12888,comment_11,Yet another update to fix checkstyle errors.,code_debt,low_quality_code
hadoop,12888,comment_13,Hi. The latest patch passes the test. The unit test is unrelated as far as I can tell:,non_debt,-
hadoop,12888,comment_14,"yes, test looks unrelated. Should that bash + security manager warning come @ info or debug?",non_debt,-
hadoop,12888,comment_15,"I went for debug: 1. to be consistent (similar to isSetsid 2. just like on windows on the other code paths, having these features disabled, on the client, has no impact. On info they would simply add noise and confusion in my opinion.",code_debt,low_quality_code
hadoop,12888,comment_16,"+1, patch applied to Hadoop 2.9+ thanks!",non_debt,-
hadoop,12923,summary,Move the test code in ipc.Client to test,architecture_debt,violation_of_modularity
hadoop,12923,description,Some code is used only by tests. Let's relocate them.,architecture_debt,violation_of_modularity
hadoop,12923,comment_0,1st patch.,non_debt,-
hadoop,12923,comment_1,Thanks  for working on this. The change looks good to me. +1 pending Jenkins.,non_debt,-
hadoop,12923,comment_3,Thanks  for reviewing the patch. I have committed this.,non_debt,-
hadoop,12934,summary,bin/mapred work for dynamic subcommands,non_debt,-
hadoop,12934,description,Do the necessary plumbing to enable mapred_subcmd_blah,non_debt,-
hadoop,12946,summary,Ensure one JVMPauseMonitor thread per JVM,non_debt,-
hadoop,12946,description,"Ensure at most one JVMPauseMonitor thread is running per JVM when there are multiple JVMPauseMonitor instances, e.g., in mini clusters. This will prevent redundant GC pause log messages while still maintaining one monitor thread running. This is a different way to fix HADOOP-12855.",design_debt,non-optimal_design
hadoop,12946,comment_0,Patch 001: * Make the thread object static. * Use a reference count to track how many instances depend on the thread. * Ensure the thread is started on the first start call. * Ensure the thread is only stopped on the last stop call * Move AbstractService specific test cases from TestJvmMetrics to a new TestAbstractService class. * Add JvmPauseMonitor test cases.,code_debt,low_quality_code
hadoop,12946,comment_2,Patch 002: * Add Apache license,non_debt,-
hadoop,12946,comment_4,Opt for HADOOP-12908.,non_debt,-
hadoop,12952,summary,/BUILDING example of zero-docs dist should skip javadocs,non_debt,-
hadoop,12952,description,"The examples for building distributions include how to create one without any documentation. But it includes the javadoc stage in the build, which is very slow. Adding skips that phase, and helps round out the parameters to a build.",code_debt,slow_algorithm
hadoop,12952,comment_0,Patch 001. Tested by pasting into a command line,non_debt,-
hadoop,12952,comment_2,"+1, thanks Steve!",non_debt,-
hadoop,12952,comment_3,Committed this to trunk and branch-2. Thanks  for the contribution!,non_debt,-
hadoop,12986,summary,"Hortonworks Data Flow (aka, NiFi)",non_debt,-
hadoop,12986,description,"1: Hortonworks Data Flow (aka, NiFi). When running the command ""bin/nifi.sh install"", it will setup the correct service file for you so that nifi will start on boot. When you look at the file, especially the ""install"" section: install() { SVC_NAME=nifi if [ ""x$2"" != ""x"" ] ; then SVC_NAME=$2 fi cp ""$0"" ""${SVC_FILE}"" sed -i ""${SVC_FILE}"" sed -i ""${SVC_FILE}"" rm -f ln -s rm -f ln -s echo ""Service ${SVC_NAME} installed"" } The problem above is that the startup and shutdown files (the ""S"" and ""K"" files) are created in a directory ""/etc/rc2.d"", however this directory exists only on RHEL. On SUSE this directory is slightly different, /etc/init.d/rc2.d So when attempting to setup the services file (for bootup purposes), the above command fails on SUSE. Worse, no error checking is performed and it will actually print a successful message!",non_debt,-
hadoop,12986,comment_0,As per your advice. Please bear with me any missing info as I am new to this. thx.,non_debt,-
hadoop,12986,comment_1,Please file this under Nifi project:,non_debt,-
hadoop,13011,summary,Clearly Document the Password Details for Keystore-based Credential Providers,non_debt,-
hadoop,13011,description,HADOOP-12942 discusses the unobviousness of the use of a default password for the keystores for keystore-based credential providers. This patch adds documentation to the that describes the different types of credential providers available and the password management details of the keystore-based ones.,non_debt,-
hadoop,13011,comment_0,"Initial patch - attempts to clearly document the password management details of the keystore-based credential providers, the options available and mechanics required for each.",non_debt,-
hadoop,13011,comment_2,"looks reasonable, but could you `` out the provider classes, like",non_debt,-
hadoop,13011,comment_3,"Sure thing, thanks for the review,",non_debt,-
hadoop,13011,comment_4,v002 addresses Steve's comments and fixes the type from the spanish Crazy to the :),non_debt,-
hadoop,13011,comment_6,Based on offline conversation with  - we agreed to add more language around default passwords and the ability to write tooling.,non_debt,-
hadoop,13011,comment_8,FYI - I plan to commit this doc only patch today.,non_debt,-
hadoop,13011,comment_9,I realized that I've yet to get a +1 on this yet. - when you have a chance can you give this a glance?,non_debt,-
hadoop,13011,comment_10,"line 129 "" This is done by leveraging the Hadoop filesystem abstraction within the provider implementation."" can this be rephrased into something concise without the word ""leveraging""? line 140, backquote line 14, same for `cat 158. Remove the `you`; make it less personal. ""Organisations ? users? "" can you quote ""side files"" not a term I'd encountered before",documentation_debt,low_quality_documentation
hadoop,13011,comment_11,"Sure, thanks",non_debt,-
hadoop,13011,comment_12,v004 addresses latest review feedback.,non_debt,-
hadoop,13011,comment_14,1,non_debt,-
hadoop,13011,comment_15,"Thanks, I will commit this to branch-2, branch-2.8 and trunk today.",non_debt,-
hadoop,13011,comment_16,"Hey Larry, did you forget to resolve this JIRA? Status is still Patch Available.",non_debt,-
hadoop,13011,comment_17,"I committed this to trunk, branch-2 and branch-2.8. Thanks for the reviews",non_debt,-
hadoop,13030,summary,Handle special characters in passwords in KMS startup script,code_debt,low_quality_code
hadoop,13030,description,{{kms.sh}} currently cannot handle special characters.,code_debt,low_quality_code
hadoop,13030,comment_0,"Attached patch 01 to fix the issue. Generally the problem is two fold: - The {{sed}} line need to escape its sed-special chars({{\/&}}). - Since the output is an XML, xml-special chars also need to be escaped({{&'""< The tarball attached is how I verified the fix. ({{test.sh}} contains only the relevant part of the {{kms.sh}} script in patch 01) Step I used to verify: - extract tarball and cd to the dir - {{source ./test-source.sh}} - {{./test.sh}} - Open up in a browser and compare that to the passwords given in test-source.sh. They're identical. Special thanks to  for helping explain the issue and provide test data + fun reproduction.",non_debt,-
hadoop,13030,comment_2,Patch 2 fixes the shellcheck warnings.,code_debt,low_quality_code
hadoop,13030,comment_4,"Patch 3 adds my first part of comment to the function, for better readability.",code_debt,low_quality_code
hadoop,13030,comment_6,"+1 LGTM, thanks Xiao for working on this. I've committed this to trunk. branch-2 and before don't have the shell script rewrite, so are somewhat different. Xiao, do you mind preparing a branch-2 patch too?",non_debt,-
hadoop,13030,comment_8,"Thank you ! Attached a patch based on branch-2.8. Verified the same way, works. (had to add {{export to test-source.sh due to kms.sh diff).",non_debt,-
hadoop,13030,comment_10,"Backport patches look good, committed to branch-2 and branch-2.8. Thank you Xiao for the contribution!",non_debt,-
hadoop,13030,comment_11,Pretty much this exact same code exists in httpfs.sh ....,code_debt,duplicated_code
hadoop,13030,comment_12,"Thanks  for pointing this out, I wasn't aware of that.. I created HADOOP-13077 to fix it and attached a patch, let's move the following work there.",non_debt,-
hadoop,13039,summary,Add documentation for configuration property for controlling maximum RPC message size.,non_debt,-
hadoop,13039,description,The RPC server enforces a maximum length on incoming messages. Messages larger than the maximum are rejected immediately. The maximum length can be tuned by setting configuration property but this is not documented in core-site.xml.,documentation_debt,outdated_documentation
hadoop,13039,comment_0,This is a follow-up from discussion on HDFS-10312.,non_debt,-
hadoop,13039,comment_1,It'd be good to add a warning that this setting should rarely need to be changed. It merits investigating whether the cause of the long RPC messages can be fixed instead.,non_debt,-
hadoop,13039,comment_2,Initial v0 patch for feedback. Thanks.,non_debt,-
hadoop,13039,comment_4,"Hi , I think we can remove this string ""as potentially malicious"". The patch looks good otherwise. Thanks!",code_debt,dead_code
hadoop,13039,comment_5,"Per-offline discussion with , the v1 patch refined the documentation to make it clearer.",documentation_debt,low_quality_documentation
hadoop,13039,comment_6,"+1 pending Jenkins, thanks .",non_debt,-
hadoop,13039,comment_7,"+1 from me too. Thank you, .",non_debt,-
hadoop,13039,comment_8,Don't know why Jenkins was not triggered. Will upload the same patch again.,non_debt,-
hadoop,13039,comment_10,Seems the precommit build is failing. Will file a bug for Yetus if it happens again.,non_debt,-
hadoop,13039,comment_12,This is for adding missing documentation for config property. No code change was involved.,documentation_debt,outdated_documentation
hadoop,13039,comment_13,+1 I committed this for 2.7.3. Thanks for the contribution  and thanks  for the code review.,non_debt,-
hadoop,13039,comment_15,Thank you  and  for your review and comments.,non_debt,-
hadoop,13039,comment_16,Closing the JIRA as part of 2.7.3 release.,non_debt,-
hadoop,13051,summary,Add Glob unit test for special characters,non_debt,-
hadoop,13051,description,"On {{branch-2}}, the below is the (incorrect) behaviour today, where paths with special characters get dropped during globStatus calls: Whereas trunk has the right behaviour, subtly fixed via the pattern library change of HADOOP-12436: (I've placed a ^M explicitly to indicate presence of the intentional hidden character) We should still add a simple test-case to cover this situation for future regressions.",test_debt,lack_of_tests
hadoop,13051,comment_0,Patch that fails in {{branch-2}} but passes in trunk after the mentioned fix.,non_debt,-
hadoop,13051,comment_4,+1. LGTM. Thanks Harsh. Please feel free to commit,non_debt,-
hadoop,13051,comment_5,+1 LGTM. Very useful patch. Could some one commit?,non_debt,-
hadoop,13051,comment_6,Thanks for the patch Harsh and the review John! Committed to trunk.,non_debt,-
hadoop,13051,comment_7,Thanks  for committing it promptly.,non_debt,-
hadoop,13054,summary,Use proto delimited IO to fix tests broken by HADOOP-12563,non_debt,-
hadoop,13054,description,"HADOOP-12563 broke some unittests (see that ticket, in comments). Switching the proto buffer read/write methods to ""writeDelimitedTo"" and ""readDelimitedFrom"" seems to fix things up.",non_debt,-
hadoop,13054,comment_0,Bug reports on failing tests in comments of HADOOP-12563,non_debt,-
hadoop,13054,comment_1,patch to fix protobuf IO problem in Credentials class,non_debt,-
hadoop,13054,comment_3,12563 was reverted. a new patch was submitted there including this edit. this ticket is obsolete.,non_debt,-
hadoop,13063,summary,Incorrect error message while setting dfs.block.size to wrong value,code_debt,low_quality_code
hadoop,13063,description,Execute in Hive See logs We need to have more informative error message here,code_debt,low_quality_code
hadoop,13063,comment_1,Please check trunk and create a patch against it if the issue is still present.,non_debt,-
hadoop,13109,summary,Add ability to edit existing token file via dtutil -alias flag,non_debt,-
hadoop,13109,description,"The first iteration of the dtutil command did not provide an operation to edit the service field on a token that has already been fetched to a file. This is a necessary feature for users who have copied a token file out of a cluster that does not support the dtutil command (hence, the token could not be aliased during get).",non_debt,-
hadoop,13109,comment_0,this patch adds new feature and unit test,non_debt,-
hadoop,13109,comment_2,patch requires whitespace diff to apply...,non_debt,-
hadoop,13109,comment_4,"Hindsight 20/20, but I'm sort of regretting how the term 'alias' was used in dtutil. Oh well. +1. Thanks! I'll commit this trunk here in a bit.",non_debt,-
hadoop,13138,summary,Unable to append to a SequenceFile with Compression.NONE.,non_debt,-
hadoop,13138,description,"Hi, I'm trying to use the append functionnality to an existing _SequenceFile_. If I set _Compression.NONE_, it works when the file is created, but when the file already exists I've a by the way it works if I specify a compression with a codec. The following exeception is thrown when the file exists because compression option is checked: This is due to the *codec* which is _null_: Thansk Mickal",non_debt,-
hadoop,13138,comment_0,Attaching the patch for the above Problem. Thanks  for reporting the issue.,non_debt,-
hadoop,13138,comment_2,", thank you for the patch. This looks good to me. I think it will be ready to go after addressing the Checkstyle nitpicks.",code_debt,low_quality_code
hadoop,13138,comment_3,Attaching the patch with fixed checkstyle nits,code_debt,low_quality_code
hadoop,13138,comment_6,", thank you for the patch. +1. I have committed this to trunk, branch-2 and branch-2.8.",non_debt,-
hadoop,13138,comment_8,Thanks Chris.,non_debt,-
hadoop,13138,comment_9,Thanks Chris,non_debt,-
hadoop,13158,summary,might throw due to null cannedACL.,non_debt,-
hadoop,13158,description,The {{cannedACL}} field of {{S3AFileSystem}} can be {{null}}. The {{toString}} implementation has an unguarded call to so there is a risk of,code_debt,low_quality_code
hadoop,13158,comment_0,"I missed this in my code review of HADOOP-13028. I saw this happen in a test run recently. Here is a patch to fix it. I reviewed the whole {{toString}} method for similar bugs, and {{cannedACL}} is the only problem. would you please review?",non_debt,-
hadoop,13158,comment_2,"Chris, we saw this at the same time, I suspect. I've fixed this as part of HADOOP-13130 patch 005. If you could review that...",non_debt,-
hadoop,13158,comment_3,"+1, stuck this patch in standalone; I'll fix up my HADOOP-13130 patch to handle it",non_debt,-
hadoop,13165,summary,S3AFileSystem toString can throw NPE when cannedACL is not set,non_debt,-
hadoop,13165,description,"In case cannedACL is not set, it could throw NPE. This was observed mainly in debugging (not in regular code path). Creating this jira just as a placeholder to fix it later.",non_debt,-
hadoop,13165,comment_0,HADOOP-13130 patch 005 already found and fixed that,non_debt,-
hadoop,13233,summary,help of stat is confusing,code_debt,low_quality_code
hadoop,13233,description,"%b is actually printing the size of a file in bytes, while in help it says filesize in blocks.",non_debt,-
hadoop,13233,comment_0,Moved to Hadoop Common project because this code is in hadoop-common module.,architecture_debt,violation_of_modularity
hadoop,13233,comment_1,"Hi ,mind if I take it over?",non_debt,-
hadoop,13233,comment_2,"Hi , you can take it over. Thanks.",non_debt,-
hadoop,13233,comment_4,"TestKDiag succeeded on my machine, I suspect it's a random failure. checkstyle could be fixed, but it would either look weird (indented differently than the other lines in Stat.DESCRIPTION) or I would have to reindent a few more lines.",code_debt,low_quality_code
hadoop,13233,comment_5,Thanks  for patch.. Mostly looks good to me. Would you update as well? I'm +1 (non-binding) if that is addressed.,non_debt,-
hadoop,13233,comment_6,"Thanks for the feedback , I've uploaded the new patch.",non_debt,-
hadoop,13233,comment_8,+1 No need to fix checkstyle warning. Failed tests not reproducible.,code_debt,low_quality_code
hadoop,13233,comment_9,"Committed the patch into trunk and branch-2. There are quite a few conflicts in branch-2.8 and below. Thanks to  for the patch,  for filing the jira and  for the review!",non_debt,-
hadoop,13353,summary,LdapGroupsMapping getPassward shouldn't return null when IOException throws,non_debt,-
hadoop,13353,description,"When IOException throws in getPassword(), getPassword() return null String, this will cause setConf() throws when check isEmpty() on null string.",non_debt,-
hadoop,13353,comment_1,The fix looks good to me. Thanks for the contribution. Would you also like to add a regression test?,test_debt,lack_of_tests
hadoop,13353,comment_2,"I see that when the IOException is thrown, the log message does not preserve its stacktrace. Could you also update the fix to include the stacktrace? Basically, change it to:",code_debt,low_quality_code
hadoop,13353,comment_3,"Thanks for the initial patch. I created a v02 patch based on the initial one, adding a regression test, removed redundant code and improved error logging.",code_debt,complex_code
hadoop,13353,comment_5,Sorry for late reply because I was on vacation. Thanks  for the v2 patch. But some code about datanode blockpool in the v2 patch seems irrelevant ?,code_debt,low_quality_code
hadoop,13353,comment_6,My bad. Sorry for being sloppy. That was for another patch.,non_debt,-
hadoop,13353,comment_7,Posting v03 patch which removes code not belong to this patch.,non_debt,-
hadoop,13353,comment_9,the test failure is unrelated. The mvnstyle error is due to an unrelated issue.,non_debt,-
hadoop,13353,comment_10,Hi could you review the patch? Or fee free to submit a patch directly. Thanks!,non_debt,-
hadoop,13353,comment_11,It LGTM. +1. Thanks,non_debt,-
hadoop,13353,comment_12,"Committed this to trunk, branch-2 and branch-2.8. Thanks  for reviewing it and for contributing the patch!",non_debt,-
hadoop,13353,comment_14,", Branch 2.8 is failing to compile after commit for this jira",non_debt,-
hadoop,13353,comment_15,"I am not able to revoke the jira, hope you can take care of it or else i can raise a new jira!",non_debt,-
hadoop,13353,comment_16,thanks for reporting this! I'll revert this commit now.,non_debt,-
hadoop,13353,comment_17,Attach version 2.8 patch,non_debt,-
hadoop,13353,comment_18,"Thanks for the patch , Seems to pass when i test locally. Will wait for jenkins run and commit it!",non_debt,-
hadoop,13353,comment_19,Committed it to Branch-2.8,non_debt,-
hadoop,13353,comment_20,Thanks!,non_debt,-
hadoop,13365,summary,Convert _OPTS to arrays to enable spaces in file paths,non_debt,-
hadoop,13365,description,"While we are mucking with all of the _OPTS variables, this is a good time to convert them to arrays so that filesystems with spaces in them can be used.",design_debt,non-optimal_design
hadoop,13365,comment_0,"I think the plan of attach should be: 1. Let users keep \_OPTS in hadoop-env.sh, etc. 2. Detect if it's not already an array by checking \_OPTS[1]. 3. If not, convert to array using eval (bash 3.x compat) This way, if users want to use \_OPTS in array format (which, for long lists of options, is significantly easier to work with), they can.",design_debt,non-optimal_design
hadoop,13365,comment_1,"Marking HADOOP-13341 as required, since it consolidates a good chunk of the _OPTS handling.",non_debt,-
hadoop,13365,comment_2,"-00: * first pass There's a lot happening here, so let's go through it: * adding some helper routines in hadoop-functions to: ** convert strings to arrays if the array doesn't already exist ** add to arrays based upon a key to dedupe * convert almost all internal users of HADOOP_OPTS and xyz_OPTS to use the array form * update some pre-existing doc references * add several unit tests * rewrite existing unit tests to use the array form To do: * more/better docs * figure out what to do about catalina? * get HADOOP-13341 committed, since this code is several times larger without it * more testing",documentation_debt,low_quality_documentation
hadoop,13365,comment_4,"One of the feedbacks from HADOOP-13341 from was that the docs should include an example of _OPTS ordering. We have an opportunity to de-dupe here. If this JIRA does end up de-duping, then that should be documented. If it doesn't de-dupe, then the docs should specifically give an example of ordering.",documentation_debt,low_quality_documentation
hadoop,13365,comment_6,-01: * quick rebase,non_debt,-
hadoop,13386,summary,Upgrade Avro to 1.8.x or later,non_debt,-
hadoop,13386,description,Avro 1.8.x makes generated classes serializable which makes them much easier to use with Spark. It would be great to upgrade Avro to 1.8.x,architecture_debt,using_obsolete_technology
hadoop,13386,comment_0,"successor to HADOOP-12527; given the discourse there I'm going to change the title of that one and close this as a duplicate. see also: for some coverage of the problem. We aren't scared of Avro, but do need to take care of its transitive dependencies. Help there showing what they are and testing all down the stack is very much appreciated",test_debt,low_coverage
hadoop,13386,comment_1,Reopening since HADOOP-14992 upgraded Avro to 1.7.7. This jira requested an upgrade to 1.8.x.,non_debt,-
hadoop,13386,comment_2,I've added you as a contributor and assigned this to you. Thanks for creating the pull request. Could you please summarize the testing you've done to validate this change? Also Steve had a question about transitive dependencies.,non_debt,-
hadoop,13386,comment_3,"Hi  Regarding the dependency, it is not really transitive dependency but run time dependency: Sqoop is using avro 1.8.1 and parquet 1.8.x for a while and Hadoop 3.1.1 contains avro 1.7.7. During the execution avro 1.7.7 will be used and the following runtime error occured when sqoop tried to import to hive in parquet format: So the version update what is defined by this task would fix our problem. As validation we executed the standard set of hdp-3.1.0 system tests with Hadoop, Hive, Sqoop, HBase etc components and we haven't found any issue in connection with the version update.",non_debt,-
hadoop,13386,comment_4,"# What extra transitive dependencies does this this change? # Has anyone tested code built with a version of hadoop *before* the upgrade running on a hadoop cluster with the upgrade? It's the changes in generated code which is going to break things here, just as with protobuf. This doesn't mean I don't support the upgrade I do but I want to know how incompatible a change it is.",non_debt,-
hadoop,13386,comment_5,"+as usual, set versions on affects/target, etc.",non_debt,-
hadoop,13386,comment_6,Now I'm +1 for Avro 1.9.0 in Hadoop 3.3.x. Avro 1.9.0 removed the dependency of Jackson 1.x. Really nice.,non_debt,-
hadoop,13386,comment_7,"1.9.0 has removed a couple of dependencies, including upgrading to Jackson 2.x. We also got rid of paranamer. Avro does only depend on Jackson, Jackson-Databind and Commons-compress. You can see the dependencies here:",build_debt,over-declared_dependencies
hadoop,13386,comment_8,"OK. We will need to tag as incompatible for precompiled avro code. But it is better, and with fewer dependencies, actually an improvement on what we get today.",build_debt,over-declared_dependencies
hadoop,13386,comment_9,Do you have any examples of the incompatibilities? We can recompile the Avro classes. There are already deprecated methods removed in 1.9.0:,non_debt,-
hadoop,13386,comment_10,"Real issue is that it forces people downstream to recompile their code too, if they have this version of avro on their CP. If they exclude it, then you can't use the compiled classes in Hadoop -but there aren't that many. Maybe: tag as incompatible and explain what to do: recompile or exclude",design_debt,non-optimal_design
hadoop,13529,summary,Do some code refactoring,non_debt,-
hadoop,13529,description,1. argument and variant naming 2. abstract utility class 3. add some comments 4. adjust some configuration 5. fix TODO 6. remove unnecessary commets 7. some bug fix 8. add some unit test,code_debt,low_quality_code
hadoop,13529,comment_4,", all unit tests passed:",non_debt,-
hadoop,13529,comment_5,some incremental updates: 1. add docs 2. remove UserInfo 3. remove comments for overrie function 4. codestyle issue,code_debt,low_quality_code
hadoop,13529,comment_7,Unit tests is OK:,non_debt,-
hadoop,13529,comment_8,Thanks for your review and offline discussion,non_debt,-
hadoop,13529,comment_10,I have merged this. Thanks!,non_debt,-
hadoop,13638,summary,KMS should set UGI's Configuration object properly,non_debt,-
hadoop,13638,description,"We found that the Configuration object in UGI in KMS server is not initialized properly, therefore it does not load core-site.xml from This becomes a problem when the Hadoop cluster uses LdapGroupsMapping for group resolution, because the UGI in KMS falls back to the default (defined in core-default.xml) and is thus not consistent with the Hadoop cluster.",non_debt,-
hadoop,13638,comment_0,"v01. a one-liner fixer to set UGI configuration correctly when KMS starts. I don't think it's easy to unit test it, but I've manually verified this fix works on a CDH 5.7.0 cluster.",non_debt,-
hadoop,13638,comment_1,+1 pending jenkins. Nice work Wei-Chiu! This is clearly a bug and I think manual testing is fine. Could you elaborate a little bit on the manual testings you've done? Thanks.,non_debt,-
hadoop,13638,comment_2,"Here's what I did to verify the patch: Configure a CDH Hadoop cluster using LdapGroupsMapping and KMS. The KMS ACL rule denies ""group1"" from decrypting the key. I added additional log at {{Groups#<init>}} to print the class name of the GroupMapping resolution object. Subsequently, I started KMS and do a few operations in a HDFS encryption zone to observe the class name printed.",non_debt,-
hadoop,13638,comment_4,"Thanks for the explanation Wei-Chiu, sgtm. Failed test looks related.",non_debt,-
hadoop,13638,comment_5,"The tests failed because sets a global variable, and each test has a client side configuration which is overwritten by it. Need to find a way to avoid interference between client and server.",non_debt,-
hadoop,13638,comment_6,Thanks for looking into the test Wei-Chiu. Looking forward to the fix. This will also give audience more time to review this. :),non_debt,-
hadoop,13638,comment_7,"v02: The Configuration object is shared by both KMS client and server in unit tests because UGI gets/sets it to a static variable. I don't see a way to isolate client's configuration from server. Fortunately, UGI-sensitive configuration names are independent between KMS client and server. As a workaround, make sure the client configurations are copied to the server's so that client can read them.",non_debt,-
hadoop,13638,comment_9,Checkstyle warnings can't be removed unless we refactor test methods.,code_debt,low_quality_code
hadoop,13638,comment_10,Thanks  for the new patch. Could you explain why do we need {{newConf = new instead of the default ctor?,non_debt,-
hadoop,13638,comment_11,That was copied from an existing test case.,code_debt,duplicated_code
hadoop,13638,comment_12,"Checkstyle is out-of-scope as Wei-Chiu mentioned. +1 to patch 2, committing this.",non_debt,-
hadoop,13638,comment_13,"I have committed this to trunk, branch-2 and branch-2.8. Let me know if you feel this should go to earlier branches, from the Target Versions field. Thanks for the contribution !",non_debt,-
hadoop,13638,comment_15,Thanks very much  for reviewing and committing the patch!,non_debt,-
hadoop,13671,summary,Fix in trunk build.,non_debt,-
hadoop,13671,description,The version 2.7 depends on which uses bcel 5.2. This does not work well with the new lamda expression. The 2.9 depends on which works around this problem by using the custom release of bcel 6.0.,non_debt,-
hadoop,13671,comment_0,The current trunk build has this:,non_debt,-
hadoop,13671,comment_2,Verified that the update pulls the new version of maven-shared-jar during build and the warning no longer appears.,non_debt,-
hadoop,13671,comment_3,1,non_debt,-
hadoop,13671,comment_4,Thanks . +1 Committing to trunk.,non_debt,-
hadoop,13671,comment_5,"Thanks, Steve and Eric for reviews.",non_debt,-
hadoop,13730,summary,"After 5 connection failures, yarn stops sending metrics graphite until restarted",non_debt,-
hadoop,13730,description,"We've had issues in production where metrics stopped. We found the following in the log files: 2016-09-02 21:44:32,493 WARN Error sending metrics to Graphite Broken pipe at Method) 2016-09-03 00:03:04,335 WARN Error sending metrics to Graphite Broken pipe at Method) 2016-09-03 00:20:35,436 WARN Error sending metrics to Graphite Connection timed out at Method) 2016-09-03 00:22:48,862 WARN Error sending metrics to Graphite Broken pipe at Method) 2016-09-03 00:24:00,270 WARN Error sending metrics to Graphite Broken pipe at Method) 2016-09-03 00:24:41,987 INFO Number of transactions: 482 Total time for transactions(ms): 9 Number of transactions batched in Syncs: 3 Number of syncs: 355 SyncTimes(ms): 342 2016-09-03 00:25:39,772 WARN Error sending metrics to Graphite Connection reset The last error was: 2016-09-03 01:13:51,619 WARN Error sending metrics to Graphite Broken pipe at Method) 2016-09-03 01:15:58,828 ERROR Too many connection failures, would not try to connect again.",non_debt,-
hadoop,13730,comment_0,"Keep on trying forever, graphite might come back.",non_debt,-
hadoop,13730,comment_2,"It's good to have retries to deal with transient outages, and race conditions in cluster launch/teardown, but at the same time, retrying forever can have adverse consequences (including, if the stack is logged, log overflows. nothing like coming in to find a 20GB error log) I'd think about using the retry policies in they are there for failures.",design_debt,non-optimal_design
hadoop,13730,comment_3,Drop 2.8 in fix version. We should only set this value when patch get committed.,non_debt,-
hadoop,13730,comment_5,"Is this still on target for 2.9.0 ? If not, can we we push this out to the next major release ?",non_debt,-
hadoop,13730,comment_6,Pushing it out from 2.9.0 due to lack of activity. Feel free to revert if required.,non_debt,-
hadoop,13730,comment_8,Pushing it out from 2.9.0 due to lack of recent activity. Feel free to revert if required.,non_debt,-
hadoop,13730,comment_9,"Bulk update: moved all 3.2.0 non-blocker issues, please move back if it is a blocker.",non_debt,-
hadoop,13732,summary,Upgrade OWASP dependency-check plugin version,non_debt,-
hadoop,13732,description,"For reasons I don't fully understand, the current version (1.3.6) of the OWASP dependency-check plugin produces an essentially empty report on trunk (3.0.0). After some research, it appears that this plugin has undergone significant work in the latest version, 1.4.3. Upgrading to this version produces the expected full report. The only gotcha is that a new-ish version of maven is required. I'm using 3.2.2; I know that 3.0.x fails with a strange error. This plugin was introduced in HADOOP-13198.",non_debt,-
hadoop,13732,comment_0,Ping,non_debt,-
hadoop,13732,comment_1,"Hi Mike, if we need to use a more recent version of Maven, then we also need to update BUILDING.txt. Could you comment on the availability of the required Maven version on a few common OSs? e.g. RHEL6, 7, Ubuntu 12/14/16.",documentation_debt,outdated_documentation
hadoop,13732,comment_2,"I'd have to make a dependency-check specific note in BUILDING.txt, which seems a little awkard. (The normal build isn't affected, of course.) I'll see what I can do. My only alternative idea is a comment around this plugin in pom.xml. I do agree it needs to be documented somewhere. * I don't even think that maven is _available_ on RHEL 6.6 * My RHEL 7.2 machine looks like it would use version 3.0.5-16 * My Ubuntu 16.04 machine is using 3.3.9 * Looks like Ubuntu 14.04 uses 3.0.5-1 The maven release history page is at",documentation_debt,outdated_documentation
hadoop,13732,comment_5,"I ran the checker locally, LGTM. will commit shortly.",non_debt,-
hadoop,13732,comment_6,"Committed to trunk, thanks Mike for the find and fix!",non_debt,-
hadoop,13768,summary,AliyunOSS: handle the failure in the batch delete operation `deleteDirs`.,non_debt,-
hadoop,13768,description,"Note in Aliyun OSS SDK, has 1000 objects limit. This needs to improve {{deleteDirs}} operation to make it pass when more objects than the limit to delete.",non_debt,-
hadoop,13768,comment_0,Result of unit test:,non_debt,-
hadoop,13768,comment_3,Take a look please.,non_debt,-
hadoop,13768,comment_4,"Looking at the following codes: 1. Please give {{l}} a more readable name. 2. Can you give some comments to explain some bit about the procedure? I (probably others) wouldn't know why it's like that without querying the SDK's manual. I know it now, there're 2 modes in the operation, one mode to return successfully deleted objects, and the other returning the deleting-failed objects. You're using the latter, and use a loop to try some times to delete and delete the failed-to-delete objects.",code_debt,low_quality_code
hadoop,13768,comment_5,get it,non_debt,-
hadoop,13768,comment_6,"assuming that the file limit is always1000, why not just list the path in 1000 blocks and issue delete requests in that size. There are ultimate limits to the size of responses in path listings (max size of an HTTP request), and inevitably heap problems well before then.",design_debt,non-optimal_design
hadoop,13768,comment_7,"Make sense. I made a mistake, and it is not possible to delete more than 1000 objects. Maybe, this jira should be to handle another issue, i.e. the failure in the batch delete operation. I will update the patch as soon as possible.",non_debt,-
hadoop,13768,comment_9,Result of unit tests:,non_debt,-
hadoop,13768,comment_12,cc,non_debt,-
hadoop,13768,comment_13,The latest patch LGTM and +1.,non_debt,-
hadoop,13768,comment_14,Committed to trunk. Thanks  for the contribution and for the review.,non_debt,-
hadoop,13770,summary,swallowed an interrupted exception,non_debt,-
hadoop,13770,description,"creates a bash shell command to verify if the system supports bash. However, its error message is misleading, and the logic should be updated. If the shell command throws an IOException, it does not imply the bash did not run successfully. If the shell command process was interrupted, its internal logic throws an which is a subclass of IOException. An example of it appeared in a recent jenkins job The test logic in starts a thread, wait it for 1 second, and interrupt the thread, expecting the thread to terminate. However, the method swallowed the interrupt, and therefore failed. The original design is not desirable, as it swallowed a potential interrupt, causing to fail. Unfortunately, Java does not allow this static method to throw exception. We should removed the static member variable, so that the method can throw the interrupt exception. The node manager should call the static method, instead of using the static member variable. This fix has an associated benefit: the tests could run faster, because it will no longer need to spawn a bash process when it uses a Shell static method variable (which happens quite often for checking what operating system Hadoop is running on)",design_debt,non-optimal_design
hadoop,13770,comment_0,Rev01: catch and log that the exception is due to interrupt.,non_debt,-
hadoop,13770,comment_2,"Rev02: the original design is not desirable, as it swallowed a potential interrupt, causing to fail. Unfortunately, Java does not allow this static method to throw exception. The rev02 removed the static member variable, so that the method can throw the interrupt exception.",design_debt,non-optimal_design
hadoop,13770,comment_4,Both test failures look unrelated to this patch,non_debt,-
hadoop,13770,comment_5,"+1 (non-binding), this patch LGTM.",non_debt,-
hadoop,13770,comment_6,"+1, kicked Jenkins again to get a fresh run. I also marked this as a Blocker for 2.8. We will not be allowed to remove the public member of a Public class once the public member ships in a release or we risk breaking backwards compatibility. Fortunately this public member hasn't been released yet, so we have a chance to fix it cleanly.",design_debt,non-optimal_design
hadoop,13770,comment_8,"Moving this back to HADOOP since the change is primarily against that project and is in response to another HADOOP JIRA that broke this, HADOOP-12441. Committing this.",non_debt,-
hadoop,13770,comment_9,"Thanks to  for the contribution and to  for additional review! I committed this to trunk, branch-2, and branch-2.8.",non_debt,-
hadoop,13795,summary,Skip in,non_debt,-
hadoop,13795,description,"Swift object store does not honor directory permissions, thus we should skip in similar to",non_debt,-
hadoop,13795,comment_0,Patch 001: * Skip Skip in,non_debt,-
hadoop,13795,comment_2,Test it manually with a dummy {{auth-keys.xml}} for Swift. The test output did show the test case skipped.,non_debt,-
hadoop,13795,comment_3,1,non_debt,-
hadoop,13795,comment_4,"Thanks  for the creating the jira and fixing the issue. Also thanks for reporting and reviewing. +1 from me too, just committed this to trunk.",non_debt,-
hadoop,13795,comment_5,Thanks  and,non_debt,-
hadoop,13975,summary,Allow DistCp to use MultiThreadedMapper,non_debt,-
hadoop,13975,description,"Although distcp allow users to control the parallelism via number of mappers, sometimes it's desirable to run fewer mappers but more threads per mapper. Since distcp is network bound (either by throughput or more frequently by latency of creating connections, opening files, reading/writing files, and closing files), this can make each mapper much more efficient. When WebHDFS protocol is used either as source or target, this MultiThreaded approach can make the HTTP connection reuse (to the NameNode) more efficient as well. In that way, a lot of resources can be shared so we can save memory and connections to NameNode.",design_debt,non-optimal_design
hadoop,13975,comment_0,"Example usage: bin/hadoop distcp -prbugp -m 8 -numThreadsPerMap 16 hdfs://targethdfs/ This uses 8 mapper, each of which with 16 threads, to do distcp. It's equivalent of running 128 mappers except this is more efficient (as long as we don't hit the resource bottleneck on a single machine). Note that is needed if you only update the client-side hadoop-tools jar but not the server (YARN nodemanager) side yet.",non_debt,-
hadoop,13975,comment_2,Fixed checkstyle issues.,code_debt,low_quality_code
hadoop,13975,comment_3,Fixed checkstyle issues.,code_debt,low_quality_code
hadoop,13975,comment_5,Added a message to for easy debugging.,code_debt,low_quality_code
hadoop,13975,comment_7,"This seems very useful! Thanks for working on this. Why is there both and in {{OptionsParser}}? It seems only one of them is used. Additionally the error message is incorrect in both of them, with one referring to {{MAX_MAPS}} and one referring to",non_debt,-
hadoop,13975,comment_8,Fixed the extra function and error message.,non_debt,-
hadoop,13975,comment_10,Fixed a bug in DynamicInputChunk that interferes with MultiThreadedMapper,non_debt,-
hadoop,13990,summary,Document KMS usage of CredentialProvider API,non_debt,-
hadoop,13990,description,Document that HADOOP-13597 enabled support for Credential Provider API.,non_debt,-
hadoop,13990,comment_0,Patch 001 * Update and index.md.vm,non_debt,-
hadoop,13990,comment_2,"+1, committing this.",non_debt,-
hadoop,13990,comment_3,Committed to trunk. Thanks  for the contribution.,non_debt,-
hadoop,13990,comment_4,Thanks  for the review and commit!,non_debt,-
hadoop,13991,summary,Retry management in NativeS3FileSystem to avoid file upload problem,non_debt,-
hadoop,13991,description,"NativeS3FileSystem does not support any retry management for failed uploading to S3. If due to socket timeout or any other network exception, file uploading to S3 bucket fails, then uploading fails and temporary file gets deleted. Connection reset Source) Source) This can be solved by using asynchronous retry management. We have made following modifications to NativeS3FileSystem to add the retry management, which is working fine in our product system, without any uploading failure:",design_debt,non-optimal_design
hadoop,13991,comment_0,"Musaddique thank your for your post and details on a fix. I'm sorry to say we aren't going to take this. That's not because there's anything wrong with it, but because we've stopped doing any work on s3n other than any emergency security work, putting all our effort into S3a. Leaving s3n alone means that we have a reference s3 connector that is pretty much guaranteed not to have any regressions, while in s3a we can do more leading edge stuff. S3a does have retry logic, a lot built into the Amazon S3 library itself, with some extra bits to deal with things that aren't retried that well (e.g. final commit of a multipart upload). # please switch to s3a as soon as you can. If you are using Hadoop 2.7.3, its stable enough for use. # and, if you want to improve s3a, please get involved on that code, ideally look at the work in HADOOP-11694 to see what to look forward to in Hadoop 2.8, and HADOOP-13204 to see the todo list where help is really welcome and that includes help testing. thanks,",defect_debt,uncorrected_known_defects
hadoop,14043,summary,Shade netty 4 dependency in hadoop-hdfs,non_debt,-
hadoop,14043,description,"During review of HADOOP-13866,  mentioned considering shading netty before putting the fix into branch-2. This would give users better experience when upgrading hadoop.",non_debt,-
hadoop,14043,comment_0,Where is netty exposed? Could we solve this by backporting the shaded client artifacts instead?,non_debt,-
hadoop,14043,comment_1,"When hbase runs mapreduce jobs, (from hadoop) would appear ahead of in the classpath, leading to",non_debt,-
hadoop,14043,comment_2,"We can backport the shaded client to branch-2, though I'd like to get the other subtasks wrapped up before we put this into what is theoretically a production release branch. I quickly grepped in HBase's pom.xml's and saw references to {{hadoop-hdfs}}, which makes me worry that it doesn't use the client artifacts. In this case, the shaded client wouldn't be sufficient.",non_debt,-
hadoop,14043,comment_3,"I have an in-progress patch for HBase to move it to client APIs. I can reprioritize that. I think it would require refactoring HBase's use of mapreduce, which I wanted to do anyways.",non_debt,-
hadoop,14043,comment_4,but the shaded client artifacts won't help if the problem is the classpath within Mapreduce jobs.  do you know off hand if the failure is in the driver that launches the jobs or in the executors?,non_debt,-
hadoop,14043,comment_5,Let me find the stack trace. The exception should be in executors.,non_debt,-
hadoop,14043,comment_7,Which netty dependency? Can you update the title to be clear on this?,non_debt,-
hadoop,14043,comment_8,"yeah, the shaded hadoop client artifacts won't help then. If you switch to the HBase shaded client, does that work? I would presume it needs to be the use of Netty 4.y, since that's where we're seeing incompatibilities (ATM 4.0 v 4.1). Did you mean the version or did you mean where in the hadoop modules Ted's talking about?",non_debt,-
hadoop,14043,comment_9,Can the shading be done in a similar way to the following:,non_debt,-
hadoop,14043,comment_10,", , , , is this critical for 2.9.0? If so, are we on track for the code freeze on 27th Oct? Thanks!",non_debt,-
hadoop,14043,comment_11,Not critical for 2.9.0,non_debt,-
hadoop,14043,comment_12,"Thanks  for the prompt response, moved it to 3.0.0.",non_debt,-
hadoop,14043,comment_13,", are we shading netty solely because we want to backport HADOOP-13866 to branch-2? I am revisiting HADOOP-13866 and thinking that it is no longer needed, see my comments there. If HADOOP-13866 is a won't fix, do we still need to shade netty?",non_debt,-
hadoop,14043,comment_14,Closing this since we won't fix HADOOP-13866. Feel free to reopen if you disagree.,non_debt,-
hadoop,14071,summary,S3a: Failed to reset the request input stream,non_debt,-
hadoop,14071,description,"When using the patch from HADOOP-14028, I fairly consistently get {{Failed to reset the request input stream}} exceptions. They're more likely to occur the larger the file that's being written (70GB in the extreme case, but it needs to be one file). Potentially relevant:",non_debt,-
hadoop,14071,comment_0,"What's happened is that the HTTP connection failed (is this long haul?), aws tried to reset the pointer (using mark/reset) and the buffer couldn't go back that far. We saw this before, thought I'd eliminated it by not buffering the input stream, but instead sending the file input stream up direct. I'll review that code. It may be we need to address this differently, simply by recognising the specific exception and retrying to send the block. That is: we implement the retry logic.",non_debt,-
hadoop,14071,comment_1,"For the & I don't think we currently set My understanding is that, based on the above, we should do this. Is that correct?",non_debt,-
hadoop,14071,comment_2,"That AWS SDK issue does look relevant. For the specific case of data source being a file, we could have the MPU request using that, rather than opening it ourselves. Will need some changes in the code, as currently the BlockOutputStream assumes the source is always some input stream...it'll have to support the option of a File, and if supplied, prefer that as the upload option.",non_debt,-
hadoop,14071,comment_3,This is short-haul (EC2 in us-east-1 to S3 in us-standard).,non_debt,-
hadoop,14071,comment_4,"OK. I'm redoing the HADOOP-14028 patch with the File ref being passed down to AWS. Due to some technical issues (""laptop is toast"") my dev time is somewhat crippled this week, so I'm not going to give a schedule for that being available. Hopefully in the next day or two I just haven't got hadoop building locally right now",non_debt,-
hadoop,14071,comment_5,"can we move discussion to HADOOP-14208, and I resolve this as a dupe? Keep discussion in one place",non_debt,-
hadoop,14071,comment_6,For sure.,non_debt,-
hadoop,14092,summary,Typo in hadoop-aws index.md,documentation_debt,low_quality_documentation
hadoop,14092,description,"In section {{Testing against different regions}}, should be",non_debt,-
hadoop,14092,comment_0,patch?,non_debt,-
hadoop,14092,comment_1,Patch 001 * Fix the typo,documentation_debt,low_quality_documentation
hadoop,14092,comment_3,"+1, committed your attention to detail is appreciated! I know its only a word in the docs, but that could cost a lot of people time. Well spotted too.",documentation_debt,low_quality_documentation
hadoop,14092,comment_5,Thanks for the review and commit! Wrong file name is even worse than other misspelled words because folks might get puzzled when they follow instructions and fail to get the result.,code_debt,low_quality_code
hadoop,14108,summary,CLI MiniCluster: add an option to specify NameNode HTTP port,non_debt,-
hadoop,14108,description,"About CLI MiniCluster, NameNode HTTP port is randomly determined. If you want to see nn-web-ui or do fsck, you need to look for the port number from the minicluster's log. It would be useful if users can specify the port number.",non_debt,-
hadoop,14108,comment_0,are you talking following..? It's already provided right..?,non_debt,-
hadoop,14108,comment_1,"Hi , We can start minicluster from a command line. This is the doc. In this way, we can't specify the port at the moment.",non_debt,-
hadoop,14108,comment_2,"oh,it's for CLI miniCluster.I just confused.",non_debt,-
hadoop,14108,comment_3,Uploaded the first patch.,non_debt,-
hadoop,14108,comment_5,"+1, verified the option can specify the NN HTTP port.",non_debt,-
hadoop,14108,comment_6,Committed this to trunk and branch-2. Thanks  for the contribution!,non_debt,-
hadoop,14108,comment_8,"Thanks for committing, !",non_debt,-
hadoop,14122,summary,Add ADLS to,non_debt,-
hadoop,14122,description,"Add to HADOOP-13687 did include at one point. , could you comment?",non_debt,-
hadoop,14122,comment_0,", my initial goal with HADOOP-13687 was to create the artifact and also set the stage for migrating each cloud storage filesystem module under the part of the build. Because of that, my early revisions of the patch included a mass movement of the codebase. I backed off on that goal though, because HADOOP-13037 wasn't yet committed, and I didn't want to invalidate patches in progress there. There is no reason not to include in the combined artifact. We just didn't do it at the time, because the focus was more on S3A and WASB. Managing this would get somewhat easier if we get to branch-2 first. I just commented on HADOOP-13037 stating that I am +1 for a merge to branch-2.",non_debt,-
hadoop,14122,comment_1,Thanks  for the info !,non_debt,-
hadoop,14122,comment_2,"I'll review this if done, and the alliyun one too into trunk. Then backport the module all the way least 2.8.x",non_debt,-
hadoop,14140,summary,"S3A reporting ""Not found on empty bucket",non_debt,-
hadoop,14140,description,"Hi: UPDATE: This stack trace is being caused because there are 0 objects in the target directory. S3A is unable to handle 0 objects  Connecting S3A to a 3rd party object store does not work. This is a publicly hosted grid and i can provide credentials if required. Please see the debug log below There are two problems - 1. Path Style setting is ignored, and S3A always uses host style addressing 2. Even when host style is specified, it is unable to proceed, see debug log (in comment below)",non_debt,-
hadoop,14140,comment_0,"vishnu, we ought to be doing this, I Think  tests this way against their in-house endpoint. Interesting here is that a 200 is coming back, yet the XML result is triggering an interpretation as a 404. # can you check out and build Hadoop branch-2.8.0; this is what is about to ship and so the last chance place to get a fix in. # Don't bother with building spark, just see if you can set up the hadoop-aws tests (only bother with the s3a ones), See: # on the failure, it'd be good to have the full stack trace; spark appears to have dropped the interesting bits (i.e we don't know exactly which operation failed) There are major changes between 2.7 and 2.8 here (everything in HADOOP-11694); hopefully this will have been addressed too. We just need your assistance in testing and debugging the problem.",non_debt,-
hadoop,14140,comment_1,"Closing as a duplicate of HADOOP-11918, ""Listing an empty s3a root directory throws FileNotFound"" As requested, please grab the branch-2.8 or branch-2 code and test there before worrying about a failure in the 2.7.x codepath. Hopefully the problems will have been fixed, and instead you can get to catch those issues which haven't been addressed yet",non_debt,-
hadoop,14140,comment_2,Stack,non_debt,-
hadoop,14140,comment_3,"Steve, I will build s3a from the 2.8-RC1 hadoop branch and see if i can replicate the issues. Can I just replace the into a spark build with hadoop 2.7, or will i need to rebuild spark also.",non_debt,-
hadoop,14140,comment_4,"don't worry about spark for now, it will only complicate your life. Replicate this in a snippet of a junit test or with the 'hadoop fs' command. Once you are confident it works, then you could look at spark # you can't mix them unless you replace every hadoop-* JAR # if you look at SPARK-7481 you can get the PR I use to build this stuff However, before you bother, consider this. There is no point running any analytics query against an empty bucket. There's no data, there's no answer. It's a corner case, which is why it didn't surface for a while. Prioritise your problems.",non_debt,-
hadoop,14292,summary,Transient failure,non_debt,-
hadoop,14292,description,"Got the test failure once, but could not reproduce it the second time. Maybe a transient ADLS error?",non_debt,-
hadoop,14292,comment_0,""" Either the resource does not exist "". Do you think it could be a consistency failure? If the error message should included the path being listed, that could help for the test and for users",non_debt,-
hadoop,14292,comment_1,"Could be a consistency issue. Digging into why this line of code did not kick in to display the path: , could you please take a look? Might need to look thru ADLS backend logs.",non_debt,-
hadoop,14292,comment_2,"Sorry for the delay. Exception string message that takes is just a default error message in this case the server had a more specific error message it sent in the response, so the SDK used that string instead. The good thing is that the trace ID is in the exception, so we can look at the logs to see if we can find anything. The bad thing is that these errors are fiendishly hard to troubleshoot, so not sure how much we will glean from just logs alone.",non_debt,-
hadoop,14292,comment_3,"For liststatus to pass, user needs: - X permission throughout the traversal path (in this case on account root) - R-X on the final destination (in this case bobdir) I couldn't find the mkdirs request for bobdir even a day ahead from the issue timestamp, and the directory is deleted since the issue too. Can you re-try starting from the directory creation with the necessary permissions set and update the JIRA as above with the exception and request timestamp. (One thing that I observed in the account ACL status is that, the service prinicpal which is attempting this request - ADLSAccessApp@SPI - is given a RWX permission on account root for access acls alone and not on default acls. Hence if any other user creates a directory on account root, this service principal will not have any permission on this new directory.)",non_debt,-
hadoop,14292,comment_4,Thanks ! {{bobdir}} probably didn't have the permission for this test case to pass. This test case expects a clean account. Filed HADOOP-14304 so that the path will not be swallowed when a remote exception occurs.,non_debt,-
hadoop,14314,summary,The OpenSolaris taxonomy link is dead in,documentation_debt,outdated_documentation
hadoop,14314,description,"Unfortunately, Oracle took down opensolaris.org, so the link is dead. The only replacement I could find with a quick search was this PDF:",documentation_debt,outdated_documentation
hadoop,14314,comment_0,"Thanks for filing this JIRA. Since we didn't find official link to replace the old one, suggest to simply remove it.",documentation_debt,outdated_documentation
hadoop,14314,comment_2,Thanks for the patches. I'll upload same patch to run jenkins for trunk.,non_debt,-
hadoop,14314,comment_4,"Actually, it might be better to delete only the link instead of the sentence.",non_debt,-
hadoop,14314,comment_5,Make sense. Uploaded new patch. And the change of doc looks like: # Before !screenshot-1.png! # After,non_debt,-
hadoop,14314,comment_6,Will upload branch-2 patch after trunk patch build finished.,non_debt,-
hadoop,14314,comment_8,+1. I think you don't need to create another patch for branch-2 since can be applied to branch-2 safely. Will commit it later.,non_debt,-
hadoop,14314,comment_9,OK! Thanks for review and committing!,non_debt,-
hadoop,14314,comment_10,"Committed to trunk, branch-3.1, branch-3.0, branch-2, branch-2.9 and branch-2.8. Thanks for the contribution, ! Also thanks  and !",non_debt,-
hadoop,14343,summary,Wrong pid file name in error message when starting secure daemon,non_debt,-
hadoop,14343,description,It will log datanode's pid file instead of JSVC's pid file.,non_debt,-
hadoop,14343,comment_1,"While we are here, let's fix the exit condition (a new error shellcheck added) by combining",non_debt,-
hadoop,14343,comment_2,", I have attached That was you meant?",non_debt,-
hadoop,14343,comment_3,Reattach last patch to kick .,non_debt,-
hadoop,14343,comment_5,", thanks for the effort on this patch. Patch LGTM. +1 , did you have anything you wanted to add?",non_debt,-
hadoop,14343,comment_7,+1 thanks committed to trunk,non_debt,-
hadoop,14343,comment_9,Thanks  and  for the review.,non_debt,-
hadoop,14351,summary,Azure: and should not use Kerberos interactive user cache,non_debt,-
hadoop,14351,description,"Currently, and use Kerberos interactive user's ticket cache if the kerberos credential is not available for or It results in usage of interactive user's ticket for impersonation, whenever services try to do File System operations as another user, which is incorrect.",non_debt,-
hadoop,14351,comment_0,Attaching the patch containing the following changes. 1. Removed the following lines when doing SPENGO connection remote service. 2. Getting the delegation token in and methods as well.,non_debt,-
hadoop,14351,comment_1,I'll have a look at the patch. Thanks for working on this.,non_debt,-
hadoop,14351,comment_2,1,non_debt,-
hadoop,14351,comment_4,Thanks  for reviewing the patch. Attached another patch containing the checkstyle fixes excluding the fix for the {{FileLength}} related warning. The below findbugs warning is unrelated to changes done in the patch which was introduced in [Commit of HADOOP-10809,code_debt,low_quality_code
hadoop,14351,comment_6,"Committed. Thanks,",non_debt,-
hadoop,14359,summary,Remove unnecessary shading of commons-httpclient,code_debt,dead_code
hadoop,14359,description,commons-httpclient dependency was removed in HADOOP-10105 but there are some settings to shade commons-httpclient. Probably they can be safely removed.,code_debt,dead_code
hadoop,14359,comment_0,Assign this to myself. I'd like to get rid of commons-httpclient and make downstream applications aware it will no longer appear in Hadoop code.,non_debt,-
hadoop,14359,comment_1,Here's a quick patch to remove commons-httpclient from all pom.xml. It compiles locally.,non_debt,-
hadoop,14359,comment_2,"+1, the patch looks good to me. I confirmed that the modules do not have transitive commons-httpclient dependency. Unfortunately, not yet. Now hadoop-yarn-project and module have transitive commons-httpclient dependency via hbase-server. * hadoop-yarn-project - HBASE-16267 removed the dependency and the fix version is 2.0.0.",build_debt,over-declared_dependencies
hadoop,14359,comment_3,Thanks for the pointer! I really wish we could get rid of commons-httpclient in Hadoop 3. ran mvn dependency:tree and I see commons-httpclient 3 is exposed in two jars: [INFO] [INFO] +- [INFO] | +- [INFO] | | \- [INFO] | +- [INFO] | +- [INFO] [INFO] +- [INFO] | +- [INFO] | | \- [INFO] | +- [INFO] | +-,code_debt,dead_code
hadoop,14359,comment_5,Committed this to trunk. Thanks  for the contribution!,non_debt,-
hadoop,14479,summary,Erasurecode testcase failures with native enabled,non_debt,-
hadoop,14479,description,"I built hadoop with ISA-L support. I took the ISA-L code from (tag v2.18.0) and built it. While running the UTs , following three testcases are failing 7, 3, Errors: 0, Skipped: 0, 1.106 sec <<< FAILURE! - in 0.029 sec <<< FAILURE! Decoding and comparing failed. 2, 0, Errors: 0, Skipped: 0, 0.591 sec - in Running # # A fatal error has been detected by the Java Runtime Environment: # # SIGSEGV (0xb) at pid=8970, # # JRE version: OpenJDK Runtime Environment (8.0_121-b13) (build # Java VM: OpenJDK 64-Bit Server VM (25.121-b13 mixed mode linux-amd64 compressed oops) # Problematic frame: # C [libc.so.6+0x8e6e4] # # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again # # An error report file with more information is saved as: # # # If you would like to submit a bug report, please visit: # # The crash happened outside the Java Virtual Machine in native code. # See problematic frame for where to report the bug. # Running 5, 1, Errors: 0, Skipped: 0, 0.559 sec <<< FAILURE! - in 0.015 sec <<< FAILURE! null",non_debt,-
hadoop,14479,comment_0,"Thanks for the report . I tried this locally too and ran into the same problem. We're supposed to be running the ISA-L tests as part of precommit, so I'm not sure what happened here. Seems like there's a test gap for ISA-L vs. the Java coder (time to revisit HDFS-11066?)  /  could you assist with debugging this?",test_debt,low_coverage
hadoop,14479,comment_1,"Sure, , I will look into it.",non_debt,-
hadoop,14479,comment_2,"Per offline  will take this. Sammi, hope you're doing great. Is this caused by different environment like Ubuntu or by the upgrade of Intel ISA-L? Did you get some chance to verify this first? It would be great if you could update on this when any progress. Thanks!",non_debt,-
hadoop,14479,comment_3,"Hi , I agree, it would be good to fill the test gap. Let's revisit HDFS-11066. Thanks!",test_debt,low_coverage
hadoop,14479,comment_4,"Hmm, I thought the test gap was majorly caused by the precommit missing ISA-L building. Quite some time ago it was done in HADOOP-12626 but removed in HADOOP-13342. I thought we should bring it back and re-enable the ISA-L building & tests in precommit. Andrew, do you think so? If sounds good, we can fire a new issue to do it. Thanks!",test_debt,low_coverage
hadoop,14479,comment_5,"SGTM, thanks Kai! The concern on HADOOP-13342 was getting a stable reference to a single version, which is something we should address when bringing this back.",non_debt,-
hadoop,14479,comment_6,HADOOP-14593 is created to handle and failure. failure is under investigation. It's pretty likely has nothing to do with ISA-L version change.,non_debt,-
hadoop,14479,comment_7,patch to solve three failed unit tests,non_debt,-
hadoop,14479,comment_8,"First of all, these three failed unit tests are not related with ISA-L version change. There are actually hidden issues. The implementation of native XOR encoder/decoder has array out of index issue which cause the JVM crash. One failed unit test is to test underlying encoder reuse by execution testCoding twice, while the underlying encoder is released explicitly by By review the context, I think is not the right place to release the encoder. 3. One failed unit test is because it doesn't consider the native RS coder situation. I have tried the patch locally. , thanks for reporting this. You can have a try with the new patch. Besides, I will close HADOOP-14593 later since I have merged fixes for 3 unit cases into one patch.",test_debt,expensive_tests
hadoop,14479,comment_10,Thanks  a lot for digging into these failure cases and providing the patch! This clarified these failures have nothing to do with ISA-L version update. The patch LGTM and +1. Will commit it shortly.,non_debt,-
hadoop,14479,comment_11,Committed to trunk. Thanks  for the contribution!,non_debt,-
hadoop,14479,comment_13,"The problem was that Intel didn't have anything that wasn't locked behind an email wall to get to the source tree. It was pretty much impossible for us to get anything that was reliable. FWIW, this isn't the only native component that isn't getting tested due to missing dependencies or just plain brokenness in the CMakefiles. I'll likely upgrade the Dockerfile bits to handle multiple OSes and architectures at some point. When I do that, dependencies will almost certainly get revisited. (I don't think anyone else is really paying attention to this stuff. Looking forward to the Jenkins upgrade, cuz a lot of non-Yetus stuff is just going to flat out break.)",build_debt,build_others
hadoop,14479,comment_14,"did we ever file that JIRA to get ISA-L re-enabled? ISA-L is required for production usage of EC, so having testing is really important.",test_debt,low_coverage
hadoop,14479,comment_15,"Thanks  for the reminder. Yes it's scheduled and we'll do it. If it can't be made in BETA1, we'll give some thorough runs against latest ISA-L manually.",non_debt,-
hadoop,14479,comment_16,"Thanks Kai. Do you mind filing a JIRA for this? Or, are we planning to use HDFS-11066?",non_debt,-
hadoop,14479,comment_17,"Ah, HDFS-11066 looks good enough. Thanks Andrew!",non_debt,-
hadoop,14634,summary,Remove jline from main Hadoop pom.xml,non_debt,-
hadoop,14634,description,"A long time ago, HADOOP-9342 removed jline from being included in the Hadoop distribution. Since then, more modules have added Zookeeper, and are pulling in jline again. Recommend excluding jline from the main Hadoop pom in order to prevent subsequent additions of Zookeeper dependencies from doing this again.",build_debt,over-declared_dependencies
hadoop,14634,comment_1,"+1, committed. Thanks ray. We all hate spurious JARs sneaking in. the ones we deliberately add are troublesome enough",code_debt,low_quality_code
hadoop,14634,comment_2,hadn't committed; my bad. Will now,non_debt,-
hadoop,14634,comment_4,Thanks for the commit Steve! It's going to be good to have all these sorts of things cleaned up in time for the final Hadoop 3 release.,code_debt,low_quality_code
hadoop,14653,summary,Update joda-time version to 2.9.9,non_debt,-
hadoop,14653,description,Update the dependency to the latest (2.9.9).,non_debt,-
hadoop,14653,comment_0,Initial patch. Still pending some testing.,non_debt,-
hadoop,14653,comment_2,"Comparing against my baseline testing, I'm not seeing any new test failures.",non_debt,-
hadoop,14653,comment_3,joda-time is not bundled with Hadoop. L&N check shows nothing.,non_debt,-
hadoop,14653,comment_4,"joda-time is bundled in hadoop as part of hadoop-tools, e.g.: Nevertheless, LGTM +1, thanks Ray!",non_debt,-
hadoop,14653,comment_5,Thanks for pointing out the jar file inclusion . Adding joda-time's NOTICE to the patch.,non_debt,-
hadoop,14653,comment_7,Unit test failure looks to be the same as HADOOP-13101.,non_debt,-
hadoop,14653,comment_8,Committed to trunk and branch-3.0. Thanks  for the review!,non_debt,-
hadoop,14692,summary,Upgrade Apache Rat,non_debt,-
hadoop,14692,description,We should upgrade Apache RAT to something modern.,architecture_debt,using_obsolete_technology
hadoop,14692,comment_0,-00: * upgrade from 10 to 12,non_debt,-
hadoop,14692,comment_1,"+1, Thanks for your help. This is needed for HDFS-12034, Going to mark that JIRA as dependent on this one.",non_debt,-
hadoop,14692,comment_3,Thanks. Committed.,non_debt,-
hadoop,14792,summary,Package on windows fail,non_debt,-
hadoop,14792,description,"{{mvn package -Pdist -Pnative-win -DskipTests -Dtar command fails on windows this is because need dos2unix conversion to avoid failure, we can add the conversion before bash execute",non_debt,-
hadoop,14792,comment_0,"as cygwin do not have dos2unix command installed by default, we can replace carriage return by awk or tr command as below",non_debt,-
hadoop,14792,comment_1,"as BUILDING.txt clearly states, cygwin is not supported, removing cygwin from PATH and using Git unix tools instead resolved this. marking as not a issue",non_debt,-
hadoop,14870,summary,backport HADOOP-14553 parallel tests to branch-2,non_debt,-
hadoop,14870,description,"Backport the HADOOP-14553 parallel test running from trunk to branch-2. There's some complexity related to The FS Contract base test being JUnit4 in branch -2, so its not a simple cherrypick.",non_debt,-
hadoop,14870,comment_0,"patch 001: branch-2 version of HADOOP-14553 patch applied & merged. Tests? All the ones which extend the JUnit 3 don't work, as they aren't getting a properly set up test account. If you leave the assumeTrue() code in from trunk they'll get uprated to an error...if you stay with the existing branch-2 tests they are skipped silently. But they are skipped, which is not what we want",non_debt,-
hadoop,14870,comment_2,"Got the tests working here...they were actually running, though the use of JUnit4 rules to name threads was amplifying confusion. Seen a test failure",code_debt,low_quality_code
hadoop,14870,comment_3,"Patch 002; removes the JUnit 4 rules & thread naming from the subclasses of testing: All tests passing except for which is covered in HADOOP-14906 & reproduced in trunk, so not a direct result of this patch",non_debt,-
hadoop,14870,comment_5,+back to normal network and HADOOP-14906 is gone; all tests are passing. I'm ready for others to play with this patch now,non_debt,-
hadoop,14870,comment_7,"Is this still on target for 2.9.0 ? If not, can we we push this out to the next major release ?",non_debt,-
hadoop,14870,comment_8,I set up nightly builds for branch-2 and loads of HDFS tests are timing out. I am wondering if this patch can help remedy that? Can you take a look and commit this after a Yetus rekick? Thanks.,non_debt,-
hadoop,14870,comment_9,"This is nothing to do with HDFS. It's Hadoop Azure tests which don't run on Yetus as it lacks the credentials. I do plan to backport it, but not this week for various reasons",non_debt,-
hadoop,14870,comment_10,Thanks for the clarification. I am moving it out of 2.9 based on your timeline.,non_debt,-
hadoop,14870,comment_11,"This is the cherry picking of HADOOP-14553 into branch-2; in sync with the latest branch-2 changes. If Yetus is happy I'll be carefully committing this. This brings branch-2 tests almost in sync w/ trunk, main diff is the use of final in declarations used in inner classes, and those bigger diffs in the subclassing of the hadoop-common FS contract tests which are still JUnit 3 in branch-2.",non_debt,-
hadoop,14870,comment_13,"+1, committed backport to branch-2",non_debt,-
hadoop,14942,summary,DistCp#cleanup() should check whether jobFS is null,code_debt,low_quality_code
hadoop,14942,description,"Over in HBASE-18975, we observed the following: came from second line below: in which case jobFS was null. A check against null should be added.",code_debt,low_quality_code
hadoop,14942,comment_1,LGTM +1 cleanup code is always good to make robust,code_debt,low_quality_code
hadoop,15026,summary,Rebase ResourceEstimator start/stop scripts for branch-2,non_debt,-
hadoop,15026,description,HADOOP-14840 introduced the which was cherry-picked from trunk to branch-2. The start/stop scripts need minor alignment with branch-2.,non_debt,-
hadoop,15026,comment_2,I have manually tested the patch before uploading.,non_debt,-
hadoop,15026,comment_4,"Thanks  for the contribution, I have committed this to branch-2/2.9/2.9.0.",non_debt,-
hadoop,15066,summary,Spurious error stopping secure datanode,non_debt,-
hadoop,15066,description,There is a spurious error when stopping a secure datanode. The error appears benign. The service was stopped correctly.,non_debt,-
hadoop,15066,comment_0,The error is from in It looks like jsvc auto-deletes the pid file when the process is killed with SIGTERM. The check for changed pid likely needs to be skipped if the pid file doesn't exist.,non_debt,-
hadoop,15066,comment_1,Thanks for the fix . Couple of comments: # The following can be replaced with elif: # We'd also need a fix in here. The pid equality checks should be skipped if the pid file no longer exists.,code_debt,low_quality_code
hadoop,15066,comment_3,"Thanks  for review. But one question I have is we do delete daemonpid file in hadoop_stop_daemon {code:java} if [[ ""${pid}"" = ""${cur_pid}"" ]]; then rm -f ""${pidfile}"" again we are trying to delete same file, not clear why we have delete logic 2 times.",non_debt,-
hadoop,15066,comment_4,That's a good question. I am not sure why the daemon pid file deletion is attempted twice. If you setup a secure cluster and try to stop the secure DN the error shows up twice.,non_debt,-
hadoop,15066,comment_7,"Bulk update: moved all 3.2.0 non-blocker issues, please move back if it is a blocker.",non_debt,-
hadoop,15228,summary,S3A Retry policy to retry on NoResponseException,non_debt,-
hadoop,15228,description,failed to respond` to something which can be retried on idempotent calls. Need to handle shaded as well as unshaded binding here.,non_debt,-
hadoop,15228,comment_1,"Looking in the AWS code, ProtocolException is also valid. As usual, shading gets in the way of coding for this.",non_debt,-
hadoop,15228,comment_2,Done in HADOOP-13786; treated as idempotent,non_debt,-
hadoop,15277,summary,remove from CLI operation log output,non_debt,-
hadoop,15277,description,"When hadoop metrics is started, a message about bean introspection appears. When using wasb or s3a,. this message appears in the client logs, because they both start metrics I propose to raise the log level to ERROR for that class in log4j.properties",non_debt,-
hadoop,15277,comment_0,Patch 001; moves log level to WARN. This is what is already used in and my local hadoop conf so I'm happy with it,non_debt,-
hadoop,15277,comment_2,1,non_debt,-
hadoop,15311,summary,HttpServer2 needs a way to configure the acceptor/selector count,non_debt,-
hadoop,15311,description,"HttpServer2 starts up with some number of acceptors and selectors, but only allows for the automatic configuration of these based off of the number of available cores: A thread pool is started of size, at minimum, {{acceptors + selectors + 1}}, so in addition to allowing for a higher tuning value under heavily loaded environments, adding configurability for this enables tuning these values down in resource constrained environments such as a MiniDFSCluster.",non_debt,-
hadoop,15311,comment_0,"Attached v000 patch with simple change & unit test. , want to take a look as a prereq for HDFS-13265?",non_debt,-
hadoop,15311,comment_1,"Since this is overwriting the {{server}} field that's set using {{\@BeforeClass}}, does that interfere with other tests?",non_debt,-
hadoop,15311,comment_2,"Mistake on my part, good catch. Thanks Chris! Attached v001 patch.",non_debt,-
hadoop,15311,comment_5,"Fixed the checkstyle warning. +1 I committed this. Thanks, Erik",non_debt,-
hadoop,15311,comment_6,Thanks Chris!,non_debt,-
hadoop,15311,comment_8,"Doing 3.1.0 RC1 now, moved all 3.1.1 (branch-3.1) fixes to 3.1.0 (branch-3.1.0)",non_debt,-
hadoop,15350,summary,[JDK10] Update maven plugin tools to fix compile error in module,non_debt,-
hadoop,15350,description,{{mvn install -DskipTests}} fails with Java 10. Upgrading maven plugin tools to 3.5.1 fixes this.,non_debt,-
hadoop,15350,comment_0,Full stack trace with -X option:,non_debt,-
hadoop,15350,comment_1,Thanks for catching the issue. I'd like to work on it.,non_debt,-
hadoop,15350,comment_2,"Sorry for late, . Actually, I don't see this error if I use Anyway, let's use the latest version.",non_debt,-
hadoop,15350,comment_4,1,non_debt,-
hadoop,15350,comment_5,Committed this to trunk. Thanks  for the contribution.,non_debt,-
hadoop,15350,comment_7,"Thanks, !",non_debt,-
hadoop,15376,summary,Remove double semi colons on imports that make Clover fall over.,non_debt,-
hadoop,15376,description,Clover will fall over if there are double semicolons on imports. The error looks like: Thankfully we only have one location with this:,non_debt,-
hadoop,15376,comment_1,1,non_debt,-
hadoop,15376,comment_2,Committed this to trunk and branch-3.1. Thanks !,non_debt,-
hadoop,15391,summary,"Add missing css file in hadoop-aws, hadoop-aliyun, hadoop-azure and modules",non_debt,-
hadoop,15391,description,"The documentation pages for hadoop-aws, hadoop-aliyun, hadoop-azure and render error (see screen-shot attached or The reason of this is that the css file is missing in these modules.",non_debt,-
hadoop,15391,comment_0,Attach the patch for adding the css file and attach the new screen-shot by fixed.,non_debt,-
hadoop,15391,comment_1,"LGTM, +1 pending Jenkins.",non_debt,-
hadoop,15391,comment_3,"Thanks for the review, . The Jenkins looks good. Will commit shortly, :).",non_debt,-
hadoop,15391,comment_4,Committed to trunk and branch-3.1. Thanks  for the review.,non_debt,-
hadoop,15476,summary,fix logging for split-dns multihome,non_debt,-
hadoop,15476,description,Fix debug log statement introduced in [HADOOP-15250].,non_debt,-
hadoop,15476,comment_1,"Thanks for catching this . Can we just remove this log message? There is another message below: For wildcard address, localAddr will be null. So perhaps we can fix this other log message to print wildcard if localAddr is null.",code_debt,low_quality_code
hadoop,15476,comment_2,"thanks for review, updated patch to address your suggestion.",non_debt,-
hadoop,15476,comment_4,+1. I've committed this. Thanks .,non_debt,-
hadoop,15476,comment_6,thanks for review and commit.,non_debt,-
hadoop,15486,summary,Make fair,non_debt,-
hadoop,15486,description,"Whenever a datanode is restarted, the registration call after the restart received by NameNode lands in via requires write lock on This registration thread is getting starved by flood of calls, which are triggered by clients those who were writing to the restarted datanode. The registration call which is waiting for write lock on is holding write lock on causing all the other RPC calls which require the lock on wait. We can make lock fair so that the registration thread will not starve.",code_debt,multi-thread_correctness
hadoop,15486,comment_0,There's no need for a config. Just make the lock always be fair. Updates to the topology should be rare compared to block placements so the throughput degradation of the fair lock will be minimal.,code_debt,multi-thread_correctness
hadoop,15486,comment_2,"Agreed, updates to NetworkTopology should be rare. Patch v001 makes the lock always fair.",non_debt,-
hadoop,15486,comment_3,+1 pending Jenkins.,non_debt,-
hadoop,15486,comment_5,Build failure is unrelated - re-triggered Jenkins.,non_debt,-
hadoop,15486,comment_7,"Committed through 3.2.0 to 2.7.7. Thanks for the contribution , thanks for the suggestion .",non_debt,-
hadoop,15494,summary,fails on Windows,non_debt,-
hadoop,15494,description,[hadoop-trunk-win #476 shows following similar errors for 21 tests: Relative path in absolute URI:,non_debt,-
hadoop,15494,comment_0,"applies to trunk. It replaces the original relative path of test base directory with absolute path of a randomized test directory.  and , can you help review?",non_debt,-
hadoop,15494,comment_2,"+1, committed",non_debt,-
hadoop,15569,summary,Expand S3A Assumed Role docs,non_debt,-
hadoop,15569,description,"The S3A assumed role doc is now where we document the permissions needed to work with buckets # detail the permissions you need for s3guard user and admin ops # and what you need for SSE-KMS This involves me working them out, so presumably get some new stack traces also: fix any errors noted in the doc",documentation_debt,low_quality_documentation
hadoop,15569,comment_0,"Patch 001 * document assumed roles better, in particularly, the permissions you need for S3Guard read and S3Guard admin. * Change structure to avoid listing which commands need which perms, and be a bit expansive in what is needed, to line up for future changes (e.g. get/set object tags) * I have tested the s3guard stuff with an assumed role.",code_debt,low_quality_code
hadoop,15569,comment_2,You need to be able to call getBucketLocation to bootstrap S3Guard: update docs to cover this.,non_debt,-
hadoop,15569,comment_3,merging into HADOOP-15583,non_debt,-
hadoop,15577,summary,Update distcp to use zero-rename s3 committers,non_debt,-
hadoop,15577,description,Hello! distcp through 3.1.0 appears to copy files and then rename them into their final/destination filename. added support for more efficient S3 committers that do not use renames. Please update distcp to use these efficient committers and no renames. Thanks!,design_debt,non-optimal_design
hadoop,15577,comment_0,"We don't need the 0-rename stuff, because distcp, except in the --atomic mode, isn't trying to do atomic operations. What we do need, is for distcp to not upload to a temp file and rename each one into place: remove that and for non-atomic uploads you eliminate the O(data) delay after each upload. Closing as a duplicate of that. *as that JIRA has no code/tests, I would support anyone who sat down to do implement the feature* There's also lots of work going on with HDFS to have an explicit multipart upload mechanism for filesystems, which can be used for a block-by-block upload to S3, this would improve distcp upload perf on files in HDFS > 1 block, as the blocks could be uploaded in parallel with locality. Keep an eye on that",test_debt,lack_of_tests
hadoop,15612,summary,Improve exception when tfile fails to load LzoCodec,non_debt,-
hadoop,15612,description,When hadoop-lzo is not on classpath you get which is probably rarely the real cause given the default class name. The real root cause is not attached to the exception thrown.,non_debt,-
hadoop,15612,comment_0,001 patch for review,non_debt,-
hadoop,15612,comment_2,resubmitting to check docker build,non_debt,-
hadoop,15612,comment_4,What about making it a Though it's not quite right,non_debt,-
hadoop,15612,comment_5,thank you for the suggestion. does not resonate with me tbh since the previously considered reasons for the exception are either 'conf key not found' or 'class not found'. I presume you allude to the missing hadoop-lzo jar but it's easy to confuse with tfile being missing which is not the case. That said it's more about the error message for me than about the exception class.,non_debt,-
hadoop,15612,comment_7,forcing codec reload in 002,non_debt,-
hadoop,15612,comment_9,"Noted. Happy with production code. w.r.t testing, I'd use LambdaTestUtils & rethrow the exception if it is wrong, as that gives you a stack trace to debug:",non_debt,-
hadoop,15612,comment_10,"hopefully looks cleaner now. Thanks for suggestion, Please review 003",non_debt,-
hadoop,15612,comment_12,LGTM +1,non_debt,-
hadoop,15612,comment_14,"Thank you for review Committed to trunk, branch-3, and branch-3.1",non_debt,-
hadoop,15612,comment_15,Updated fixed version to 3.1.2 given this don't exist in branch-3.1.1,non_debt,-
hadoop,15645,summary,fails if bucket has per-bucket binding to DDB,non_debt,-
hadoop,15645,description,"If your bucket has a per-bucket setting to use S3Guard, then the test can fail as fs-only data can creep into the metastore. The rawfs should use clearBucketOption to clear the metastore binding, so guarantee a real raw fs",non_debt,-
hadoop,15645,comment_0,"Patch 001 * better setup of implementations, especially bucket config, where per-bucket settings are cleared * and both check the metastore type of the test FS, skip if not correct -and include details on store in the message. * testDiff test erases test FS in testDiff setup through rawFS and metastore FS, in case it gets contaminated This addresses the double setup of dynamodb being picked up on a local test run, the ddb metastore getting tainted with stuff which shouldn't be there, and the s3 bucket getting stuff into it which the getFileSystem() fs doesn't see/delete.",design_debt,non-optimal_design
hadoop,15645,comment_1,"Testing: us-west-1 w/ test setup of S3Guard enabled in the bucket, options of {{-Ds3guard}} and {{{-Ds3guard -Ddynamodb}}. In the first of those options because my test bucket is per-bucket bonded to s3, the local test is now skipped properly",non_debt,-
hadoop,15645,comment_3,002 - Rebase onto current head ( ),non_debt,-
hadoop,15645,comment_5,LGTM. Merged.,non_debt,-
hadoop,15645,comment_7,"thanks. FWIW, we do like to have an explicit +1 in the comment list, because LGTM can often be used as a not-quite final comment...you don't want people like me committing stuff to early, do you?",non_debt,-
hadoop,15647,summary,"dependency checker, dockerfile test",non_debt,-
hadoop,15647,description,None,non_debt,-
hadoop,15647,comment_0,"2 runs kicked off, normal and rbt.",non_debt,-
hadoop,15666,summary,ABFS: Compatibility tests can fail,non_debt,-
hadoop,15666,description,"Sounds like other folks aren't hitting this, but I'm seeing failures in 2 compatibility test classes:",non_debt,-
hadoop,15666,comment_0,"Hey , are you still seeing this test failed for you?",non_debt,-
hadoop,15666,comment_1,Nope - I noticed at the end of last week it wasn't showing up any more.,non_debt,-
hadoop,15736,summary,Trash : Negative Value For Deletion Interval Leads To Abnormal Behaviour.,non_debt,-
hadoop,15736,description,If deletion interval ( ) is somehow configured negative. The trash gets enabled since the value is not zero. It even changes the emptier interval to that negative value since the value needs to be less than or equal to deletion interval. But the emptier due to negative value doesn't get up throws Ultimately the trash gets piled up since no emptier work is being done.,non_debt,-
hadoop,15736,comment_3,"+1, LGTM",non_debt,-
hadoop,15736,comment_4,"Committed to trunk, branch-3.1 and branch-3.0 Thanks  for contribution",non_debt,-
hadoop,15742,summary,Log if ipc backoff is enabled in CallQueueManager,non_debt,-
hadoop,15742,description,Currently we don't log the info of ipc backoff. It will look good to print this as well so that makes users know if we enable this.,code_debt,low_quality_code
hadoop,15742,comment_0,Is this going to flood the logs if @ info?. Is it important enough to merit a mention ? Otherwise * log at debug * use SLF4J string interpolation to build up the strings if logging at debug is enabled,non_debt,-
hadoop,15742,comment_1,"Hi this won't flood the log, we just add ipc backoff info in existing info. This only print once when construct the call queue manager",non_debt,-
hadoop,15742,comment_2,This patch makes users know that they enabled this feature.,non_debt,-
hadoop,15742,comment_3, could you use string interpolation way as mentioned? Update this like following:,non_debt,-
hadoop,15742,comment_5,"Hi,, I have used string interpolation way in The following output logs were tested in my local.",non_debt,-
hadoop,15742,comment_7,"Other than checkstyle issue, +1 for others.",code_debt,low_quality_code
hadoop,15742,comment_8,1,non_debt,-
hadoop,15742,comment_10,"LGTM, +1. Will hold off the commit until at the end of today in case there are some other comments.",non_debt,-
hadoop,15742,comment_12,"Committed this to trunk. Thanks  for the contribution and also thanks additional reviews, :).",non_debt,-
hadoop,15742,comment_13,:D,non_debt,-
hadoop,15759,summary,AliyunOSS: update oss-sdk version to 3.0.0,non_debt,-
hadoop,15759,description,"OSS Java SDK 3.0.0 adds 7 And Assumes Role Credentials function bases on that sdk offers, so we update its version.",non_debt,-
hadoop,15759,comment_0,Attach 001 patch.,non_debt,-
hadoop,15759,comment_2,"Please help to review this patch. Thanks! Below is test result based on this patch. [INFO] [INFO] [INFO] T E S T S [INFO] [INFO] Running [INFO] 2, 0, Errors: 0, Skipped: 0, 1.397 s - in [INFO] Running [INFO] 5, 0, Errors: 0, Skipped: 0, 5.095 s - in [INFO] Running [INFO] 19, 0, Errors: 0, Skipped: 0, 5.673 s - in [INFO] Running [INFO] 7, 0, Errors: 0, Skipped: 0, 5.191 s - in [INFO] Running [INFO] 8, 0, Errors: 0, Skipped: 0, 3.435 s - in [INFO] Running [INFO] 18, 0, Errors: 0, Skipped: 0, 7.82 s - in [INFO] Running [INFO] 6, 0, Errors: 0, Skipped: 0, 28.708 s - in [INFO] Running [INFO] 8, 0, Errors: 0, Skipped: 0, 4.569 s - in [INFO] Running [INFO] 9, 0, Errors: 0, Skipped: 0, 2.921 s - in [INFO] Running [WARNING] 11, 0, Errors: 0, Skipped: 2, 4.066 s - in [INFO] Running [INFO] 6, 0, Errors: 0, Skipped: 0, 2.243 s - in [INFO] Running [INFO] 2, 0, Errors: 0, Skipped: 0, 12.197 s - in [INFO] Running [INFO] 6, 0, Errors: 0, Skipped: 0, 74.243 s - in [INFO] Running [INFO] 50, 0, Errors: 0, Skipped: 0, 14.781 s - in [INFO] [INFO] Results: [INFO] [WARNING] 157, 0, Errors: 0, Skipped: 2",non_debt,-
hadoop,15759,comment_4,"Below is test result based on branch-2 [INFO] [INFO] T E S T S [INFO] [INFO] Running [INFO] 2, 0, Errors: 0, Skipped: 0, 1.851 s - in [INFO] Running [INFO] 5, 0, Errors: 0, Skipped: 0, 11.613 s - in [INFO] Running [INFO] 19, 0, Errors: 0, Skipped: 0, 7.817 s - in [INFO] Running [INFO] 7, 0, Errors: 0, Skipped: 0, 7.196 s - in [INFO] Running [INFO] 8, 0, Errors: 0, Skipped: 0, 5.042 s - in [INFO] Running [INFO] 18, 0, Errors: 0, Skipped: 0, 11.092 s - in [INFO] Running [INFO] 4, 0, Errors: 0, Skipped: 0, 55.971 s - in [INFO] Running [INFO] 8, 0, Errors: 0, Skipped: 0, 6.749 s - in [INFO] Running [INFO] 9, 0, Errors: 0, Skipped: 0, 3.827 s - in [INFO] Running [WARNING] 11, 0, Errors: 0, Skipped: 2, 5.195 s - in [INFO] Running [INFO] 6, 0, Errors: 0, Skipped: 0, 2.826 s - in [INFO] Running [INFO] 2, 0, Errors: 0, Skipped: 0, 33.99 s - in [INFO] Running [INFO] 6, 0, Errors: 0, Skipped: 0, 225.558 s - in [INFO] Running [INFO] 39, 0, Errors: 0, Skipped: 0, 18.497 s - in [INFO] [INFO] Results: [INFO] [WARNING] 144, 0, Errors: 0, Skipped: 2",non_debt,-
hadoop,15759,comment_5,"Thanks  for the contribution. + 1, committed to trunk, branch-3.1, branch-3.2, branch-2 and branch-2.9.",non_debt,-
hadoop,15759,comment_6,Thanks  :),non_debt,-
hadoop,15759,comment_7,"Hi , I found JSON license is included in the transitive dependency of oss-sdk 3.0.0, and the license cannot be included. Would you check HADOOP-15992?",non_debt,-
hadoop,15759,comment_9,This commit is reverted via HADOOP-15992. We cannot reopen this because 2.9.2 is already released.,non_debt,-
hadoop,15859,summary,mistakes a class for an instance,non_debt,-
hadoop,15859,description,"As a follow up to HADOOP-15820, I was doing more testing on ZSTD compression and still encountered segfaults in the JVM in HBase after that fix. I took a deeper look and realized there is still another bug, which looks like it's that we are actually [calling on the ""remaining"" variable on the class itself (instead of an instance of that class) because the Java stub for the native C init() function [is marked leading to memory corruption and a crash during GC later. Initially I thought we would fix this by changing the Java init() method to be non-static, but it looks like the ""remaining"" setInt() call is actually unnecessary anyway, because in reset() we [set ""remaining"" to 0 right after calling the JNI init() So init() doesn't have to be changed to an instance method, we can leave it as static, but remove the JNI init() call's ""remaining"" setInt() call altogether. Furthermore we should probably clean up the class/instance distinction in the C file because that's what led to this confusion. There are some other methods where the distinction is incorrect or ambiguous, we should fix them to prevent this from happening again. I talked to  who further pointed out the ZStandardCompressor also has similar problems and needs to be fixed too.",code_debt,low_quality_code
hadoop,15859,comment_0,Attached a patch that removes the JNI setting of the remaining field per Ben's analysis above and cleans up the naming re: objects vs. classes in the JNI function arguments.,code_debt,low_quality_code
hadoop,15859,comment_2,+1 We've applied the same patch internally.,non_debt,-
hadoop,15859,comment_4,"Thanks for the review, Kihwal! I committed this to trunk, branch-3.2, branch-3.1, branch-3.0, branch-2, and branch-2.9.",non_debt,-
hadoop,16007,summary,Order of property settings is incorrect when includes are processed,non_debt,-
hadoop,16007,description,"If a configuration file contains a setting for a property then later includes another file that also sets that property to a different value then the property will be parsed incorrectly. For example, consider the following configuration file: with the contents of as: Parsing this configuration should result in myprop=val2, but it actually results in myprop=val1.",non_debt,-
hadoop,16007,comment_0,I tracked this behavior change down to HADOOP-15554. I believe the problem stems from parsing the included sub-resource directly into the Configuration {{properties}} member which bypasses the ordering of properties returned by the Parser#parse method.,non_debt,-
hadoop,16007,comment_1,"Really? I thought I'd been seeing the ""correct"" behaviour, but maybe not. I do chained /nested XIncludes though",non_debt,-
hadoop,16007,comment_2,"The behavior will only be noticed if an included resource overrides a previously set property from the same resource doing the include. If the include was overriding a value from a previously parsed resource (like core-default.xml) then the problem does not manifest. The parser directly sets the included properties on the conf as a side-effect of parsing but the non-included properties are returned as a parse result and those results are iterated to set them. The sideband processing of includes effectively reverses the order in which properties are processed if the xinclude appears after the property setting in the original resource. Here's the simple code I used to test it: Using this sample code with a core-site.xml and included file setup as described in the JIRA description, The following shows what I get at two adjacent commits on the trunk line: So the above shows the broken behavior. core-site.xml set myconf to val1 then xincluded another file which set it to val2, yet the property acts as if the xinclude occurred at the top of core-site.xml. Moving one commit earlier in time shows the expected behavior:",non_debt,-
hadoop,16007,comment_3,Hi  and  Thanks for reporting same. I think this is the same issue reported during 3.2.0 RC0 time. Correct? Does this need some revert or partial revert of HADOOP-15554 as I could see a good set of changes in that work? I think I need to hold on to RC once this is resolved. Pls correct me otherwise. Thanks.,non_debt,-
hadoop,16007,comment_4,"This is not quite the same issue as discovered during the RC0 voting period, as that's HADOOP-15973. Eric and I have been discussing this quite a bit offline, and he said that rolling back to the commit before HADOOP-15554 did not fix HADOOP-15973, so they are related but slightly different issues. We _think_ there's a way to fix both of them with the same change, and Eric is actively working on that. I agree that we should hold the RC for these fixes, as not loading the intended config settings properly could lead to very bad behavior depending upon the property which was accidentally, silently dropped after upgrading.",non_debt,-
hadoop,16007,comment_5,This was fixed by HADOOP-15973.,non_debt,-
hadoop,16013,summary,DecayRpcScheduler decay thread should run as a daemon,non_debt,-
hadoop,16013,description,"sets up a {{Timer}} to schedule a decay of the weights it tracks: However this Timer is not set up as a daemon thread. I have seen this cause my JVM to refuse to exit when running, for example, with FairCallQueue enabled.",design_debt,non-optimal_design
hadoop,16013,comment_0,Attaching a v000 patch which simply uses this constructor for {{Timer}} instead of the no-arg:,non_debt,-
hadoop,16013,comment_2,"+1 on v000 patch, the failed tests seem unrelated and passed in my local run. I've committed to trunk and branch-2, thanks for the contribution !",non_debt,-
hadoop,16013,comment_3,Nice! Thanks !,non_debt,-
hadoop,16044,summary,ABFS: Better exception handling of DNS errors followup,code_debt,low_quality_code
hadoop,16044,description,This is a follow up for HADOOP-15662 as the 001 patch of HADOOP-15662 is already committed.,non_debt,-
hadoop,16044,comment_0,"Following up for -HADOOP-15662, c-ompared to L173: - Updated the format to: *Unknown host name: %s. Retrying to resolve the host name...* - Replace *""ex.getMessage()*"" with although they both return host name string, but the *getHost()* seems to be more readable code. - Removed the else if logic as it breaks the current exception trace. Test: testUnknownHost() would take about 14 minutes due to the retry, I verified the warning format in the console: I haven't come up with a good way to retrieve the log msg for test, any suggestions? Testes against US west account passed: All tests passed my US west account: XNS account oauth 35, 0, Errors: 0, Skipped: 0 324, 0, Errors: 0, Skipped: 22 168, 0, Errors: 0, Skipped: 21 XNS account sharedKey: 35, 0, Errors: 0, Skipped: 0 324, 0, Errors: 0, Skipped: 20 168, 0, Errors: 0, Skipped: 15 non-xns account sharedKe: 35, 0, Errors: 0, Skipped: 0 324, 0, Errors: 0, Skipped: 206 168, 0, Errors: 0, Skipped: 15",code_debt,low_quality_code
hadoop,16044,comment_2,"+1, LGTM",non_debt,-
hadoop,16044,comment_3,"This is a silly question, but why is being retried? Because JVMs do have a history of caching negative DNS lookup results. Looking at the latest javadocs, is set to 10s, so it will eventually come up, but you do have to trust DNS to be updating its records Elsewhere i the code we're just treating this as unrecoverable. Which may not be correct action in a world of dynamicness...but if we do start changing this policy, we should think about doing it consistently everywhere (including HA failover events)",design_debt,non-optimal_design
hadoop,16044,comment_4,"ABFS has been retrying on since it previewed because our understanding is that this exception is thrown for transient name resolution failures. Our retry policy last longer than the typical DNS TTL (or negative cache) of 5 minutes, so the driver could recover and enable a long running task to complete successfully. WASB also retries for these. I expect ADL retries too, although have not confirmed. Mostly we do this for status quo, I mean, it is less likely to cause a regression if we keep the current behavior. With that said, if you have evidence this is a bad design, we should change it. I see that we do the opposite for S3, but I don't know what led to that decision nor do I have a good sense for the behavior in the wild, so I don't know what's best. Certainly retrying is not going to increase the recovery time on the node in question.",design_debt,non-optimal_design
hadoop,16044,comment_5,"bq With that said, if you have evidence this is a bad design, we should change it. no, I don't think it is a bad design. I'm curious. In a classic physical deployment, DNS failures are a bad sign. And, because the JVM cached -ve DNS results *forever* , spinning never fixed things. If the JVMs have stopped doing this, then in a dynamic world, this makes sense. I wondering something broader, which is: is the assumption that not worth retrying"" no longer valid? And if so, what to do about all those existing uses?",design_debt,non-optimal_design
hadoop,16044,comment_6,"Good questions, and the answers are complicated by JVM implementations which may not honor DNS TTL for negative caching. Testing with fault injection could shed light here, but not sure how easy/difficult this is? Could we commit this change while we dig into this more?",test_debt,lack_of_tests
hadoop,16044,comment_7,"+1, committed to branch-3.2+ thanks for explaining your DNS behaviour",non_debt,-
hadoop,16160,summary,fails in branch-2.8,non_debt,-
hadoop,16160,comment_0,"If AdlFS contract test is not enabled, {{fs}} is null in When calling it calls and then it calls {{fs.delete()}}, finally NPE occurs. This issue is fixed by HADOOP-14170 in 2.9+, so I'll backport HADOOP-14170 to fix this issue.",test_debt,lack_of_tests
hadoop,16160,comment_1,Cherry-picked HADOOP-14170 to branch-2.8. Closing.,non_debt,-
hadoop,16207,summary,Improved S3A MR tests,non_debt,-
hadoop,16207,description,"Reported failure of in validation runs of HADOOP-16186; assertIsDirectory with s3guard enabled and a parallel test run: Path ""is recorded as deleted by S3Guard"" The file is there but there's a tombstone. Possibilities * some race condition with another test * tombstones aren't timing out * committers aren't creating that base dir in a way which cleans up S3Guard's tombstones. Remember: we do have to delete that dest dir before the committer runs unless overwrite==true, so at the start of the run there will be a tombstone. It should be overwritten by a success.",non_debt,-
hadoop,16207,comment_0,Stack,non_debt,-
hadoop,16207,comment_1,"could be more fundamental as in ""I'm not sure the committers are correctly telling S3Guard about parent directories"". After each PUT is manifest, we call finishedWrite() , but that seems to do more about purging spurious deleted files, rather than adding dir entries into S3Guard. Provided mkdirs() is called Proposed: build a list of all directories which need to exist as part of a job commit, and only create those entries The other strategy is for to mkdir on the parent. That's a bit more expensive though. Better to not worry about whether it exists and do all this stuff during job commit only",design_debt,non-optimal_design
hadoop,16207,comment_2,"I think this failure is not intermittent, It's happening all the time for me if I run parallel tests. This is actually a returning error when running the tests for HADOOP-16210, so I will refer to this ticket that we know about this and this is not something that the guava update causes.",non_debt,-
hadoop,16207,comment_3,"FIx HADOOP-16184 and provided this is a non-auth test run, this will act as regression test to make sure the fix works in real situations",non_debt,-
hadoop,16207,comment_4,suspecting a race condition in >1 test. If we isolate paths this should go away,non_debt,-
hadoop,16207,comment_5,"Working on this. Finally got a log. And (currently) doesn't exist. Assumption: all the miniYarnClusters are sharing the same /tmp staging dir, so that when one is shutdown while another is running, the second one fails as all its staging files go away -in which case yes, it is a race condition. At least this time.",non_debt,-
hadoop,16207,comment_6,"Staging problem is fixed by MAPREDUCE-6521, and as it only seems to surface when the cluster FS is local (unconfirmed) then its not likely to be the cause of the previous failures (when HDFS was used as the cluster FS) And, given it seems to be a race condition, doesn't explain why we'd see failures during sequential test runs.",non_debt,-
hadoop,16207,comment_7,"Seeing several MR job failures when running tests on HADOOP-16445. always fail when run with -Ds3guard -Ddynamo -Dauth (These fail when starting with a clean DDB table as well) The test setup seems broken to me. * Cluster set up happens with createCluster(new JobConf()) * After this, creates the MRJob with ... -* JobConf will only read core-site.xml ... so the command line parameters -Ds3guard, -Ddynamo -Dauth don't make a difference. Adding in auth-keys.xml or core-site.xml fixed all the test failures for me. (With the additions, the JobConf used by the cluster has these configs, and the tests do what they're supposed to). That isn't the correct fix though. Making sure the test configuration is used to create the JobConf for the cluster and jobs would allow the test properties to work. That said, I did see 3 empty (and marked as deleted) files - part_0000, part_0001, _SUCCESS in the s3guard table. I suspect this is a result of the committer trying to access a file on the client, getting a cached FileSystem instance (same UGI), and the getFileStatus (maybe) creates these S3Guard DDB entries?",non_debt,-
hadoop,16207,comment_8,"Also, to run the tests in parallel - the jobs need to start using a different directory name. Currently, all of them use testMRJob (The method name in the common class that all tests inherit from). The issue with the local dir conflict is a MR configuration afaik (Likely the MR tmp dir config property). YARN clusters should already be able to run in parallel (different ports, random dir names, etc) I'd also be careful trying to run too many of these in parallel, given the amount of memory they consume. Maybe a different parallelism flag for any tests running on a cluster? How about simplifying the code and letting the tests reside in the same class, which makes the code easier to read and allows sharing a cluster more easily. Haven't seen the WIP patch - but sharing a cluster across different tests, which may or may not trigger at the same time seems like it may cause problems. The tests also use a 1 s sleep for the InconsistentFS to get into a consistent state. That can lead to flakiness in the tests. A higher sleep, unless InconsistentFS can be set up with an actual waitForConsistency method which is not time based.",test_debt,flaky_test
hadoop,16207,comment_9,"-Attached a simple patch which fixes just the test failures. Doesn't do anything with parallelism, changing dir names to be different across tests etc. Can submit this in a separate jira, if this one is being used for parallelizing the tests.- Switched to",non_debt,-
hadoop,16207,comment_10,"Final patch replaces the committer-specific terasort and MR test jobs with parameterization of the (now single tests) and use of file:// over hdfs:// as the cluster FS. The parameterization ensures that only one of the specific committer tests run at a time -overloads of the test machines are less likely, and so the suites can be pulled back into the parallel phase. There's also more detailed validation of the stage outputs of the terasorting; if one test fails the rest are all skipped. This and the fact that job output is stored under means failures should be more debuggable. We also have the s3guard operations log enabled; on guarded runs this tracks all calls made of a store, so acts as the log of what changes were made there. If we see intermittent issues here again (And after the HADOOP-16570 changes) then we are better positioned to understand the failures",non_debt,-
hadoop,16226,summary,new Path(String str) does not remove all the trailing slashes of str,non_debt,-
hadoop,16226,description,"new Path(String str) calls Path.normalizePath to normalize the str, however, it does not remove all the slashes of the str. Javadoc says ""remove any trailing path separators"", but it removes only one trailing slash. Credit: This issue was found by  in HDFS-14369.",non_debt,-
hadoop,16226,comment_1,thanks for the patch . Maybe use a simple double-slash comment to avoid this checkstyle issue: +1 pending ^,code_debt,low_quality_code
hadoop,16226,comment_2,Thanks  for the review. Added a period to fix the checkstyle warning.,code_debt,low_quality_code
hadoop,16226,comment_4,Thanx  for the quick fix. LGTM. :),non_debt,-
hadoop,16226,comment_5,1,non_debt,-
hadoop,16226,comment_6,Committed this to trunk and branch-3.2. Thanks  and  for reviewing this.,non_debt,-
hadoop,16230,summary,Correct findbug ignores for unjustified issues during update to guava to 27.0-jre in hadoop-project,non_debt,-
hadoop,16230,description,"In HADOOP-16220 Ive added instead of So it should be instead of The following description is correct, but the code was not: *Null passed for non-null parameter of String, Object, Object, Object) in In we call for this call findbug assumes that {{Argument 4 might be null but must not be null}}, but Guava 27.0's java.lang.String, java.lang.Object, java.lang.Object, java.lang.Object)}} is annotated like the following: so we have {{@Nullable}} on each parameter for the method. I don't see this warning as justified, or need to be fixed.",non_debt,-
hadoop,16230,comment_0,"where we with this? You've closed the first PR? I assume another one is coming, yes?",non_debt,-
hadoop,16230,comment_1,I will fix findbugs issues in HADOOP-16210 to see the whole picture. It seems like I was able to create patch that will fix all findbugs - in code or skip if unjustified.,non_debt,-
hadoop,16230,comment_2,I'll resolve it as won't fix. I will fix this in HADOOP-16210.,non_debt,-
hadoop,16265,summary,is not consistent between default value and manual settings.,code_debt,low_quality_code
hadoop,16265,description,"When call getTimeDuration like this: {color:#333333}If ""nn.interval"" is set manually or configured in xml file, 10000 will be retrurned.{color} If not, 10 will be returned while 10000 is expected. The logic is not consistent.",code_debt,low_quality_code
hadoop,16265,comment_0,"It is a little wired to return only the raw literal number when default time units and literal number is provided. It may mislead us to unexpected behave if we just take into account its name and parameters. Though changing long 10 to string '10s' will return expected result, I think it should return the same result when long 10 and default time unit SECOND is given.",code_debt,low_quality_code
hadoop,16265,comment_1,Maybe I should move it to hadoop-common project.,architecture_debt,violation_of_modularity
hadoop,16265,comment_2,"Hey , this is a good catch. It looks like  and I missed this when doing HDFS-14346. Would it not be simpler to just do: It seems a little weird to me to convert a number to a string just so that we can re-parse it into a number.",design_debt,non-optimal_design
hadoop,16265,comment_3,", thanks. Good point. I will change it.",non_debt,-
hadoop,16265,comment_4,Thanks  for catching this! Should we use {{defaultUnit}} instead of in the latest patch v2?,code_debt,low_quality_code
hadoop,16265,comment_5,", thanks. Sorry for the mistake.",non_debt,-
hadoop,16265,comment_6,"Hey  - I got  to add you as a contributor so that you can have JIRAs assigned to you. I took the liberty of assigning this to you and marking the patch as available so that Jenkins will run. By the way, when submitting a patch against {{trunk}}, you don't need to include the branch in the patch name. This is only necessary when submitting a patch against other branches; trunk is the default.",non_debt,-
hadoop,16265,comment_8,+1 on patch v3.,non_debt,-
hadoop,16265,comment_9,", thanks. 'trunk' will be removed next time.",non_debt,-
hadoop,16265,comment_10,+1 from me as well. I just committed this to {{trunk}} ~ {{branch-3.0}}. Thank you for your contribution !,non_debt,-
hadoop,16291,summary,HDFS Permissions Guide appears incorrect about,documentation_debt,low_quality_documentation
hadoop,16291,description,Fix some errors in the HDFS Permissions doc. Noticed this when reviewing HADOOP-16251. The FS Permissions seems to mark a lot of permissions as Not Applicable (N/A) when that is not the case. In particular getFileInfo (getFileStatus) checks READ permission bit as it should.,documentation_debt,low_quality_documentation
hadoop,16291,comment_0,"Docs are correct. That's the HA state check, not a permissions check.",non_debt,-
hadoop,16291,comment_1,Thanks . Thought it was strange the docs were wrong for this long. Was going to ask for a sanity check on this JIRA but you beat me to it.,documentation_debt,low_quality_documentation
hadoop,16318,summary,Upgrade JUnit from 4 to 5 in hadoop security,non_debt,-
hadoop,16318,description,Upgrade JUnit from 4 to 5 in hadoop security,non_debt,-
hadoop,16318,comment_0,"replaced assertions, its argument orders, and annotations except Test(timeout=... / Test(expected=... cases. - I replaced with ""git grep -l"" and ""sed"" commands below: then replaced wild card import with single class imports. - I would like to fix Test annotations with timeout/expected parameters in other tickets, because it seems the best alternative for global timeout in JUnit5 is not determined yet.",non_debt,-
hadoop,16318,comment_3,"Thanks  for the patch! * Are the unit test failures related to the patch? If the answer is yes, you need to fix the unit tests. * Would you fix checkstyle warnings? * All the change is in hadoop-common project. I'll move this issue from HDFS to HADOOP.",code_debt,low_quality_code
hadoop,16318,comment_5,"we have the global timeouts One concern I have with all this migration is that it's going to make backporting harder; I've already hit backport issues with the move from to # How urgent does this migration need to be? # could we mandate it more for new tests, before migrating everything else?",non_debt,-
hadoop,16318,comment_6,"JUnit 4 is still maintained, so I think the migration is not so urgent for now. +1, I'll send an e-mail to Hadoop dev MLs.",non_debt,-
hadoop,16332,summary,Remove S3A's depedency on http core,non_debt,-
hadoop,16332,description,HADOOP-16085 added a dependency on apache httpcore just to get a constant of an http error code. This is a needless dependency which can only cause problems downstream. replace the external constant with an internal one and remove the new dependency.,build_debt,over-declared_dependencies
hadoop,16345,summary,Potential NPE when instantiating FairCallQueue metrics,non_debt,-
hadoop,16345,description,"initially reported the following issue on HADOOP-16266: It would appera that when HADOOP-15481 added support for FCQ stats to the metrics2 system, it opens up the potential for the {{MetricsProxy}} to be used _before_ the delegate has been set, causing an NPE.",non_debt,-
hadoop,16345,comment_0,"Within the constructor for {{FairCallQueue}}, we have: After HADOOP-15481, the constructor for {{MetricsProxy}} looks like: Immediately upon creation of the metrics proxy, it is registered with the metrics system, so its {{getQueueSizes()}} and methods can be called at any time. Currently, those methods assume that {{delegate}} is non-null, but this is only true after {{setDelegate()}} has been called for the first time. Since this occurs after registration, there is a window of time where the metrics system may attempt to read one of the metrics methods, but the delegate has not yet been set, causing an NPE. I have attached a simple v000 patch which solves this issue. I don't see any easy way to test it beyond adding some timing injection logic between the 2nd and 3rd line in the first code snippet, but that seems overkill for this situation. I am open to other ideas. cc  who helped commit HADOOP-15481.",non_debt,-
hadoop,16345,comment_2,"v000 patch LGTM, +1",non_debt,-
hadoop,16345,comment_3,"Thanks ! , do you want to take a look? Otherwise I will commit early next week.",non_debt,-
hadoop,16345,comment_4,"LGTM, Thanks for the fix!",non_debt,-
hadoop,16345,comment_6,"I fixed this in all of the branches that have HADOOP-15481, {{branch-2}} ~ {{trunk}}. I checked the build failure above and it was due to {{mvn}} not being installed on the Jenkins box.. Thanks again for the help !",non_debt,-
hadoop,16359,summary,Bundle ZSTD native in branch-2,non_debt,-
hadoop,16359,description,HADOOP-13578 introduced ZSTD codecs but the backport to branch-2 didn't include the bundle change in which should be included.,non_debt,-
hadoop,16359,comment_1,", : could you review this? Thanks!",non_debt,-
hadoop,16359,comment_2,"I took a look and this patch matches the code committed in the {{trunk}} version. I'll give a tentative +1 based on that, but I won't pretend to understand how the native libs or the shell code is set up, so I would prefer to get a review from someone else more familiar if possible.",non_debt,-
hadoop,16359,comment_3,"I will review.  did some libzstd work before, so FYI.",non_debt,-
hadoop,16359,comment_4,"Precommit test doesn't compile the code and bundle the native lib, which is what I would like to see. , I guess you had already tested so I will +1, but if our precommit test doesn't build with zstd lib then we probably want to figure out how to add it. As I can see, the ZStandard library is getting quite some traction these days.",non_debt,-
hadoop,16359,comment_5,yes I tried the patch on 2.9 and it worked - before the patch the native libs are not bundled under Not sure how to test this - does other codecs have unit tests for the bundling part?,test_debt,lack_of_tests
hadoop,16359,comment_6,+1 I'll file a separate Yetus Jira for the native build flag.,non_debt,-
hadoop,16359,comment_7,"Thanks, pushed to branch-2 and branch-2.9",non_debt,-
hadoop,16359,comment_8,Filed YETUS-895 for the precommit issue,non_debt,-
hadoop,16359,comment_9,Thanks  and !,non_debt,-
hadoop,16409,summary,Allow authoritative mode on non-qualified paths,non_debt,-
hadoop,16409,description,"currently requires a qualified URI (e.g. s3a://bucket/path) which is how I see this being used most immediately, but it also make sense for someone to just be able to configure /path, if all of their buckets follow that pattern, or if they're providing configuration already in a bucket-specific context (e.g. job-level configs, etc.) Just need to qualify whatever is passed in to allowAuthoritative to make that work. Also, in HADOOP-16396 Gabor pointed out a few whitepace nits that I neglected to fix before merging.",code_debt,low_quality_code
hadoop,16409,comment_0,"+1, committed to trunk.",non_debt,-
hadoop,16435,summary,RpcMetrics should not be retained forever,non_debt,-
hadoop,16435,description,* RpcMetrics objects are registered into * although there is a shutdown() call (which is actually invoked) it doesn't unregisters itself from the * RpcDetailedMetrics has the same issue background * hiveserver2 slowly eats up memory when running simple queries in new sessions (select 1) * every session opens a tezsession * tezsession has rpcmetrics * with a 150M heap after around 30 session the jvm gets outofmemmory,non_debt,-
hadoop,16435,comment_1,"This fix makes sense to me. Hi , could you attach the information of the java heap if available?",non_debt,-
hadoop,16435,comment_3,I've uploaded some more info; including a full heapdump somewhere around the heap gets filled up,non_debt,-
hadoop,16435,comment_4,will be called when the server is stopped. so I wonder whether the solution is ok?,non_debt,-
hadoop,16435,comment_5,"yes, it will be unregistered when the server is stopped - if I don't fully understand your concern; could you be more specific?",non_debt,-
hadoop,16435,comment_7,I mean that the session is end is not identity that the server is end .,non_debt,-
hadoop,16435,comment_8,I don't think this is about a single session - these metrics are only used in the [IPC If you create an server and then shut it down - would you expect its metrics to be retained? (IMHO: if it silently retaines any information related to the actual instance that's a resource leak),code_debt,low_quality_code
hadoop,16435,comment_9,"thank you for the information. \+1, I confirmed there are 30\+ RpcMetrics retained in the heap dump. Let's unregister this.",non_debt,-
hadoop,16435,comment_11,"Committed this to trunk, branch-3.2, branch-3.1, branch-2, branch-2.9, and branch-2.8. Thanks  for the contribution and thanks  for the comments.",non_debt,-
hadoop,16461,summary,Regression: FileSystem cache lock parses XML within the lock,non_debt,-
hadoop,16461,description,"The lock now has a ShutdownHook creation, which ends up doing which ends up doing a ""new Configuration()"" within the locked section. This indirectly hurts the cache hit scenarios as well, since if the lock on this is held, then the other section cannot be entered either. slowing down the RawLocalFileSystem when there are other threads creating HDFS FileSystem objects at the same time.",design_debt,non-optimal_design
hadoop,16461,comment_0,"gopal, can you give us files & lines rather than just code snippets. thanks",non_debt,-
hadoop,16461,comment_1,Linked the lines up and opened a PR.,non_debt,-
hadoop,16461,comment_3,"+1, committed if this affects earlier branches, let me know & Ill cherry pick",non_debt,-
hadoop,16461,comment_4,Commit applies to branch-3.2 and branch-3.1. I'll cherry pick the commit into these two branches.,non_debt,-
hadoop,16504,summary,Increase default from 128 to 256,non_debt,-
hadoop,16504,description,"Because default value is too small, TCP's ListenDrop indicator along with the rpc request large. The upper limit of the system's semi-join queue is 65636 and maximum number of fully connected queues is 1024. I think this default value should be adjusted.",design_debt,non-optimal_design
hadoop,16504,comment_0,"Hi Thank you very much for paying attention to this issue. I have worked it, so I assign again to me.Hope you do not mind.",non_debt,-
hadoop,16504,comment_1,"it's ok, I haven't started yet.",non_debt,-
hadoop,16504,comment_2,"Hi  thanks for raising the issue. Would you please describe what's the intended use of this change? Is it meant for NameNode, or other services? What's the impact of this change? Like, does it reduce client connection timeout when NN is busy? Thanks",non_debt,-
hadoop,16504,comment_3,"Note, you also should change the default value in core-site.xml too.",non_debt,-
hadoop,16504,comment_4,"The problem now is that since listen queue is full, TCP can drop many packet and resend packet after a while. So tcp connection to be slow or even timed out. Changing this default value can reduce client connection timeout when the request is large for nn or dn. Update the v1 patch and upload it. Thank you.",code_debt,slow_algorithm
hadoop,16504,comment_5,ping  Could you help continue review it? Thank you.,non_debt,-
hadoop,16504,comment_6,Looks reasonable to me. We even have customers increasing this number to 2k or even 16k. The only potential downside is it could result in more memory usage. Not sure how much it could be but I don't think that's a concern for typical deployments. I'd like to wait for a day or two for any one else to have a chance to assess & comment before I commit this patch.,design_debt,non-optimal_design
hadoop,16504,comment_7,hi  no one comment for a few days and this patch should be no problem. Should we commit this patch? Thank you.,non_debt,-
hadoop,16504,comment_8,+1 committing this now.,non_debt,-
hadoop,16504,comment_9,Thanks !,non_debt,-
hadoop,16520,summary,Race condition in DDB table init and waiting threads,non_debt,-
hadoop,16520,description,"s3guard threads waiting for table creation completion can be scheduled before the creating thread, look for the version marker and then fail. window will be sleep times in AWS SDK",non_debt,-
hadoop,16520,comment_1,I'll try to solve this the other way: add the version marker in IFF the table lacks version marker AND empty.,non_debt,-
hadoop,16520,comment_2,see also HADOOP-16349,non_debt,-
hadoop,16520,comment_3,+1 on #1576 from Committing. Thanks.,non_debt,-
hadoop,16523,summary,Minor spell mistake in comment (PR#388),code_debt,low_quality_code
hadoop,16523,description,File a jira for PR#388.,non_debt,-
hadoop,16550,summary,Spark config name error on the Launching Applications Using Docker Containers page,non_debt,-
hadoop,16550,description,"On the ""Launching Applications Using Docker Containers"" page at the ""Example: Spark"" section the Spark config for configuring the environment variables for the application master the config prefix are wrong: - - The correct ones: - - See",non_debt,-
hadoop,16550,comment_0,could you please take a look to this issue and PR?,non_debt,-
hadoop,16550,comment_1,"Sure, thanks for the contribution . LGTM, +1. on the PR",non_debt,-
hadoop,16550,comment_2,Merged PR #9,non_debt,-
hadoop,16601,summary,Add support for hardware crc32 of nativetask checksums on aarch64 arch,non_debt,-
hadoop,16601,description,"Add support for aarch64 CRC instructions in nativetask module, optimize the CRC32 and CRC32C. Use the benchmark tools : nttest , the improvement is quite substantial: *CRC32 Zlib polynomial 0x04C11DB7* *CRC32C Castagnoli polynomial 0x1EDC6F41*",non_debt,-
hadoop,16601,comment_1,"* it's best if you file a github PR with this JIRA at the start of the title * Look at the Yetus complaints about style, swap tabs for spaces we really love unit tests, and new patches to improve that coverage * you MUST run the native tests on arm64 and give us the results; yetus won't be testing your code * and if you can think of new tests to put into test_bulk_crc32.c, that'd be good too. Are there ways to break things?",test_debt,low_coverage
hadoop,16601,comment_3,"Thanks~ * Modify the patch according the Yetus's suggestion. * Run the native tests on arm64 , the execute file is attaching the results file: * The issue is optimized for performance, adding new tests in test_bulk_crc32.c if need it. * Testing the hadoop test case (include: with the test tool HiBench-HiBench-7.0 , all testcases had passed.",code_debt,slow_algorithm
hadoop,16602,summary,mvn package fails in hadoop-aws,non_debt,-
hadoop,16602,description,mvn package seems to fail in hadoop-aws,non_debt,-
hadoop,16602,comment_1,"+1, merged to trunk thanks for spotting and fixing this!",non_debt,-
hadoop,16602,comment_3,", Thank you for your review!",non_debt,-
hadoop,16607,summary,s3a attempts to look up password/encryption fail if JCEKS file unreadable,non_debt,-
hadoop,16607,description,"Hive deployments can use a JCEKs file to store secrets, which it sets up To be readable only by the Hive user, listing it under When it tries to create an S3A FS instance as another user, via a doAs{} clause, the S3A FS getPassword() call fails on the subsequent -even if the secret it is looking for is in the XML file or, as in the case of encryption settings, or session key undefined. I can you point the blame at hive for this -it's the one with a forbidden JCEKS file on the provider path, but I think it is easiest to fix in S3AUtils than in hive, and safer then changing Configuration. ABFS is likely to see the same problem. I propose an option to set the fallback policy. I initially thought about always handling this: Catching the exception, attempting to downgrade to Reading XML and if that fails rethrowing the caught exception. However, this will do the wrong thing if the option is completely undefined, As is common with the encryption settings. I don't want to simply default to log and continue here though, as it may be a legitimate failure -such as when you really do want to read secrets from such a source. Issue: what fallback policies? * fail: fail fast. today's policy; the default. * ignore: log and continue We could try and be clever in future. To get away with that, we would have to declare which options were considered compulsory and re-throw the caught Exception if no value was found in the XML file. That can be a future enhancement -but it is why I want the policy to be an enumeration, rather than a simple boolean. Tests: should be straightforward; set to a non-existent file and expected to be processed according to the settings.",design_debt,non-optimal_design
hadoop,16607,comment_0,"The only thing we could change code-wise is to fallback better in S3A And ABFS when the client gets an error trying to open/read a JCEKS:// URL reference to a file. Larry and I have discussed having a slightly different URL e.g. ""JCEKS2://"" which delegates to a specific reader that will convert access exceptions into ""no entry found"" responses -this can then be used in hive and elsewhere. This will keep any exceptions swallowing tricks out of the object stores; while providing a straightforward way for applications to use it when desired. Probably not that hard to do -though I have never actually looked at the code.",non_debt,-
hbase,21,summary,hbase jar has hbase-default.xml at top-level rather than under a conf dir,architecture_debt,violation_of_modularity
hbase,21,description,I don't think it'll be found at this location. Verify.,non_debt,-
hbase,21,comment_0,Fix for 0.2.0.,non_debt,-
hbase,21,comment_1,"Turns out, hbase-default.xml at top-level in jar is right place to put it.. not under a conf. Resolving as invalid.",non_debt,-
hbase,23,summary,UI listing regions should be sorted by address and show additional region state,non_debt,-
hbase,23,description,"Currently, looking at a cluster under load, you'll often trip over the disorientating listing of more than one region with a null end key (Was seen by Billy yesterday and psaab today). UI should list out all of its attributes. Also sort region listings by server address so easier finding servers.",code_debt,low_quality_code
hbase,23,comment_0,web ui should list hostnames instead of or in addition to server addresses -- from paul saab (Make sure the hostname in ISA instance in HServerAddress is cached -- and that we're not doing a lookup each time).,non_debt,-
hbase,23,comment_1,Do for 0.16 release.,non_debt,-
hbase,23,comment_2,HBase needs at least the encoded region name added. Do for 0.2.0.,non_debt,-
hbase,23,comment_3,"Other suggestions apart from the few above: + Fix the load column so its requests per second, count of regions and also lists new 'avg. load' attribute from the new balancing code in TRUNK + Perhaps fix the 'Tables' listing so that instead it was divided into user and catalog tables ('Online META Regions' is a bad title IMO -- gives wrong impression that these are just regions and not members of actual catalog tables). Clicking on a table would give a list of all regions that comprise that table. Each line of this table would also list where the region is hosted. Clicking on the address of the region will take you to the hosting regionserver.",non_debt,-
hbase,23,comment_4,Above its suggested that we add encoded region name to region listings. We should also list is region is online or offline and if it has daughter regions if a split.,non_debt,-
hbase,23,comment_5,I'll have a look into it.,non_debt,-
hbase,23,comment_6,"I see that the jsp code is really HQL-coupled (ShowCommand, TableFormatter, ReturnMsg, etc). Should it stay like this for the moment?",non_debt,-
hbase,23,comment_7,"Ugh. Yes, for the moment.",non_debt,-
hbase,23,comment_8,"For the hostname, does it replace the address or not? Also, I've checked in ISA and the hostname is a property For the load average, I've searched a bit and I can't find the information. Any ideas?",non_debt,-
hbase,23,comment_9,For the moment shown regions are only those which are online. So you suggest having 2 tables or 1 with an extra column?,non_debt,-
hbase,23,comment_10,"Yeah, I suppose showing offlined regions would be a bit tough unless you read the .META. Leave this one off the list. Its possible to figure what you need using shell and only fellas debugging will want the info (though if easy enough, you could print out whether a region has 'references' -- this would demark the regions that still hold references to parent region). On hostname, it is available? If so, great. I'd say replace the IP w/ hostname. Thats what most would prefer. Regards load, there is You could print this out in UI as one of the cluster properties (load and average). Otherwise, I there is the loadToServers map which I think we're already reading. Good stuff Jean-Daniel.",non_debt,-
hbase,23,comment_11,"Funny how there is a onlineRegions attribute in HRegionServer but no offlineRegions. At first glance it seems logic that the two exists but it isn't. Bit misleading. Yeah in ISA. This is the same as in the dfshealth.jsp page in hadoop ui. I added a getHostname method in HServerAddress just like getPort or getBindAddress to get it in master.jsp Ok. Indeed, this is why I was wondering why that average load by server was nowhere to be found.",non_debt,-
hbase,23,comment_12,"Oh and one more thing, look at the code which generates the tables table : Here comes the HQL coupling to decouple a bit, I guess.",non_debt,-
hbase,23,comment_13,Yeah. HQL has been configured to output xhtml so you can just toString the result of the execute.,non_debt,-
hbase,23,comment_14,"Things to look at in particular : + In table.jsp, there is an IOE that is not managed. There is no other place in the webapps were exceptions are thrown so I decided to do it like this since this exception wouldn't mean anything special. + In HTable, I copied code from getStartKeys to create the getRegionsInfo. This should be fixed with the MetaRegion refactoring. Also, I have to return a Map of HRegionInfo and HServerAddress because there is no other way to get information about a region's regionserver (I think).",non_debt,-
hbase,23,comment_15,Forgot to change CHANGES and I fixed a comment in HTable. Review please.,non_debt,-
hbase,23,comment_16,"FYI, leave out the CHANGES.txt changes. Your patch will fail if someone has made a commit ahead of yours. I tried the patch. Looks really good. I love that we're now showing server names and thanks for renaming catalog tables section. Would suggest that no reason catalog tables shouldn't be clickable as user tables are. Usually will be one region only but even so, the new Table page shows where that region is located and gives a never-before available quick-path to the hosting region server. The Is Split column in the table will probably never be true especially when we are doing this: Would suggest you remove it (Sorry - -my fault for suggesting it in the first place) Would you mind fixing the requests calcuation? Its kinda weird at the moment. Its requests per (default 3 seconds). It should be showing requests per second. Thats what people expect. On the code, FYI, the hadoop convention is two-spaces for tabs. Regards your TODO, that you've duplicated code until we add MetaTable, thats fine. Was wondering if the .META. table was made of multiple regions and your scanner had to cross from one to the other, do you handle that case? Otherwise, the patch is great.",code_debt,duplicated_code
hbase,23,comment_17,"It think so. The first loop scans over each region of .META. Regards the calculations, I searched in the whole source and it doesn't seem to be used, only referred to in the text so I think I will just remove it. Correct me if I'm wrong. So that means that they are clickable? I'm confused.",code_debt,dead_code
hbase,23,comment_18,"Review please. Things that changed : + Request load is now expressed in req/sec. + ROOT and META tables are clickable. The fact that these tables are not handled like user tables internally impacts the way I could show information. Suggestions appreciated. + Small indentation change. - Removed ""is split"" information on regions.",code_debt,low_quality_code
hbase,23,comment_19,"Committed with a few tweaks (updated help text, used new getHostname everywhere so no more IP addresses). Below is commit message. Thanks for the patch Jean-Daniel.",non_debt,-
hbase,76,summary,Purge servers of Text,non_debt,-
hbase,76,description,"Chatting with Jim while looking at profiler outputs, we should make an effort at purging the servers of the Text type so HRegionServer doesn't ever have to deal in Characters and the concomitant encode/decode to UTF-8. Toward this end, we'd make changes like moving HStoreKey to have four rather than 3 data members: column family, column family qualifier, row + timestamp done as a basic Writable -- -- and a long rather than a Text column, Text row and a timestamp long. This would save on our having to do the relatively expensive 'find' of the column family separator inside in extractFamily (",design_debt,non-optimal_design
hbase,76,comment_0,"If TextSequence obeyed ISA sematics wrt Text, would that solve the problem? Or should we move from Text and TextSequence to String?",non_debt,-
hbase,76,comment_1,"I think we should split the column into family and qualifier anyway, because it will benefit us when it comes time to switch mapfile implementation.",design_debt,non-optimal_design
hbase,76,comment_2,"TS made a difference profiling saving on Text creations but it can only be used in a few places; its dangerous using it everywhere (the underlying Text can move from under it). Profiling, an HSK that had family and qualifier members rather just than a column that we then have to split in many places would save us lots of object creations and CPU.",design_debt,non-optimal_design
hbase,76,comment_3,"If TS really ISA Text, then the Text won't be able to move out from under it because TS won't HAVEA Text anymore.",non_debt,-
hbase,76,comment_4,Then we might as well just make Text instances rather than TS's (Point of TS was pseudo-substring w/o having to create new instances).,non_debt,-
hbase,76,comment_5,"Here is a little test program I wrote to test the speed of serialization of Text vs String. While String is a little slower than Text, it isn't by much. String also has the advantage of being immutable once created. Test results: Serialized 1000000 Strings in 547 milliseconds Deserialized 1000000 Strings in 860 milliseconds Serialized 1000000 Text objects in 531 milliseconds Deserialized 1000000 Text objects in 500 milliseconds",non_debt,-
hbase,76,comment_6,I think the important extraction from this issue is that we should split the column name into separate column family and cell qualifier text instances in HStoreKey. Then we'd never have to search the column name for the separator.,design_debt,non-optimal_design
hbase,76,comment_7,"Agreed on separation. The comparison of String and Text was just something I wanted to settle in my mind since I had heard that String was far less efficient than Text. Turns out, not so much.",code_debt,slow_algorithm
hbase,76,comment_8,Attach the code used doing the test jim. Will help understand the timings.,non_debt,-
hbase,76,comment_9,??? its already here.,non_debt,-
hbase,76,comment_10,Pardon me. Didn't look for attachment. Test contrasts String's native UTF-8ing with Text's and then construction of either from bytes. Looks like the Text UTF8'ing ain't that much faster than String's. The big difference deserializing is kinda odd -- String is doing extra work? Text and String though are different animals I suppose; the one is backed by UTF-8 bytes while the other is backed by UTF-16BE.,non_debt,-
hbase,76,comment_11,"Just thought it was an interesting comparison. It might have different results if I had used real UTF-8 characters instead of just ASCII, don't know.",non_debt,-
hbase,76,comment_12,HBASE-82 removed Text from servers.,non_debt,-
hbase,93,summary,[Hbase Shell] Error in Help-string of create command.,non_debt,-
hbase,93,description,"- VECTOR_SIZE=n NUM_HASH=n], "" + VECTOR_SIZE=n NUM_HASH=n], """,non_debt,-
hbase,93,comment_0,submitting.,non_debt,-
hbase,93,comment_2,Committed. Thanks for that patch Edward.,non_debt,-
hbase,154,summary,[hbase] Master marks region offline when it is recovering from a region server death,non_debt,-
hbase,154,description,"While looking for regions that were being served by a downed region server, the master will mark any region the server was serving offline.",non_debt,-
hbase,154,comment_1,"Tests passed. Committed to trunk and back-ported, committed to 0.16.0",non_debt,-
hbase,182,summary,hbase shell: phantom columns show up from select command,non_debt,-
hbase,182,description,"Note the phantom value for test:b in row 2. I looked at the code, and it looks like incorrectly fails to call results.clear() every time it calls scan.next(). However, I also think that the key,",non_debt,-
hbase,182,comment_0,"Sorry Michael for debugging pain. This issue has been fixed in TRUNK (I know you are using released hbase). Up on IRC, you suggest that we do a clear internally to next on the passed Map. I think this a good idea. Let me do that as part of this issue.",non_debt,-
hbase,182,comment_1,"oh, thanks.",non_debt,-
hbase,182,comment_3,"Reran locally tests that 'failed', tests that usually never fail and they passed. Resubmitting.",non_debt,-
hbase,182,comment_5,Committed clearing of map on each next invocation.,non_debt,-
hbase,203,summary,hbase not spliting when the total size of region reaches max region size * 1.5,non_debt,-
hbase,203,description,right now a region may get larger then the max size set in the conf HRegion.needsSplit Checks the largest column to see if its larger then max region size * 1.5 and then desides to split or not But if we have more then one column the region could be vary large example Say we have 10 columns all about the same size lets say 40MB and the max file size is 64MB we would not split even thought the region size is 400MB well over the 96MB needed to trip a split to happen.,non_debt,-
hbase,203,comment_0,"Jim makes the point that the config. is actually the size of any one HStore HStoreFile. Reading the description, I can see that it could be interpreted as region size. Lets fix that at least as part of this issue.",non_debt,-
hbase,203,comment_1,Clarified documentation. Committed with changes for HADOOP-2525,non_debt,-
hbase,205,summary,[hbase] REST servlet throws NPE when any value node has an empty string,non_debt,-
hbase,205,description,"If you PUT/POST an XML entity body that has an empty string as the value (<value></value>), the servlet will fail to decode it properly.",non_debt,-
hbase,205,comment_0,"This patch resolves the problem. If an empty value is passed in, it will be handled appropriately.",non_debt,-
hbase,205,comment_1,Sending to Hudson.,non_debt,-
hbase,205,comment_3,Committed. Resolving. Thanks for the patch Bryan.,non_debt,-
hbase,206,summary,hbase table filename problem,non_debt,-
hbase,206,description,running ver 0.15.0 I store web pages in hbase and use the urls as row keys like google does with bigtable but it seams that with foward slashes ( / ) as the row key breaks the path for the hbase filenames example starting off one of my tables has this file name but when it trys to split it will have these file names The / in the row key from the url is breaking the path name to the hregion file. This causes the region server to exit. I assume it would kill all region servers in a pool by assigning each one the table and each would die on trying the split the table. Easy solution for this would be the key/filename need to be escaped but I am not sure how thats done in java.,non_debt,-
hbase,206,comment_0,This issue is fixed in Trunk.,non_debt,-
hbase,239,summary,HBase | Incorrect classpath in binary version of Hadoop,non_debt,-
hbase,239,description,The HBase classpath is incorrect in binary version downloads of Hadoop. I've attached a patch that Stack generated after fixing the bug on my local machine.,non_debt,-
hbase,239,comment_0,A patch to fix the issue with the HBase classpath.,non_debt,-
hbase,239,comment_1,Build and passes all tests locally. Trying against hudson.,non_debt,-
hbase,239,comment_3,Committed w/ below message. Resolving. M bin/hbase Had a hard-coded name for the hbase jar. Fix so allows for version in jar name.,code_debt,low_quality_code
hbase,274,summary,Add to MapFile a getClosest that returns key that comes just-before if key not present (Currently does just-after only).,non_debt,-
hbase,274,description,"The list of regions that make up a table in hbase are effectively kept in a mapfile. Regions are identified by the first row contained by that region. To find the region that contains a particular row, we need to be able to search the mapfile of regions to find the closest matching row that falls just-before the searched-for key rather than the just-after that is current mapfile getClosest behavior.",non_debt,-
hbase,274,comment_0,Amended mapfile and mapfile unit test.,non_debt,-
hbase,274,comment_1,v2 removes some dross included by misstack.,non_debt,-
hbase,274,comment_2,Passing to hudson.,non_debt,-
hbase,274,comment_4,v3 fixes javadoc warning.,documentation_debt,low_quality_documentation
hbase,274,comment_5,Going to hudson again with fix for javadoc warning.,non_debt,-
hbase,274,comment_7,Failed because I'd forgotten to add the TruncateCommand class as part of a previous commit (HADOOP-2240). Retry.,non_debt,-
hbase,274,comment_8,Retrying patch.,non_debt,-
hbase,274,comment_10,"Failure is in an unrelated hbase test. Hopefully this failure doesn't get in the way of this patch getting reviewed. Hbase needs this feature in MapFile so the hbase client will work against large clusters, particularly those undergoing churn: upload or failures, etc.",non_debt,-
hbase,274,comment_11,+1 This looks fine to me.,non_debt,-
hbase,274,comment_12,+1 This should make my HBase cache patch much more efficient.,code_debt,slow_algorithm
hbase,274,comment_13,I've just committed this. Thanks stack!,non_debt,-
hbase,282,summary,[hbase] provide multiple language bindings for HBase,non_debt,-
hbase,282,description,"There have been a number of requests for multiple language bindings for HBase. While there is now a REST interface, this may not be suited for high-volume applications. A couple of suggested approaches have been proposed: - Provide a Thrift based API (very fast socket based but some of the languages are not well supported) - Provide a JSON based API over sockets. (faster than REST, but probably slower than Thrift) Others?",non_debt,-
hbase,282,comment_0,"-1 on JSON over sockets. JSON is probably more lightweight than XML in REST, true, but we'd still need some basic transmission protocol, like REST. However, I'm not convinced that JSON would make that much of a difference in the long run, since JSON is a printable format like XML, which means we'd still need to encode the binary data, which I have to think is the slow part. +1 on Thrift. It seems to be the best option. If other languages haven't got Thrift libraries, the solution is to make one or use REST. As more time passes, Thrift will support more and more languages, so this will be less of a problem. I've created a draft Thrift spec on the HBase wiki at Feel free to make suggestions or changes.",non_debt,-
hbase,282,comment_1,"+1 on both JSON, Thrift.",non_debt,-
hbase,282,comment_2,Related:,non_debt,-
hbase,282,comment_3,"I'm looking at implementing the server-side of the Thrift API. Let me know if there's already work in progress going on beyond the spec on the wiki. As a matter of introduction, I'm a colleague of Chad, Michael, and Jim over at Powerset and I've been working on various non-Hbase/Hadoop projects that use Thrift.",non_debt,-
hbase,282,comment_4,"diff that adds the Hbase.thrift API file, a ThriftServer class, and some example clients",non_debt,-
hbase,282,comment_5,"Thrift Java runtime library, taken from the facebook SVN repo at revision 746",non_debt,-
hbase,282,comment_6,"I attached a patch file and a thrift Java runtime jar file that implements a Thrift service for Hbase. The patch includes the Hbase.thrift interface defintion, a ThriftServer class which can be launched from './bin/hbase thrift', and some example clients which are in the src/examples/thrift directory. I'm also attaching the Hbase.thrift file directly so it's easy to view for comments. Here's what's included in the package.html: * Package Description Provides an HBase Thrift service. This directory contains a Thrift interface definition file for an Hbase RPC service and a Java server implementation. * What is Thrift? ""Thrift is a software framework for scalable cross-language services development. It combines a powerful software stack with a code generation engine to build services that work efficiently and seamlessly between C++, Java, Python, PHP, and Ruby. Thrift was developed at Facebook, and we are now releasing it as open source."" For additional information, see Facebook has announced their intent to migrate Thrift into Apache Incubator. * Description The Hbase API is defined in the file Hbase.thrift. A server-side implementation of the API is in ThriftServer. The generated interfaces, types, and RPC utility files are checked into SVN under the directory. The files were generated by running the commands: thrift -strict -java Hbase.thrift mv . rm -rf gen-java The 'thrift' binary is the Thrift compiler, and it is distributed as a part of the Thrift package. Additionally, specific language runtime libraries are a part of the Thrift package. A version of the Java runtime is checked into SVN under the hbase/lib directory. The version of Thrift used to generate the Java files is revision 746 from the SVN repository. The ThriftServer is run like: ./bin/hbase thrift [-h|--help] [-p|--port PORT] The default port is 9090. -",non_debt,-
hbase,282,comment_7,Hbase Thrift API,non_debt,-
hbase,282,comment_8,+1 on patch. I committed the libthrift.jar so that your patch will build on hudson when you want to submit it there.,non_debt,-
hbase,282,comment_9,re-uploading to apply patch,non_debt,-
hbase,282,comment_10,re-uploading,non_debt,-
hbase,282,comment_13,"Committed. Resolving (Thrift IDL facilitiates, IMV, 'multiple language bindings'). Thanks for the patch Dave.",non_debt,-
hbase,325,summary,[hbase] Compaction cleanup; less deleting + prevent possible file leaks,code_debt,low_quality_code
hbase,325,description,"This issue is being created so I can commit the compaction patch that just passed hudson over in HADOOP-2283. That issue is about trouble accessing hdfs. It should stay open since haven't yet figured whats up. As a by-product of the investigation, the compaction patch was generated.",non_debt,-
hbase,325,comment_0,Resolving. Committed the compaction.patch from over in HADOOP-2283.,non_debt,-
hbase,369,summary,[hbase] Make HRegionInterface more like that of HTable,non_debt,-
hbase,369,description,"The client api (as defined by HTable and HBaseAdmin are not insignificantly different from HRegionInterface (wire protocol). They could be made much more similar by implementing a couple of new Writable classes (e.g., SortedMapWritable).",code_debt,low_quality_code
hbase,369,comment_0,Tests run on Linux and Windows. See if we can also get a +1 from Hudson,non_debt,-
hbase,369,comment_1,"Committed. Changes: New class MapWritable replaces KeyedData and HBaseAdmin, HConnectionManager, HMaster, HRegionInterface, HRegionServer, HTable, TestScanner2: - getRow returns MapWritable instead of array of KeyedData - next returns MapWritable instead of array of KeyedData GroupingTableMap, IdentityTableMap, TableInputFormat, TableMap, TableOutputFormat, TestTableMapReduce: - use MapWritable instead of KeyedData and",non_debt,-
hbase,396,summary,hbase contrib javadoc on apache.org,non_debt,-
hbase,396,description,"Currently javadoc for hbase contrib does not show up anywhere at Below is some discussion from hadoop-dev list on how to do the hbase contrib javadoc build. From: Doug Cutting Re: javadoc for hbase on apache.org I'd vote for including it as we have other contrib documentation, as a separate section in the main javadoc tree. Doug Michael Stack wrote: > St.Ack",documentation_debt,low_quality_documentation
hbase,396,comment_0,Attached is a patch that amends the top-level build.xml adding hbase to include build of hbase contrib. javadoc,non_debt,-
hbase,396,comment_1,Patch available.,non_debt,-
hbase,396,comment_3,Dear Hadoop QA (Nigel): Please rerun the test of this patch. Patch should succeed now after the commit of HADOOP-1402 to fix warnings. Thanks.,non_debt,-
hbase,396,comment_5,"I just committed this. Thanks, Michael!",non_debt,-
hbase,413,summary,add mailing list to gmane.org,non_debt,-
hbase,413,description,do we want our mailing list on gmane? I use it for tracking hadoop when we where on there so I thank it would be helpfull and can not thank of any reason not to. looking at there FAQ we need to subscribe the list to them The subscribe form is here,non_debt,-
hbase,413,comment_0,Submitted requests for hbase mailing lists to be added to gmane.org,non_debt,-
hbase,413,comment_1,I received email today stating that hbase-dev and hbase-user are available on gmane.org at and respectively.,non_debt,-
hbase,414,summary,Move client classes into client package,architecture_debt,violation_of_modularity
hbase,414,description,Let's move all the client classes into the o.a.h.h.client package. Files to move: * HTable * HBaseAdmin * HConnection * HConnectionManager Is there anything else I am missing? Obviously a lot of the tests will get moved too.,architecture_debt,violation_of_modularity
hbase,414,comment_0,"Here's a patch that moves everything around. To apply, delete: * * * * * * * * then apply the patch, which will create new (moved) versions of the deleted files, which contain edits, so it's not just a simple move. Tests pass locally.",non_debt,-
hbase,414,comment_1,Please review.,non_debt,-
hbase,414,comment_2,So why doesn't the patch have the deletes in it?,non_debt,-
hbase,414,comment_3,Do you know a way to have svn diff include deletes and adds?,non_debt,-
hbase,414,comment_4,"To get adds, you need to do svn add filename before svn diff. To get deletes, you need to do svn before doing svn diff",non_debt,-
hbase,414,comment_5,I've already done that. The .patch file does not contain whatever information is necessary to tell svn to delete the deleted files. You have to do that manually.,non_debt,-
hbase,414,comment_6,... or you learn about the -E switch on patch. That's cool too.,non_debt,-
hbase,414,comment_7,Applied patch and ran tests which passed. +1,non_debt,-
hbase,414,comment_8,I just committed this.,non_debt,-
hbase,424,summary,Should be able to enable/disable .META. table,non_debt,-
hbase,424,description,None,non_debt,-
hbase,424,comment_0,Why can't we do this? Is it a shell problem or a HBase region server problem or what?,non_debt,-
hbase,424,comment_1,A rule on regionserver side IIRC,non_debt,-
hbase,424,comment_2,"Only the -ROOT- table cannot be disabled. IMO, if the root table needs to be modified, it should be done off-line. However, I see no restrictions on offlining the .META. table",non_debt,-
hbase,424,comment_3,"Try the shell. Will throw an exception. Will help figure where the problem if any is. Run 'disable "".META.""'. Maybe its a non-issue now?",non_debt,-
hbase,424,comment_4,Removed client side checks and fixed bug in master. Committed.,non_debt,-
hbase,424,comment_5,"This doesn't work, at least on TRUNK. Reopening. Here is a clean install:",non_debt,-
hbase,424,comment_6,Enable table was waiting for the META region to come on-line. That doesn't work if you're trying to enable the META table. Committed.,non_debt,-
hbase,426,summary,hbase can't find remote filesystem,non_debt,-
hbase,426,description,"If filesystem is remote, e.g. its an Hadoop DFS running ""over there"", there is no means of pointing hbase at it currently (unless you count copying hadoop-site.xml into hbase/conf). Should be possible to just set a fully qualified hbase.rootdir and that should be sufficient for hbase figuring the fs (needs to be backported to 0.1 too).",non_debt,-
hbase,426,comment_0,Made it a blocker (for 0.2 and 0.1),non_debt,-
hbase,426,comment_1,"There was a suggestion of having the hbase.rootdir parameter be a filesystem and path combined, like This seems like it'd be a really simple, clear, useful way to specify where your files go. Also, I think there should be a reasonable default value for this in hbase-site.xml, so that you KNOW that you have to change it. Putting it in hbase-default.xml would just be confusing.",non_debt,-
hbase,426,comment_2,Change hbase.rootdir to expectation is that its fully-qualified: i.e. includes the filesystem implementation and full location information. Addressed also issue Billy Pearson found where we still referred to hadoop.tmp.dir. Removed fs.default.name setting in hbase-site.xml for test. Tests now use home directory in minidfs as hbase.rootdir. Bulk of the patch is setting into HBaseConfiguration the rootdir (minidfs randomly picks a port). Also removed 'default rootdir'. HBase should fail if you haven't specified root dir for you hbase.,non_debt,-
hbase,426,comment_3,Minor changes to getting started so its clearer how you point at a filesystem.,non_debt,-
hbase,426,comment_4,v3 includes test I failed to update.,non_debt,-
hbase,426,comment_5,Committed to branch and TRUNK,non_debt,-
hbase,439,summary,Add hbase logs path to svn:ignore list,non_debt,-
hbase,439,description,Please add hbase logs path to svn:ignore list.,non_debt,-
hbase,439,comment_0,I have no hbase logs directory in my workspace. What is the path in your workspace where the logs directory appears?,non_debt,-
hbase,439,comment_1,I only removed the sign '#'. # export - hbase-env.sh at 44 line.,non_debt,-
hbase,439,comment_2,I added this to $HBASE_HOME ($HBASE_HOME/logs is where hbase logs now go by default). Thanks for noticing Edward.,non_debt,-
hbase,459,summary,should be like NSRE exception?,non_debt,-
hbase,459,description,"Jim was just running a loading test. It failed because of a At first I thought it HBASE-428 -- because we were making regions w/ same start and end key -- but looking closer, it looks like the client is getting a WRE thought its making a legitimate request; it just happens to be asking for a row from the top-half of a region just as it split. I would think that on WRE, the client would at least retry in same manner in which it does when it gets a NSRE? Here is the split detail: Here is the WRE:",non_debt,-
hbase,459,comment_0,"If an issue, needs backporting to 0.1.0",non_debt,-
hbase,459,comment_1,"You mean the client doesn't already retry in the case of a WRE? If it doesn't, then this is just an oversight.",non_debt,-
hbase,459,comment_2,"Looking at code, to me it looks like we should be retrying.... asking jim if he has client-side logs.",non_debt,-
hbase,459,comment_3,"What version was this test run on? Trunk? Before or after RegionManager refactoring? If 0.2.0, after refactor, then it's possible this issue is because of some of the ugliness in HBASE-473. If regions were unreasonably being offlined over and over, it's possible that you'd get WREs because an expected region would never be online.",non_debt,-
hbase,459,comment_4,This would have been a near-TRUNK patched version of hbase (was Jim testing his big patch). Want to close this?,non_debt,-
hbase,459,comment_5,"Is this only reproducible with Jim's (now defunct) patch? If so, we should close this.",non_debt,-
hbase,459,comment_6,Closing. Was seen running a patch since abandoned.,non_debt,-
hbase,467,summary,Move HStore(^Key)*.java into o.a.h.h.store,non_debt,-
hbase,467,description,"Per Jim's suggestions on HBase-419, let's move all the store-related files into a subpackage. It should either be o.a.h.h.store or If we push it down another level, I think that we should make sure we never use any of those files outside of",design_debt,non-optimal_design
hbase,467,comment_0,HStore.isReference is the only method used outside of HRegionServer. I can move that somewhere else.,non_debt,-
hbase,467,comment_1,"This is looking ugly. HRegion references a number of methods in HStore and HStoreFile. HStore references static methods from HRegion. HRegion and HStore are also tightly coupled with HLog. We can probably factor out the inner classes of Hregion, HStore and HStoreFile into but trying to tease these apart using the current plan will either be a) a ton of work or b) turn out to not be possible. Comments, please!",design_debt,non-optimal_design
hbase,467,comment_2,"Looking at the issue further, it appears that making subpackages for region, store, etc inner classes is not going to work well. Although many of the inner classes are static, in order for the parent class to access them, too much would have to be made public. Just factoring out inner classes (into and making them package scope would yield the highest containment.",design_debt,non-optimal_design
hbase,467,comment_3,"On second thought, I am going to put HBASE-467 on the back burner until HBASE-469 is completed. Since the original patch was based on unrefactored code, the changes will be easier to apply now than later.",code_debt,low_quality_code
hbase,467,comment_4,I'm fine w/ all being under o.a.h.h.r rather than under subpackages under o.a.h.h.r -- especially if its loads of work. Introducing o.a.h.h.r package is sufficient improvement over what we had previous. Good stuff.,architecture_debt,violation_of_modularity
hbase,467,comment_5,So let's resolve won't fix?,non_debt,-
hbase,467,comment_6,There is just too much intermodule coupling to do this properly without breaking encapsulation.,code_debt,low_quality_code
hbase,492,summary,hbase TRUNK does not build against hadoop TRUNK,non_debt,-
hbase,492,description,"When I build Hadoop's library from TRUNK and then build Hbase from TRUNK, when I replace Hbase hadoop's libraries with the one built from TRUNK, I get the following error.",non_debt,-
hbase,492,comment_0,That was painful! Apparently Hadoop-trunk changed so that it no longer provides defaults for hadoop.log.dir and for mapred.output.dir.,non_debt,-
hbase,506,summary,"When an exception has to escape ServerCallable due to exhausted retries, show all the exceptions that lead to this situation",non_debt,-
hbase,506,description,"Every so often, we find ourselves trying to debug a problem that happens in HTable where we exhaust all our retries trying to contact the region server hosting the region we want to operate on. Oftentimes the last exception that comes out is something like which should just never be the case. As a way to improve our debugging capabilities, when we decide to throw an exception out of ServerCallable, let's show not just the last exception but all the exceptions that caused all the retries in the first place. This will help us understand the sequence of events that led to us running out of retries.",design_debt,non-optimal_design
hbase,506,comment_0,"Here's a patch to add this functionality for 0.1. (Nearly the same patch would apply to trunk, but let's see if this works first.)",non_debt,-
hbase,506,comment_1,+1 on patch. Falls into the debugging tools category -- will help prove/disprove IRC theory that WREs just happen to be the last of a set of retryes -- so fine to apply to 0.1 branch.,non_debt,-
hbase,506,comment_2,"I saw below running tests on patch: [junit] 1, 1, Errors: 0, 72.628 sec [junit] Test FAILED Do you see same?",non_debt,-
hbase,506,comment_3,"Don't see that failure in my test suite because the TestEmptyMetaInfo test doesn't exist yet. HBASE-27 hasn't been applied, right?",non_debt,-
hbase,506,comment_4,Right. My test bed was polluted w/ HBASE-27.,non_debt,-
hbase,506,comment_5,Here's the same thing but for trunk.,non_debt,-
hbase,506,comment_6,Please review.,non_debt,-
hbase,506,comment_7,"That is really odd that TestEmptyMetaInfo should fail. Basically what it does is open the META table, stick in a bunch of rows that don't have info:regioninfo in them and wait for the master to clean them up.",non_debt,-
hbase,506,comment_8,"I ran TestEmptyMetaInfo against this patch now that it's committed, and it works fine.",non_debt,-
hbase,506,comment_9,I just committed this to 0.1 and trunk.,non_debt,-
hbase,568,summary,region offline after DELETE,non_debt,-
hbase,568,description,"I have a very simple table named 'titles' that I'm playing around with. After entering hql I get the following output: 08/04/07 15:09:15 INFO hbase.HBaseAdmin: Disabled table titles Exception in thread ""main"" Every succeeding attempt to query the table results in the following output: Exception in thread ""main"" region offline: Deleting a specific column on the other hand, as in: hql is no problem - I get this output: 08/04/07 17:08:02 INFO hbase.HBaseAdmin: Disabled table titles 08/04/07 17:08:02 INFO hbase.HBaseAdmin: Enabled table titles and everything's all right ever after.",non_debt,-
hbase,568,comment_0,HQL bug. HQL will be removed for 0.2.0.,non_debt,-
hbase,568,comment_1,This has bitten me as well. Is there something credible to replace HQL for simple CLI administrative tasks?,non_debt,-
hbase,775,summary,max mapfiles to compact at one time,non_debt,-
hbase,775,description,Currently we compact all map files with no upper limit this could cause a regionserver to OOME if the compaction get behind and the number of mapfiles build up.,non_debt,-
hbase,775,comment_0,This should not be hard to do. Making a blocker for 0.3.0. We can move it out later.,non_debt,-
hbase,775,comment_1,"Billy, you think that you could have a go at this one? Doesn't look too hard. Add a new configuration into hbase-default.xml named something like In the HStore constructor, you would read its setting and save it off to a data member. See for an example. Then you'd add something around line #785 in HStore where you'd only compact the amount. Also, checkout HBASE-64. So much has changed since that issue was originally filed but there are a load of your ideas in there about how we should do compactions. Perhaps distill out the good stuff into new, more pointed issues?",non_debt,-
hbase,775,comment_2,I went with compaction size based on file sizes not count. count is to hard to figure a default for when you do not know what the column count will be on any users setup. This should solve the problem when we are running a normal compaction but this will not solve The issue when it comes to compacting and force is true or if we are compacting after a split and have references. Please review stack The only thing missing I would like to have a debug line in there to let us know when we are hitting the compaction limit In case a user is setting it to low and running in to problems down the road but not sure if its work the extra code and vars to log that.,non_debt,-
hbase,775,comment_3,"Hey Billy: Thanks for the patch. You made size the determinant. I may be wrong, but I didn't think size mattered; the compaction is not done in RAM but by streaming a set of inputs to an output.",non_debt,-
hbase,775,comment_4,I thought we did it in Ram if its just streaming then there should not be a max. we should do all we can on each compaction that would be less work on the next compaction. I assume there some kind of index that is in memory so we keep track of max versions etc so that would be the only factor to limit the count or size of the compaction. Sense we do not compact in memory I do not see any reason for this issue then so it would be resolved as won't fix HBASE-64 should also be resolved as HBASE 785 solved that problem,non_debt,-
hbase,775,comment_5,If there is some other reason any one would see us needing it let me know and I will make a patch based on the count of file compacted at one time,non_debt,-
hbase,775,comment_6,"I'm tempted to leave hbase-64 open for now since the original justification in hbase-64 on why there should be a maximum number of files to compact at any one time is convincing, to me at least. But HBASE-64 became a mess pulled in all directions. Because of this, I closed hbase-64 and opened a new issue on a maximum number of files per compaction -- perferably the smaller ones first (HBASE-834). I'll close this one since it was based on a misunderstanding. Thanks Billy.",non_debt,-
hbase,798,summary,Provide Client API to explicitly lock and unlock rows,non_debt,-
hbase,798,description,"We need to be able to perform a series of reads from and writes to a single row without any potential interference from other clients. Unfortunately this is a bit involved because normal reads currently do not acquire row locks, so it requires adding additional get/getRow calls that obtain and release a row lock. In addition, there will be two additional client calls, lockRow/unlockRow, which actually acquire and release the locks. Though each lock is associated with an HRegion, this will be tracked within the HRegionServer. When a lock is acquired from the client, it is handled much like a Scanner. We obtain the row lock from the HRegion, store the region name and lock identifier in a synchronized Map, and also obtain a lease to ensure that the lock will eventually be released even if the client dies. This also required adding a RowLockListener (implements LeaseListener) private class in HRS to handle row lock lease expiration. HRS.lockRow will return a long lockId (as openScanner does) that will be used in subsequent client calls to reuse this existing row lock. These calls will check that the lock is valid and perform the operations without any locking (wrappers around get*, new versions of batchUpdate, openScanner, etc). This is going to really add some noise to the list of available HTable/client methods so I'm not sure if it's something people would want to commit into a normal release. Regardless this does provide some very convenient functionality that may be useful to others. We are also looking into Clint Morgan's HBASE-669, but one major downside is that there is a significant amount of overhead involved. This row locking is already built in and this will only extend the API to allow clients to work with them directly. There is little to no overhead at all. The only (obvious) performance consideration is that this should only used where necessary as rows will not be locked and unlocked as quickly with round-trip client calls. In our design, we will have specific notes in our schema about which tables (or even which families or columns) must be accessed with row locks at all times and which do not. This is my first attempt at adding any additional functionality, so comments, criticism, code reviews are encouraged. I should have a patch up tomorrow.",non_debt,-
hbase,798,comment_0,Suggestions: Add a subclass of HTable named something like LockingHTable and add your lock versions to this subclass.,non_debt,-
hbase,798,comment_1,"After discussing with Jim on IRC, there won't be very many additional functions exposed through HTable. It will actually just be one new version of each of: batchUpdate, deleteAll x 2, and deleteFamily. So it would be those 4, plus the lockRow/unlockRow. You think it would be okay to include just those in HTable w/o subclassing it?",design_debt,non-optimal_design
hbase,798,comment_2,"I""m fine w/ that. You might want to subclass anyways just because it groups this new functionality nicely.",design_debt,non-optimal_design
hbase,798,comment_3,"Here's my first go. This adds lockRow and unlockRow methods to the client/HTable. The return type of lockRow is a new client object RowLock (contains long lockid and byte[] row). unlockRow takes a RowLock as argument. Also adds new versions of commit, deleteAll, and deleteFamily that also can take RowLock's. Within HRS I created new versions of the same functions which take the actual lock ids: long in HRS (randomized id as with scannerids, used for leases, etc), Integer in HR (existing type used for row locks). Existing functions, which do not explicitly take locks, now call the same HR functions but with nulls as row locks. Internally HR checks if it was passed a valid row lock or not. If it's a null, it obtains a row lock and releases it at the end of the call. If it's valid, it makes use of the lock, and does nothing at the end. Otherwise an exception is thrown. Code has not been completely cleaned up, but this demonstrates how the client row locks are being implemented.",code_debt,low_quality_code
hbase,798,comment_4,"Patch seems nice Jonathan. Unfortunately I didn't have the chance to apply it since the path of the modification refers to ""jonsrc"" instead of ""src"". Also, I think you forgot to include the class RowLock.",non_debt,-
hbase,798,comment_5,"I'm going to work on another patch now which adds a kind of ""upsert/increment"" functionality. I have a few particular and frequent use cases in mind that others may have a need for. Rough APIs below... - - - I'm going to build this patch on top of the previous patch because some helper functions and classes will be useful.",non_debt,-
hbase,798,comment_6,"Ah, I must have goofed my diff line. Will take a look and put the patch back up.",non_debt,-
hbase,798,comment_8,"On HRS side, I would not create new methods, but add RowLock as an additional parameter. Pass null from the client if no row lock was supplied, or something like that. Basically HRS methods usually expect all parameters and then deals with those that are not supplied.",non_debt,-
hbase,798,comment_9,This patch should properly apply to trunk.,non_debt,-
hbase,798,comment_10,"HBASE-798 implementation will allow the requested behavior from HBASE-493. However it may still be worth implementing as it is nearly as flexible as the explicit lock/unlocks, but requires only two round-trips in the average case whereas lock, read, write, unlock is four every time.",non_debt,-
hbase,798,comment_11,"Jim, I may have misspoken in my explanation. There are only single HRS methods that take all arguments. Well, there are two of deleteAll but there's different HR implementations. public void deleteAll(final byte [] regionName, final byte [] row, final byte [] column, final long timestamp, final long lockId) public void deleteAll(final byte [] regionName, final byte [] row, final long timestamp, final long lockId) public void deleteFamily(byte [] regionName, byte [] row, byte [] family, long timestamp, final long lockId) public void batchUpdate(final byte [] regionName, BatchUpdate b, final long lockId)",code_debt,duplicated_code
hbase,798,comment_12,"In our discussions we overlooked getRow, which calls HR.getFull, which does obtain row locks. Will make a new patch to add that. Atomic increments discussed above have been moved to HBASE-803",non_debt,-
hbase,798,comment_13,Applies to latest TRUNK. Were mismatches due to the HBASE-805 patch I submitted earlier. This cleans up the code a bit and adds getRow/getFull methods with optional locks.,non_debt,-
hbase,798,comment_14,This patch is only a line break fix to allow this to apply against TRUNK (once HBASE-819 has been committed).,non_debt,-
hbase,798,comment_15,Ready for review,non_debt,-
hbase,798,comment_16,"Reviewed patch. -1 because: - should renew the lease if it is passed an outstanding lock id - should use as it is much more efficient than - In HRegion, factor out the multiple occurrances of: into a private or protected method, such as: - isRowLocked should be protected or private",code_debt,slow_algorithm
hbase,798,comment_17,"Thanks for review Jim! 1) Agreed. 2) Okay. I was using the identical structure used to keep track of scanners: Map<String, InternalScanner HashMap<String, InternalScanner Should that be changed as well? 3) Yes, I had it that way before but changed it for some reason. I think because I need to do a check at the end, but this checks the passed-in argument which is unchanged by the getLock function call. So you are correct, will change. 4) Yup. Will wait to hear back regarding synchronizedMap(new Hashmap()) versus ConcurrentHashMap() before posting new patch. Thanks again, Jim!",non_debt,-
hbase,798,comment_18,Contains changes outlined by Jim. Also updates scanners in HRegion to use ConcurrentHashMap rather than,non_debt,-
hbase,798,comment_19,+1 Committed to trunk. Thanks for the patch Jonathan!,non_debt,-
hbase,798,comment_20,Neither RowLock nor was included in the patch. This is causing compilation errors. Please supply ASAP!,non_debt,-
hbase,798,comment_21,Adds RowLock.java and My apologies :),non_debt,-
hbase,798,comment_22,Added classes missing from final patch that were present in previous versions. Fix a number of compilation problems. All regression tests now pass.,non_debt,-
hbase,827,summary,Failed to open scanner in shell after series of action executed,non_debt,-
hbase,827,description,"Background introduction : I was intent to implement a large queue-liked structure based on HBase. Different from normal queue structure, in my structure, a pop action means 'randomly' pick one elements(which is key/value pair from a row) in queue, and decrease the queueSize of that queue. For performance consideration, I implemented pop action by following steps : 1.get the queue size 2.random a number between 0~queueSize 3.take step2 as a qualifier, get the value of that element, and put the last element's value to the original qualifier. 4.decrease the queue size. (I didn't delete the last elements since storage space is not my primary consideration) When the queueSize and number of pop action become large, hbase failed to open a scanner. (Even the data of database is correct) My program can be split into few steps: 1.create a large queue. 2.pop until the queue empty 3.keep popping the queue 4.delete the queue ( delete a row with deleteAll ) 5.create a small queue with same name (i.e: same row) After create a small queue, I checked every key/value in the hbase and found it is correct. But when I tried to open a scanner, it got Unknown Exception as follows: In shell : NativeException: Trying to contact region server 127.0.0.1:54141 for region row '', but failed after 3 attempts.",non_debt,-
hbase,827,comment_0,Code to reproduce problem : MyClient.java jstack log : jstack.log Hbase log : hbase.log (From empty database to scanner failed),non_debt,-
hbase,827,comment_1,"I grab the MyClient.java and latest trunk (r686572) to try it out. So, the example program do the following things: 1. insert 40000 columns under one col family 2. insert a number into the same col family to record the total # of cols in this family 3. repeat 100,000 times, do read one column value, modify it, insert back 4. delete everything 5. repeat 1 but with much smaller columns to insert Then I try to get a scanner from java client: The table in question has only few columns but lots of deleted records (about 140,000). The scanner needs some time to finish it's job. The line above requires 240 seconds to finish (that's 4 mins). However, in hirb.rb, we have 3) 3) These are smaller than hbase-default.xml, and this is why scan shell fails.",non_debt,-
hbase,827,comment_3,Thanks for Rong-en and stack's solution : ),non_debt,-
hbase,847,summary,new API: HTable.getRow with numVersion specified,non_debt,-
hbase,847,description,"I'd like to be able to call HTable.getRow with numVersions, and get multiple versions for each column.",non_debt,-
hbase,847,comment_0,"What needs to be done: - bump versionID - change: - add overloads to getRow: - replace: All getRow(String...) methods should call: Similarly all getRow(byte[]...) methods should call: which will use the new getRow api in HRegionInterface described above. Modify to match the change in HRegionInterface. This will require corresponding changes to HRegion.getFull, and Multiple values and timestamps for the same column:family can be stored in a single Cell using either of the constructors:",non_debt,-
hbase,847,comment_1,"Patch for the issue. OK, this is my first big(-ish) patch, so I am sure I am missing something :) Anyway, updates hbase as Jim Kellerman suggested. RowResult#getRow-s don't have any documentation yet. I will update them with a later patch. I also want to update scanners so that you can ask for multiple versions from them too (not done yet). (Also includes patch from HBASE-892.)",documentation_debt,outdated_documentation
hbase,847,comment_2,"Patch does not apply. Patches must be in svn diff format to be accepted. Please add a test case to demonstrate that getting multiple versions works (should also include multiple versions with timestamp specified) Please do not include a patch for HBASE-52 and HBASE-33 in this patch. Even though they are similar, changes to scanners are more difficult. We try to limit the scope of a single patch in general. Insure that the sub issues of this Jira, HBASE-857, HBASE-31 and HBASE-44 are addressed. Thanks.",test_debt,lack_of_tests
hbase,847,comment_3,"Again, thanks for comments. I will update as you suggested with a new patch. Btw, a question: Do you think it is a good idea to change Cell so that if it stores multiple <timestamp, value> pairs, those pairs are sorted? I mean, the value with the latest timestamp will be returned first during an iteration?",non_debt,-
hbase,847,comment_4,"That would be nice, but will require substantial changes to and Memcache.getFull At first glance, however, changes are required there just to be able to get multiple versions in the first place.",non_debt,-
hbase,847,comment_5,Do people think this should wait after HBASE-880 since that issue will change all APIs anyway or shall I work on a new patch now?,non_debt,-
hbase,847,comment_6,"New version of patch. Same as the last one except - Added a new test case - Changed Cell to keep a reverse sorted map of timestamp-- Also changed iteration. Cell now iterates over Entry<Long, byte[]- Added javadoc for new overloads There is a small bug. If, say, your table is configured to keep last 3 versions and you have just written code that makes 5 updates to a row/column (with timestamps, t1, t2, t3, t4, t5.) Now if you try asking for 5 versions, you will only get t5, t4 and t3. But if you ask for 5 versions starting from t4, you will get t4, t3, -t2- (at least until table is compacted). I don't know if this will be too much of a problem. I also should note that HTable#get also behaves like this. About subtasks: I think HBASE-857 and HBASE-44 are covered. I am not sure about HBASE-31. Is it useful to get just timestamps and not values?",non_debt,-
hbase,847,comment_7,Patch looks good. Let me study it more and try it locally and try and get it into 0.19.0. Good stuff.,non_debt,-
hbase,847,comment_8,"Thanks for comments, stack. I am OK with this issue (or HBASE-44 etc) being fixed in 0.19 or 0.20.",non_debt,-
hbase,847,comment_9,Committed. Thanks for the patch Doacan.,non_debt,-
hbase,894,summary,[shell] Should be able to copy-paste table description to create new table,non_debt,-
hbase,894,description,I want to create a new table based off the description of an old. You'd think I could just copy the description of the old in the shell but it doesn't work. Our 'describe' emission cannot be used as input on a subsequent create. Below I copied the output that describes one table and tried to create a new table named 'x' with it:,non_debt,-
hbase,894,comment_0,"It need to convert the arguments of version/ttl/length from string to integer. However, even use to_i to try to convert to integer, the jruby returns unexpected Long. It's said that it's resolved in jruby 1.1.2. Maybe the version in hbase is smaller?",non_debt,-
hbase,894,comment_1,Thanks Sishen. HBase version of jruby is 1.1.2. Let me update it to 1.1.4 (HBASE-896). It was just released.,non_debt,-
hbase,894,comment_2,I updated jruby Sishen. Maybe that'll fix it?,non_debt,-
hbase,894,comment_3,"I think it's not. The bug is solved in 1.1.5. Also, since some changes from 1.1.2 to 1.1.4 and the functionality is broken now. Will check more.",non_debt,-
hbase,894,comment_4,Let me know if you'd like me to back out HBASE-896.,non_debt,-
hbase,894,comment_5,Would Long.intValue() help? Pass it the long returned by jruby?,non_debt,-
hbase,894,comment_6,"The patch should work but it contains some patches to HBASE-890. However, not much changes there.",non_debt,-
hbase,894,comment_7,Yes. the patch is based on jruby 1.1.2. So it's better to rollback the jruby version to 1.1.2.,non_debt,-
hbase,894,comment_8,I reverted HBASE-896 Sishen. I don't follow your comment. HBASE-890 has fix for this issue? And did you intend to attach a patch here? Thanks.,non_debt,-
hbase,894,comment_9,"No. HBase-890 is fix the issue about alter table which enable user to add/delete/edit column families for a table. So it has some changes to HBase.rb. Also, this issue also modified the file HBase.rb. So the problem is that my patch including these two parts due to my local file.",non_debt,-
hbase,894,comment_10,Ok. I applied HBASE-890. Hope that helps.,non_debt,-
hbase,894,comment_11,Sure. A new patch only including patches to this issue. :),non_debt,-
hbase,894,comment_12,Committed. Thanks for the patch Sishen.,non_debt,-
hbase,980,summary,"Undo core of HBASE-975, caching of start and end row",non_debt,-
hbase,980,description,"Profiling, I learned that the HBASE-975 makes things worse rather than better. For every Reader opened -- one is opened per store file when we open a region as well as a Reader per file when we compact and then another Reader whenever a Scanner is opened -- the change adds about 4 seeks and at least in the case of compacting and scanning, to no benefit. Even where it is of benefit, when going against HalfMapFiles or when many Store files and we're testing to see if row is in file, it looks like the number of seeks saved are miniscule -- definetly not something that would show up in timings. This issue is about undoing the get of first and last key on open of a store file, the heart of HBASE-975 (975 included a bunch of cleanup refactoring. That'll stay). Profiling seeks, I did notice that we do an extra seek during a get, a reset that takes us to the start of the file. Then internally to getClosest, the core of our get, we're also doing a seek to closest index. Let me try undoing the extra seek and see if it breaks things.",design_debt,non-optimal_design
hbase,980,comment_0,Commit message: Patch passes all tests.,non_debt,-
hbase,980,comment_1,Committed.,non_debt,-
hbase,991,summary,Update the mapred package document examples so they work with TRUNK/0.19.0.,non_debt,-
hbase,991,description,"The examples in package doc. are old making mention of the long deprecated Text, etc. Update them.",documentation_debt,outdated_documentation
hbase,991,comment_0,"Cleanup of the mapreduce examples. Started new Will point folks at examples in here since its hard keeping up examples that have been modified so they'll sit in javadoc. Also changed HbaseMapWritable so it can take byte [] for values, not just Writable. Makes sense passing byte [] rather than make a new, temporary to go from map to reduce.",documentation_debt,low_quality_documentation
hbase,991,comment_1,Committed.,non_debt,-
hbase,998,summary,Narrow getClosestRowBefore by passing column family,non_debt,-
hbase,998,description,"Currently when we do we're usually just interested in catalog table edits to the info family. As its written, we'll also go trawl the region historian column family though its irrelevant and worse, if we got a row out of here with no corresponding info entry, we'd be hosed. Add being able to narrow the scope of the getClosestRowBefore by passing column family to dive in.",non_debt,-
hbase,998,comment_0,Committed. Needed for HBASE-999.,non_debt,-
hbase,1012,summary,[performance] Try doctoring a dfsclient so it shortcircuits hdfs when blocks are local,non_debt,-
hbase,1012,description,Ning Li up on list has stated that getting blocks using hdfs though the block is local takes almost the same amount of time as accesing the block over the network. See if can do something smarter when the data is known to be local short-circuiting hdfs if we can in a subclass of DFSClient (George Porter suggestion).,design_debt,non-optimal_design
hbase,1012,comment_0,Linked to HADOOP-4801,non_debt,-
hbase,1012,comment_1,Do we still want to do this?,non_debt,-
hbase,1012,comment_2,"Leave this issue open I'd say. Todd Lipcon has made nice progress over in HADOOP-4801. There is critical pushback, ""why such a difference -- maybe we should look more into why the canonical path is so slow"", but the numbers are too good to leave on the table.",non_debt,-
hbase,1012,comment_3,"This is done, available in hdfs.",non_debt,-
hbase,1031,summary,Add the Zookeeper jar,non_debt,-
hbase,1031,description,None,non_debt,-
hbase,1031,comment_0,Added the 3.0.1 jar.,non_debt,-
hbase,1033,summary,get and getRow with latest timestamp don't find requested columns,non_debt,-
hbase,1033,description,"If there is a row with more timestamps than versions (which can happen if only some cells are newer than others in other columns), get and getRow fail to find columns with newer timestamps.",non_debt,-
hbase,1033,comment_0,Example row,non_debt,-
hbase,1033,comment_1,"In the example above, a get for the row for column page:url at latest timestamp returns nothing. getRow with no columns specified and for latest timestamp return columns: { title , anchor:all_inlinks , anchor:anchor_text , } but does not return page:url or page:content get for the row for column page:url specifying all versions returns all three versions of page:url",non_debt,-
hbase,1033,comment_2,"The problem appears not to be with timestamps, but sometimes columns are not returned when, in fact, they do exist. Currently this is not reproducible with a simple test case. Will open a new Jira when I can identify what is going on here.",non_debt,-
hbase,1039,summary,Compaction fails if bloomfilters are enabled,non_debt,-
hbase,1039,description,"From Thibaut up on the list. As soon as hbase tries to compact the table, the following exception appears in the logfile: (Other compactations also work fine without any errors) 2008-11-30 00:55:57,769 ERROR Compaction failed for region maxValue must be Because the region cannot compact and/or split, it is soon dead after (re)assignment.",non_debt,-
hbase,1039,comment_0,Getting more information from Thibaut via email...,non_debt,-
hbase,1039,comment_1,There seems to be some talk that HFS will incorporate bloom filter code. Anyone know the status on this or how it will impact the need for hbase to implement this?,non_debt,-
hbase,1039,comment_2,"One crucial detail it seems is that the bloomfilter related exception happens even when no bloomfilters are enabled in the schema. There are also DFS related exceptions. From Thibaut: I created all the tables from scratch and didn't change them at run time. The schema for all the tables right now is as followed. (data is a bytearray of a serialized google buffer object) {NAME = I reran everything from scratch with the new table scheme and got the same exception again, just on a different table this time: (Disabling the bloomfilter, compression and the blockcache doesn't seem to have any effect) 2008-11-30 23:22:20,774 ERROR Compaction failed for region maxValue must be The log file is also full of these kind of errors: (before and after) 2008-11-30 23:22:44,500 INFO IPC Server handler 16 on 60020, call from x.x.x.203:52747: error: Name: 8976385860586379110 Source) Shortly afterwards I got the dfs error on the regionserver again, on a different table though (and might be completely unreleated and not important?): 2008-11-30 23:26:55,885 WARN Exception while reading from of from x.x.x.204:50010: Premeture EOF from inputStream Source) Datnode entries related to that block: 08/11/30 22:44:19 INFO dfs.DataNode: Receiving block src: /x.x.x.204:44313 dest: /x.x.x.204:50010 08/11/30 22:44:41 INFO dfs.DataNode: Received block of size 33554432 from /x.x.x.204 08/11/30 22:44:41 INFO dfs.DataNode: PacketResponder 3 for block terminating 08/11/30 22:53:18 WARN dfs.DataNode: infoPort=50075, ipcPort=50020):Got exception while serving to /x.x.x.204: 480000 millis timeout while waiting for channel to be ready for write. ch",non_debt,-
hbase,1039,comment_3,"You can neither enable nor disable bloomfilters once a column has data in it. If you enable it on a table with existing data, compact assumes all stores have a bloom filter and will NPE because it cannot read it. If you disable bloom filters on a table that has data in it, compact will fail because the store file knows it has a bloom filter. It is easier to fix the latter than the former by going through the store files and deleting the bloom filter file. Enabling bloom filters after the table has data in it is much harder as all store files must be read and a bloom filter created for each. It would be better to disallow the enabling of bloom filters once the table has been created. This would at least prevent shooting yourself in the foot.",non_debt,-
hbase,1039,comment_4,"According to the reporter (Thibaut, on hbase-user@)), the table schema never uses bloomfilters yet the bloomfilter related exceptions occur.",non_debt,-
hbase,1039,comment_5,"Why not have the bloom Writer just not build a bloom filter if ALL inputs don't already have blooms rather than NPE (in getReaders, if an input doesn't have nrows, set it to -1)? Could output a warning and just carry on. New flushes will include bloom filters so subsequent compactions will have bloom filters to hand. Eventually all inputs will have bloom filters and only then on compaction, write out compacted file with blooms. Adding disallow set/unset would be awkward in implementation; i.e. providing the appropriate context that determines when a flag is settable or not in HTD.",non_debt,-
hbase,1039,comment_6,Do you have an issue id where this is discussed Bruce? That you'd get the bloom filter exception on table that doesn't have it enabled -- or that never had it enabled in the past is odd... difficult to explain.,non_debt,-
hbase,1039,comment_7,"Correction from Thibaut on list: He originally said 8 node cluster, but now makes mention of only single regionserver. So one problem seems related to blooms, probably also seeing load-related issues.",non_debt,-
hbase,1039,comment_8,"+1 on the idea of allowing alteration of table schema to enable bloomfilters after the fact. Current situation with maxValue must be > 0"" exceptions in that case violate principle of least surprise.",non_debt,-
hbase,1039,comment_9,"Writing, if illegal filter parameters passed, continue with a warning. If reading, if can't find the filter file, continue with warning.",non_debt,-
hbase,1039,comment_10,Bring into 0.19.0. Effects apurtell. Compactions are failing and so his regions continue to grow. Fellas can't undo bloomfilters w/o throwing away data w/o this patch.,non_debt,-
hbase,1039,comment_11,"+1 on the 1039 patch. I'll try keeping my data and seeing if this will clear the problem. At least if the compactions succeed, that's a good step forward.",non_debt,-
hbase,1039,comment_12,Committed. Waiting on confirmation that this patch actually works before closing.,non_debt,-
hbase,1039,comment_13,"Confirmed. I see the warning in the regionserver logs, then messages that indicate compaction completed successfully.",non_debt,-
hbase,1054,summary,Index NPE on scanning,non_debt,-
hbase,1054,description,"I had some tables refuse to find their indexes even though they were defined and had been updated. Scanning .META. I see that some regions from the table don't have the indexes... Oddly, during the .META. scan, I would see that I had 3 entries per table. (I have very little data in each table, def not splitting yet) But when I visited the UI it showed just one region per table... This patch also addes some toStrings that helped in diagnostics, and a new null check that I found I needed in IndexedTableScanner",design_debt,non-optimal_design
hbase,1054,comment_0,"I'm having trouble re-provoking this issue, so I'm not sure exactly what was going on here. Perhaps my issue was fixed simply by updating to the newest trunk... I was wrong about the HTableDescriptor copy constructor being the issue, its only called in tests. But I still think this little patch has some value, so you can see defined indexes in the shell...",non_debt,-
hbase,1054,comment_1,The multiple region issue was due to a messed up deployment by me (multiple hbase data dirs overlayed on top of each other). Sorry for the noise. I would still like the patch to go through as it fixes this NPE I was getting too.,non_debt,-
hbase,1054,comment_2,Comitted. Passes local tests. Thanks for the patch Clint.,non_debt,-
hbase,1089,summary,Add count of regions on filesystem to master UI; add percentage online as difference between whats open and whats on filesystem,non_debt,-
hbase,1089,description,Jim Firby idea. Shouldn't be hard to add.,non_debt,-
hbase,1089,comment_0,I'd like to work this issue. Does 'regions on filesystem' mean the regions that are unassigned to regionservers?,non_debt,-
hbase,1089,comment_1,I believe 'regions on filesystem' is a simple count of the number of directories under for each table.,non_debt,-
hbase,1089,comment_2,"Yeah, I think it an interrogation of whats on the fileysystem under $HBASE_HOMEDIR. Skip things like log files and contents of compaction.dir. It'd be a rough count. No way to tell if a region online/offline at the moment.",non_debt,-
hbase,1089,comment_3,"Oh, I see. Thanks, andrew & stack.",non_debt,-
hbase,1089,comment_4,attach a simple patch.,non_debt,-
hbase,1089,comment_5,Committed. Thanks for the nice patch Samuel. We might consider adding this to hbase 0.19.1. Could be used to indicate cruft in the hbase.rootdir; e.g. there is loads of cruft in pset hbase.rootdir since its been migrated across multiple versions.,design_debt,non-optimal_design
hbase,1133,summary,Master does not reload META on IOE,non_debt,-
hbase,1133,description,"This never recalibrates: 2009-01-18 01:35:30,906 WARN Scan one META region: {regionname: .META.,,1, startKey: All datanodes 10.30.94.34:50010 are bad. Aborting... Method) ...",non_debt,-
hbase,1133,comment_0,Usually means no datanodes running? Is that your case? How'd it happen?,non_debt,-
hbase,1133,comment_1,"Needed to up FD limit again. Meantime, datanodes had trouble and some were restarted. After, all datanodes were up and reporting to namenode within time limits again, but MetaScanner was unhappy, throwing the IOE above. Is there anything that can be done on IOE in MetaScanner like this? If not, never mind.",non_debt,-
hbase,1133,comment_2,I think we'd need HBASE-1084,non_debt,-
hbase,1133,comment_3,"Seems to have been fixed via HBASE-1084 (and upwards, towards HDFS where the issue lied). Please reopen if am wrong.",non_debt,-
hbase,1165,summary,Make the Region Servers resistent to Master failures,non_debt,-
hbase,1165,description,None,non_debt,-
hbase,1165,comment_0,Duplicate of HBASE-1205,non_debt,-
hbase,1247,summary,checkAndSave doesn't Write Ahead Log,non_debt,-
hbase,1247,comment_0,Committed TRUNK and branch.,non_debt,-
hbase,1263,summary,Optimize for single-version families,non_debt,-
hbase,1263,description,"As some of us have been discussing, allowing the client to manually set the timestamp of a put breaks the general semantics of versioning and I'd like to see it removed as part of HBASE-880 (a more appropriate place to debate that). However, one trick being used when you don't want the overhead of versions on a frequently updated column (which are only cleared on compactions even if set to 1), was to use the same timestamp. Since that would create an identical key it would just overwrite the value not create a new version. It's a very common use-case, and this hack is being used as part of the committed increment ops from Rather than making a special optimization for counters, an optimization on single-version families that never stores more than one version of a column.",design_debt,non-optimal_design
hbase,1263,comment_0,"One idea would be to create a special KeyValue comparator that looked at row and column only and ignored timestamp. Stack, it still seems pretty clunky having different KV comparators that stores must be aware of. At least lots of branchy code at the beginning. We also talked about potentially allowing descending order or custom comparators... would there be a ""simple"" way to make the comparator an additional and optional family setting?",non_debt,-
hbase,1263,comment_1,"I totally agree that we should not have a system that have the same timestamp in multiple places, that will brake the whole model and will make earlying out impossible when we are doing that based on time. So if we go along with deleting entries from the memCache we could just let Delete(ts) delete itself too if it finds that version in memcache, doing it like that means that we don't get any overhead of multiple versions and extra deletes hanging out for no use.",non_debt,-
hbase,1263,comment_2,We already have notion of comparator that ignores timestamps -- needed to count versions -- and a comparator that ignores type so we can see if something has been deleted (though type in the key is different). Would be easy enough to specify comparator with no timestamp on a family. Descending sort ain't going to happen. Whole hbase system has to have same basic comparator else fhit hits the san when regions split and get entries in .META. (Can't have the .META. order sometimes lexographically and then at other times reverse lexicographically all in the one table).,non_debt,-
hbase,1263,comment_3,"Good point, stack. I knew there was something preventing us from doing exotic comparators :)",non_debt,-
hbase,1263,comment_4,"Moving this out. Non-critical feature. If it shows up before we cut 0.20.0, we'll include it. Moving out for now.",non_debt,-
hbase,1271,summary,Allow multiple tests to run on one machine,non_debt,-
hbase,1271,description,"Currently, if we try to run two tests on one machine (e.g. in two checkouts) the second one will fail because its servers won't be able to bind to ports. We should use random ports in our servers in the tests to fix this.",non_debt,-
hbase,1271,comment_0,"Fixing to region server: * Previously all info servers for HRS were bind on one port, so it was imposible to start multiple HRS with info servers on one node * Now if port is in use, info server binds on next port (port++) Fixing to test framework: * LocalHBaseCluster. If Masters port is busy... try port++ * Same thing if port is busy try next one. Still need to check working of Not shure for my changes. How I was testing my changes: * Run 3 HRS and master on one laptop (info server is turned on), everything is working... jsp GUI for master show links for all HRS info servers * Run ant test task with all tests... + running at same time JUnits... no colisions * Run 2 ant tasks with all tests at same time. No collisions.",non_debt,-
hbase,1271,comment_1,"Nitay, I hope you will look throw my changes to maybe something can go to ZK testing core P.S. Thx, larsgeorge for help!",non_debt,-
hbase,1271,comment_2,"Evgeny, - I like your approach with the ++port, but we need to note that you are changing it for the general use case, not just the tests. Any use of RegionServer or LocalHBaseCluster will now have the ++port logic in it. This means that a user who sets his port in a config to 5000 may spin up a server on port 5002 instead of producing an error. It's not clear to me that that is a good change necessarily? - LOG.info(""Faild binding Master to ... should fix spelling of ""Failed"" Looks great on the ZooKeeper side of things and otherwise. Thanks for taking this on :).",documentation_debt,low_quality_documentation
hbase,1271,comment_3,"* LocalHBaseCluster - is not so general thing... as I understand it is used for testing.. never for real cluster... so my change will simplify this testing by providing automatic another port binding. * HRS InfoServer is really general thing... but port of InfoServer is not so critical... we can document it like a future... ""Automatic InfoServer port binding"". And if you want explicit port setting we can add console param for HRS start without this changes we cant star't several HRS on one node with info servers * thx... will fix my spelling",code_debt,complex_code
hbase,1271,comment_4,"I don't think that's true Evgeny. LocalHBaseCluster is used for users in ""local"" mode. Testing uses MiniHBaseCluster, which calls out to a LocalHBaseCluster. From HMaster code:",non_debt,-
hbase,1271,comment_5,I know about it..... but for what is used local mode? I guess for testing. If you want local DB you can run MySQL.... or something else,non_debt,-
hbase,1271,comment_6,"Anyway I can try to move port changes to MiniHBaseCluster level.... But are we really need it? If user starts second hbase locally... without changing config (same jar), he will get something like this: ""Hey! this port is busy. I found another port and bind to it. So have a rest and don't worry about reconfiguring and other stuff."" Will user be so disappoint because of it? :)",non_debt,-
hbase,1271,comment_7,Have an idea.... we can add parameter to config (hbase-site)... parameter name: auto.port.binding value: true/false To enable or disable this future. Suggest to turn on it by default. New users loves to play with local mode but don't like to reconfigure hbase... and manually setting another port.... Same pram can be done for InfoServer port,non_debt,-
hbase,1271,comment_8,* moved port++ from LocalHBaseCluster to MiniHBaseCluster level * fixed spelling :) * Still auto port binding for Info Server. I think it is really useful.,documentation_debt,low_quality_documentation
hbase,1271,comment_9,"Looks good to me, assuming the auto port binding for info server is okay. What do other folks think?",non_debt,-
hbase,1271,comment_10,About effect of adding auto binding: * HBase will work same in all cases when it was working (running one HRS with info server per node) * But after patch it will work in cases when HBase wasn't working (run several hrs with info servers that all trying to bind to same port bring an error) So the scope of hbase usage is extended.,non_debt,-
hbase,1271,comment_11,"Evgeny: I think the port++ should only be in test context. Elsewhere, if already service on the named port, I think it more helpful if we fail to bind (Other service could be old instance of hbase or a service user didn't realize they had). Port++, user will have to hunt for UI. So I like your idea of the auto.port config. with it being off by default but on in tests (See at the configuration we use in test context -- it could be on here). Just by way of FYI, LocalHBaseCluster, as Nitay says above is not just for testing. Its the default server used when you start up hbase. Its kinda weird because you would never use it in a production context but it has to work really well because its what new people try first; if this don't work well, then they won't stick around to try more of hbase. Your patch has ^M (Carriage returns) in it. Can you purge them? I'd suggest you put the two log lines below together as one: + LOG.info(""Failed binding http info server to port: "" + port); + port++; + LOG.info(""Attempt to bind http info server to port: "" + port); Do it in all BindException handlers, I'd suggest. Otherwise, patch looks great. Thanks E.",non_debt,-
hbase,1271,comment_12,Everything done as mr. Stack wished. :) To enable auto port binding for info servers use:,non_debt,-
hbase,1271,comment_13,"Committed. In testing, there seem to be issues with hdfs still but this is a good start. Thanks for the patch Evgeny.",non_debt,-
hbase,1282,summary,TestThriftServer is failing on TRUNK,non_debt,-
hbase,1282,description,Testcase: testAll took 88.129 sec FAILED expected:<0,non_debt,-
hbase,1282,comment_0,The following expectation is no longer met:,non_debt,-
hbase,1282,comment_1,"Delete table on 'tableB' deletes both 'tableB' and 'tableA'. 2009-03-21 17:52:50,415 DEBUG [main] deleteTable: table=tableB 2009-03-21 17:52:50,416 DEBUG [main] Cache hit for row < This isn't supposed to happen: 2009-03-21 17:52:50,432 DEBUG [IPC Server handler 4 on 60000] DELETING region <<< 2009-03-21 17:52:50,437 DEBUG [IPC Server handler 4 on 60000] DELETING region 2009-03-21 17:52:50,439 INFO [IPC Server handler 4 on 60000] deleted table: tableB 2009-03-21 17:52:50,442 INFO [main] Deleted tableB",non_debt,-
hbase,1282,comment_2,Resolved by HBASE-1284,non_debt,-
hbase,1298,summary,master.jsp & table.jsp do not URI Encode table or region names in links,non_debt,-
hbase,1298,description,"is a key in my ""userdata"" table which happens to be the start key for a region named lists a link to which is incorrect because the "" + "" character is not properly URI Encoded. This results in a misleading user error message: ""This region is no longer available. It may be due to a split, a merge or the name changed. "" manually escaping the "" + "" character as ""%2B"" produces the correct output. A quick skim of master.jsp suggests it has a similar problem: it doesn't URI Encode table names when constructing links to table.jsp",code_debt,low_quality_code
hbase,1298,comment_1,Verified problem also exists in 0.19.1 and trunk.,non_debt,-
hbase,1298,comment_2,Attached patch fixes href issues with non URL safe characters in region names.,non_debt,-
hbase,1298,comment_3,Shows regions with unsafe characters.,non_debt,-
hbase,1298,comment_4,Shows in the address bar how the region names are now encoded properly.,non_debt,-
hbase,1298,comment_5,Thanks for the patch Lars.,non_debt,-
hbase,1309,summary,HFile rejects key in Memcache with empty value,non_debt,-
hbase,1309,description,"2009-04-05 02:12:56,497 FATAL Replay of hlog required. Forcing server shutdown region: Caused by: Value cannot be null or empty ... 3 more",non_debt,-
hbase,1309,comment_0,Changed issue title after some initial digging in the code. Null values is one thing. Should empty values just be ignored rather than cause exceptions? Actually empty values are valid also: Apps may only want to test for presence of a key.,non_debt,-
hbase,1309,comment_1,From Ryan on hbase-dev@: > are getting...,non_debt,-
hbase,1309,comment_2,"In my opinion HBase should not dictate to users what they can or cannot store into the system. I did not say anything about empty keys, which of course makes no sense. The particular circumstance in this place is a crawling application that stores content indexed by SHA-1 hash but also includes a family 'url:' which stores, for each unique object according to hash, the list of URLs where it was encountered. The URLs are stored as column qualifiers to produce a list of unique values with no need for scanning for duplicate elimination etc. There is no need to store data also because all the data is already in the key.",design_debt,non-optimal_design
hbase,1309,comment_3,A more basic case is booleans as existence tests on keys.,non_debt,-
hbase,1309,comment_4,If there are no objections I'm going to apply the patch attached to this issue.,non_debt,-
hbase,1309,comment_5,Committed to trunk. Passes all local tests. Tested under concurrent write/scan load.,non_debt,-
hbase,1337,summary,Using byte[] compare instead of long compare when needed to convert to long first.,non_debt,-
hbase,1337,description,When comparing timestamps in the KeyValues tests have shown that it is faster to do a byte[] compare on the timestamps rather than converting to longs and then do the long compare. In the case where you have one byte[] and one long that you need to compare it is better to convert both to long and do the long compare. Some numbers: Compare byte[] to byte[] timer 1042 ns Compare bytes2long to bytes2long timer 3143 ns Compare long to long timer 281 ns bytes2long timer 1328 ns long2bytes timer 1349 ns,code_debt,slow_algorithm
hbase,1337,comment_0,Can we close this Erik? You implemented a bunch of long compares this way in 1304.,non_debt,-
hbase,1337,comment_1,"I think we can, I think it is good for now, we can keep it in mind for going the extra mile when we have everything else under control for like 0.21 or so.",non_debt,-
hbase,1337,comment_2,resolving because erik said it's ok,non_debt,-
hbase,1343,summary,is broken due to wrong use of Mutation constructor,non_debt,-
hbase,1343,description,Generating thrift bindings with a current release of thrift breaks the python demo client. The attached patch fixes the demo client code.,non_debt,-
hbase,1343,comment_0,"So, what version of thrift are you using Felix? I think idea is hold to our current thrift version till there is a thrift 1.0 release -- then update it all.",non_debt,-
hbase,1343,comment_1,Ancient issue,non_debt,-
hbase,1359,summary,After a large truncating table HBase becomes unresponsive,non_debt,-
hbase,1359,description,"If you see I removed and ip or something for security reasons Once I truncate the table, hbase freaks out for about 10 seconds and all the thrift servers die. Thrift server log: 2009-04-02 12:09:08,971 INFO Retrying connect to server: /:60020. Already tried 0 time(s). You see this a bunch of times and then it times out The hbase shell 13:01:08 INFO Quorum servers: Truncating t2; it may take a while Disabling table... 09/04/30 13:01:19 INFO client.HBaseAdmin: Disabled t2 0 row(s) in 10.3417 seconds Dropping table... 09/04/30 13:01:19 INFO client.HBaseAdmin: Deleted t2 0 row(s) in 0.1592 seconds Creating table... 0 row(s) in 14.7567 seconds undefined local variable or method `lsit' for #<Object:0x3bbe9a50 from (hbase):3 undefined local variable or method `lsit' for #<Object:0x3bbe9a50 from (hbase):4 null from `proc essRow' from `metaScan' from `metaScan' from `list Tables' from `listTables' from `invoke0' from `invoke' from `invoke' from `invoke' from ndling' from `invoke' from `call' from `cacheAndCal l' from `call' from `interpret' from `interpret' ... 113 levels... from `call' from `call ' from `call' from `cacheAndCal l' from `call' from `__file_ _' from `__file__ ' from `load' from `runScript' from `runNormally' from `runFromMain' from `run' from `run' from `main' from `list' from",code_debt,slow_algorithm
hbase,1359,comment_0,This works a bit better but the table seems to never disable even still.,non_debt,-
hbase,1359,comment_1,"Pulling into 0.20.0. Have to be able to disable tables. posix, is it unreponsive on the shell?",non_debt,-
hbase,1359,comment_2,"Disable and drop now work so I would say this is not a blocking issue. This is a rather large table maybe 40/50 regions. $date; echo ""disable 't5'""| .../hbase shell ;date Wed Jul 15 11:36:21 EDT 2009 HBase Shell; enter 0.20.0-dev, r794282, Wed Jul 15 10:29:12 EDT 2009 disable 't5' 09/07/15 11:36:24 DEBUG Created new HBASE_INSTANCES 09/07/15 11:36:24 DEBUG Got ZooKeeper event, state: SyncConnected, type: None, path: null 09/07/15 11:36:24 DEBUG Read ZNode /hbase/master got :60000 09/07/15 11:36:49 DEBUG Created new HBASE_INSTANCES 09/07/15 11:36:49 DEBUG Got ZooKeeper event, state: SyncConnected, type: None, path: null 09/07/15 11:36:49 DEBUG Read ZNode got :60020 09/07/15 11:36:49 DEBUG Found ROOT at :60020 09/07/15 11:36:50 DEBUG Cached location address: :60020, regioninfo: REGION = :$date; echo ""drop 't5'""| shell ;date Wed Jul 15 11:37:39 EDT 2009 HBase Shell; enter 0.20.0-dev, r794282, Wed Jul 15 10:29:12 EDT 2009 drop't5' 09/07/15 11:37:42 DEBUG Created new HBASE_INSTANCES 09/07/15 11:37:42 DEBUG Got ZooKeeper event, state: SyncConnected, type: None, path: null 09/07/15 11:37:42 DEBUG Read ZNode /hbase/master got :60000 09/07/15 11:37:43 DEBUG Created new HBASE_INSTANCES 09/07/15 11:37:43 DEBUG Got ZooKeeper event, state: SyncConnected, type: None, path: null 09/07/15 11:37:43 DEBUG Read ZNode got :60020 09/07/15 11:37:43 DEBUG Found ROOT at :60020 09/07/15 11:37:44 DEBUG Cached location address: :60020, regioninfo: REGION =09/07/15 11:37:47 DEBUG Read ZNode got :60020 09/07/15 11:37:47 DEBUG Found ROOT at :60020 09/07/15 11:37:47 DEBUG Cached location address: :60020, regioninfo: REGION =09/07/15 11:39:37 DEBUG Closed connection with ZooKeeper 09/07/15 11:39:37 INFO client.HBaseAdmin: Deleted t5 0 row(s) in 3.3580 seconds 0 row(s) in 118.0930 seconds Wed Jul 15 11:39:42 EDT 2009 09/07/15 11:36:53 DEBUG Read ZNode got :60020 09/07/15 11:36:54 DEBUG Found ROOT at :60020 09/07/15 11:36:54 DEBUG Cached location address: :60020, regioninfo: REGION =09/07/15 11:37:04 DEBUG client.HBaseAdmin: Sleep. Waiting for all regions to be disabled from t5 09/07/15 11:37:06 DEBUG client.HBaseAdmin: Wake. Waiting for all regions to be disabled from t5 09/07/15 11:37:06 DEBUG Cache hit for row <09/07/15 11:37:09 DEBUG Cache hit for row <09/07/15 11:37:16 INFO client.HBaseAdmin: Disabled t5 0 row(s) in 51.0380 seconds Wed Jul 15 11:37:16 EDT 2009",non_debt,-
hbase,1359,comment_3,"Oooh I am totally wrong that broke the table, even after a restart list and simple commands don't work 09/07/15 11:50:07 ERROR Note this table shouldn't even exist.",non_debt,-
hbase,1359,comment_4,"I just tried disable on table of 1500 regions: Doing it a few times, each time it gets above.",non_debt,-
hbase,1359,comment_5,Eventually I got this on repeated disablings:,non_debt,-
hbase,1359,comment_6,Please update Alex and try again. I think things should be a good bit better now after hbase-1583 went in.,non_debt,-
hbase,1359,comment_7,"Rerunning my test. On Jul 17, 2009 12:41am, ""stack (JIRA)"" <jira@apache.org> wrote:",non_debt,-
hbase,1359,comment_8,I just dropped a +1000 region table and everything worked fine. I say close it out.,non_debt,-
hbase,1359,comment_9,+1 on closing as fixed by HBASE-1583,non_debt,-
hbase,1359,comment_10,Resolving at Alex's prompting.,non_debt,-
hbase,1359,comment_11,Work in hbase-1583 fixed this.,non_debt,-
hbase,1396,summary,Remove unused sequencefile and mapfile config. from hbase-default.xml,code_debt,dead_code
hbase,1396,description,None,non_debt,-
hbase,1396,comment_0,Committed a few days ago.,non_debt,-
hbase,1396,comment_1,was created after commit. JIRA was down at the time.,non_debt,-
hbase,1484,summary,commit log split writes files with newest edits first (since hbase-1430); should be other way round,non_debt,-
hbase,1484,description,None,non_debt,-
hbase,1484,comment_0,Fix for wrong order of edits. M Added simple test that verifies only single region in a produced split file and that edits are in ascending order. M Make splitLog return List of split logs written -- helps in unit tests. Walk the LinkedList of edits in reverse so edits come out in the order in which they were added writing split file. Added toString to HLogEdit.,non_debt,-
hbase,1484,comment_1,"I get a NPE: This fixes it: Index:  (revision 782058) +++ (working copy) @@ -888,7 +888,8 @@ SequenceFile.Writer w = conf, logfile, HLogKey.class, HLogEdit.class, - logWriters.put(key, new w)); + wap = new w); + logWriters.put(key, wap); if { LOG.debug(""Creating new hlog file writer for path "" + logfile + "" and region "" +",non_debt,-
hbase,1484,comment_2,Thanks Clint. Applied your patch to branch (Trunk was fine).,non_debt,-
hbase,1492,summary,make region split size a table setting,non_debt,-
hbase,1492,description,Make the region split size a table configuration parameter. It would help make a smaller but higher concurrent table split sooner and improve scalability on smaller data sets.,design_debt,non-optimal_design
hbase,1492,comment_0,"Isn't it already like that? The max file size parameter is on HTD or in the shell you can use the ""alter"" command with table_att method.",non_debt,-
hbase,1492,comment_1,This was done a long time ago. See HBASE-62 and HBASE-42.,non_debt,-
hbase,1497,summary,Web UI does not show up when running a distributed cluster,non_debt,-
hbase,1497,description,"Web UI seems to work when running in pseudo-distributed mode. However, when running on our dev cluster the master UI does not work. Navigating (any browser, tried many) to: gives a directory listing, showing webapps/ Eventually leading to which gives a 404 as above.",non_debt,-
hbase,1497,comment_0,Looks like HBASE-1395. For sure you have that fix in place?,non_debt,-
hbase,1497,comment_1,classpath issue,non_debt,-
hbase,1529,summary,familyMap not invalidated when Result is (re)read as a Writable,non_debt,-
hbase,1529,description,None,non_debt,-
hbase,1529,comment_0,Committed without review as trivial fix.,non_debt,-
hbase,1558,summary,deletes use but no one translates that into 'now',non_debt,-
hbase,1558,description,Deletes don't update MAX_TIMESTAMP -> now like puts do.,non_debt,-
hbase,1558,comment_0,it needs to be done in HRegion so we can put the proper TS into the WAL.,non_debt,-
hbase,1558,comment_1,"here's a prototype fix, but we need tests.",test_debt,lack_of_tests
hbase,1558,comment_2,Here is a version with tests,non_debt,-
hbase,1558,comment_3,"+1, patch v2.",non_debt,-
hbase,1558,comment_4,Thanks Ryan.,non_debt,-
hbase,1592,summary,TestLruBlockCache failing on Hudson,non_debt,-
hbase,1592,description,TestLruBlockCache is failing on Hudson:,non_debt,-
hbase,1592,comment_0,Duplicate of HBASE-1591,non_debt,-
hbase,1614,summary,single zk node buckling under small node? connections never timing out?,non_debt,-
hbase,1614,description,"A few (Irfan on the list and jgray) report that a single node zk seems insufficient supporting small cluster < 20 nodes. This seems odd (though, yeah, we're recommending serious deploys must have a quorum of (Check zk stat to see if it'll dump connections)",non_debt,-
hbase,1614,comment_0,It is important to remind that the ZK doc is very clear that a ZK server should have its own disk and even its own machine. I bet that people put the ZK servers on the same machines as the RS servers which is a cheap way to make it work but not really a good way. So a single ZK instance on the same node as a RS/TT/DN is probably the worst setup ever.,non_debt,-
hbase,1614,comment_1,"Understood. But unless I'm missing something, the zk node is doing close to nothing but idle. If its zk writing logs that is causing it to go missing, it would be good to know that is the case. I'd be interested in learning tools to interogate zk instance -- see if its releasing connections and how many its hosting at any one time and if its stuck log writing to a disk that is being hogged by other processes.",non_debt,-
hbase,1614,comment_2,Good point.,non_debt,-
hbase,1614,comment_3,Clients not caching root may have put heavy load on ZK. Confirm.,non_debt,-
hbase,1614,comment_4,"I vote to close this issue as resolved. We fixed some stuff for ZK that should prevent this from happening. If this arises again on latest trunk or in the RC, we can create a new issue.",non_debt,-
hbase,1614,comment_5,Moving out of 0.20.0. We should be able to definitively tell a buckling zk from one that is heavily loaded. Don't think this should be in way of the RC.,non_debt,-
hbase,1614,comment_6,"I vote that we close this issue as fixed or invalid. It was fixed from other ZK fixes, namely not asking for ROOT location every time. 0.21 usage of ZK will expand greatly and if this happens again it should be dealt with in another jira.",non_debt,-
hbase,1614,comment_7,Closing because of Jon's rationale above.,non_debt,-
hbase,1614,comment_8,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,1634,summary,UI and shell list throw NPE,non_debt,-
hbase,1634,description,"I have a .META. from bryan duxbury that has a row with only historian data. List does whole row, not just info, so if only historian entries, a row is returned. We then try and get a regioninfo from this row and NPE. As to why this happens, jd suggests that since we moved to async historian updates, row may have been deleted already. Confirm.",non_debt,-
hbase,1634,comment_0,Here is sample NPE: This is not same as the old NPE we used to get in BaseScanner back in the day.,non_debt,-
hbase,1634,comment_1,"So, J-D speculates that what could be happening here is that historian events are coming in after a .META. entry for a region has been deleted. 1633 adds emission of the row causing the problem.",non_debt,-
hbase,1634,comment_2,"I am getting the same NPE after hbase crashed, after restart the hbase, the NPE shows up from UI master.jsp. Below is the log when hbase crashed, thought it might provide some context for why NPE happens later. Also, I understand it is not the old NPE in BaseScanner, but I apply the same fix (basically replace getRegionInfo with it seem to work (UI is up). But I have no idea if it will cause other problems somewhere else.... 2009-07-13 16:42:22,338 WARN Scan one META region: {server: 10.10.30.148:60 020, regionname: .META.,,1, startKey: Name: -39524747077288 85071 Source) Method) 2009-07-13 16:42:22,339 FATAL Shutting down HBase cluster: file system not avai lable File system is not available Caused by: Filesystem closed ... 5 more 2009-07-13 16:42:22,339 INFO All 1 .META. region(s) scanned 2009-07-13 16:42:22,339 INFO All 1 .META. region(s) scanned",non_debt,-
hbase,1634,comment_3,"Haijun Cao are you on TRUNK or on 0.19.x hbase? If on TRUNK, it would be interesting to see if you have HBASE-1638 condition. Otherwise, its something else and should be self-healing. Update to latest TRUNK to pick up some fixes. One of the fixes prints out the name of the row that is provoking the NPE. Try deleting that row from .META. table from the shell. Does that fix it? If not, may be HBASE-1638. You can figure out the file with bad entry by doing something like the following: $ ./bin/hbase |grep historian ... substituting your filenames in the above and your path to your hbase install. This will emit a row if has historian family entry. If you see one, then you have HBASE-1638. Somethings up. has a fixup script for the file with the bad entry in it. See issue for how to run it. After amending the file in .META., redeploy meta or just restart the cluster.",non_debt,-
hbase,1634,comment_4,"ok, i may have hbase-1638 after all, I am a little confused about the 1638 description at first. I will try the script in 1638 and see if it solves the problem. thanks, Stack! Haijun",non_debt,-
hbase,1634,comment_5,"I am on TRUNK. I don't think I have 1638 condition. BTW, after ""fixing"" the UI, I try to count TestTable using hbase shell. The count can't pass region I scan the content of .META. table and notice that there is a row that contains only ""historian""? I am new to hbase (about 3 days old), so I really don't know what all this means. Just wondering, to recover from this, can I manually delete all entries from .META. table? So that it does not stuck there? Is it safe? Thanks! Haijun column=info:server, 3328532 value=1247543311613 3328532 value=Region assigned to se 2904014 rver value=Region opened on server : h 2904014 value=Region split from: TestTab 2904014",non_debt,-
hbase,1634,comment_6,"Yes, try deleting. hbase Can you reproduce this condition? If so, I'd be interested in how. Thanks.",non_debt,-
hbase,1634,comment_7,"Stack, For 1638, here is how I get it: setup: hadoop-0.20.0 (distributed mode, 1 regionserver) hbase trunk (distributed mode, 1 datanode) zookeeper-3.2.0 (standalone mode) everything run in one machine (linux ubuntu). reproduce steps: use PE program (nomapred) to populate TestTable with 10million records (10 client threads, took 30 min, not bad). run PE as MapReduce program to sequetialWrite 2million records, the map reduce program started, then hbase crashed, the crash stack is pasted in the previous comment, here is the exact m/r command:",non_debt,-
hbase,1634,comment_8,"Stack, I run the 1638 script to purge historian from info, and info from historian (yes, there is info in historian as well). This fix the NPE problem in this ticket(1634). But I still can't count pass region 0001192603, so I try to run hbase Trying to contact region server 10.10.30.148:60020 for region .META.,,1, row '', but failed after 3 attempts. Exceptions: No 44 in Method) Caused by: No 44 in",non_debt,-
hbase,1634,comment_9,Making a blocker. Jeff Croft saw this too (voixd up on irc). He sent me his .META. and I was able to confirm errant record. How is this happening? Will try running the Haijun Cao prescription to see if I can manufacture it local.,non_debt,-
hbase,1634,comment_10,"Stack, Just so you know, I am able to reproduce again. It happens while doing sequentialWrite 100x1m records, this time, master/region servers did not crash (so the crash log I mentioned in the earlier comment may be misleading). sequentialWrite are still successful, just UI not working. I applied the purgewrongfamily script and NPE is gone. I am going to run the test again with DEBUG mode, will email you the log if I am able to reproduce. Haijun",non_debt,-
hbase,1634,comment_11,@Haijun Excellent. Thanks.,non_debt,-
hbase,1634,comment_12,Trying Haijun's receipe for generating issue.,non_debt,-
hbase,1634,comment_13,"Andrew's fix for HBASE-1706 has same effect as the one I suggested for listTables where we only look in the 'info' column. That should fix this NPE. Will continue over in HBASE-1634 trying to replicate root of this issue, using Haijun's prescription, an entry added to the wrong family.",non_debt,-
hbase,1640,summary,Allow passing arguments to jruby script run when run by bin/hbase shell,non_debt,-
hbase,1640,description,"If we pass a ruby script, can't give it args; they are eaten by the bin/hbase shell start up wrapper. Fix is simple:",non_debt,-
hbase,1640,comment_0,Committed small change,non_debt,-
hbase,1655,summary,Usability improvements to HTablePool,design_debt,non-optimal_design
hbase,1655,description,"A discussion on the HBase user mailing list led to some suggested improvements for the class. I will be submitting a patch that contains the following changes to HTablePool: * Remove constructors that were not used. * Change access to remaining contstructor from public to private to enforce use of the static factory method getPool. * Change internal map from TreeMap to HashMap because I couldn't see any reason it needed to be sorted. * Remove HBaseConfiguration and tableName member variables since they aren't really properties of the pool itself. They are associated with the HTable that should get instantiated when one is requested from the pool, but not already there.",code_debt,dead_code
hbase,1655,comment_0,A patch with modified HTablePool is now attached. I also modified code in the stargate package that was using HTablePool.,non_debt,-
hbase,1655,comment_1,"Patch looks pretty good. Small nitpicky issues with some needless reorderings of stuff and also some tabbing oddities. Can be fixed on commit. I remember why we're using TreeMap now instead of HashMap. HashMap with byte[] as a key does not work with default comparator, and you cannot pass one in. There is no real downside to using TreeMap unless you have hundreds or thousands of tables, and even then logarithmic on the client-side on the order of 100-1000 is negligible. More efficient in memory but a little dirtier on the GC... however this is almost exclusively client-side (and there's ever only one) so really makes no difference IMO. So, back to TreeMap. The API seems to have grown quite a bit. Do we need all these permutations of getTable() ? Could we drop the String taking ones besides the simplest one? (This is why we don't support String in most of the HBase API now, leads to very long and ugly APIs) The default getTable(byte [] tableName) also requires instantiating a new each time internally, even if we are reusing an existing HTable... Whether there is a significant overhead or not to that, we should avoid it when unnecessary. Typical usage is probably to not supply your own HBC, so if someone wants that level of control, give them the ability to build the Pool themselves rather than expose the friendlier, higher-level methods with all the different ways to instantiate the pool. Blah, this is better said in a new patch....",code_debt,low_quality_code
hbase,1655,comment_2,"This doesn't include any stargate stuff, only issues there are the tab issues, reordering, and whether to expose the pools or not. I'm not sure we want to remove the ability to work with HTablePool directly or not. This patch re-publics a bunch of stuff, cuts down on the API quite a bit, and reduces re-instantiation of HBC when getting pools that already exist and will always reuse the original when getting tables. We should definitely keep that. Otherwise the changes are open to discussion, I'm not particularly sold on my own patch, just providing it so we have a point of reference for discussion. The biggest thing to figure out is whether we even expose HTablePool non-statically to the user. If not, we go down the path of a much longer easy-to-use API... /me sleeping on it",code_debt,low_quality_code
hbase,1655,comment_3,"Ken, added you as a contributor so you can now assign yourself issues and such.",non_debt,-
hbase,1655,comment_4,"A few questions/comments on the comments: - Why does the key to a HashMap need a comparator? - I much prefer the type of String vs byte[] for the tableName throughout all the method signatures and as a key to the internal map, but I tried to keep it as-is because it seemed to fit more with other HBase code. Can we just change it to String throughout and go back to the HashMap? - I am not a fan of the current API either. I'd much prefer that the HTablePool be instantiated by a client and we get rid of the static methods and static Map altogether. This fits in much more nicely for people like me who are using IoC containers like Spring. It also allows the ability to have multiple HTablePool instances, maybe each with their own configuration (which is currently just the max size). - Sorry about the tab/spaces issue. I didn't clean it up carefully enough. - Sorry about the reordering of imports. I use Eclipse which does this automatically and I was too lazy to try to restore the order back to the way it was. I'll try harder next time. So what is the next step? Should I make a new patch with an HTablePool that is meant to be instantiated and w/o the static methods?",code_debt,low_quality_code
hbase,1655,comment_5,"I agree on most of your points... We have an internal HTable pooler (we call it HBaseRef) that works more like you describe. - When using byte[] in a HashMap it will actually end up acting on the address of the byte[] rather than the bytes contained within. You have to use a wrapping class that would yield hashCode() and equals() methods if you want byte[] as the key in a HashMap. - We can switch it all to String, but underneath these are byte[]s so it's just more consistent to actually use byte[]. If we can keep the API as small as possible, let's just include both. - There's no reason we can't do both of these things. Make the constructor public and you can instantiate them manually. I guess in your framework you might end up re-instantiating the static map multiple times? The reason I like it is because you don't have to pass the pool around your code, you can always just reference the static instance of everything. My vote would be to try to do both. If we end up - No worries on the little stuff... all part of the initiation process ;) Do you think there is a way we could retain the static stuff and still allow HTablePool to be instantiable? We should just write a nice long class comment once we settle on the design with the different ways to use it, etc... I would vote to keep the static stuff as short as possible, maybe just 2 static methods to get an HTable (w/ and w/o the size param). No statics to get the HTablePools. You have to instantiate then and that's where you would be able to set manual HBC and such. The other thing is to make sure we aren't constructing HBCs constantly. Good stuff, Ken. Thanks for figuring this.",non_debt,-
hbase,1655,comment_6,"- Even though TreeMap uses the comparator rather than the equals method to compare keys, using a byte[] as the key seems to break the contract of a java.util.Map. The last paragraph of the [Map says that impls are free to avoid calling equals() but the containsKey(Object key) method should ""return true if and only if this map contains a mapping for a key k such that (key==null ? k==null : key.equals(k))"". As you mentioned, you'd need a byte[] wrapper in order to be compliant. So a byte[] key will work in a TreeMap (and not in a HashMap), but we'd break the Map contract. Seems better to just use String and HashMap which works well and satisfies the Map contract. - Allowing both the static methods and the public construction would bring me back to the complaint I had originally with this class. :) It becomes confusing for a user trying to quickly understand how he/she is supposed to interact with this class. I guess this can all be explained away with documentation, but, it doesn't feel ""good"". - Also, yes, there would be a problem with re-instantiating the static map multiple times, but this could be prevented by implementing a Singleton pattern so that the static access operates on an internal singleton instance with a member Map rather than a static Map. - I agree that there is no reason to construct HBC's for each invocation. An internal HBC instance can be constructed and reused. - Is there a checkstyle.xml file for HBase? That would make it easy to check my code for formatting problems before submitting patches.",design_debt,non-optimal_design
hbase,1655,comment_7,".bq So a byte[] key will work in a TreeMap (and not in a HashMap), but we'd break the Map contract. Seems better to just use String and HashMap which works well and satisfies the Map contract. Looking at the TreeMap equals implementation, yes, it breaks the above contract for Map. We're impure (insert lots of sack cloth and ashes, self-flagellation, etc., here -- smile). We are zealous about using byte [] everywhere. As Jon allows above, client-side, we can relax some. IMO, we should not be allowing public construction and static methods. +1 on Singleton to prevent multiple instantiations. There is no checkstyle (maybe there is one up in hadoop). In general 80 characters per line and two spaces for tab. Anything else we'll take. Don't worry about it too much.",design_debt,non-optimal_design
hbase,1655,comment_8,"Moved into 0.20.0, we need to have a concise/consistent story for this client-facing class.",non_debt,-
hbase,1655,comment_9,"Rewrite of, and commented version of HTablePool, worked on by Ken and I yesterday. Discussion to follow from Ken ;)",non_debt,-
hbase,1655,comment_10,Attaching patch file reflecting several discussions about the best design for the HTablePool.,non_debt,-
hbase,1655,comment_11,"I've submitted another patch, that reflects a different approach taken to improve HTablePool. In the new approach, there will be no static methods at all. Instead, the normal usage of the class will be to instantiate an HTablePool directly and use the getTable and putTable methods to get an HTable and return an HTable to the pool. The pool class will automatically instantiate a new HTable in the case that no HTable for the given table name is in the pool OR if the internal pool for that table name is full (has more elements than its configured max size). The default max size is Integer.MAX_VALUE. While we lose the convenience of calling static methods, we gain a design with less code that is easy to understand, easy to use in an IoC container, and easy to unit test. This patch also includes a small modification to HTable that allows an instantiation to HTable with a null HBaseConfiguration object. This makes it possible to unit test classes such as HTablePool that internally use an HTable without requiring an environment in which the HTable can get and use a connection. Finally, this patch includes modifications to the stargate package reflecting the new HTablePool API. Let me know if this is suitable for inclusion in the project. Thanks to Jon Gray and Stack for their help and input.",design_debt,non-optimal_design
hbase,1655,comment_12,"I'm +1 on the changes described. I'm not a big fan of putting a big comment in the HTable constructor javadoc about being able to pass it a null. IMO Something like that belongs in a // comment in the code, not exposed in the javadoc of the most client-facing class we have. It's for developers, not users. Users who pass a null HBC will soon learn that this will not work, why do they care about unit testing?",documentation_debt,low_quality_documentation
hbase,1655,comment_13,"+1 Also, I agree with jgray about leaving out mention of allowing nulls in the HTable constructor.",non_debt,-
hbase,1655,comment_14,"+1 on removing the comment about allowing nulls. Your rational makes sense to me. Do I need to make a new patch, or can the commiter just remove the comments from HTable after applying the current patch?",non_debt,-
hbase,1655,comment_15,Will take care of it on commit.,non_debt,-
hbase,1655,comment_16,"Committed after receiving +1s from apurtell (original author) and ken. New test added passes. Removed HTable comments during commit. Thanks, Ken!",non_debt,-
hbase,1668,summary,hbase-1609 broke unit test,non_debt,-
hbase,1668,description,"was test added to fix hbase-810, puts happening while an outstanding scanner and then a split. We used to wait on the scanner to finish so was possibility of deadlock. We no longer wait so this condition that we fixed no longer happens. I'm going to just remove the test since whats happening now is that on split, the concurrent puts are just failing with NSRE, which is what you'd expect if a split under them.",non_debt,-
hbase,1668,comment_0,Committed. Small change so didn't get review.,non_debt,-
hbase,1723,summary,getRowWithColumnsTs changed behavior,non_debt,-
hbase,1723,description,"The method of the thrift interface changed behavior from version 0.19 to 0.20: In 0.19, it returned only cells with exactly the given timestamp, 0.20 it to returns cells with a timestamp before (not including) the given timestamp. It needs to be clearified, which one is the desired behavior. I attach a patch to make 0.20 conform with 0.19 (only return cells with exactly the given timestamp), if this is what is wanted.",non_debt,-
hbase,1723,comment_0,"Use get.setTimeStamp instead of get.setTimeRange This patch rearranges getRowWithColumnsTs a bit, because the original version was unneccessary repetitive.",code_debt,duplicated_code
hbase,1723,comment_1,"Patch looks good. Yes, the method should work as it used to. I don't think this issue of enough import to sink the current 0.20.0 RC1. Lets leave it filed against 0.20.0 and add it in if we end up making a new RC. Will commit later to TRUNK and if it doesn't make 0.20.0, to 0.20.1.",non_debt,-
hbase,1723,comment_2,Committed to 0.20 branch and TRUNK. Thanks for the patch Mathias.,non_debt,-
hbase,1723,comment_3,"Reopening. Breaks TestThriftServer. Removed patch from 0.20 branch and TRUNK. Mathias, looks like the unit test needs to be changed to match this change in behavior. Any chance of your taking a look?",non_debt,-
hbase,1723,comment_4,"Sorry for the inconveniences caused. I'm trying to fix the tests, yet my problem is, that the test-case failes even without the patch. When I run the TestThriftServer test, I get an exception and later it fails with No server address listed in -ROOT- for region .META.,,1"" What am I missing?",non_debt,-
hbase,1723,comment_5,"No worries Matias. Thanks for looking out for our thrift API. For first error, make the hadoop jar come before the hbase jar or the dir with hbase webapps in it.... .CLASSPATH is a little tricky. Or run on the cmdline: ant test Ant test has CLASSPATH set right. On second error, my guess is that its a follow on from first? Test passes for me.",non_debt,-
hbase,1723,comment_6,Move to 0.20.1,non_debt,-
hbase,1723,comment_7,substitute setTimeRange with setTimeStamp in several methods for consistent behavior. Also adjust TestThriftServer.,design_debt,non-optimal_design
hbase,1723,comment_8,"@Tim: I know you are on your holidays but should we apply this to 0.20.1? It changes behavior, so its like 0.19.x, but changes it. Need a thrift-head to say yeah or nay!",non_debt,-
hbase,1723,comment_9,"I vaguely recall this changing after Ryan updated it for the new java client? So was maybe intentional. If not, I think the current thrift interface suggests the 19 behavior anyway, so +1. ~Tim.",non_debt,-
hbase,1723,comment_10,"setTimeStamp' says 'retrieve values AT this exact timestamp'. When I Was fixing the thrift tests, the tests suggested that the API was 'retrieve me a vision of the past that is at this timestamp' which implies Retrieving exactly TS=X is rarely correct behaviour since you rely on millisecond precise matching, and unless you are using explicit TS, its fairly unlikely this will match. Whereas saying 'all cells older than TS=X' works much better.",non_debt,-
hbase,1723,comment_11,Ok. The behavior is different from 0.19.x but lets leave it as is since its more likely what folks want. Moving out of 0.20.1.,non_debt,-
hbase,1723,comment_12,"I believe this is another issue that is overridden by the new Thrift API from HBASE-1744 and it can be closed as ""won't fix""",non_debt,-
hbase,1723,comment_13,Subsumed by new Lars Francke thrift 0.2 API work HBASE-1744,non_debt,-
hbase,1753,summary,Deadlock in client using deprecated APIs in a multi-threaded application,non_debt,-
hbase,1753,description,"There is a deadlock somewhere in HTable or HConnectionManager when using the deprecated API in a multi-threaded application. I haven't had a chance to look at the thread dump in detail yet, but will attach it to this issue",non_debt,-
hbase,1753,comment_0,Thread dump after deadlock.,non_debt,-
hbase,1753,comment_1,"I've seen this. The block is on locateRegion. I saw it on the gario cluster. It happens at startup when lots of threads. In his case, he was able to slow the thread rollout and then all ran well. Threads are blocked here: Here is thread that has the lock: We're doing something dumb in the way we go through this synchronization block... In garios case, the table was big. I think that the case here too. In the gario case, suggestion was a call to prime the HTable cache by single-threadedly reading the .META. content. He's testing currently.",non_debt,-
hbase,1753,comment_2,"I just ran into this on the pset cluster. 100 threads are using same instance of HTable. All are trying to do a put. If regions moved around, all threads are blocked while the HTable figures new location. Takes a good while for the 100 threads to get through the synchronization block. This upload application should be making an HTable per thread or using htablepool. Resolving as invalid.",non_debt,-
hbase,1753,comment_3,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,1770,summary,does not flush the writeBuffer when its size is set to a value lower than its current size.,non_debt,-
hbase,1770,description,"When setting the size of the write buffer to a value lower than the current size of data in the write buffer, the content of the write buffer should be flushed so it does not occupy in memory more than its new size for an extended period of time.",non_debt,-
hbase,1770,comment_0,Adds call to flushCommits if the new size of the write buffer is lower than the current size of the data in the buffer. Side effect is that setWriteBufferSize now throws IOException as propagated from flushCommits.,non_debt,-
hbase,1770,comment_1,Patch looks good. Javadoc is a little bit odd because it seems like it's asking the client to call flushCommits rather than it happening automatically. Can fix on commit. Should we put this into 0.20 branch and trunk?,documentation_debt,low_quality_documentation
hbase,1770,comment_2,"Indeed my intent was to write 'calls flushCommits', not 'call flushCommits'.",non_debt,-
hbase,1770,comment_3,+1 on adding to branch and trunk,non_debt,-
hbase,1770,comment_4,"Committed to trunk and branch. Thanks for the patch, Mathias.",non_debt,-
hbase,1773,summary,Fix broken tests (setWriteBuffer now throws IOE),non_debt,-
hbase,1773,description,None,non_debt,-
hbase,1773,comment_0,Committed:,non_debt,-
hbase,1773,comment_1,Thanks stack.,non_debt,-
hbase,1785,summary,zk_dump command requires zoo.cfg,non_debt,-
hbase,1785,description,"the zk_dump command depends on zoo.cfg existing, which means it doesnt work when there is no zoo.cfg (a valid cluster config) and the quorum is in hbase-site.xml",non_debt,-
hbase,1785,comment_0,"oh wait im dumb, not an issue.",non_debt,-
hbase,1785,comment_1,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,1849,summary,HTable doesn't work well at the core of a multi-threaded server; e.g. webserver,non_debt,-
hbase,1849,description,"HTable must do the following: + Sit in a shell or simple client -- e.g. Map or Reduce task -- and feed and read from HBase single-threadedly. It does this job OK. + Sit at core of a multithreaded server (100s of threads) -- a webserver or thrift gateway -- and keep the throughput high. Its currently not good at this job. In the way of our achieving the second in the list above are the following: + HTable must seekout and cache region locations. It keeps cache down in HConnectionManager. One is shared by all HTable instances if the HTable instance was made with same HBaseConfiguration instance. Lookups of regions is inside a synchronize block; if the region wanted is in the cache, the lock is held a short time. Otherwise, must wait till trip to server completed (may require retries). Meantime all other work is blocked even if we're using HTablePool. + Regardless of the identity of the HBaseConfiguration, Hadoop RPC has ONE Connection open to a server at a time; request and response are multiplexed over this single connection. Broken stuff: + Puts are synchronized to protect the write buffer so only one thread at a time appends but flushcommit is open for any thread to call it. Once the write buffer is full, all Puts block until its freed again. This looks like hang if hundreds of threads and each write is to a random region in a big table and each write has to have its region looked-up (There may be some other brokenness in here because this bottleneck seems to last longer than it should even if hundreds of threads). Ideas: + Query of the cache does not block all access to the cache. We only block access if wanted region is being looked up so other reads and writes to regions we know the location of can go ahead. + nio'd client and server",non_debt,-
hbase,1849,comment_0,"Great issue, stack. Also, we need to consider how to best add additional threading to HTable. Specifically, for which Erik and I are working on right now over in HBASE-1845. We are not at all taking advantage of running batch puts in parallel right now, and it's especially important for MultiGet which could drastically improve performance by distributing the calls in parallel. Additional discussion of the specifics should happen in the other issue, just wanted to link these up.",design_debt,non-optimal_design
hbase,1849,comment_1,See the thread dump in for example of how client can get hung up on synchronized batch put in particular.,non_debt,-
hbase,1849,comment_2,"I've been working on this for the past 2 weeks, although I'm guessing that my solution won't be really satisfactory for this issue. I wrote another HBase client from scratch, and it's been written from the ground up to work well in a multi-threaded environment. I'll open-source it in a few days, stay tuned.",design_debt,non-optimal_design
hbase,1849,comment_3,some of the original complaints have been fixed. HTablePool does some things. The advice has generally been dont share HTable between threads. The granularity of the locks in HCM were improved and while not all better there are substantial improvements since this issue was filed.,design_debt,non-optimal_design
hbase,1849,comment_4,@Benit: Bring it on!,non_debt,-
hbase,1849,comment_5,"It's not perfect, but the client has come a long way. No action on this issue for a long time, resolving as Incomplete",defect_debt,uncorrected_known_defects
hbase,1863,summary,"HbaseObjectWritable does not support unknown Writable, and writes code twice for known writables",design_debt,non-optimal_design
hbase,1863,description,"does not support read/write of unknown Writable object (will throw in addition, writing a known Writable object, e.g., HColumnDescriptor, will write the code twice. furthermore, it may be useful to change addToMap from private to public. not causing any problem with hbase, but will be nice to have the above corrected, especially part of the code is already there.",design_debt,non-optimal_design
hbase,1863,comment_0,Should be done as part of HBASE-2182.,non_debt,-
hbase,1863,comment_1,We don't have an HBaseObjectWritable anymore. Resolving as no longer valid.,non_debt,-
hbase,1897,summary,WAL is spread all over code-base and presumption is its SequenceFile; encapsulate inside a wal subpackage,non_debt,-
hbase,1897,description,"Presumption is that WAL is a sequencefile. I just spent some time looking again at my old buddy SF and its kinda heavy-duty for our needs. Do we need the sync bytes it writes into the stream every time you call a sync? Maybe it'd help recovering logs of edits? We don't need the compression by record or block, metadata info, etc. Its also currently unsuited because its sync actually doesn't do a sync on the backing stream: We should move all of our HLog stuff into a wal package and rename the classes as WAL, WALEdit, etc. Splitting code and replay code should reference a WAL.Reader rather than a etc.",design_debt,non-optimal_design
hbase,1897,comment_0,HBASE-1756 is about refactoring HLog and I have some stuff done already.,non_debt,-
hbase,1897,comment_1,Closing as dulicate of hbase-1756 (Adding comment there instead),non_debt,-
hbase,1922,summary,failing on pristine branch,non_debt,-
hbase,1922,description,Testcase: testCreateTable took 20.725 sec Testcase: took 18.646 sec FAILED expected:<true at Testcase: testTableExist took 18.002 sec,non_debt,-
hbase,1922,comment_0,Yes the fix is in HBASE-1918 sorry.,non_debt,-
hbase,1922,comment_1,Duplicate of HBASE-1918.,non_debt,-
hbase,1948,summary,updates to existing tables do not get reflected after a complete HBase + Hadoop + Zookeeper restart,non_debt,-
hbase,1948,description,"I have an existing hbase table with data in it. When I restart hadoop, hbase and zookeeper, I am able to read all of the data that existed in the table prior to the restart. However when I write data to the table, it does not get reflected. When I do a disable <table> and then an enable <table> on an hbase shell. The data that was written to the table now appears and the table is up to date. However, even after this my thrift client still sees the old data and not the updated values.",non_debt,-
hbase,1948,comment_0,Tell us more please Gautam. How big is your table? How many regions/rows? It should be easy enough on our part to reproduce if single-machine mode. Send us your hbase-site.xml and hdfs-site.xml if you don't mind (you can X'out the machine names). Attach them here to this issue. Thanks.,non_debt,-
hbase,1948,comment_1,"Hi Stack, I think this problem is seen regardless of the size of the table, but I have a table that's 'column-heavy'. I have 2 rows and about 10K+ columns per row. Database size is about 25MB. No instance of HBase region server is started up because I have set to false. I have Hbase set to manage my Zookeeper instance. I believe that the problem may be related to the fact that hadoop's namenode may rearrange block information when it starts up and this is not reflected in hbase which still attempts to use old locations. I hope that helps. Please let me know if you need more information. I will attach my hdfs-site.xml and hbase-site.xml. Since this is my test/dev environment the settings are pretty minimal",non_debt,-
hbase,1948,comment_2,Attaching hbase-site.xml and hdfs-site.xml as requested.,non_debt,-
hbase,1948,comment_3,"Gautam: Thanks for posting config. My guess is that what you are seeing is an idiosyncracy standalone mode. We should test it though. Do you think the fs is moving blocks around on the opened hbase dfsciients? There's no replicas in your filesystem, right? You are using local filesystem or hdfs with one datanode?",non_debt,-
hbase,1948,comment_4,"There are no replicas. I'm using HDFS with one datanode. It doesnt seem to me that it's opened hbase clients because hbase is started up after hadoop. Here's the series of steps that might help replicate this situation. 1. start hadoop 2. start hbase 3. start thrift 4. load data into table 5. stop thrift(normal shutdown) 6. stop hbase (normal shutdown) 7. stop hadoop(normal shutdown) 8. edit some of the existing row(s) and/or column(s) in the database through thrift PHP client or C++ client 9. lookup the data using hbase shell 10. the updated data does not show up. 11. disable the table and then enable it using hbase shell 12. the updated data shows up now Sometimes I see some really wierd behavor though, wherein initially every alternate value is updated and the others are not, say columns 1,3,5,7,9 show updated values and 2,4,5,8,10 show old values. Then after I disable and enable the table columns 1,3,5,7,9 now show the old values and columns 2,4,6,8,10 show the new updated values!",non_debt,-
hbase,1948,comment_5,It also appears that this problem is not restricted to single-machine mode. I get a regionnotserving exception when I try this on a clustered setup.,non_debt,-
hbase,1948,comment_6,"RNSE is something else; the region is not on line (if the RNSE is showing log as ERROR). Study your master logs. Do you see the region in question being deployed? If so, see to which regionserver. Check its logs to see whats up. Thanks Gautam.",non_debt,-
hbase,1948,comment_7,"Stack, I agree that we can just ignore the clustered case for now and just focus on the simple single machine test case. I suspect that whatever is the fix for the single machine mode should fix the clustered situation as well. I think it's a matter of adding this functionality : hbase regionservers request a refreshed set of block location information from hadoop dfs (assuming that this is a simple call to get the info) when they start up, and update their internal points to the data.",non_debt,-
hbase,1948,comment_8,Reopen or file new issue if still relevant with modern HBase versions,non_debt,-
hbase,1964,summary,"Enter temporary ""safe mode"" to ride over transient FS layer problems",non_debt,-
hbase,1964,description,"When a hadoop/hbase cluster is under heavy load it will inevitably reach a tipping point where data is lost or corrupted. A graceful method is needed to put the cluster into safe mode until more resources can be added or the load on the cluster has been reduced. St.Ack has suggested the following short-term task: ""Meantime, it should be possible to have a cron run a script that checks cluster resources from time-to-time -- e.g. how full hdfs is, how much each regionserver is carrying -- and when it determines the needle is in the red, flip the cluster to be read-only.""",design_debt,non-optimal_design
hbase,1964,comment_0,"We take exception to this statement. One can corrupt an Oracle database by overcommitting RAM such that the kernel panics in get_free_page (on Linux). There is no substitute for competent monitoring and administration of production systems, especially ones which try to support terascale or petascale storage and computation over 10s or 100s of servers. However, certainly it is the case that HBase has opportunities to sense overloading and take self preserving actions where currently it does not.",non_debt,-
hbase,1964,comment_1,"Refocus this issue as ""Enter temporary ""safe mode"" to ride over transient FS layer problems"", as part of ride over restart.",non_debt,-
hbase,1964,comment_2,Moved from 0.21 to 0.22 just after merge of old 0.20 branch into TRUNK.,non_debt,-
hbase,1964,comment_3,Moving out of 0.92.0. Pull it back in if you think different.,non_debt,-
hbase,1964,comment_4,Superseded by HBASE-8338 and related.,non_debt,-
hbase,1968,summary,Give clients access to the write buffer,non_debt,-
hbase,1968,description,"From a Trend dev team: Their further analysis explains in detail the scenario, which I will summarize here: 1) An invalid put is added to the writeBuffer by put(Put put). It will trigger a once it goes to the region server. 2) At some point, the buffer is flushed. 3) When the invalid put is processed, an exception is thrown. The finally clause of flushCommits() removes all successful puts from the writebuffer list but the failed put remains at the top. This entry becomes an immovable blocker which prevents any subsequent entry from being processed. 4) Subsequent puts will add more entries to the write buffer until the buffer limit is reached, compounding the problem by allowing more edits to be queued which can never be processed. A workaround could be for the client to call getWriteBuffer() -- on trunk -- and remove the entry at the head of the list manually, but without the patch on this issue, the client cannot get access to the list on branch.",design_debt,non-optimal_design
hbase,1968,comment_1,Committed trivial patch to 0.20 branch.,non_debt,-
hbase,1968,comment_2,Any reason not to apply it on trunk?,non_debt,-
hbase,1968,comment_3,getWriteBuffer is already available on trunk.,non_debt,-
hbase,1968,comment_4,Sorry brain lag ;),non_debt,-
hbase,1990,summary,Add methods accepting strings for family/qualifier in client,non_debt,-
hbase,1990,description,"Consider the following client code... byte b[] = result.getValue( ); put.add( Bytes.toBytes( ""value"") ); ... the requirement to supply family and qualifiers as bytes causes code to get cluttered and verbose. At worst, it scares peoples un-necessarily about HBase development, and at best, developers inevitably will get tired of doing all this casting and then add their own wrapper classes around the HBase client to make their code more readable. I would like to see something like this in the API... byte b[] = result.getValue( ""family""), ""qualifier"" ); put.add( ""family"", ""qualifer"", Bytes.toBytes( ""value"") ); ... where the Hbase client can perform the required Bytes.toBytes() conversion behind the scenes.",code_debt,complex_code
hbase,1990,comment_0,This is no problem. As long as nobody votes this down I'll put in the convenience methods.,non_debt,-
hbase,1990,comment_1,+1 But you might want to wait Andrew. I think Doug might have someone for the job (Doug?).,non_debt,-
hbase,1990,comment_2,"I'll give it a shot. :-) Two questions: 1) what timeframe? (i.e., I'm expecting to try this 'soon' but not by tomorrow morning). Even though this is small, this would be my first attempt at an HBase change and I don't want to screw it up. 2) I am assuming that the value is still going to take 'byte[]'. I think it would be nice to have methods that also took String (et al.) and ""did the right thing"" internally, but the .get methods would get a little more complicated since overloading doesn't work with return values (and they would probably need to be something like getString and getInt, etc.). Any changes along this line I think would require a separate JIRA ticket.",non_debt,-
hbase,1990,comment_3,"No worries. Having convenience functions to address values with strings at user request I think is fine. But, HBase is really type agnostic with respect to both keys and values. Storing something as String does not and should not imply the String type is suitable representation for the value, or Integer, or Boolean, etc.",non_debt,-
hbase,1990,comment_4,What about lightweight wrapper classes in contrib that expose these String based methods? Or - if possible and you want to go crazy - adds generics that denote the type you want. Internally you use that to call the right Bytes.toXYZ() method. That way this leaves the API unchanged being type agnostic and that wrapper would take care of more than just String types?,non_debt,-
hbase,1990,comment_5,"Lars, this sounds sweet. Can you sketch more what a genericised (sp?) HTable would look like?",non_debt,-
hbase,1990,comment_6,Will do. Let me try to do a proof in a unit test and then propose a solution.,non_debt,-
hbase,1990,comment_7,"is a sample of how we could add generics support to the classes. Now, this could be in a contrib package, but maybe even in the standard API. Biggest issue is of course the API change again. And we want to keep the API as simple as possible I would assume and with byte[] it is already as generic as possible. I would opt for those classes to be in contrib, fully implemented of course and properly layed out and tested. My classes above are just an attempt to show a general idea, which is how to make a wrapper framework that adds all the various type support. What do you all think?",non_debt,-
hbase,1990,comment_8,"I like this direction. It does more than just String. Regards the API change, if we changed all Put, Delete, Get, etc. to do generics instead, would it just add warnings since the old and new should erase to the same types? Or would old compile not compile at all?",non_debt,-
hbase,1990,comment_9,"If we add in generic HTable, I think that would make this a non-0.20 change... Would be done in 0.21.",non_debt,-
hbase,1990,comment_10,"The concept of is good, but unfortunately it might not work even for simpler use cases. Let's discuss the following example: The above example forces you to pick a data type for values at the instantiation of the Put object. But in most cases (at least in our software) we have different data types in a row such as Long, String, Custom Object etc. Even a typical relational database table always have multiple data types in a row. If you exclude the value and keep the value as byte array, it should be sufficient for 80% of the use cases. (Even though we have many columns where the column name is not a string, they are a minority)",design_debt,non-optimal_design
hbase,1990,comment_11,"Vaibhav, I think the idea was to use that as an optional ""wrapper"" around the purely byte oriented low level classes. Also, you can always still do this and do the value conversion to byte yourself. Or we add a ""Object"" type parameter and convert the value from Long, String, etc. internally (like an ""auto boxing"" or marshalling).",non_debt,-
hbase,1990,comment_12,"@Vaibhav Good point @Lars You mean byte [] , rather byte in your example above? If so, that'd work. As would our suggestion of an LCD Object. Vaibhav, what you think of Lars' suggestion?",non_debt,-
hbase,1990,comment_13,It will be great if we could do autoboxing somehow for most common data types. That would certainly help.,non_debt,-
hbase,1990,comment_14,"Moving out of 0.20.3 I like Lars suggestion and looks like it would work after some discussion but this change in API better belongs in 0.21 IMO. Since 0.21 is not too far away, I'd say no harm punting it till then. If anyone disagrees, please put it back into 0.20.3.",non_debt,-
hbase,1990,comment_15,Moved from 0.21 to 0.22 just after merge of old 0.20 branch into TRUNK.,non_debt,-
hbase,1990,comment_16,Moving out of 0.92. Move it back in if you think differently.,non_debt,-
hbase,1990,comment_17,Moving out of 0.92. Move it back in if you think differently.,non_debt,-
hbase,1990,comment_18,Not wanted badly enough I'd say,non_debt,-
hbase,2003,summary,[shell] deleteall ignores column if specified,non_debt,-
hbase,2003,description,"In the shell, a delete must match the value's coordinates exactly. By default the delete command uses the latest timestamp but you can provide on explicitly. So you have to delete each version independent of the others if there are multiple versions of a value. The command 'deleteall' is supposed to clear out a whole row or a whole column of values: but the code won't work as advertised: 'column' is ignored.",non_debt,-
hbase,2003,comment_0,Committed to trunk and 0.20 branch.,non_debt,-
hbase,2035,summary,Binary values are formatted wrong in shell,non_debt,-
hbase,2035,description,"Binary values in the shell don't seem to be formatted correctly. For example: In this case we insert a single byte (double quotes needed for it to interpret the hex value correctly), but when formatted, it appears as 3 bytes in octal. The same thing happens when the data is inserted via the Java api. For example, this code: Prints out {{\x91}} And then accessing via shell gives:",non_debt,-
hbase,2035,comment_0,I've seen this. Its annoying and presents difficultly working binary keys.,non_debt,-
hbase,2035,comment_1,Any byte that is > 0x79 does the weird octal 3-byte thingy. Looking at outputing using hbase's,non_debt,-
hbase,2035,comment_2,Patch that makes the shell do binary as hex. I tried Dave's combos above and it seems to do right thing. I'll just commit.,non_debt,-
hbase,2035,comment_3,Patch is different for branch. Problem is made worse by the makeColumnName ruby method.,non_debt,-
hbase,2035,comment_4,Committed branch and trunk.,non_debt,-
hbase,2060,summary,Missing closing tag in mapreduce package-info.java,non_debt,-
hbase,2060,description,There is a closing tag missing which makes half the page appear in fixed width font.,non_debt,-
hbase,2060,comment_0,Patch fixes tag. Trivial fix.,non_debt,-
hbase,2060,comment_1,Committed to trunk and 0.20 branch. Thanks for the patch Lars!,non_debt,-
hbase,2068,summary,"MetricsRate is missing ""registry"" parameter",non_debt,-
hbase,2068,description,I am trying to get the graphing going using Edward Capriolo's great JMX to Cacti I checked and I am missing the request rate in the JMX MBean: I checked the code and the difference between requests and the other attributes is that MetricsRate does not register itself in the MetricsRegistry used by the dynamic MBean like for example the MetricsLongValue does.,non_debt,-
hbase,2068,comment_0,"For this to work MetricsRate has to implement MetricsBase, just like MetricsLongValue etc. does. During creation of the MBean the iterates over the MetricsRegistry and declares the attributes dynamically. That is where the ""request"" falls off the plate. This only concerns JMX btw. as using the file or Ganglia based context employs the MetricsRecord directly.",non_debt,-
hbase,2068,comment_1,@Gary Any comment on above? Is it an oversight in the jmx hookup work?,non_debt,-
hbase,2068,comment_2,Another inconsistency is that MasterMetrics uses a MetricsIntValue and the uses a MetricsRate class I suggest after fixing MetricsRate to replace the MasterMetrics one with it as well. I am also wondering if it would have been better to call it MetricsIntRate to indicate the size of the internal counter.,code_debt,low_quality_code
hbase,2068,comment_3,Patch adds MetricsRate support for adding itself to the MetricsRegistry and also exchanged the MetricsIntValue in MasterMetrics for the MetricsRate class.,non_debt,-
hbase,2068,comment_4,"Looks good, committed to trunk and branch.",non_debt,-
hbase,2068,comment_5,"It looks like MetricsRate is not going to work with the Hadoop dynamic MBean support for JMX Trying out this patch, I get this message in logs: 2009-12-30 14:43:45,010 ERROR unknown metrics type: and the requests attributes do not show up in JConsole for the RS or master stats. Traced the log message back to So for this to work with JMX, we'll need to extend with another class that understands MetricsRate. I'll pull together a patch for that.",non_debt,-
hbase,2068,comment_6,Thanks Gary! Another option would be to simply use the same MetricsIntValue plus reset to zero as the MasterMetrics employed. That does work with what Hadoop has now.,non_debt,-
hbase,2068,comment_7,Reopened to address issue found by Gary. As this can be fixed quite quickly one way or the other I reopened this issue.,non_debt,-
hbase,2068,comment_8,"This patch adds JMX support for MetricsRate instances, by adding a MetricsMBeanBase class which understands HBase metrics classes.",non_debt,-
hbase,2068,comment_9,Patch adding MetricsMBeanBase to support exporting MetricsRate under JMX.,non_debt,-
hbase,2068,comment_10,Previous patch was missing the new MetricsMBeanBase class and unit test,non_debt,-
hbase,2068,comment_11,Patch against trunk adding MetricsMBeanBase and unit test,non_debt,-
hbase,2068,comment_12,"Fantastic Gary, I give that a shot asap and report back.",non_debt,-
hbase,2068,comment_13,"+1 Works great! There are few now obsolete imports, like in the changed Statistics classes. Could remove on commit. Otherwise please commit.",code_debt,low_quality_code
hbase,2068,comment_14,Committed. Thanks for the patches lads. I removed the unused import. Added a couple of licenses too. Excellent.,code_debt,low_quality_code
hbase,2075,summary,HBaseMaster requires HDFS superuser privileges due to waitOnSafeMode,non_debt,-
hbase,2075,description,Repro: 1) Enable dfs.permissions 2) Start HBaseMaster in a different linux user account from HDFS. I get the following exception in the log. It looks like waitOnSafeMode requires HDFS superuser privileges which I do not grant to HBase.,non_debt,-
hbase,2075,comment_0,"That was added in HBASE-1960. Maybe we should just catch the exception, log a WARN and continue?",non_debt,-
hbase,2075,comment_1,Committed attached patch which should resolve this issue to trunk and branch. Just catches the exception and continues. This code catches what should be a fairly rare corner case (except for up on EC2) so warning will cause more confusion than help.,non_debt,-
hbase,2075,comment_2,"Fair enough, thx Andrew!",non_debt,-
hbase,2085,summary,StringBuffer -> StringBuilder - conversion of references as necessary,non_debt,-
hbase,2085,description,Some references in toString() converted from StringBuffer to StringBuilder as concurrency is probably not needed in those contexts as the references do not get out of scope.,design_debt,non-optimal_design
hbase,2085,comment_0,Committed to TRUNK. Thanks for the patch Kay Kay.,non_debt,-
hbase,2085,comment_1,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,2096,summary,Runtime error on HMaster and HRegionServer because dynamic instantiation using getConstructor fails,non_debt,-
hbase,2096,description,"When starting the regionserver (issue also reproduces on master), I get the following error:  2010-01-06 15:10:48,208 INFO 2010-01-06 15:10:48,211 ERROR Can not start region server because ",non_debt,-
hbase,2096,comment_0,Patch that changes getConstructor call,non_debt,-
hbase,2096,comment_1,Stale,non_debt,-
hbase,2100,summary,[EC2] Adjust fs.file-max,non_debt,-
hbase,2100,description,From Robert Gibbon up on hbase-user@: Update the remote init script appropriately.,non_debt,-
hbase,2100,comment_0,Committed.,non_debt,-
hbase,2122,summary,[Stargate] Initializing scanners column families doesn't work,non_debt,-
hbase,2122,description,"Similar to HBASE-2120 the handling of column families with scanner does not work correctly. The issue is in (line 62), and I'm attaching a patch for both trunk and the 0.20 branch.",non_debt,-
hbase,2122,comment_0,Applied to trunk and 0.20 branch. Thanks for the fixes Greg!,non_debt,-
hbase,2133,summary,Increase default number of client handlers,non_debt,-
hbase,2133,description,"Any reason not to just go ahead and change hbase-default.xml to include: ? The current default for both, 10, is anemic.",non_debt,-
hbase,2133,comment_0,"Patrick Hunt reminds that is a per host limit, so just ignore that bit.",non_debt,-
hbase,2133,comment_1,100 may be a bit much. I thought we'd set this up once before already but I just looked at TRUNK and its 10. 25? Default in datanode is 3 or something. That should go up too.,non_debt,-
hbase,2133,comment_2,hmmm its 10 in branch too...,non_debt,-
hbase,2133,comment_3,Committed following change to trunk and 0.20 branch: \\ We might want to increase this again depending on user feedback. I know I needed 100 to avoid trouble with high read/write load once above ~200 regions/server.,non_debt,-
hbase,2152,summary,Add default files into conf,non_debt,-
hbase,2152,description,If there are no objections I would like to commit these two default files. They go along with the earlier added JMX setting in hbase-env.sh.,non_debt,-
hbase,2152,comment_0,"Sure, but can't you start jmx without requiring a login if you pass the right args? Should this be the default rather than a login?",non_debt,-
hbase,2152,comment_1,"Yes, you can monitor local processes without a password. You can also disable password checking for remote processes by setting It is true that the default hbase-env.sh references these files in comments: So it is a bit confusing to refer to the files when they're not there by default. We could change this to simply reference the default files in the JRE. Those will already exist with sample formats. The only problem there is that java wants the password file to only have permissions of ""600"" and be owned the the user account running the process. Which often won't mesh for people running hbase (JDK owned by root, Hadoop/HBase running as normal user). So in that case, we could change the default hbase-env.sh to something like:",non_debt,-
hbase,2152,comment_2,"The latter seems better to me (sorry if I should have spoken up earlier). What do you fellas think? There's no password access on an hbase cluster generally for the webui, etc.",non_debt,-
hbase,2152,comment_3,"Here's a patch to change the hbase-env.sh comments to include a password-less remote JMX setup by default. This seems okay to me, as it's not enabled unless uncommented, and the comments point you to how to enable password access as well. Lars, what do you think?",non_debt,-
hbase,2152,comment_4,I am fine with removing it. The only reason I added it in the first place was that I followed Ed Capriolo's guide and made use of them. Given we have no passwords anywhere else it seems OK to remove them. The JMX ports are internal like the IPC ones so a user needs to have LAN access anyways. Seems good in the overall scheme of things.,non_debt,-
hbase,2152,comment_5,You going to commit Lars?,non_debt,-
hbase,2152,comment_6,Committed to branch and trunk. Thanks Lars and Gary.,non_debt,-
hbase,2241,summary,Change balancer sloppyness from 0.1 to 0.3,non_debt,-
hbase,2241,description,"This is a quick workaround until we do a better balancer. Taking a region off line when cluster is under load is bad news. Latency goes up as we wait on regions to come up in new locations. The load balancer should only cut in if the cluster is way out of alignment. I'd argue that 10% deviance from the avg. is not good enough reason moving regions around when cluster is under load. Balancer already has a knack for cutting in at most inopportune moments: during cluster startup, when new node is added to a small cluster, or moving a region just after its been opened on a node. We'll need to do a better balancer but meantime lets just allow that region loading can be sloppier, say 20% or 30% off the average before balancer cuts in.",code_debt,slow_algorithm
hbase,2241,comment_0,Changes slop from .1 to .3.,non_debt,-
hbase,2241,comment_1,Committed branch and trunk.,non_debt,-
hbase,2247,summary,[performance] Small scans should run like a random-read,non_debt,-
hbase,2247,description,"hbase-2180 added pread whenever we do a get -- random-read -- and kept the old sync+position+read for when scanning. In between is the case of small scans. Small scans of 0-100 or so rows where the cells are small will likely fit a single hfile blocksize, especially if its the default 64k. We should recognize small scans and flip to random-read to satisfy (somehow). It'll up the performance a bit.",code_debt,slow_algorithm
hbase,2247,comment_0,"Hi stack I think We can new smallScan() method in HTable, the method use pread to read data and the method return all data in one hfile block. I think we can let users to decide use smallScan() method or getScanner method.",non_debt,-
hbase,2247,comment_1,-1 on adding a new API that duplicates another but with one distinction. Make this a Scan option instead.,code_debt,duplicated_code
hbase,2247,comment_2,"or we can take the hint information in the Scan object,",non_debt,-
hbase,2247,comment_3,+1 on adding one option in Scan.,non_debt,-
hbase,2247,comment_4,Recently added short scan does this. Resolving as fixed.,non_debt,-
hbase,2263,summary,[stargate] multiuser mode: authenticator for zookeeper,non_debt,-
hbase,2263,description,Add an authenticator module for zookeeper. Use a tree like:,non_debt,-
hbase,2263,comment_0,Committed to trunk and 0.20 branch.,non_debt,-
hbase,2295,summary,Row locks may deadlock with themselves,non_debt,-
hbase,2295,description,"Row locks in HRegion are keyed by a int-sized hash of the row key. It's perfectly possible for two rows to hash to the same key. So, if any client tries to lock both rows, it will deadlock with itself. Switching to a 64-bit hash is an improvement but still sketchy.",non_debt,-
hbase,2295,comment_0,are you talking about the multiPut where multiple rows could be updated atomically by an app?,non_debt,-
hbase,2295,comment_1,"The multiput does not promise nor do multi row atomic puts. It actually just calls the array put API. On Mar 7, 2010 11:11 AM, ""dhruba borthakur (JIRA)"" <jira@apache.org [ dhruba borthakur commented on HBASE-2295: are you talking about the multiPut where multiple rows could be updated atomically by an app? perfectly possible for two rows to hash to the same key. So, if any client tries to lock both rows, it will deadlock with itself. Switching to a 64-bit hash is an improvement but still sketchy. -- This message is automatically generated by JIRA. - You can reply to this email to add a comment to the issue online.",non_debt,-
hbase,2295,comment_2,"1. Changed locksToRows to be a Set (instead of a Map) 2. Generate sequential lockids, and in the case of collision use a Random number to start from a new origin",non_debt,-
hbase,2295,comment_3,"The patch I uploaded is for hbase 0.20. BTW, when I downloaded the hbase trunk, I do not see any build.xml files in the source repo at all. Do I need to use something other than ""ant test""?",non_debt,-
hbase,2295,comment_4,"Hey Dhruba, I don't think this patch works right. You can't have a HashSet<byte[]> since byte[]'s hashcode is identity based, not content, right?",non_debt,-
hbase,2295,comment_5,"I agree, that is the case for all native items like byte[]. I will change it.",non_debt,-
hbase,2295,comment_6,"@Dhruba If you want to use a SortedSet and pass a comparator instead, so you can retain byte [] elements, there is a bytes comparator here: Otherwise, patch looks good. Regards TRUNK, its been mavenized so to build it, you need maven -- see -- and then to build and run tests do something like mvn install",design_debt,non-optimal_design
hbase,2295,comment_7,This patch incorporates Todd's and Stack's comments. And is for hbase trunk.,non_debt,-
hbase,2295,comment_8,Committed to trunk and branch. Thanks for patch Dhruba.,non_debt,-
hbase,2295,comment_9,"This already went in, but one quick note on the final patch: rather than checking locksToRows.size() Does this need to be a followup JIRA?",non_debt,-
hbase,2295,comment_10,I agree isEmpty is better than size... I'll just put fix in under this issue. Thanks Todd. I changed it to do this instead on trunk and branch:,code_debt,low_quality_code
hbase,2295,comment_11,Marking these as fixed against 0.21.0 rather than against 0.20.5.,non_debt,-
hbase,2295,comment_12,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,2327,summary,[EC2] Allocate elastic IP addresses for ZK and master nodes,non_debt,-
hbase,2327,description,"Amazon EC2 supports Elastic IP Addresses to implement the effect of having a static IP address for public servers running on EC2. Up on hbase-users@ there was some recent discussion, confirmed, that when an EC2 instance queries the external DNS name of an elastic IP address, EC2 DNS returns the internal IP address of the instance to which the elastic IP address is bound, so it is safe to use elastic IPs for the ZK and master nodes. We gain the ability to do transparent replacement of one instance, e.g. failed, with another without incurring any additional cost. Update and to allocate elastic IPs: and then assign the elastic IP address to the appropriate instance(s): and then get the external DNS name to use when performing substitutions on master and slave configs: When shutting down the cluster, just release the elastic IPs after terminating the instances: NOTE: AWS accounts default to a limit of 5 Elastic IP addresses but most will run with 1 master and 3 or 1 ZK instances. And, the ZK ensemble can be shared. A follow up issue can address making scripts to launch replacements for failed instances transparently.",non_debt,-
hbase,2327,comment_0,Tested a few times. Works ok. I'm a bit concerned about what would happen if the elastic IP association takes too long. Committed to trunk and 0.20 branch. We can revisit if there are problems in practice.,non_debt,-
hbase,2327,comment_1,Marking these as fixed against 0.21.0 rather than against 0.20.5.,non_debt,-
hbase,2327,comment_2,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,2341,summary,Suite of test scripts that a.) load a cluster with a verifiable dataset and b.) do random kills of in small cluster,non_debt,-
hbase,2341,description,"We just filed hbase-2340 but discussion up on irc has it that we need something more hardcore than pussy-footing inside a single jvm as hdfs-2340 does. The point was made (tlipcon) that its hard to ensure real recovery working if all is in the one JVM. So, this issue is about scripts that can: + load a cluster with a dataset that we can 'verify' as in we can tell if it has holes in it, if data has been lost. + script that does random kill of a random node on some random occasion + Script that can check cluster for data loss All above should work while cluster is under load. The above would not sit under junit. This looks like a suite that we'd want to run up in ec2 using Andrew's scripts and our donated aws credits.",non_debt,-
hbase,2341,comment_0,"Thanks for filing, Stack, and sorry for the confusion around th epurpose of HBASE-2340. I put a couple implementation ideas in comments in that ticket:",non_debt,-
hbase,2341,comment_1,"Timely. I just put together a first cut of scripted PE up on EC2 using our EC2 scripts. Requires scripts from current head of trunk or 0.20 branch. Fix up the attached script to choose one of the public AMIs. But what I want to do is take this bash recipe which works well enough (but is not totally robust) and convert it into a web service that allows one to - Select a base AMI (with HBase public AMIs as default) and instance types - Upload replacement Hadoop or HBase jars, additional jars for lib/ - Upload additional files, i.e. test scripts - Execute something on the command line Each test gets its own transient cluster. All output and logs are collected from the cluster just before it is terminated and made available on an ongoing basis for posterity. Given what I've been working with recently and am most familiar with, I'd implement it as a servlet: can use the jars provided by AWS with their command line tools directly, jsch, etc.",non_debt,-
hbase,2341,comment_2,"Having an EC2 harness for this to quickly bring it up on ""cloud"" hardware would be sweet. Really excited by that idea. Can we do that work in a separate JIRA though? Keep this to generating the test scripts themselves, and another JIRA to deploy and run the tests in an EC2 cluster?",non_debt,-
hbase,2341,comment_3,I opened HBASE-2343,non_debt,-
hbase,2341,comment_4,"Here's a bit of a hacked together end-to-end verifiable write test. To run: a) A few times, perhaps on a few machines: [these will run until you ctrl-C them, and write little files into /dev/shm] b) Once you've ctrl-Ced the writers, something like: Of course, you can insert various nasty things between (a) and (b) here. I also configured my cluster with 1MB memstores and 4MB split threshold to keep things exciting.",non_debt,-
hbase,2341,comment_5,Updated version of the editor test that does verification in parallel threads.,non_debt,-
hbase,2341,comment_6,VE script looks good to me.,non_debt,-
hbase,2341,comment_7,I still want to write a gremlin script that will do nasty things on a cluster to go along with this. Then we can run the two together for a week or so to be really confident that stuff works.,non_debt,-
hbase,2341,comment_8,Cosmin suggests that we instrument the code with a coverage tool while running a long-running cluster test. We can then identify edge cases / dark corners better (and write tests to exercise them),test_debt,lack_of_tests
hbase,2341,comment_9,"This is a manual test that tries to kill region servers and data nodes, restarts them and verifies the data load. I was running on a 3 RS cluster, and was not able to get to the point that verifies if the read verification works ok. The patch will add the source code to the following location in the HBase source: usage: HBaseTest -zk <Zookeeper node -load <num keys -read <start key -kill <HBase root path>:<HDFS root path>:<minutes between kills>:<RS kill %>:<#keys to verify>",non_debt,-
hbase,2341,comment_10,Fix wrong imports.,non_debt,-
hbase,2341,comment_11,"I've started work on some python based fault injections here: The work is very preliminary, and I plan on continuing to develop it over the next couple of weeks, but would be happy to have other people contribute. Once it's reached a more thorough state we could look at including it right in the HBase source, though it's generally useful so I plan to keep it on github as well.",non_debt,-
hbase,2341,comment_12,Bulk move of 0.20.5 issues into 0.21.0 after vote that we merge branch into TRUNK up on list.,non_debt,-
hbase,2341,comment_13,Good to have but moving out of next release.,non_debt,-
hbase,2341,comment_14,The integration test suite does this. Resolving as done/duplicate,non_debt,-
hbase,2412,summary,[stargate],non_debt,-
hbase,2412,description,A version of PE that works with Stargate. Patch includes a number of fixes for multiuser mode and the client library also.,non_debt,-
hbase,2412,comment_0,Committed to trunk and branch.,non_debt,-
hbase,2555,summary,Get rid of,design_debt,non-optimal_design
hbase,2555,description,"Now that we have HFile, it seems that the constant is no longer useful. I've grepped around the codebase and couldn't find it used elsewhere. Perhaps kept around in case folks decide to store their HBase data into a MapFile?",code_debt,dead_code
hbase,2555,comment_0,Patch removes constant and two unused HCD methods which use constant. HBase does not support anything but HFiles right now.,code_debt,dead_code
hbase,2555,comment_1,1,non_debt,-
hbase,2555,comment_2,"Committed to trunk. Thanks for catching these jeff, thanks for review stack.",non_debt,-
hbase,2555,comment_3,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,2585,summary,NPE in HRegionServer when attempting to assignSplitDaughter,non_debt,-
hbase,2585,description,"While inserting a large number of rows into a HBase table (approx 100M) we got this error many times (usually do to multiple attempts) on one of our region servers: 2010-05-19 16:51:30,745 WARN Attempt=1 Source) Method)",non_debt,-
hbase,2585,comment_0,Here is code from 0.20.4 around line 535: Was the meta region online at the time?,non_debt,-
hbase,2585,comment_1,The meta region was online and mostly working. I say mostly because we were having a weird problem where for a few regions (most worked just fine) the master would get a region open message but the meta table was never updated with the assignment information. Because it was a test cluster we just formated the namenode and started over.,non_debt,-
hbase,2585,comment_2,"At a minimum, lets fix the dumb NPE and throw a better exception. Looking at code, it looks like the RS will just retry later to report the split (See ~#544 in HRegionServer).",code_debt,low_quality_code
hbase,2585,comment_3,Stale,non_debt,-
hbase,2621,summary,Fix broken link in HFile Javadoc,documentation_debt,low_quality_documentation
hbase,2621,description,"There's a bad link in the HFile javadoc; should point to HBASE-61, but instead point to HBASE-3315. Also cleaned up the reference to TFile.",documentation_debt,low_quality_documentation
hbase,2621,comment_0,Note that patch is relative to trunk/. I can change if necessary.,non_debt,-
hbase,2621,comment_1,"Commited. Thanks for the fix, Jeff.",non_debt,-
hbase,2621,comment_2,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,2657,summary,TestTableResource is broken in trunk,non_debt,-
hbase,2657,description,Column name is illegal -- its 'test:' -- when constructing a HCD.,non_debt,-
hbase,2657,comment_0,There is also hbase-2658. For now I've loosed a test check to be tightened again over in fix for hbase-2658.,non_debt,-
hbase,2657,comment_1,Pass a column family name minus the ':' creating HCD. Also changed check so its starts with rather than equals till HBASE-2658 is fixed so test will pass.,non_debt,-
hbase,2657,comment_2,Committed small change.,non_debt,-
hbase,2657,comment_3,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,2694,summary,Move RS to Master region open/close messaging into ZooKeeper,non_debt,-
hbase,2694,description,"As a first step towards HBASE-2485, this issue is about changing the message flow of opening and closing of regions without actually changing the implementation of what happens on both the Master and RegionServer sides. This way we can debug the messaging changes before the introduction of more significant changes to the master architecture and handling of regions in transition.",non_debt,-
hbase,2694,comment_0,"This issue will include: - Introduction of an {{UNASSIGNED}} znode/directory into ZK which will contain a znode for each region in transition. Data of the region znodes will contain the state of the region (closing, closed, opening, opened) - Introduction of a custom executor service, which is a special type of named thread pool which will be used to process events - Introduction of interface which is the type executed by the - Two event handlers, and to handle open/close events on the master side. For this jira, they will actually not contain new logic and will just call the existing open/close implementations. The existing message flow for moving a region from one RS to another (close then open): # M -# RS1 -# M: Offline in META # M: MetaScanner picks up unassigned region # M -# RS2 -# RS2 -# M: Add opened region to meta The new message flow: # M -# RS1 -# ZK -# RS1 -# ZK -# M: MetaScanner picks up unassigned region # M -# RS2 -# RS2 -# ZK -# M -> ZK: Delete ZNode",non_debt,-
hbase,2694,comment_1,"Adding the patch for this... not verified that the merge with the latest code is correct from my private branch. In my branch (staler version of code), the unit tests are passing and the cluster load and verify test is passing.",non_debt,-
hbase,2694,comment_2,Second pass at the patch. Incorporates changes from in-person review with Todd and Stack. Unit tests pass.,non_debt,-
hbase,2694,comment_6,"Final patch for commit. Includes changes from Todd's comments, added a few licenses, and also fixed mixed newlines on final patch file.",code_debt,low_quality_code
hbase,2694,comment_7,"Committed final patch. Awesome work Karthik. Thanks to stack, todd, kannan, and everyone else for all the time spent reviewing. 1 down, 6 to go :)",non_debt,-
hbase,2694,comment_8,"Woohoo, patch finally committed :) Thanks for the reviews guys, and thanks for the follow up and commit Jonathan - that was super quick!",non_debt,-
hbase,2694,comment_9,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,2775,summary,Update of hadoop jar in HBASE-2771 broke TestMultiClusters,non_debt,-
hbase,2775,description,TestMultiClusters failing following HBASE-2771,non_debt,-
hbase,2775,comment_0,"Dug into MiniDFSCluster on latest hadoop-20-append branch. Looks like this is our problem: This means you can't have multiple MiniDFSClusters in a single jvm. Odd regression, digging more.",non_debt,-
hbase,2775,comment_1,Introduced by HDFS-909,non_debt,-
hbase,2775,comment_2,See HDFS-1240,non_debt,-
hbase,2775,comment_3,This will be fixed when HDFS-1240 gets committed (thanks todd),non_debt,-
hbase,2775,comment_4,In the mean time I created if anyone is interested.,non_debt,-
hbase,2775,comment_5,Change our POM to pull that j-d? Our tests will pass again won't they if this is in place?,non_debt,-
hbase,2775,comment_6,"Modified the pom file, closing.",non_debt,-
hbase,2775,comment_7,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,2779,summary,Build a -src tgz to sit beside our -bin tgz when you call maven assembly:assembly,non_debt,-
hbase,2779,description,Reinstitute a patch of Paul Smiths w/ some amendements.,non_debt,-
hbase,2779,comment_0,"Patch that does the following when you call mvn ... assembly:assemble, it creates the following in target dir, [INFO] Building tar : [INFO] Building tar : This is the new one: If you undo it, it looks like this: Inside in this dir I can do a mvn to regenerate the tgzs and so on.",non_debt,-
hbase,2779,comment_1,Don't put hbase-webapps at top-level. Include KNOWN-BUGS.,non_debt,-
hbase,2779,comment_2,Couple tweaks Stack and I did on IRC,non_debt,-
hbase,2779,comment_3,Committed to TRUNK.,non_debt,-
hbase,2779,comment_4,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,2795,summary,On review HBASE-2707 has problem in that we'll get stuck in the delay queue and never come out,non_debt,-
hbase,2795,description,"I committed the hbase-2707 patch yesterday but on second thoughts, it has a flaw in that if nothing in the todo queue, we then poll the delayedtodo queue. If we fall into the latter and it has not elements, then we'll never come out; there are no notifyalls going on to wake us up. Patch coming.",non_debt,-
hbase,2795,comment_0,Here is a test that proves my assumption that 2707 patch has a flaw is wrong. I committed this test under the aegis of this issue. Now I'm going to resolve this invalid.,non_debt,-
hbase,2795,comment_1,Invalid. Not an actual problem.,non_debt,-
hbase,2796,summary,Backport of 2707 to 0.20 branch,non_debt,-
hbase,2796,description,"Backport the hbase-2707 fix to the 0.20 branch. If 2707 happens, it hoses cluster...",non_debt,-
hbase,2796,comment_0,This should do it. Need to test on a cluster.,non_debt,-
hbase,2796,comment_1,Committed to 0.20 branch.,non_debt,-
hbase,2796,comment_2,Broke build. Undoing. Rare. Not needed for 0.20.6. Moving to 0.20.7.,non_debt,-
hbase,2838,summary,Replication metrics,non_debt,-
hbase,2838,description,"Replication needs to publish metrics about its performance: - WALEdits read, filtered, sent to slave clusters, applied on slaves - size of batches sent/received - ms spent on reading, sending, applying edits This can be done using HadoopMetrics. Also we need to publish information not related to performance: - size of each HLog queues - age of the last replicated edit in each queue - time of last successful replication These informations can hardly be graphed, but we still need to represent them somehow. It has to be accessible by web UI, shell, and other tools in general. I don't feel strongly about creating a new public method on HRS's interface, and I'm not sure publishing those in ZooKeeper is a good idea either (why add another indirection?). Still wondering about a better solution.",non_debt,-
hbase,2838,comment_0,"you can publish the other stats via hadoop metrics as well. dont publish the long of how old the longest one is, but publish the delay time, ie: the time difference. in a graph normally this will hover near 0, but during times of trouble it may climb thus making a clear indication that something is wrong. Another metric you can track is queue linger time - how long do items remain in various queues before being processed. You'd probably have to track and average this. On Thu, Jul 15, 2010 at 5:42 PM, Jean-Daniel Cryans (JIRA)",non_debt,-
hbase,2838,comment_1,"I'm guessing these would be added in as additional attributes in For the metrics values, look at: - averaged rate of occurrances per time unit (requests/sec) - counter reset to 0 at end of sample period - same as a long counter - time-based rate reset to 0 at end of sample period (RPC queue time) These should map pretty well to the values you want to publish. The time-varying ones also expose min and max values for the sample period (or since last reset call) as well.",non_debt,-
hbase,2838,comment_7,"Committed to trunk, thanks for reviewing Stack!",non_debt,-
hbase,2838,comment_8,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,2854,summary,broken tests on trunk,non_debt,-
hbase,2854,description,the following tests are broken: All of the TestQueryMatcher tests.,non_debt,-
hbase,2854,comment_0,committed,non_debt,-
hbase,2854,comment_1,"The tests were broken because the Timestamp was not being set and it used to take the default value which is Long.Max_Value. The default timerange is 0 to Long.Max_Value (exclusive) so the default timestamp will never exist within default timerange. I created a patch which uses the existing extra variable to handle this case. I don't think this change is required but some tests in future which do not set the timestamp may fail again if this is not used. Note: Even after this patch, there is no way we can create a finite timerange which includes Long.Max_Value .",non_debt,-
hbase,2854,comment_2,@Pranav Maybe its ok that LATEST_TIMESTAMP breaks things? That way we find the places where its coming in unexpected?,non_debt,-
hbase,2854,comment_3,@stack: Agreed. I think the code fix isn't necessary.,non_debt,-
hbase,2854,comment_4,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,2886,summary,Add search box to site,non_debt,-
hbase,2886,description,Add search box to HBase site which directs users to This was discussed on mailing list:,non_debt,-
hbase,2886,comment_0,"Added patch applicable to trunk. It adds default site template file (site.vm) with direct injection of search box in it. Please let me know if another way of adding search box is preferable, or this one needs adjustments.",non_debt,-
hbase,2886,comment_1,"Thats a bit radical bringing in the site.vm and editing it directly but I suppose it necessary given where you've put the search box (I like where you put the search box). Now we've added site.vm, other customizations on mvn template will come easier. Thanks for the patch Alex. Committed.",non_debt,-
hbase,2886,comment_2,"Oops, it looks like the search box got lost, probably during the Feb 10 release. Stack, any way you can revive it?",non_debt,-
hbase,2886,comment_3,@Otis Sorry about that. It should be back now (Page will have 'Last Published: 2011-02-22' on it). Not sure how that happened. Will keep an eye on it.,non_debt,-
hbase,2886,comment_4,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,2925,summary,LRU of breaks if HBaseConfiguration is changed,non_debt,-
hbase,2925,description,"caches the created {{TableServer}} in {{HBASE_INSTANCES}} (a {{LinkedHashMap}} ) which is keyed by the configuration instance itself. Given the current implementation of {{hashCode()}} (and {{equals()}}) of the hash code of the configuration is changed if any of its properties are changed, which will cause the keys of {{HBASE_INSTANCES}} to be inconsistent with the hashtable that contains them, making some entries unreachable. In this case, when the map's LRU strategy needs to remove the oldest entry, it tries to remove it based on the oldest key, which no longer gives the original hash code, therefore the lookup in doesn't actually remove anything. This has been observed to lead to OOM errors in long running clients.",non_debt,-
hbase,2925,comment_0,This is a class that I put together that illustrates this problem and replicate it. Could be used as a basis for the unit test for a patch.,non_debt,-
hbase,2925,comment_1,"The only fix I could think of is to clone the configuration before inserting in the new {{TableServer}} in This way, LRU will work since the key cannot be changed since no other object holds a reference to it. This will add the overhead of creating a new instance with every connection, which is minimal if the cache works as it should.",non_debt,-
hbase,2925,comment_2,"@Robert What if we just removed the dump hash and equals and used object equality instead? I think it a bit much trying to equate Configuration objects; i.e. they are the same if they have ""same"" config where ""same"" is every properties's hash equates. If we were to go the route of trying to equate Configurations by the properties they carry, we should equate the values 'true', 'True', 'TRUE', and if a boolean !0 (anything but zero). Thanks for filing this issue.",non_debt,-
hbase,2925,comment_3,Hmm... Looks like I thought removing hashcode from HBC a good idea a while back too (See comment in HBASE-1976).,non_debt,-
hbase,2925,comment_4,"Here's a start. Its not done yet but shows direction: i.e. removing hashcode and equals from HBC. TODO, is test that prove that HBASE-1251 is still fixed (I think thing to do here is just read configs down in TableServers out of the conf each time rather than once up front so if number of retries is changed mid-use, subsequent invocations will pick up new config. -- let me see).",requirement_debt,requirement_partially_implemented
hbase,2925,comment_5,"Oh Robert, I just noticed that you did not grant Apache license on your test code. If intentional, np, I'll back your test out of my patch. Just say so.",non_debt,-
hbase,2925,comment_6,"@stack, thank you for picking up this issue. Feel free to reuse the code provided in in any way you want. (Let me know if this is not enough I could submit another file with Apache license added.) Back to the discussion, I agree that removing the {{hasCode()}} and {{equals()}} methods does resolve this issue. However, I wonder if it defeats the purpose of having a connection cache from the first place. You also alluded to the problem of the configuration settings changing after the connection is open, which, if we rely solely on Object equality, could lead to subtle error or unexpected behavior. Hence my suggestion to pin down the configuration once the TableServers instance is created. The disadvantage there is that small/unrelated changes in the configuration will invalidate the cache entry as well. If that is deemed a real problem given the common use cases, I was thinking maybe we define ""equality"" of the configuration for the purposes of caching as the equality of all properties that start with {{hbase.}} or {{zk.}} or any other prefix whose properties could be used as possible connection parameters. Not sure if the comparison becomes too slow in that case, but what is more important is whether correctness is compromised if we make the assumption that only properties starting with those prefixes affect the connection.",design_debt,non-optimal_design
hbase,2925,comment_7,"@Robert ...I wonder if it defeats the purpose of having a connection cache from the first place. As I see it, the main benefit to the cache is saving on region lookups, setup of zk connection, and master proxy setup. Having the likes of the following config cached is secondary: e.g. In fact, I'm now thinking that if a user changes any of the above in a Configuration that is being used as a key in that its ok if we don't notice the change as long as we doc the fact that the Configuration is read on instantiation of the HTable and then no more. I don't tthink this should 'surprise' the user too much and they can just go create new HTable with the new Configuration if they really want their new config. to take hold (Making things work this way is similar to what you suggest only you would copy the Configuration before adding it as a key). As to your last suggestion, I think we should move away from trying to equate Configurations at all; there be daemons that way. Let me know what you think. If you are agreeable, I'll work up the patch some more mostly adding doc clarifying what we've agreed here.",documentation_debt,outdated_documentation
hbase,2925,comment_8,This is what I had in mind... Sorry if I didn't make it clear in previous comments. I agree that these semantics are good for all the use cases I could think of. So feel free to proceed with your patch.,non_debt,-
hbase,2925,comment_9,"This version of the patch just adds javadoc to HTable explaining the advantage of shared Configuration -- the sharing of zookeeper connection, cache of region locations, etc. If you have a minute, give it a gander Robert and if its good w/ you, I'll go ahead and commit.",documentation_debt,outdated_documentation
hbase,2925,comment_10,You may have left out a couple of print statements in TestHCM. Not sure if that was intentional. Otherwise it's good to go.,code_debt,low_quality_code
hbase,2925,comment_11,"@Robert Yeah I removed some, left others but also added asserts so tests should fail is regression. Thanks for doing up the test. I just committed v2.",non_debt,-
hbase,2925,comment_12,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,2969,summary,missing sync in,non_debt,-
hbase,2969,description,"Considering that the method _getTable(String)_ in is invoked by multiple threads, it may happen that while _'queue == null'_ is true, it is possible to have a queue mapped to that name into the tables map when queue)'_ is executed. However, I don't think it will cause any harm because the overwritten queue will eventually be garbage-collected. :-)",design_debt,non-optimal_design
hbase,2969,comment_0,Now using,non_debt,-
hbase,2969,comment_1,Sorry. I've used the wrong action.,non_debt,-
hbase,2969,comment_2,Thanks for the patch Guilherme. Committed to trunk,non_debt,-
hbase,2969,comment_3,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,3057,summary,Race condition when closing regions that causes flakiness in TestRestartCluster,test_debt,flaky_test
hbase,3057,description,"In we spin up cluster, create three tables, shut it down, start it back up, and ensure we still have three regions. A subtle race condition during the first shutdown makes it so the flush of META doesn't finish so when we start back up there are no user regions. I'm not sure if there are reasons the ordering is as such, but the is the section of code in CloseRegionHandler around line 118: We remove from the online map of regions before actually closing. But what the main run() loop in the RS is waiting on to determine when it can shut down is that the online region map is empty. Any reason not to swap these two and do the close before removing from online regions?",code_debt,multi-thread_correctness
hbase,3057,comment_0,+1 Looks like mistake on my part (I love unit tests).,non_debt,-
hbase,3057,comment_1,Committed to trunk,non_debt,-
hbase,3057,comment_2,"I'm not sure if this is the same problem. If not I'll open another issue. testClusterRestart fails for me most of the time. I've attached two logs. The first is unaltered trunk, the second again slightly modified (so line numbers etc. won't match up) but a slightly different error.",non_debt,-
hbase,3057,comment_3,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,3072,summary,Client should be interruptible; APIs should all change to let out Interrupted Exception,non_debt,-
hbase,3072,description,This issue comes of discussion over in HBASE-3064. That issue hacks in some basic interruptibility. We should revamp client so all APIs can be interrupted/throw IE.,non_debt,-
hbase,3072,comment_0,What would you like to do with this issue ?,non_debt,-
hbase,3072,comment_1,Too inspecific. Resolving later. No work done on it.,non_debt,-
hbase,3072,comment_2,"fwiw, some work has been done in HBASE-10337 and others. The strategy to limit the impact on the interface was to use There is a test for get so if it breaks we will see it. Not all paths (puts...) are covered however.",non_debt,-
hbase,3173,summary,HBase 2984 breaks ability to specify BLOOMFILTER & COMPRESSION via shell,non_debt,-
hbase,3173,description,HBase 2984 breaks ability to specify BLOOMFILTER & COMPRESSION via shell,non_debt,-
hbase,3173,comment_0,"I tried the patch, although it fixes my breakage, I see that this fix breaks something that was possible before: This was possible before. You might just look at what was done, or I can do it since it's my fault you're fixing it :)",non_debt,-
hbase,3173,comment_1,Aah. I have always been using upper case stuff :). So didn't catch it. We might need to throw in a toUpperCase() in admin.rb. JD: Want to make that additional change and commit? In I see something like:,non_debt,-
hbase,3173,comment_2,Committed to branch and trunk (after verifying it works). Modelled fix on Igor's hbase-3310.,non_debt,-
hbase,3173,comment_3,This fix was included in 0.90.0.,non_debt,-
hbase,3173,comment_4,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,3181,summary,"Review, document, and fix up timeout logic",non_debt,-
hbase,3181,description,"In some of the testing Stack and I have been doing, we've uncovered some issues with concurrent RS failure and when the Master is under heavy load. It's led to situations where we handle ZK events far after they actually occur and have uncovered some issues in our timeout logic. This jira is about reviewing the timeout semantics, especially around ZK usage, and ensuring that we handle things appropriately.",non_debt,-
hbase,3181,comment_0,"Working on a document that goes over all this stuff, but I'd like stack to give a go at my current patch. There's a few fairly big fixes, not just to timeouts but also to server shutdown handling, and I'd like to see if it fixes the issues he's been seeing. Putting patch up on RB.",documentation_debt,low_quality_documentation
hbase,3181,comment_10,This last patch is looking really good. A rolling restart worked for first time and a MR rowcount confirmed table all up. Trying some more....,non_debt,-
hbase,3181,comment_13,Committed to trunk. Thanks for testing and review stack.,non_debt,-
hbase,3181,comment_14,Final version of patch committed.,non_debt,-
hbase,3181,comment_15,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,3188,summary,Review locking around,non_debt,-
hbase,3188,description,is currently a so does not require synchronization. But sometimes we do multiple operations and we synchronize on it. But other times we don't synchronize on it at all. Let's review and make sure we're doing the right thing. Also see if we still need this Don't we disable load balancer / expiration during startup or no?,non_debt,-
hbase,3188,comment_0,Stale,non_debt,-
hbase,3228,summary,Deprecate multiput for 0.90.. its going away in favor of mutliaction,non_debt,-
hbase,3228,description,None,non_debt,-
hbase,3228,comment_0,This stuff is already deprecated in favor of MultiAction.,non_debt,-
hbase,3228,comment_1,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,3252,summary,sometimes fails due to a race condition in test notification,non_debt,-
hbase,3252,description,"sometimes fails with errors like the following: Running the test locally this can happen as much as 25-50% of the time. It looks like this is due to a basic race condition in the way the test is structured. The test code uses: But, since zkListener is instantiated (and registered with ZooKeeperWatcher) prior to secondTracker (which is always the source of the failure), zkListener will be notified first of the change and there is a race condition between the subsequent test assertions and the secondTracker notification. Attaching a patch with a simple fix of just instantiating secondTracker prior to zkListener so that it's registered (and notified) first.",non_debt,-
hbase,3252,comment_0,Trivial patch to make the test reliably pass. Just moves secondTracker registration ahead of zkListener.,non_debt,-
hbase,3252,comment_1,"The problem fixed is only with the test code itself, not the underlying ZK notification, so it's not critical. But marking for 0.90.0 so that we can get all tests passing on hudson. Feel free to boot it if you disagree.",non_debt,-
hbase,3252,comment_2,Committed. Thanks for the patch Gary.,non_debt,-
hbase,3252,comment_3,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,3257,summary,Coprocessors: Extend server side integration API to include HLog operations,non_debt,-
hbase,3257,description,"Coprocessor based extensions should be able to: * Observe, rewrite, or skip WALEdits as they are being written to the WAL * Write arbitrary content into WALEdits * Act on contents of WALEdits in the regionserver context during reconstruction (update: remove WALEdit monitoring at master split. No need to use scope to control the behavior here, since it could be done by a coprocessor implementation.)",non_debt,-
hbase,3257,comment_0,Need master side cp extension to support master log splitting monitoring.,non_debt,-
hbase,3257,comment_1,"Patch was put to review board, but it's not forwarded to jira. Here is the review request: Summary: Coprocessors: Extend server side integration API to include HLog operations Coprocessor based extensions should be able to: - Observe, rewrite, or skip WALEdits as they are being written to the WAL - Write arbitrary content into WALEdits - Act on contents of WALEdits in the regionserver context during reconstruction Code changes: - a new coprocessor interface WALCPObserver is added which provides preWALWrite() and postWALWrite() upcalls to HLog, before and after at doWrite(). - added 2 new upcalls for RegionObserver for WAL replay, preWALRestore() and postWALRestore(). - a sample implementation -- -- was create which can add, remove, modify WALEdit before writing it to WAL. - test cases which use to test WAL write and replay. - added coprocessor loading at TestHLog to make sure it doesn't affect HLog. I need feedback for: - The new cp interface name -- WALCPObserver -- is not perfect. The ideal name is WALObserver but it's been used already. Other options include HLogObserver(H is not preferred), - No support for monitor master log splitting. I don't have a use case to support the requirement right now.",code_debt,low_quality_code
hbase,3257,comment_2,Committed to trunk after a week on RB. Tests pass locally.,non_debt,-
hbase,3257,comment_4,Sorry I didn't get to a review. This is a great addition.,non_debt,-
hbase,3257,comment_5,Did a quick review. +1 on commit. There are a few things you might consider in my remarks. The one that is a little worrying is notion that CP Host is instantiated even if CP is not configured on (This may just be me misreading code).,non_debt,-
hbase,3257,comment_6,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,3276,summary,delete followed by a put with the same timestamp,non_debt,-
hbase,3276,description,"[Note: This issue is relevant only for cases that don't use the default ""time"" based versions, but provide/manage versions explicitly.] The fix for HBASE-1485 ensures that if there are multiple puts with the same timestamp the later one wins. However, if there is a delete for a specific timestamp, then the later put doesn't win. Say for example the following is the sequence of operations: put row/col/v1 - value1 deleteColumn row/col/v1 put row/col/v1 - value2 Without the deleteColumn(), HBASE-1485 ensures that ""value2"" is the winner. However, with the deleteColumn() thrown into the mix, the delete wins, and one cannot insert a new value at that version. [The only, unsatisfactory, workaround at this point seems to be trigger a major compaction. The major compact would clear the delete marker, and allow new cells to be created with that version again.]  Seems like it might not be too complicated to extend the fix for HBASE-1485 to also respect ordering between delete/put operations. I'll look into this further.",non_debt,-
hbase,3276,comment_0,"I wrote: <<< [Note: This issue is relevant only for cases that don't use the default ""time"" based versions, but provide/manage versions explicitly.] On second thoughts, this should also help cases where a delete followed by a put arrives within the same millisec.",non_debt,-
hbase,3276,comment_1,"Kannan, this is easy. I can take care of this.",non_debt,-
hbase,3276,comment_9,"On flushes, if we did what a minor compaction now does (after HBASE-3048), i.e. process etc. then a HFile would never contain a value that should be suppressed. And with regards to multiple HFiles containing conflicting data (i.e. corresponding to same TS), we could pick the ""sequenceId"" of the HFile to resolve the winner. (HBASE-1485 fix also relies on sequenceId ordering of HFiles to resolve winners between entries coming from multiple files).",non_debt,-
hbase,3276,comment_10,"I'm worried that an implicit ordering opens us to problems in the future. The kind that involve ""i lost my data and there is no way to recover it"". To that end, I propose we implement HBASE-2856, specifically my comment which talks about bringing the memstoreTS (ish, an equivalent but not quite identical value) down into the HFile. It will have many benefits, including fixing this JIRA, and also fixing the ACID stuff that has been waylaid for lack of this change.",non_debt,-
hbase,3276,comment_11,A mislaid now-duplicate of more recent issues filed on the same topic.,non_debt,-
hbase,3324,summary,LRU block cache configuration improvements,non_debt,-
hbase,3324,description,"The block cache has lots of configuration parameters but they aren't using Configuration like they should. It would also be nice to have a better way of doing hit ratios, like a rolling window.",design_debt,non-optimal_design
hbase,3324,comment_0,Forgot I already opened HBASE-3306 for hit ratio improvement,non_debt,-
hbase,3324,comment_1,Moving out of 0.92.0. Pull it back in if you think different.,non_debt,-
hbase,3324,comment_2,Part dup of HBASE-3306 and stale. Resolving.,non_debt,-
hbase,3368,summary,Split message can come in before region opened message; results in 'Region has been PENDING_CLOSE for too long' cycle,non_debt,-
hbase,3368,description,"Another good one. Look at these excerpts from master log: ... so the split will have cleared the parent from in-memory data structures and then the open handler will add them back (though region is offlined, split). Then the balancer runs....... only no one is holding the region thats being balanced. Over on XXX185 I see the open and then split at these times: So, the fact that it takes the Master a while to get around to the zk watcher processing messes us up. Root problem is that we're using two different message buses, zk and then heartbeat. Intent is to do all over zk and remove hearbeat but looking at what to do for 0.90.0.",non_debt,-
hbase,3368,comment_0,add SPLITTING and SPLIT states into zk? we don't want to do load balancing during splits anyways and this would prevent that and put splits more inlien with other stuff.,non_debt,-
hbase,3368,comment_1,"Agreed that we should move all messaging up into zk and remove heartbeating carrying messages but I was thinking that for 0.90, delete of region clears it from HMaster in-memory too. The worst that could happen is balancer ran meantime. If so, it'll fail close of a region not opened anywhere but the in-memory PENDING_CLOSE would be cleared by the delete-of-region cleanup. What you think?",architecture_debt,violation_of_modularity
hbase,3368,comment_2,This is what I'm thinking:,non_debt,-
hbase,3368,comment_3,"As we discussed, I think comment is a little misleading with ""just in case"". I agree that this should not be necessary but that's because the current design is suboptimal. Otherwise this seems fine. When were we doing the regionOffline previously?",design_debt,non-optimal_design
hbase,3368,comment_4,Committed branch and trunk. Thanks for review Jon. We call regionOffline when split comes in... its called elsewhere on disable. Check it out. HBASE-1502 is how we'll fix this properly. I'll not open new issue. I just added comment over there that its needed for 0.92.,non_debt,-
hbase,3368,comment_6,"There is a problem with this 'fix'. It leaves a region in RIT and its not cleared because this happens: Above happens because on receipt of the split message, we offline parent which involves: .. i.e. we remove the region from RIT on receipt of RIT though its in OPENING or OPENED state.",non_debt,-
hbase,3368,comment_7,"Reopening. Here is full transcript for problem region, a region now stuck in RIT: Workaround is deleting the items from zk. Seems to work fine.",non_debt,-
hbase,3368,comment_8,"Oh, we don't have the PENDING_CLOSE for too long cycle anymore so committed code fixed that at least.",non_debt,-
hbase,3368,comment_9,Real fix for this one is HBASE-1502,non_debt,-
hbase,3368,comment_10,Making critical rather than blocker since don't have cycle and hacky workaround. Filing against 0.92 for now.,non_debt,-
hbase,3368,comment_11,"In my continuous splitting test, killing machines, this scenario is easy to manufacture.",non_debt,-
hbase,3368,comment_12,Start in on patch for this where we add a 'splits' dir to zk.,non_debt,-
hbase,3368,comment_13,This adds testing of the,non_debt,-
hbase,3368,comment_14,"Talking w/ Jon, we should look into creating new SPLITTING state only its the RS that sets this state rather than Master that is orchestrating state in ZK. We'd need to deal with Master doing an unassign while a split was going on (We'd need to be able to reject a close). There are probably other edge cases to consider; e.g. when a region in RIT, balancer won't run. Upsides are that this seems to fit naturally under umbrella (though the dir up in zk is called 'unassigned' -- we should change that). Talking more, the way I'm going could have issues. We can't guarantee that we won't have the same issue on occasion; e.g. the open state is handled by an executor and in frentic times, executors may be backedup.... whereas handling of the split would be done up in the zk callback. This would seem to indicate that split handling too should be done in an executor -- on both sides for the transaction. So, some exploration, and even then the patch is starting to look big.",non_debt,-
hbase,3368,comment_15,"Fixed by ""HBASE-3559 Move report of split to master OFF the heartbeat channel""",non_debt,-
hbase,3368,comment_16,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,3408,summary,AssignmentManager,non_debt,-
hbase,3408,description,"If AssignmentManager tries to move a region to an invalid destination server, rather than choosing a random server as intended, it throws an NPE. Line 1009 should check if if (existingPlan == null || forceNewPlan || != null && { I triggered it by trying to manually move regions around, probably to an invalid destination server. I'm not currently able to build the project to test if that's the extent of the problem, so here's a little more info... It leaves a stranded until the master and/or regionserver are restarted and causes problems like the following. ""hbck -fix"" was unable to repair it. 2011-01-04 00:14:10,948 DEBUG Scanned 4287 catalog row(s) and gc'd 0 unreferenced parent region(s) 2011-01-04 00:14:18,574 DEBUG Not running balancer because 1 region(s) in transition: state=OFFLINE, ts=1294118046139} 2011-01-04 00:14:36,142 INFO Regions in transition timed out: state=OFFLINE, ts=1294118046139 2011-01-04 00:14:36,142 INFO Region has been OFFLINE for too long, reassigning to a random server 2011-01-04 00:14:36,142 DEBUG Forcing OFFLINE; state=OFFLINE, ts=1294118046139 2011-01-04 00:14:36,142 ERROR Caught exception at (i think this is .90.0RC1, so same bug on a different line number)",non_debt,-
hbase,3408,comment_0,Thank you for the patch Matt. Applied to branch and trunk.,non_debt,-
hbase,3408,comment_2,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,3510,summary,Add thread name for IPC reader threads,non_debt,-
hbase,3510,description,"The IPC readers come out of a thread pool but have no name, which is annoying.",design_debt,non-optimal_design
hbase,3510,comment_0,+1 Same comment as for your other thread naming patch.,non_debt,-
hbase,3510,comment_1,Same comment as last time - it's difficult to get to the RS name from here. So I just added the port - that makes it consistent with the other IPC threads.,design_debt,non-optimal_design
hbase,3510,comment_3,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,3528,summary,Maven should restrict more exported deps,non_debt,-
hbase,3528,description,"Our maven build exports a lot of deps, and they flow to clients who depend on us. for example we shouldnt export etc. Clients should be able to depend on any version of the above libraries or NOT depend on them.",design_debt,non-optimal_design
hbase,3528,comment_0,Isn't this the same as HBASE-2170? Our Maven build only declares all dependencies necessary to build and run HBase. If you leave out one of those dependencies you will run into problems trying to develop or run HBase server. The solution is to modularize HBase.,non_debt,-
hbase,3528,comment_1,@Lars Yes. @Ryan Want to add your remark to hbase-2170 and kill this one?,non_debt,-
hbase,3528,comment_2,Moving out of 0.92.0. Pull it back in if you think different.,non_debt,-
hbase,3528,comment_3,HBase has been modularized and there are on-going efforts to do more.,non_debt,-
hbase,3543,summary,"Get rid of ""delete forward""",non_debt,-
hbase,3543,description,"There was a conversation on the mailing list about removing this ""misfeature"" but I couldn't find the associated JIRA. If it's already been filed, feel free to mark this one as a duplicate.",non_debt,-
hbase,3543,comment_0,Thanks for filing this Jeff. Should get addressed by hbase-2856.,non_debt,-
hbase,3547,summary,Make hbase run on 0.21,non_debt,-
hbase,3547,description,None,non_debt,-
hbase,3547,comment_0,"Stack, Am thinking of contributing to this and work during next week's Hackathon. Would like to get a handle on the work - is this something a new person can pick up ? How complex is the work ? And is it critical - don't want to be in the high critical path on my first few pieces of work. Cheers & Thanks <k/>",non_debt,-
hbase,3547,comment_1,Krishna: This would be a nice addition. You'll be messing w/ the difference in 0.20.x and 0.21.x append. You'll probably add a bit of reflection on top of the reflection we already have for figuring what APIs are available doing sync/flush. Shouldn't be too tough. Doing it at the hackathon you'd have a few fellas at your shoulder to give you pointers should you get stuck.,non_debt,-
hbase,3547,comment_2,"Krishna, I'm resolving this as a duplicate of HBASE-3547. Please do your work against that issue going forward. Thanks.",non_debt,-
hbase,3547,comment_3,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,3571,summary,Why do we have a lock in CompactSplitThread when only ever one instance of this thread?,non_debt,-
hbase,3571,description,"This lock in CompactSplitThread doesn't do anything best as I can tell: If two instances of CompactSplitThread, it would not prevent the two threads contending since its local to the instance. Remove it.",code_debt,dead_code
hbase,3571,comment_0,"I looked at the code, and it looks like this was fixed in",non_debt,-
hbase,3571,comment_1,Closing as duplicate. Assigned Joey because he did the work to figure it is a duplicate (thanks Joey -- added you as contributor).,non_debt,-
hbase,3571,comment_2,Thanks Stack!,non_debt,-
hbase,3597,summary,ageOfLastAppliedOp should update after cluster replication failures,non_debt,-
hbase,3597,description,"The value of ageOfLastAppliedOp in JMX doesn't update after replication starts failing, and it should. See:",non_debt,-
hbase,3597,comment_0,"btw. this is filed under ""replication"" component because there is no ""cluster replication"" component. Maybe it should be added?",non_debt,-
hbase,3597,comment_1,"We're not big users of that field, but AFAIK ""replication"" is the right one as it's the name of the package.",non_debt,-
hbase,3597,comment_2,"I guess I would be the one working on this and I won't have time for 0.90.2, so punting.",non_debt,-
hbase,3597,comment_3,"Patch that I think will fix the issue, it adds a way to refresh the age of the last shipped edit in case replication fails.",non_debt,-
hbase,3597,comment_4,Looks fine. Aren't all those expensive?,code_debt,slow_algorithm
hbase,3597,comment_5,"That would be called on average once or twice per second on a normal cluster, I'm pretty sure is a few order of magnitudes more expensive than what those metrics are doing.",code_debt,slow_algorithm
hbase,3597,comment_6,"Committed to branch and trunk, talked to Stack offline and he thought he saw that it was calling currentTimeMillis for every edit but it's only for the last one we ship so he was ok with it.",non_debt,-
hbase,3597,comment_7,1,non_debt,-
hbase,3597,comment_9,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,3625,summary,improve/fix support excluding Tests via Maven -D property,non_debt,-
hbase,3625,description,"Currently the surefire plugin configuration defines the following exclusion: AFAICT the '{{/$**}}' does not resolve to anything meaningful. Adding support to exclude one or more tests via Maven property, i.e.",design_debt,non-optimal_design
hbase,3625,comment_0,"Maven surefire plugin supports a The surefire configuration supports test exclusions by providing a Test Java file path {{/Test**.java}}, the {{<exclusion Supporting exclusion for Supporting exclusion for a single test, is easy: If the {{-Dtest.exclude=}} property is not specified, the pattern will not resolve to a testcase class. Supporting exclusion for multiple tests can be done easily if full PATH patterns are specified, Again, if the property is not specified, the pattern will not resolve to testcases classes. Both properties can be defaulted in the {{<properties Then a single exclusion is needed for both single test class and test file patterns.",non_debt,-
hbase,3625,comment_1,mvn test -Dtest=TestHRegion currently works. What else are you worried about?,non_debt,-
hbase,3625,comment_2,"Ryan, I want to be able to exclude tests using a -D property when running mvn",non_debt,-
hbase,3625,comment_3,"Alejandro, would you mind posting a patch against hbase trunk?",non_debt,-
hbase,3625,comment_4,Alejandro's on vacation so I uploaded his patch he made against an internal git repo here.,non_debt,-
hbase,3625,comment_5,1,non_debt,-
hbase,3625,comment_6,Marking patch available. Looks like something that'd be useful in trunk and branch.,non_debt,-
hbase,3625,comment_8,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,3641,summary,is not using the correct variable,non_debt,-
hbase,3641,description,This should be {{hitCount.get()}},non_debt,-
hbase,3641,comment_0,Patch looks odd: The method is called getHitCachingCount and it was returning state of hitCachingCount... which seems right?,non_debt,-
hbase,3641,comment_1,"Wow. And I guess we know who screwed it up in the first place =P Thanks for catching that, Stack. This patch should be right.",non_debt,-
hbase,3641,comment_2,+1 (You must be a little busy Jon -- smile),non_debt,-
hbase,3641,comment_3,Committed to branch and trunk. Thanks stack.,non_debt,-
hbase,3641,comment_5,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,3673,summary,Reduce HTable Pool Contention Using Concurrent Collections,non_debt,-
hbase,3673,description,"In the case of medium-to-large sized HTable pools, the amount of time the client spends blocking on the underlying map and queue data structures turns out to be quite significant. Using an efficient wait-free implementation of maps and queues might serve to reduce the contention on the pool. In particular, I was wondering if we should replace the synchronized map with a concurrent hash map, and linked list with a concurrent linked queue.",design_debt,non-optimal_design
hbase,3673,comment_0,Patch looks good Karthick. You tried it? Any noticeable difference? Good on you K.,non_debt,-
hbase,3673,comment_1,"The patch did seem to make a difference in our particular use case, in terms of the average time it took to get a htable from the pool. For the sake of a more formal evaluation, I put together a benchmark (see attached test case), which did show noticeable difference. Specifically, when you've a htable pool of size 150, and a worker pool of 100 threads, where each thread does a get followed by a put a million times, the total time spent was 7775614 ms with the patch, as opposed to 12654820 ms without. That's turns out to be a 40% improvement. That said, using different htable pools for different use cases (e.g., different htables) might be the way to go, as that will tend to reduce the level of concurrency on any given htable pool.",code_debt,slow_algorithm
hbase,3673,comment_2,Committed to TRUNK. Thanks for the patch Karthick.,non_debt,-
hbase,3673,comment_4,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,3696,summary,No durability when running with LocalFileSystem,non_debt,-
hbase,3696,description,"LocalFileSystem in Hadoop doesn't currently implement sync(), so when we're running in that case, we don't have any durability. This isn't a huge deal since it isn't a realistic deployment scenario, but it's probably worth documenting. It caused some confusion for a user when a table disappeared after killing a standalone instance that was hosting its data in the local FS.",requirement_debt,requirement_partially_implemented
hbase,3696,comment_1,"This confused me a bit today while trying to debug HBase replication running locally; you have to roll the HLog in order for ReplicationSource to pick up and replicate your edits. +1 to a fix (but, no idea what that'd be; an implementation of sync() for LocalFileSystem?)",non_debt,-
hbase,3696,comment_2,Is this added foot-note sufficient?,non_debt,-
hbase,3696,comment_4,Doc patch of mine got applied via HBASE-9379.,non_debt,-
hbase,3696,comment_5,"Can this be closed or can the DOC component be removed, based on the previous comment?",non_debt,-
hbase,3696,comment_6,"Resolving as implemented by HBASE-11218 (which should go in soon) at least for standalone. Data loss on fs has been doc'd also. Hopefully HBASE-11218 will do pseudo distributed mode too. If not, doc will need tweaking. Can do in new issue.",documentation_debt,outdated_documentation
hbase,3713,summary,Hmaster had crashed as disabling table,non_debt,-
hbase,3713,description,"Operation step: 1, startup cluster with HA master 2, the active master crashed while it is creating table with region 3, backup master become active. 4, I want to drop the table 5, the active master crashed So the issue is that if a region was closed and disabled when the first master was running, it won't be assigned anywhere and won't be in transition either (it's called being in RIT in the code). When the new master comes around, and disable is called, it does a check to see if the region is in RIT but not if it was already disabled, and fails on NPE because it's not assigned to anyone.",non_debt,-
hbase,3713,comment_0,"Thank you for digging in. While I see the sequence described as being relatively 'rare' in operation, it does expose a 'hole' that others might fall in to doing other than the above described sequence.",non_debt,-
hbase,3713,comment_1,Moving out of 0.92.0. Pull it back in if you think different.,non_debt,-
hbase,3713,comment_2,"I believe that is fixed in 0.94+, but need to verify.  Wanna have a look?",non_debt,-
hbase,3713,comment_3,"Yes, i think many of these have got fixed in recent versions. We can close this and if we encounter something similar we can track it down in the new JIRA.",non_debt,-
hbase,3713,comment_4,"Closing as DUP of some of the current changes (don't have jiras, though)",non_debt,-
hbase,3751,summary,Book.xml - fixing unit of measure in 2 metrics. added description for compactionQueue,non_debt,-
hbase,3751,description,"The patch I submitted yesterday for 'blockCacheFree' and 'blockCacheSize' said these were in MB. After re-re-confirming against our own cluster, these numbers are actually in bytes. Also added further description of what's actually in the compactionQueueSize (e.g., it's the number of stores targeted for compaction in the region).",non_debt,-
hbase,3751,comment_0,Thanks for the patch Doug. Committed to TRUNK.,non_debt,-
hbase,3751,comment_2,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,3807,summary,Fix units in RS UI metrics,non_debt,-
hbase,3807,description,Currently the metrics are a mix of MB and bytes. Its confusing.,design_debt,non-optimal_design
hbase,3807,comment_0,Moving out of 0.92.0. Pull it back in if you think different.,non_debt,-
hbase,3807,comment_1,"Proposing to change from the following formats request=0.0, regions=8, stores=8, storefiles=8, rootIndexSizeKB=3, memstoreSize=0, flushQueueSize=0, usedHeap=41, maxHeap=995, blockCacheCount=9, stores=1, storefiles=2, storefileSizeMB=0, memstoreSizeMB=0, rootIndexSizeKB=0, to numberOfStores=8, rootIndexSizeKB=4, maxHeapMB=995, blockCacheCount=11, rootIndexSizeKB=1, Also proposing the following metric in bytes blockCacheSize ,blockCacheFree planning to MB with full precision like If this looks good ,i can upload the patch for trunk. If needed i can prepare the patch for 90.X also.",non_debt,-
hbase,3807,comment_2,Your proposal looks excellent Subramanian. Would suggest two digits after the decimal point only is needed (There is a limitDecimalTo2 in Hadoop StringUtils class if that is of any help).,non_debt,-
hbase,3807,comment_3,"numberOfStores=8, rootIndexSizeKB=3, memstoreSizeMB=0, flushQueueSize=0, usedHeapMB=38, maxHeapMB=995, blockCacheCount=4,",non_debt,-
hbase,3807,comment_4,"Also modifying the debug log in LruBlockCache since theres duplicate percentages{%%} in cachingHitsRatio & hitRatio 2011-08-09 11:39:52,227 DEBUG LRU Stats: total=960.27 KB, free=198.75 MB, max=199.69 MB, blocks=2, accesses=19, hits=17, cachingAccesses=19, cachingHits=17, evictions=0, evicted=0, evictedPerRun=NaN after fix 2011-08-10 10:58:59,062 DEBUG LRU Stats: total=1.65 MB, free=197.53 MB, max=199.18 MB, blocks=4, accesses=83, hits=79, cachingAccesses=83, cachingHits=79, evictions=0, evicted=0, evictedPerRun=NaN",non_debt,-
hbase,3807,comment_5,Removing the unused code for determining time elapsed in RegionServerMetrics : int seconds = - if (seconds == 0) { seconds = 1; } Will upload the patch ASAP,code_debt,dead_code
hbase,3807,comment_6,Very nice. Thank you Subramanian. Applied to TRUNK,non_debt,-
hbase,3807,comment_8,Assigning to Subramanian.,non_debt,-
hbase,3807,comment_9,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,3840,summary,Add sanity checks on Configurations to make sure hbase confs have been loaded,design_debt,non-optimal_design
hbase,3840,description,A common user error (and even hbase dev error) is to pass a vanilla Hadoop Configuration into HBase methods that expect to see all of the relevant hbase defaults from hbase-default.xml. This often results in NPE or issues locating ZK. We should add a method like which ensures that the conf has incorporated hbase-default.xml. We can do this by checking for existence of,design_debt,non-optimal_design
hbase,3840,comment_0,Moving out of 0.92.0. Pull it back in if you think different.,non_debt,-
hbase,3840,comment_1,There appears to be no interest in this for over a year. Closing. Please reopen if you disagree.,non_debt,-
hbase,3936,summary,Incremental bulk load support for Increments,non_debt,-
hbase,3936,description,"From ""The bulk load feature uses a MapReduce job to output table data in HBase's internal data format, and then directly loads the data files into a running cluster. Using bulk load will use less CPU and network than going via the HBase API."" I have been working with a specific implementation of, and can envision, a class of applications that reduce data into a large collection of counters, perhaps building projections of the data in many dimensions in the process. One can use Hadoop MapReduce as the engine to accomplish this for a given data set and use to move the result into place for live serving. MR is natural for summation over very large counter sets: emit counter increments for the data set and projections thereof in mappers, use combiners for partial aggregation, use reducers to do final summation into HFiles. However, it is not possible to then merge in a set of updates to an existing table built in the manner above without either 1) joining the table data and the update set into a large MR temporary set, followed by a complete rewrite of the table; or 2) posting all of the updates as Increments via the HBase API, impacting any other concurrent users of the HBase service, and perhaps taking 10-100 times longer than if updates could be computed directly into HFiles like the original import. Both of these alternatives are expensive in terms of CPU and time; one is also expensive in terms of disk. I propose adding incremental bulk load support for Increments. Here is a sketch of a possible",non_debt,-
hbase,3936,comment_0,"This looks like a very specific usecase, Andy. Are you still interested in working on this? Should we keep it open?",non_debt,-
hbase,3936,comment_1,Resolving as Later.,non_debt,-
hbase,4016,summary,doesn't have a consistent behavior when the field that we are incrementing is less than 8 bytes long,non_debt,-
hbase,4016,description,"We wanted to use an int (32-bit) atomic counter and we initialize it with a certain value when the row is created. Later, we increment the counter using This call results in one of two outcomes. 1. The call succeeds, but the column value now is a long (64-bit) and is corrupt (by additional data that was in the buffer read). 2. Throws offset (65547) + length (8) exceed the capacity of the array: 65551 Source) Based on our incorrect usage of counters (initializing it with a 32 bit value and later using it as a counter), I would expect that we fail consistently with mode 2 rather than silently corrupting data with mode 1. However, the exception is thrown only rarely and I am not sure what determines the case to be executed. I am wondering if this has something to do with flush. Here is a HRegion unit test that can reproduce this problem. We modified our code to initialize the counter with a 64 bit value. But, I was also wondering if something has to change in to handle inconsistent counter sizes gracefully without corrupting existing data. Please let me know if you need additional information.",design_debt,non-optimal_design
hbase,4016,comment_0,"Your code is storing strings in the cells, but expects a big-endian encoded long, not a string.",design_debt,non-optimal_design
hbase,4016,comment_1,"Todd, I reviewed this with Praveen before he created this jira and his point is that the error reporting seems buggy when trying to increment a cell that doesn't contain a long, not that a string or int cannot be incremented.",non_debt,-
hbase,4016,comment_2,"The cell that is being incremented, (row1,fam1,qual1) store an int (4 bytes) not a string. I think you are confusing the row key with the cell value. There was a minor issue with the old patch and I have revised it to fix it. The new one is available at I am reopening the issue.",non_debt,-
hbase,4016,comment_3,"Fixed this. Btw - Praveen, your test is broken. even if you comment out l long result; try { result = fam1, qual1, 1, true); fail(""Expected to fail here""); } catch (Exception exception) { // Expected. }, the assertICV's still fail.",non_debt,-
hbase,4016,comment_4,checks length of stored value before incrementing.,non_debt,-
hbase,4016,comment_5,also fixed Praveen's tests.,non_debt,-
hbase,4016,comment_6,Applied to TRUNK. Thanks for the patch Li Pi.,non_debt,-
hbase,4016,comment_8,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,4088,summary,npes in server shutdown,non_debt,-
hbase,4088,description,"2011-07-11 10:26:01,268 ERROR Caught throwable while processing event M_SERVER_SHUTDOWN 2011-07-11 10:26:01,268 INFO Splitting logs for 2011-07-11 10:26:01,269 ERROR Caught throwable while processing event M_SERVER_SHUTDOWN",non_debt,-
hbase,4088,comment_0,I have seen this when we are shutting down.,non_debt,-
hbase,4088,comment_1,Small fix to logging message -- check for null before getting list size.,code_debt,low_quality_code
hbase,4088,comment_2,Applied small fix to branch and trunk.,non_debt,-
hbase,4088,comment_4,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,4116,summary,[stargate] in row spec parse,non_debt,-
hbase,4116,description,"From user@hbase, Allan Yan writes: There might be a bug for REST web service to get rows with given startRow and endRow. For example, to get a list of rows with startRow=testrow1, endRow=testrow2, I send GET request: curl And got This was because in the RowSpec.java, parseRowKeys method, startRow value was changed: After change to this, it works: I've also created a unit test method in",non_debt,-
hbase,4116,comment_0,Converted Allan's description into a patch.,non_debt,-
hbase,4116,comment_1,"Committed to 0.90 branch and trunk. All tests pass locally, including new test.",non_debt,-
hbase,4247,summary,Add isAborted method to the Abortable interface,non_debt,-
hbase,4247,description,Add a new method isAborted() to the Abortable interface,non_debt,-
hbase,4247,comment_0,"Firstly in the process of working on this. I have made the following assumptions 1. if abort() has a null implementation, then a. isAborted() will have a null implementation ( returns false ) 2. if abort() throws a RuntimeException(), then a. create a boolean abort b.set this to true in abort() c. return abort in isAbort() I hope this would be fine. Secondly there are Instances where in abort() implementation this.stopped and this.closed are manipulated. Shouldn't this be done in stop() and close() methods of Stoppable and Closable interfaces? This is a little misleading isn't it ? If it really has to be stopped when aborted then why not call stop() from abort(). I feel it makes it more meaningful",non_debt,-
hbase,4247,comment_1,"On 1., agree. I agree on 2. too. On your 'Secondly', yes. That seems dirty. Agree on calling stop from abort (Doesn't it do this in a few places? IIRC).",code_debt,low_quality_code
hbase,4247,comment_2,I just made the changes and ran tests. There's one issue. In as I mentioned before this.closed is being set to true in the abort() method and I changed this to call close(stopProxy) method indeed instead of manipulating this.closed to true. But unfortunatly TestMergeTool started failing because of this change. So my question is that was this intended closed in the first place ??,non_debt,-
hbase,4247,comment_3,TestMergeTool passed in build 2155. Can you publish your patch along with the test failure ? Thanks,non_debt,-
hbase,4247,comment_4,TestMerge tool is failing because of the changes I made I am sure. The change is basically this: Original My question is close is closing zookeeper session and other thing. But does changing this leads to other test cases like TestMergeTool failing. Does setting this.closed=true have any meaning at all in abort without actually closing ?,non_debt,-
hbase,4247,comment_5,Plz excuse me about the formatting. I didn't know the Markup tags the codes should be placed under,code_debt,low_quality_code
hbase,4247,comment_6,"See for Code formatting tags for JIRA @Akash: If you cannot edit your previous comment, I can format it for you.",non_debt,-
hbase,4247,comment_7,implements HConnection which inturn implements Abortable,non_debt,-
hbase,4247,comment_8,"According to the javadoc for Abortable: This means the first if block in the above abort() method is supposed to handle almost all calls to the method. To try to validate the above judgement, I issued the following two searches among all test output files: There was nothing returned. TestMergeTool is a special case where HBase cluster is supposed to be unavailable. Looking at getMaster() and locateRegion(), this.closed being true would make them return immediately. Hope this helps with your question.",non_debt,-
hbase,4247,comment_9,I agree with that. But on a more logical level doesn't setting this.closed = true mean that close() method was called on it and the zookeeper session itself was closed ? setting this.closed = true without executing close() shouldn't be done even for special cases right ? If such is that case then this.aborted itself should be used instead of closed right ??,non_debt,-
hbase,4247,comment_10,I thought abort executed close?,non_debt,-
hbase,4247,comment_11,In under the subclass If we call close() instead of this tests are failing( one of them being TestMergeTools ). I was wondering if some change should be made out here. Somehow I feel its not right set this.closed out here? Or if this is some special case and could be left untouched. Thanks,non_debt,-
hbase,4247,comment_12,I think this is special. This is client-side. I thought you were talking server-side. I misunderstood. Do what you think makes most sense Akash. This client-side stuff is tricky.,non_debt,-
hbase,4247,comment_13,Adding Javadoc to the isAbortable() method in Abortable Interface,non_debt,-
hbase,4247,comment_14,Committed to TRUNK. Thank you for the patch Akash.,non_debt,-
hbase,4247,comment_16,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,4250,summary,"developer.xml - restructuring, plus adding common patch feedback",code_debt,low_quality_code
hbase,4250,description,"* Did some restructuring of sections * Added section for common patch feedback (based on my own experiences in a recent patch). ** I ran these past Todd & JD at this week's hackathon. * mentioning ReviewBoard in patch submission process ** this didn't appear anywhere before. I still need to add more on tips on using it, but this is a start.",code_debt,low_quality_code
hbase,4303,summary,has bad quoting,code_debt,low_quality_code
hbase,4303,description,"Currently it's outputting: REGION = Notice the missing quotes around tableName, etc.",code_debt,low_quality_code
hbase,4303,comment_0,Trivial fix,non_debt,-
hbase,4303,comment_1,I committed this without review since it's trivial,non_debt,-
hbase,4303,comment_3,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,4311,summary,generates table that fails in hbck.,non_debt,-
hbase,4311,description,"When refactoring TestHBaseFsckRepair to add more hbck test cases, I noticed that uses an existing table with empty region, adds more regions, and then attempts to remove the region. The region remains in meta and is causes hbck to report at inconsistency. Ideally these test table generation utility functions should generate clean tables.",design_debt,non-optimal_design
hbase,4311,comment_0,The description is slightly wrong. The entry is removed from meta but it may be assigned and its hdfs hfile stuff is still present which causes hbck to complain.,documentation_debt,low_quality_documentation
hbase,4311,comment_1,"I agree we should create hbck clean tables. FYI, the gymnastics being done in createMultiRegion precedes the createTable that took a list of regions. In other words, I don't think we need do it this way any more. Could just do byte[][]) I believe. (You might not be able to do the above w/ the calls though)",non_debt,-
hbase,4311,comment_2,"Thanks for the pointer, I'll update table creation in HBASE-4313's refactor of hbck tests. The existing version and the current dev version of the hbck tests does a somewhat hacky cleanup to make the table clean before messing it up again.",code_debt,low_quality_code
hbase,4311,comment_3,Used other mechanisms to create tables for hbck / HBASE-5128. Might be better to create new issue to remove or deprecate these methods (if this hasn't happened yet).,non_debt,-
hbase,4340,summary,Hbase can't balance if encountered exception,non_debt,-
hbase,4340,description,"Version: 0.90.4 Cluster : 40 boxes As I saw below logs. It said that balance couldn't work because of a dead RS. I dug deeply and found two issues: 1. shutdownhandler didn't clear numProcessing deal with some exceptions. It seems whatever exceptions we should clear the flag or close master. 2. ""dead regionserver(s): is inaccurate. The dead sever should be "" 158-1-130-",non_debt,-
hbase,4340,comment_0,"I have made a patch, Please review.",non_debt,-
hbase,4340,comment_1,"The NPE happened on this line in MetaReader.java: The patch looks reasonable since there is no action taken if hris is null. Have you tested the patch on a cluster, Jinchao ?",non_debt,-
hbase,4340,comment_2,"Yes, All test cases have passed.",non_debt,-
hbase,4340,comment_3,Patch for TRUNK has been integrated. Original patch integrated to 0.90 branch. Thanks for the patch Jinchao.,non_debt,-
hbase,4340,comment_5,"Thanks for your work. Ted. I want to patch through to review, and then make a trunk patch. All test case passed need two hours. :)",non_debt,-
hbase,4340,comment_6,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,4341,summary,HRS#closeAllRegions should take care of HRS#onlineRegions's weak consistency,non_debt,-
hbase,4341,description,"This's the reason of why did get failure . In this test, one case was timeout and cause the whole test process got killed. [logs] Here's the related logs(From [Analysis] One region was opened during the RS's stopping. This is method of HRS#onlineRegions is a ConcurrentHashMap. So walk down this map may not get all the data if some entries are been added during the traverse. Once one region was missed, it can't be closed anymore. And this regionserver will not be stopped normally. Then the following logs occurred:",design_debt,non-optimal_design
hbase,4341,comment_0,The above analysis makes sense to me. You have a patch Jieshan?,non_debt,-
hbase,4341,comment_1,I'm trying to make the patch. Hope I can submit it today.,non_debt,-
hbase,4341,comment_2,You are a good man.,non_debt,-
hbase,4341,comment_3,The patch is reasonable.,non_debt,-
hbase,4341,comment_4,I agree.,non_debt,-
hbase,4341,comment_5,Applied to branch and trunk. Thank you for the patch Jieshan.,non_debt,-
hbase,4341,comment_7,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,4437,summary,Update hadoop in 0.92 (0.20.205?),non_debt,-
hbase,4437,description,"We ship with branch-0.20-append a few versions back from the tip. If 205 comes out and hbase works on it, we should ship 0.92 with it (while also ensuring it work w/ 0.22 and 0.23 branches).",non_debt,-
hbase,4437,comment_0,"Would make sense to work on 205, can't be that far off from CDH3 anyways once it gets sync.",non_debt,-
hbase,4437,comment_1,"Agree with targeting 205 as well. Since it includes security, that would also mean security wouldn't need to override the Hadoop version for build. The Hadoop security code in 205 does have some changes vs. what's in the current CDH3, but that won't make a difference for current HBase.",non_debt,-
hbase,4437,comment_2,Trying with 205. Looks like we lose our blue border in UI in 0.90.x hbase. Lars Francke figured out why a while back. Need to revisit.,non_debt,-
hbase,4437,comment_3,Move to 0.20.205.0 hadoop. Also includes edits to our jsp and jamon templates commenting out DOCTYPE to get around bug where css is served as text/html. See tail of HBASE-2110 for discussion.,design_debt,non-optimal_design
hbase,4437,comment_7,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,4459,summary,"HbaseObjectWritable code is a byte, we will eventually run out of codes",non_debt,-
hbase,4459,description,"There are about 90 classes/codes in HbaseObjectWritable currently and Byte.MAX_VALUE is 127. In addition, anyone wanting to add custom classes but not break compatibility might want to leave a gap before using codes and that's difficult in such limited space. Eventually we should get rid of this pattern that makes compatibility difficult (better client/server protocol handshake) but we should probably at least bump this to a short for 0.94.",design_debt,non-optimal_design
hbase,4459,comment_0,+1 on making code of type short.,non_debt,-
hbase,4459,comment_1,"We probably have to worry that someone has stored somewhere persistent, in which case just making the type a short is not Perhaps we should make it a varint, which I believe will encode the same for any existing data, but would be multibyte-capable?",non_debt,-
hbase,4459,comment_2,"@Todd: which ""varint"" are you referring to here?",non_debt,-
hbase,4459,comment_3,The scheme in looks like it's for small positive bytes:,non_debt,-
hbase,4459,comment_4,So this means we can replace out.writeByte(code) in with Shall we pull this JIRA into 0.92 ?,non_debt,-
hbase,4459,comment_5,I'm fine with pulling into 0.92 since it doesn't break any compatibility.,non_debt,-
hbase,4459,comment_6,Pls review the patch.,non_debt,-
hbase,4459,comment_7,- Why is Queue added within the scope of this JIRA? Seems unrelated. - Can you remove the unnecessary import re-org at the top? - Can we have a unit test which shows the backwards compatibility of this? Thanks for working on this Ram.,code_debt,low_quality_code
hbase,4459,comment_8,What is this about? Is it necessary to this patch? And this: Looks good Ram. Any chance of a test to prove it works the way it used to?,test_debt,lack_of_tests
hbase,4459,comment_9,Latest patch with testcases. Text and byte[] objects is persisted using current HbaseObjectWritable and read back with enhanced patch),non_debt,-
hbase,4459,comment_10,1,non_debt,-
hbase,4459,comment_11,Commited to trunk and 0.92 branch. Thanks for the review Stack and Jon.,non_debt,-
hbase,4459,comment_14,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,4660,summary,Place to publish RegionServer information such as webuiport and coprocessors loaded,non_debt,-
hbase,4660,description,"HBASE-4070 added loaded CoProcessors to HServerLoad which seems like wrong place to carry this info. We need a locus for static info of this type such as loaded CoProcessors and webuiport as well as stuff like how many cpus, RAM, etc: e.g. in regionserver znode or available on invocation of an HRegionServer method (master can ask HRegionServer when it needs it).",non_debt,-
hbase,4660,comment_0,This issue didn't come up during code review. Agree that HSL is overloaded and at least should be renamed.,non_debt,-
hbase,4660,comment_1,This is what hbase-4070 does... its kinda wonky having list of regionserver coprocessors showing in the UI load.,non_debt,-
hbase,4660,comment_2,I agree; but I also think it would be nice to separate all the comma-delimited values into separate <td>s.,non_debt,-
hbase,4660,comment_3,Loaded CPs shouldn't be showing at all in load is my point.,non_debt,-
hbase,4660,comment_4,I agree; I went off on a tangent about the HTML presentation which is a separate display issue.,non_debt,-
hbase,4660,comment_5,"I think this is fixed, at least from a UI perspective. Closing unless someone says otherwise",non_debt,-
hbase,4726,summary,RS should close region if it fails to mark it as 'OPENED'.,non_debt,-
hbase,4726,description,Currently if a RS fails to mark a region as 'OPENED' it only logs an error. It will leave the region open - this has caused duplicate region assignments in one of our production clusters.,non_debt,-
hbase,4726,comment_0,This is the scenario we saw: 1. Region r is opened on RS1 and ZNode is updated with 2. But for some reason update to META takes a long time. (Not sure why yet) 3. hbck reports r is still unassigned - since update to META has not happened yet. 4. hbck -fix is run. 5. Now master tries to assign r to RS2. The following two happen simultaneously: 6a. Master now processes the old RS2ZK_REGION_OPENED update from RS1. This updates META and clears ZK. 6a. RS2 onlines the r - but when it tries to set state to it gets a ZK NoNodeException error.,non_debt,-
hbase,4726,comment_1,This may also be fixed by HBASE-4540.,non_debt,-
hbase,4726,comment_2,"Marking as duplicate and closing, unless there is an objection.",non_debt,-
hbase,4734,summary,[bulk load] Warn if bulk load directory contained no files,non_debt,-
hbase,4734,description,Bulk load exits if no files are found in the specified directory. This can happen if a directory has been bulk loaded already (bulk load renames/moves files). It would be good to provide some sort of warning when this happens.,non_debt,-
hbase,4734,comment_0,"Trivial usability improvement. Should apply on trunk, 0.92 as well as 0.90 after HBASE-4718.",non_debt,-
hbase,4734,comment_3,Trunk and branch. Thanks for the patch Jon.,non_debt,-
hbase,4734,comment_6,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,4740,summary,[bulk load] the HBASE-4552 API can't tell if errors on region server are recoverable,non_debt,-
hbase,4740,description,"Running more frequently seems to show that it has become flaky. It is difficult to tell if this is because of a unrecoverable failure or a recoverable failure. To make this visiable from test and for users, we need to make a change to bulkload call's interface on HRegionServer. The change should make successful rpcs return true, recoverable failures return false, and unrecoverable failure throw an IOException. This is an RPC change, so it would be really good to get this api right before the final 0.92 goes out.",test_debt,flaky_test
hbase,4740,comment_0,"While reworking the tests for recoverable and simulated unrecoverable failures with the updated api, I noticed that there are some problems in the test cases I previously wrote. There will be some significant changes with the tests in this patch as well. Oddly I have a case where splitting was not happening in a particular test case but is in another.",non_debt,-
hbase,4740,comment_1,"Raise to blocker, either fix or undo for release. +1 on fix.",non_debt,-
hbase,4740,comment_2,Ends up that I was splitting in the wrong place and splitting an empty region returns scary error messages when it should say return an innocuous one.,non_debt,-
hbase,4740,comment_3,Review here:,non_debt,-
hbase,4740,comment_5,"Is this right Jon, setting maxRetries to 0 by default? This is a little odd looking but thats it (could let auto-boxing take care of the boolean to Boolean): This code looks a lot like whats in the populateTable method: Code dup? Patch seems fine. Did you try it on a cluster Jon?",non_debt,-
hbase,4740,comment_6,"@Stack Yeah, 0 is actually the original behavior in the pre-HBASE-4552 version it I think would just eat exceptions and bail out without completing. It is more complicated because of bulk atomicity. Will update boolean if it works -- there is some template checking in another place so assumed it needed boxed type. The difference is that the version uses a different instance. I'll refactor to exclude that portion and require it in the test. I tried the previous version with a small data set on psuedo-dist cluster and live cluster. For this particular patch I tried this one by looping the relevant unit tests 100 times and seeing that they passed all the time. I haven't tested this exact version on real cluster.",code_debt,low_quality_code
hbase,4740,comment_7,@Jon Sounds good. Was wondering if you think this issue will be done soon (test and Ted's comments?) -- its only blocker on 0.92 currently (I'm sure that will change -- we'll soon have a million blockers -- but hopefully can get an RC up before the million blockers come in). Good on you Jon.,non_debt,-
hbase,4740,comment_8,Updated patch addressed comments in review and on jira.,non_debt,-
hbase,4740,comment_9,"@Stack I've updated the patch -- if this is insufficient, I'm probably going to be spotty for a week or so.",non_debt,-
hbase,4740,comment_10,Patch testing v2.,non_debt,-
hbase,4740,comment_11,These tests passed based on patch v2:,non_debt,-
hbase,4740,comment_12,+1 on patch v2.,non_debt,-
hbase,4740,comment_13,Patch v3 is same as v2.,non_debt,-
hbase,4740,comment_15,Some test failures were due to 'Too many open files' I couldn't reproduce failures:,non_debt,-
hbase,4740,comment_16,Integrated to 0.92 and TRUNK. Thanks for the patch Jonathan. Thanks for the review Stack.,non_debt,-
hbase,4740,comment_18,Can we close this now Ted?,non_debt,-
hbase,4740,comment_19,I think the test failure in HBase-0.92 build 119 shouldn't be caused by this JIRA. We can resolve this.,non_debt,-
hbase,4740,comment_21,Committed to trunk and 0.92 branch. Thanks for the patch Jon.,non_debt,-
hbase,4740,comment_22,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,4778,summary,Don't ignore corrupt StoreFiles when opening a region,non_debt,-
hbase,4778,description,"We used to ignore StoreFiles that failed to open, which led to a situation when only a subset of regions was opened, and HBase did not return results to clients for the affected set of keys. This change makes sure we propagate IOExceptions coming from an attempt to open a StoreFile all the way up to where it will lead to a failure to open the whole region. This way we can avoid returning corrupt data to the application.",non_debt,-
hbase,4778,comment_0,Part of 89-fb to trunk port. See r1181594,non_debt,-
hbase,4778,comment_2,What are users expected to do after IOException fails region open ? I assume such scenario happened at FB. It is nice to know the remedies.,non_debt,-
hbase,4778,comment_3,"@Ted: In this particular case, I think we ended up finding that HDFS wasn't reading truncated HFiles properly. Overall though, the idea is to prioritize data integrity & consistency over availability. We shouldn't be silently opening regions with missing data. We should instead understand why the data is missing. If someone wants to add a flag and allow this to happen, then that's fine.",code_debt,low_quality_code
hbase,4778,comment_4,I agree.,non_debt,-
hbase,4778,comment_5,+1 on patch.,non_debt,-
hbase,4778,comment_6,Committed to 0.92.,non_debt,-
hbase,4804,summary,Minor Dyslexia in CHANGES.txt,documentation_debt,low_quality_documentation
hbase,4804,description,I was going through the 0.92 CHANGES and found are a few entries in CHANGES.txt where jira numbers don't match up descriptions.,documentation_debt,low_quality_documentation
hbase,4804,comment_0,Do we all need to go see a doctor Jon?,non_debt,-
hbase,4804,comment_1,Committed branch and trunk. Thanks jon.,non_debt,-
hbase,4804,comment_2,Haha.. I have a spelling problem and a tendency to omit words which may be incurable. :),documentation_debt,low_quality_documentation
hbase,4804,comment_6,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,4859,summary,Correctly PreWarm HBCK ThreadPool,non_debt,-
hbase,4859,description,"See description at HBASE-3553. We had a patch ready for this in HBASE-3620 but never applied it publicly. Testing showed massive speedup in HBCK, especially when RegionServers were down or had long response times.",code_debt,slow_algorithm
hbase,4859,comment_0,patch applies on 0.92 & 0.94,non_debt,-
hbase,4859,comment_1,+1 on patch.,non_debt,-
hbase,4859,comment_2,Looks like HBASE-3767 solves this in a more elegant and scalable fashion. Will incorporate their changes instead of the original patch.,non_debt,-
hbase,4859,comment_3,"version 2 of the patch, modeled after HBASE-4859 behavior",non_debt,-
hbase,4859,comment_5,Committed trunk and branch. Thank N.,non_debt,-
hbase,4859,comment_10,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,4864,summary,occasionally fails,non_debt,-
hbase,4864,description,"looks this logs: It seems that we should wait region is added to online region set. I made a patch, Please review.",non_debt,-
hbase,4864,comment_0,+1 on patch.,non_debt,-
hbase,4864,comment_2,Integrated to 0.92 and TRUNK. Thanks for the patch Jinchao.,non_debt,-
hbase,4909,summary,Detailed Block Cache Metrics,non_debt,-
hbase,4909,description,Moving issue w/ no recent movement out of 0.95,non_debt,-
hbase,4909,comment_0,Need to port from 89-fb. See SVN #1181972,non_debt,-
hbase,4909,comment_1,"If no one is working on this issue, i can help porting from 89-fb. Thanks.",non_debt,-
hbase,4909,comment_2,"Yes please, +1 fo mo metrix!",non_debt,-
hbase,4909,comment_3,Any plans for working on this soon? Otherwise please move to 0.96.,non_debt,-
hbase,4909,comment_4,"Moving out of 0.94, pull back if you disagree.",non_debt,-
hbase,4909,comment_5,"Here is the commit Nicolas is referring to: r1181972 | nspiegelberg | 2011-10-11 10:45:00 -0700 (Tue, 11 Oct 2011) | 25 lines Refactored and more detailed block read/cache and bloom metrics Summary: As we keep adding more granular block read and block cache usage statistics, there is a combinatorial explosion of the number of cases we have to monitor, especially when we want both per-column family / block type statistics and aggregate statistics on one or both of these dimensions. I am trying to unclutter HFile readers, LruBlockCache, StoreFile, etc. by creating a centralized class that knows how to update all kinds of per-column family/block type statistics. Test Plan: Run all unit tests. New unit test. Deploy to one region server in dark launch and compare the new output of hbaseStats.py to the old one (take a diff of the set of keys). Reviewers: pritam, liyintang, jgray, kannan Reviewed By: kannan CC: , hbase@lists, dist-storage@lists, kannan Differential Revision: 321147 Looking at svn diff the commit is all We have this. It is differently named, it is CacheStats. So, we have this detail. It came in with HBASE-4027, the slab cache issue. We need more but we have this much now so resolving as implemented.",non_debt,-
hbase,4909,comment_6,Subsumed by HBASE-4027,non_debt,-
hbase,4911,summary,Clean shutdown,code_debt,low_quality_code
hbase,4911,description,None,non_debt,-
hbase,4911,comment_0,Talked with Mikhail more about this issue. It is fixed in trunk currently,non_debt,-
hbase,5039,summary,"[book] book.xml - more cleanup of Arch chapter on regions, adding a FAQ entry",code_debt,low_quality_code
hbase,5039,description,book.xml * Arch chapter/Regions. clearing up a little more in region assignment * FAQ. Adding an architecture section. * MapReduce chapter. Fixed nit that Ian Varley brought to my attention on RDBMS summary.,code_debt,low_quality_code
hbase,5052,summary,"The path where a dynamically loaded coprocessor jar is copied on the local file system depends on the region name (and implicitly, the start key)",non_debt,-
hbase,5052,description,"When loading a coprocessor from hdfs, the jar file gets copied to a path on the local filesystem, which depends on the region name, and the region start key. The name is ""cleaned"", but not enough, so when you have filesystem unfriendly characters (/?:, etc), the coprocessor is not loaded, and an error is thrown",non_debt,-
hbase,5052,comment_0,"We could work on better cleaning the region name, but I think it is far better to depend on something like the hashed name",design_debt,non-optimal_design
hbase,5052,comment_2,@Andrei: Have you considered possible collision introduced by the proposed change ?,non_debt,-
hbase,5052,comment_4,+1 on patch. itself isn't stable.,non_debt,-
hbase,5052,comment_5,Committed branch and trunk. Thanks for the patch Andrei.,non_debt,-
hbase,5052,comment_10,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,5101,summary,Add a max number of regions per regionserver limit,non_debt,-
hbase,5101,description,"In a testing environment, a cluster got to a state with more than 1500 regions per region server, and essentially became stuck and unavailable. We could add a limit to the number of regions that a region server can serve to prevent this from happening. This looks like it could be implemented in the core or as a coprocessor.",non_debt,-
hbase,5101,comment_0,This would disable splitting on the RS at the limit?,non_debt,-
hbase,5101,comment_1,@Jon: Can you describe how the cluster got into this state ? I guess a lot of region servers crashed ? Otherwise there should have been better planning for the region size.,non_debt,-
hbase,5101,comment_2,"@Andrew I think that would be the idea. The hope is to avoid getting region servers into trouble and to give an admin some warning when they are approaching trouble (maybe reached some percentage of region limit). @Ted I've been purposely testing using a stress configuration with heavy write load that purposely requires flushes (4 MB), splits (64MB) and compactions all the time. Along the way region servers crash (which is fine -- fault injection is part of this workload). I've encountered some situations where folks don't know the distribution of their row keys (or don't have uniform row key distributions). This could be a useful go-between in situations where region pre-splitting with dynamic splitting off may not be effective.",non_debt,-
hbase,5101,comment_3,How do we calculate region limit ?,non_debt,-
hbase,5101,comment_4,HBASE-4365 is related to this JIRA.,non_debt,-
hbase,5101,comment_5,@Ted First cut -- simply some configured number of max regions. Maybe it would stop splitting at a point where it can handle if some number of rs's going down and its regions are reassigned. There could possibly be a limit on tables as well -- and we could put cap on the # of region servers/table and # of regions/region server.,non_debt,-
hbase,5101,comment_6,"@Jon: I think HBASE-4365 tried to tackle the same problem in a different way. Meaning, a new RegionSplitPolicy implementation should be created. Shall we continue along that route ?",non_debt,-
hbase,5101,comment_7,Isnt this (HBASE-2844)?,non_debt,-
hbase,5101,comment_8,"This is a dupe of HBASE-2844, and later discussion lead to something similar to HBASE-2956.",non_debt,-
hbase,5101,comment_9,"@Todd Looks like it, closing as a dupe.",non_debt,-
hbase,5110,summary,code enhancement - remove unnecessary if-checks in every loop in HLog class,code_debt,complex_code
hbase,5110,description,"The HLog class (method has unnecessary if check in a loop. static byte [][] long oldestWALseqid, final Map<byte [], Long // This method is static so it can be unit tested the easier. List<byte [] for (Map.Entry<byte [], Long if <= oldestWALseqid) { if (regions == null) regions = new ArrayList<byte [] } } return regions == null? null: regions.toArray(new byte [][] } The following change is suggested static byte [][] long oldestWALseqid, final Map<byte [], Long // This method is static so it can be unit tested the easier. List<byte [] for (Map.Entry<byte [], Long if <= oldestWALseqid) { } } return regions.size() == 0? null: regions.toArray(new byte [][] }",code_debt,low_quality_code
hbase,5110,comment_0,why? This isn't a hot code path...,non_debt,-
hbase,5110,comment_1,"Hi Todd, There was a long discussion on this on the mailing list, it was decided to open a JIRA. Mikael.S",non_debt,-
hbase,5110,comment_2,"Ah, I missed that thread... I just wanted to clarify if this is for readability or performance... do you see this function getting called a lot in a write workload? Your comments on the mailing list thread indicate that it's performance sensitive, but I don't see how that would be the case.",code_debt,slow_algorithm
hbase,5110,comment_3,"I see it a lot in the heavy write scenario when major compaction occurs in the background, but to be realistic even when i see this method called 2000 times during a test of 5 hours, across a cluster of 10 RS (each RS log contains +/-200 calls of this method), i don't think this method present a performance problem. So for my point of view this is more readability issue before it becomes a performance problem. It is strange to me to see code asking each time in an iteration for object existence especially if creating the object is not heavy task.",code_debt,low_quality_code
hbase,5210,summary,HFiles are missing from an incremental load,non_debt,-
hbase,5210,description,"We run an overnight map/reduce job that loads data from an external source and adds that data to an existing HBase table. The input files have been loaded into hdfs. The map/reduce job uses the HFileOutputFormat (and the to create HFiles which are subsequently added to the HBase table. On at least two separate occasions (that we know of), a range of output would be missing for a given day. The range of keys for the missing values corresponded to those of a particular region. This implied that a complete HFile somehow went missing from the job. Further investigation revealed the following: * Two different reducers (running in separate JVMs and thus separate class loaders) * in the same server can end up using the same file names for their * HFiles. The scenario is as follows: * 1. Both reducers start near the same time. * 2. The first reducer reaches the point where it wants to write its first file. * 3. It uses the StoreFile class which contains a static Random object * which is initialized by default using a timestamp. * 4. The file name is generated using the random number generator. * 5. The file name is checked against other existing files. * 6. The file is written into temporary files in a directory named * after the reducer attempt. * 7. The second reduce task reaches the same point, but its StoreClass * (which is now in the file system's cache) gets loaded within the * time resolution of the OS and thus initializes its Random() * object with the same seed as the first task. * 8. The second task also checks for an existing file with the name * generated by the random number generator and finds no conflict * because each task is writing files in its own temporary folder. * 9. The first task finishes and gets its temporary files committed * to the ""real"" folder specified for output of the HFiles. * 10. The second task then reaches its own conclusion and commits its * files (moveTaskOutputs). The released Hadoop code just overwrites * any files with the same name. No warning messages or anything. * The first task's HFiles just go missing. * * Note: The reducers here are NOT different attempts at the same * reduce task. They are different reduce tasks so data is * really lost. I am currently testing a fix in which I have added code to the Hadoop method to check for a conflict with an existing file in the final output folder and to rename the HFile if needed. This may not be appropriate for all uses of FileOutputFormat. So I have put this into a new class which is then used by a subclass of HFileOutputFormat. Subclassing of FileOutputCommitter itself was a bit more of a problem due to private declarations. I don't know if my approach is the best fix for the problem. If someone more knowledgeable than myself deems that it is, I will be happy to share what I have done and by that time I may have some information on the results.",non_debt,-
hbase,5210,comment_0,Patch for discussion.,non_debt,-
hbase,5210,comment_1,@Larry: Have you filed a MAPREDUCE- JIRA ? might be the right place for the change.,non_debt,-
hbase,5210,comment_2,"@Andrew: I looked at your suggested patch. It may not work as well as you hope since it depends on System.nanoTime() changing rapidly which it may not do on all systems. There are discussions in other blogs about this. I believe that java.util.Random uses System.nanoTime() to do default seeding, and it has not been terribly successful in my case. @Zhiyong: I believe that you are correct in that this is the appropriate place to make a change. However, I did not suggest this as a MAPREDUCE change because the existing behavior may be correct for applications other than generating HFiles. I can imagine situations in which one would want the latest version of a file produced by several reducers to be the only one left at the end of a map/reduce job. However, it's definitely not appropriate when producing HFiles for an incremental load. My own solution which I am testing now is to clone FileOutputCommitter and add logic to the moveTaskOutputs() method that creates a new name for any conflicting files. FileOutputCommitter had too many components that were private to make it easy to subclass. I subclassed FileOutputFormat to use the new output committer class and then I used that subclass in my map/reduce job. I included a logging statement in the new moveTaskOutputs() method so that I can tell when the rename logic is triggered. It may take awhile to see if the logic is successful since the two occurrences that I was able to track down occurred two months apart in a job that runs nightly.",non_debt,-
hbase,5210,comment_3,"Any fix in getRandomFilename will just reduce the chance of file name collision. Since this a rare case, I think it may be better to just fail the task if failed to commit the files in the moveTaskOutputs(), without overwriting the existing files. In HDFS 0.23, rename() takes an option not to overwrite. With HADOOP 0.20, we can just do our best to check any conflicts before committing the files.",code_debt,low_quality_code
hbase,5210,comment_4,"I prefer Lawrence's approach. The only consideration is that it takes relatively long for the proposed change in to be published, reviewed and pushed upstream.",non_debt,-
hbase,5210,comment_5,Why not change the output file name to be based on the task attempt ID? There is already a unique id for each task available...,non_debt,-
hbase,5210,comment_6,I like this one. It's really simple and clean.,non_debt,-
hbase,5210,comment_7,"@Todd: Two questions about your solution: 1. If we were to form a file name from just the numeric digits of the task attempt ID, that would be 23 digits. As I look at the file names for HBase tables, they seem to be 18-19 digits long. Do you know if there are any assumptions made in other HBase code about the length of file names for store files? 2. In the unlikely event that there was a name conflict with an HFile created by a reducer, what should happen then? (The job number looks like it might roll at 10000 jobs - I don't know if anyone has gotten that far without restarting Map/Reduce.) It still seems to me that the safest solution is a change to HFileOutputFormat to use a new output committer class that adds rename logic to moveTaskOutputs(). These changes could be implemented strictly in the HBase code tree without having to involve the underlying Hadoop implementation.",non_debt,-
hbase,5210,comment_8,"In Store.java, when we bulk-load the MR output, we rename to a randomly generated filename in the region directory, using a UUID, it looks like. So the names of the MR output files should be inconsequential.",non_debt,-
hbase,5210,comment_9,Can this issue be reproduced in a more modern HBase? Can we close this as WON'T FIX as we sunset the 0.90 line?,non_debt,-
hbase,5210,comment_10,Closing for now,non_debt,-
hbase,5217,summary,"Reenable the thrift tests, and add a new one for getRegionInfo",non_debt,-
hbase,5217,description,"At some point we disabled tests for the thrift server. In addition, it looks like the getRegionInfo no longer functions. I'd like to reenable the tests and add one for getRegionInfo. I had to write this to test my changes in HBASE-2600 anyway. I figured I would break it out. We shouldn't commit it until we have fixed getting the regioninfo from the thriftserver.",test_debt,low_coverage
hbase,5217,comment_0,We depend on a functioning getRegionInfo in thrift.,non_debt,-
hbase,5217,comment_4,Expect this build to fail until the dependent jira is done.,non_debt,-
hbase,5217,comment_5,I attached my own jenkins run. The one failing test is due to 2600 not handling migrations yet.,non_debt,-
hbase,5217,comment_7,remerged,non_debt,-
hbase,5217,comment_10,v2,non_debt,-
hbase,5217,comment_12,@Alex Is this dependent on another issue being committed first? In doTestGetRegionInfo the tabs are wrong. Should be two spaces like the rest of the file. With your changes are we going to start a cluster each time (Was the testAll method trying to avoid our making a cluster each time)?,code_debt,low_quality_code
hbase,5217,comment_13,This <bold * Runs all of the tests under a single JUnit test method. We - * consolidate all testing to one method because - * is prone to when there are three or more - * JUnit test methods. - * - * @throws Exception I think this is mostly fixed now though.,non_debt,-
hbase,5217,comment_14,Fixed spacing,documentation_debt,low_quality_documentation
hbase,5217,comment_16,Attach a patch w/ --no-prefix Alex... jenkins don't know how to apply a/src... paths.,non_debt,-
hbase,5217,comment_17,done,non_debt,-
hbase,5217,comment_19,Your patch still doesn't apply Boss.,non_debt,-
hbase,5217,comment_20,"Indeed, it is tied to HBASE-2600.",non_debt,-
hbase,5217,comment_21,I rebased today,non_debt,-
hbase,5226,summary,[book] troubleshooting.xml - add client slowdown when calling admin APIs issue to Client section,non_debt,-
hbase,5226,description,troubleshooting.xml - adding client slowdown when calling admin APIs issue (HBASE-5073) to the Client section.,non_debt,-
hbase,5282,summary,Possible file handle leak with truncated HLog file.,code_debt,low_quality_code
hbase,5282,description,"When debugging hbck, found that the code responsible for this exception can leak open file handles.",code_debt,low_quality_code
hbase,5282,comment_0,"When debugging, open region file was attempting to open either a truncated or 0 size hlogfile (which is throws IOException at out from getReader), and leaking a handle on every open attempt. Patch applies on 0.92 and trunk.",code_debt,low_quality_code
hbase,5282,comment_1,reader.close() may throw IOE. I think we should protect the execution of status.cleanup().,non_debt,-
hbase,5282,comment_2,"True, but is only used in one place, wrapped with {{try catch}} that checks for IOE and seems like reasonable behavior: What do you mean by protect status.cleanup()? Check for {{status == null}}? (it cannot be).",non_debt,-
hbase,5282,comment_3,"What I meant is that if close() throws IOE, status.cleanup() would be skipped. status.cleanup() can be placed before the call to close().",non_debt,-
hbase,5282,comment_4,"Ah, got it. Good catch.",non_debt,-
hbase,5282,comment_5,Updated to call status.cleanup() before close.,non_debt,-
hbase,5282,comment_8,+1 on patch v2.,non_debt,-
hbase,5282,comment_9,I'll commit later today.,non_debt,-
hbase,5282,comment_10,+1 on v2,non_debt,-
hbase,5282,comment_11,"First code commit! Thanks for the review Ted, Lars!",non_debt,-
hbase,5294,summary,Make sure javadoc is included in tarball bundle when we release,non_debt,-
hbase,5294,description,0.92.0 doesn't have javadoc in the tarball. Fix.,non_debt,-
hbase,5294,comment_0,This patch only generates javadocs for the artifact when the release profile (-Prelease) is used. I figured the build is slow enough already - no need to slow down the common case. Tested with: 'mvn clean site install assembly:single -Dmaven.test.skip -Prelease' and manually verified the resulting tarball included javadocs.,non_debt,-
hbase,5294,comment_1,Committed to branch and trunk. Thanks for the patch Shaneal (though now need to fix licenses -- smile).,non_debt,-
hbase,5329,summary,"addRowLock() may allocate duplicate lock id, causing the client to be blocked",non_debt,-
hbase,5329,description,"In addRowLock(),rand may generate duplicated lock id, it may cause regionserver throw client will be blocked until old rowlock is released.",non_debt,-
hbase,5329,comment_0,"I think we should call putIfAbsent(). If the return value from putIfAbsent() is not null, we should try to regenerate a new lockId. Do you want to upload a patch ?",non_debt,-
hbase,5329,comment_1,It seems addScanner() might have similar issue.,non_debt,-
hbase,5329,comment_2,"I had raised that fact about getScanner() a while ago... Isn't that *incredibly* unlikely, though. nextLong generates all 2^64 possible longs with roughly equal probability. The chances of the same long being generated are astronomically small. From the log attached, you actually saw this happening? Are you sure this due to a duplicate random number?",non_debt,-
hbase,5329,comment_3,"Although rare, this is possible - considering duplicate scanner Id with lock Id, etc. Here is related code:",non_debt,-
hbase,5329,comment_4,"Even if you get 1000 scanners per second you'd have to do that for 1 trillion (1.000.000.000) years to come into the ballpark of 2^64. The laws of big numbers are not intuitive. It is billions times more likely that your power fails, there're random bit errors, your harddrive dies, or datacenter is hit my a meteorite :). From that viewpoint this is in fact not possible, because your hardware is not going to live long enough to experience this. I really think we do not have to change this.",non_debt,-
hbase,5329,comment_5,"sorry, i didn`t describle it clearly. I thinks there is possibility that Random.nextLong() generate same number. To get consequences what the same number lead to I changed code follow this, and got the",non_debt,-
hbase,5329,comment_6,"Agree with Lars on the extreme unlikeliness of this. But also agree with Ted; the putIfAbsent pattern should be an easy default for all things like this, rather than even discussing it. (Especially since we've already got a ConcurrentMap here). Attaching a casual patch that does that. (You may not like the do loop style, feel free to replace with something you find more readable.)",non_debt,-
hbase,5329,comment_7,Simple patch to use putIfAbsent,non_debt,-
hbase,5329,comment_8,Thanks for activating this JIRA. Indentation is off for the above block. I prefer the old way of keeping long lockId so that the above parsing can be omitted.,code_debt,low_quality_code
hbase,5329,comment_9,Patch v2 does some cosmetics changes based on Ian's patch.,non_debt,-
hbase,5329,comment_10,"Cool, thanks for the updated patch Ted. Agreed about the lockId change, that's clearer. Thanks!",non_debt,-
hbase,5329,comment_12,"I ran manually and it passed. Integrated to trunk. Thanks for the patch, Ian.",non_debt,-
hbase,5329,comment_15,Patch was applied a while back.,non_debt,-
hbase,5329,comment_16,Marking closed.,non_debt,-
hbase,5333,summary,"Introduce Memstore ""backpressure"" for writes",non_debt,-
hbase,5333,description,"Currently if the cannot keep up with the writeload, we block writers up to milliseconds (default is 90000). Would be nice if there was a concept of a soft ""backpressure"" that slows writing clients gracefully *before* we reach this condition. From the log: ""2012-02-04 00:00:06,963 WARN Region <table",design_debt,non-optimal_design
hbase,5333,comment_0,I've done some brainstorming with Stack and the result was HBASE-5162.,non_debt,-
hbase,5333,comment_1,"Checked w/ Lars & Jesse, agreed that this is just a dup of 5162.",non_debt,-
hbase,5391,summary,index.html - adding years to news section,non_debt,-
hbase,5391,description,"There are multiple years listed in the news section, but we only had Month and day. Adding years.",non_debt,-
hbase,5420,summary,TestImportTsv does not shut down MR Cluster correctly (fails against 0.23 hadoop),non_debt,-
hbase,5420,description,"Test calls but never calls This causes failures with -Dhadoop.profile=23 when both testMROnTable and are run, because the cluster cannot start up properly for the second test.",non_debt,-
hbase,5420,comment_0,+1 on patch. Mind resubmitting w/ --no-prefix please Gregory and then doing 'submit patch' to try it against hadoopqa?,non_debt,-
hbase,5420,comment_2,Committed branch and trunk. Thanks for the patch Gregory.,non_debt,-
hbase,5466,summary,Opening a table also opens the metatable and never closes it.,non_debt,-
hbase,5466,description,"Having upgraded to CDH3U3 version of hbase we found we had a zookeeper connection leak, tracking it down we found that closing the connection will only close the zookeeper connection if all calls to get the connection have been closed, there is incCount and decCount in the HConnection class, When a table is opened it makes a call to the metascanner class which opens a connection to the meta table, this table never gets closed. This caused the count in the HConnection class to never return to zero meaning that the zookeeper connection will not close when we close all the tables or calling true);",code_debt,low_quality_code
hbase,5466,comment_0,patch to make sure the metatable gets closed when the table is opened before it falls out of scope,non_debt,-
hbase,5466,comment_1,Thanks for the finding. We use two spaces for indentation. Can you regenerate patch ? Refer to HBASE-3678.,code_debt,low_quality_code
hbase,5466,comment_2,patch regenerated thanks for the link to that jira task.,non_debt,-
hbase,5466,comment_5,+1 on patch (except for the spacing that is not like the rest of the file),code_debt,low_quality_code
hbase,5466,comment_6,"TestZooKeeper passed locally with patch v2. There should be a space between } and finally, finally and {, if and (, ) and { Overall, +1 on patch v2. Please fix formatting in v3.",code_debt,low_quality_code
hbase,5466,comment_7,Thanks for the feedback this is my first patch to hbase.,non_debt,-
hbase,5466,comment_8,Patch v3 looks good to me. @Stack: Should this go to 0.92.1 ?,non_debt,-
hbase,5466,comment_10,@Ted Yes please.,non_debt,-
hbase,5466,comment_11,"Integrated to 0.92 and TRUNK. Thanks for the patch Ashley. Thanks for the review, Stack.",non_debt,-
hbase,5466,comment_16,"Hi, I'm running into this problem on a 0.90 cluster. The patch seems to apply cleanly to 0.90 branch - could a committer add it there too?",non_debt,-
hbase,5466,comment_17,Integrated to 0.90.7 as well.,non_debt,-
hbase,5466,comment_18,Committed to 0.90 branch at Shaneal's request.,non_debt,-
hbase,5466,comment_19,"This happend pre-0.94 branch, added fixver that this is in 0.94.",non_debt,-
hbase,5523,summary,Fix Delete Timerange logic for KEEP_DELETED_CELLS,non_debt,-
hbase,5523,description,"A Delete at time T marks a Put at time T as deleted. In parent I invented special logic that insert a virtual millisecond into the tr if the encountered KV is a delete marker. This was so that there is a way to specify a timerange that would allow to see the put but not the delete: Discussed this today with a coworker and he convinced me that this is very confusing and also not needed. When we have a Delete and Put at the same time T, there *is* not timerange that can include the Put but not the Delete. So I will change the code to this (and fix the tests): It's easier to understand, and does not lead to strange scenarios when the TS is used as a controlled counter. Needs to be done before 0.94 goes out.",design_debt,non-optimal_design
hbase,5523,comment_0,Here's a patch.,non_debt,-
hbase,5523,comment_2,+1 on patch.,non_debt,-
hbase,5523,comment_3,"I remember the initial motivation for the +1 shift now. If somebody accidentally places a Delete at T it would not be possible to get at any Puts of T with normal Scan API. The +1 allow setting an interval that includes the Puts but not the Delete. I.e. setting the range to [0,T+1) would include the Puts and Deletes. (note that the lower bound inclusive and the upper bound is exclusive hence the [x,y) notation). With the +1 shift [0,T+1) would not contain the Delete, but [0,T+2) would. This is very confusing, since [0,T+1) doesn't actually mean that when it comes to deletes. To recover the above mentioned Puts one could use a raw scan, instead. I'm going to commit the attached patch; it makes these scenarios much clearer.",design_debt,non-optimal_design
hbase,5523,comment_4,Committed to 0.94 and trunk. Thanks for the review Ted. Thanks for alerting me to the weirdness James (Taylor),non_debt,-
hbase,5554,summary,"""hadoop.native.lib"" config is deprecated",non_debt,-
hbase,5554,description,"When using HBase shell, we see: should be used.",non_debt,-
hbase,5554,comment_0,Since has been deprecated for both hadoop1 and hadoop2 the fix should be fine.,non_debt,-
hbase,5554,comment_1,Dup of HBASE-5697,non_debt,-
hbase,5591,summary,is identical to Bytes.getBytes(),non_debt,-
hbase,5591,description,None,non_debt,-
hbase,5591,comment_0,"sc requested code review of ""HBASE-5591 [jira] is identical to Bytes.getBytes()"". Reviewers: tedyu, dhruba, JIRA is identical to Bytes.getBytes() Remove the redundant method. Task ID: # Blame Rev: TEST PLAN Revert Plan: Tags: REVISION DETAIL AFFECTED FILES MANAGE HERALD DIFFERENTIAL RULES WHY DID I GET THIS EMAIL? Tip: use the X-Herald-Rules header to filter Herald messages in your client.",code_debt,duplicated_code
hbase,5591,comment_1,"tedyu has accepted the revision ""HBASE-5591 [jira] is identical to Bytes.getBytes()"". If tests pass. REVISION DETAIL BRANCH getbytes",non_debt,-
hbase,5591,comment_3,"stack has commented on the revision ""HBASE-5591 [jira] is identical to Bytes.getBytes()"". Otherwise patch looks good. INLINE COMMENTS Why do you think this comment is here Scott? Its ominous lookinh. REVISION DETAIL BRANCH getbytes",non_debt,-
hbase,5591,comment_4,Integrated to trunk.,non_debt,-
hbase,5591,comment_5,"sc has commented on the revision ""HBASE-5591 [jira] is identical to Bytes.getBytes()"". INLINE COMMENTS It was added by me actually. Because I checked bb) and found that it's different from this one. But this one is the same as bb). These names are really confusing. REVISION DETAIL BRANCH getbytes",code_debt,low_quality_code
hbase,5591,comment_7,"stack has commented on the revision ""HBASE-5591 [jira] is identical to Bytes.getBytes()"". INLINE COMMENTS Funny. If it was added by you, then I'd say its your prerogative to remove it! Good stuff. REVISION DETAIL BRANCH getbytes",non_debt,-
hbase,5591,comment_8,"sc has commented on the revision ""HBASE-5591 [jira] is identical to Bytes.getBytes()"". INLINE COMMENTS Sorry for the trouble. Thanks for bearing with me :) REVISION DETAIL BRANCH getbytes",non_debt,-
hbase,5591,comment_10,"sc has closed the revision ""HBASE-5591 [jira] is identical to Bytes.getBytes()"". REVISION DETAIL To: tedyu, dhruba, JIRA, sc Cc: stack",non_debt,-
hbase,5591,comment_11,This was committed to trunk a while back but will leave it open until Scott grants Apache permission.  Would you mind granting permission for this already committed patch else we'll have to remove it from trunk. Thanks boss.,non_debt,-
hbase,5591,comment_13,"Yes, permission granted. I couldn't find the UI for granting the permission. Does this statement in the comment count?",non_debt,-
hbase,5591,comment_14,"Thank you Scott. No check box any more. If you don't want to include it, you say so in a comment (Might be easier on you fellas with your auto-posting phabricator). Thanks for perm.",non_debt,-
hbase,5595,summary,Fix in 0.92 when running on local filesystem,non_debt,-
hbase,5595,description,Fix this ugly exception that shows when running 0.92.1 when on local filesystem:,code_debt,low_quality_code
hbase,5595,comment_0,Ted remarks on his seeing the above over in,non_debt,-
hbase,5595,comment_1,Attached the patch for trunk. I feel we can just skip the exception if as Zhihong Yu mentioned in other JIRA HBASE-5568 and also can provide the log info clearly. Patch contains the same.,non_debt,-
hbase,5595,comment_3,"Thanks for the patch Uma. Just what the doctor ordered. Applied to 0.92, 0.94 and trunk",non_debt,-
hbase,5595,comment_4,Thanks for the patch Uma. Added you to the contributor list.,non_debt,-
hbase,5595,comment_5,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,5635,summary,"If getTaskList() returns null, splitlogWorker would go down and it won't serve any requests",non_debt,-
hbase,5635,description,"During the hlog split operation if all the zookeepers are down ,then the paths will be returned as null and the splitworker thread wil be exited Now this regionserver wil not be able to acquire any other tasks since the splitworker thread is exited Please find the attached code for more details in the",non_debt,-
hbase,5635,comment_0,"Yes, I think, continuing without SplitLogWroker may not be a good behaviour. Because that particular regionServer may have more capacity to take up the new regions. With the current behaviour it may not compete for taking any new splilog work. I feel we can retry for some times and then we can shutdown regionServer? or other option is to retry forever on any ZK exception. And can exit only on interrupted exception. Also i am seeing this issue may be bit dangerous bacause, if ZK is not available for some time, all RegionServer may face this problem and no one will take up the splitlog work. will return null only if node does not exist. If it is not able to find any children then it will return empty list. So, will be always set. On Other keeperExceptions like ZK unavalability and all, we have to handle.",design_debt,non-optimal_design
hbase,5635,comment_1,In if getTaskList() returns null it is comming out form taskLoop(). Here when ever getTaskList() return null this thread should wait and when ever it will be notified it will continue the work. So splitLogWorker thread wont be exited Attached the patch with this change.,non_debt,-
hbase,5635,comment_2,Please change the wording in the following log: In this case the worker thread is not exiting.,code_debt,low_quality_code
hbase,5635,comment_3,@Chinna Currently there is no retry mechanism working in getTaskList. Though there is a loop there is no retry. One more thing is we can differentiate if really there was a problem in setting a watch by returning null and if the watch was success but there is no child for the split node. In that case i think we can return an empty list. Also as the thread is just going for a wait state without watch getting set successfully the thread will keep waiting.,non_debt,-
hbase,5635,comment_4,Updated the patch with retry logic in getTaskList(). So if getTaskList() after retry if it returns null the splitLogWorker will be exited.,non_debt,-
hbase,5635,comment_5,Typo. First line should read 'it gets the' Second line should read 'worker thread exited'. Please reduce the above interval. Do we need a timeout for the newly added loop ?,documentation_debt,low_quality_documentation
hbase,5635,comment_6,"If we have timeout, then if the zk connection takes a little longer than the timeout how to make the worker thread to be alive? That is the reason an infinite loop was tried.",non_debt,-
hbase,5635,comment_7,Then we should log a message saying what we wait for every X minutes so that user doesn't have to use jstack.,design_debt,non-optimal_design
hbase,5635,comment_8,Updated the patch with log message for retry and corrected the typo.,documentation_debt,low_quality_documentation
hbase,5635,comment_10,@Chinna I think logging after the sleep makes sense and also addresses Ted's concern.,non_debt,-
hbase,5635,comment_11,As per the patch the below variable is of no use now,code_debt,dead_code
hbase,5635,comment_12,@Anoop: is used by SplitLogManager Does anyone have comment about latest patch ?,non_debt,-
hbase,5635,comment_13,@devs I plan to commit this to trunk tomorrow. Pls provide your comments.,non_debt,-
hbase,5635,comment_14,+1 on patch. I wonder how many more problems like this we have lurking in the worker threads. Can you do a 0.94.1 patch as well?,non_debt,-
hbase,5635,comment_15,Thanks Lars. Sure i will make a patch for 0.94 as well and commit both tomorrow.,non_debt,-
hbase,5635,comment_16,Reattached the patch and prepared one for 0.94 also.,non_debt,-
hbase,5635,comment_18,The above failure in TestSplitLogManager QA does not repeat. Prev run on QA has passed. Will look into it more before committing.,non_debt,-
hbase,5635,comment_19,"Committed to trunk and 0.94. The testcase failure is not due to this patch. Ran TestSplitLogManager multiple times and did not fail. Thanks for the patch Chinna. Thanks for the review Ted, Lars and Anoop.",non_debt,-
hbase,5635,comment_22,@Ted I mean the variable zkretries not the config parameter. Yes this parameter is used in SplitLogManager. :),non_debt,-
hbase,5636,summary,TestTableMapReduce doesn't work properly.,non_debt,-
hbase,5636,description,No map function is called because there are no test data put before test starts. The following three tests are in the same situation: - - -,non_debt,-
hbase,5636,comment_0,Very good find. It seems that HBASE-4503 might have caused it. Do you plan to work on this?,non_debt,-
hbase,5636,comment_1,"Yes, I'm going to work this week or weekend.",non_debt,-
hbase,5636,comment_2,I found another bug when I was working on HBASE-5636.,non_debt,-
hbase,5636,comment_3,"I filed the new issue of the bug. By the way, the TestCase name is typo, isn't it? I will rename it to",documentation_debt,low_quality_documentation
hbase,5636,comment_4,"I'm sorry, was not included in 0.92.x.",non_debt,-
hbase,5636,comment_5,I attached a patch file for trunk. This includes patches for,non_debt,-
hbase,5636,comment_6,"Since is a new file, can you outline the changes you made on top of ? I noticed this:",non_debt,-
hbase,5636,comment_8,"Oh, I'm very sorry. Here is the diff of",non_debt,-
hbase,5636,comment_9,Changes for and couldn't apply cleanly. Can you upload a new patch for trunk ? Thanks,non_debt,-
hbase,5636,comment_10,I attached a patch v2. Maybe I forgot --no-prefix option of git-diff command.,non_debt,-
hbase,5636,comment_12,"Without patch from HBASE-5663, I saw the following in test output:",non_debt,-
hbase,5636,comment_13,"Integrated to trunk and 0.94. Thanks for the patch, Takuya.",non_debt,-
hbase,5636,comment_17,@Ted: It looks like this patch is introducing intermittent unit test failures in the trunk. Can we revert or fix?,non_debt,-
hbase,5636,comment_18,"A careful check of the failed test: would reveal the following: This should have been fixed by MAPREDUCE-3583 Once hadoop 1.0.2 comes out, we won't see the above exceptions.",non_debt,-
hbase,5636,comment_19,@Ted: we should list the tests as expected failures without Hadoop 1.0.2 then. Hadoop QA should be able to point out when a new patch introduces test failures & avoid forcing the user to understand the context of features that other people are working on.,non_debt,-
hbase,5636,comment_20,You're suggesting we refine the logic of finding failed tests in ? That sounds good.,non_debt,-
hbase,5908,summary,should not use append to corrupt the HLog,non_debt,-
hbase,5908,description,"fails against a version of hadoop with The failure: Append is not supported. Please see the dfs.support.append configuration parameter."" Instead of using append, we can probably just: - copy over the contents to a new file - append the garbage to the new file - copy back to the old file",non_debt,-
hbase,5908,comment_0,+1 pending Jenkins,non_debt,-
hbase,5908,comment_1,"@Gregory: I would suggest referencing other JIRAs by their names only, such as HADOOP-8230. This way we would easily see whether the JIRA has been resolved.",non_debt,-
hbase,5908,comment_3,"@Ted: Good idea, I'll do that from now on.",non_debt,-
hbase,5908,comment_4,Those tests fail for me locally even without patch applied.,non_debt,-
hbase,5908,comment_5,"I'll commit this momentarily to 0.90, 0.92, 0.94, and trunk (whew)",non_debt,-
hbase,5958,summary,Replace ByteString.copyFrom with new ByteString(),non_debt,-
hbase,5958,description,"ByteString.copyFrom makes a copy of a byte array in case it is changed in other thread. In most case, we don't need to worry about that. We should avoid copying the bytes for performance issue.",code_debt,slow_algorithm
hbase,5958,comment_0,Hi Jimmy. The constructor you referenced above is private... are you suggesting using reflection to access it or something?,non_debt,-
hbase,5958,comment_1,"Too bad. I didn't realize it is private. Why is that? It is immutable any way, right?",non_debt,-
hbase,5958,comment_2,"The protobuf people have a strong reticence to allowing ByteString construction without a copy. One solution would be to switch to protostuff, which does allow this kind of thing, but would be a bigger change. (though it's still wire-compatible)",non_debt,-
hbase,5958,comment_3,"Did you look at this Jimmy? ""But yes, if you start with a byte[], and you want a ByteString with the same content, you are going to need to make a copy, because ByteString has to guarantee immutability.""",non_debt,-
hbase,5958,comment_4,Boo to buffer copies!,non_debt,-
hbase,5958,comment_5,"In my mind the two possible solutions are : 1) use protostuff 2) use my earlier idea of a ""data dictionary"" at the front of our RPC requests. Essentially, we'd change the RPC request/response mechanism so that before each message (or perhaps after), we send a list of KeyValues (or byte strings). Then in the actual Put/Result/whatever protos, we just use vint32s to refer back to the dictionary. That would allow us to manually send out the KeyValues using a CodedOutputStream rather than having to build a full protobuf which includes them, if that makes sense.",non_debt,-
hbase,5958,comment_6,Looks like adding protostuff could help elsewhere; it looks like we can generate classes as part of the build w/o having to rely on external pb compiler being on the build path. Shouldn't we do data dictionary whether protostuff or not? Seems like a generally good idea (tm),non_debt,-
hbase,5958,comment_7,Protostuff is usually a little bit faster as well. I haven't personally run those benchmarks in a while and it looks like the most recent are not up yet. But still something to consider.,code_debt,slow_algorithm
hbase,5958,comment_8,"KeyValue is kind of immutable. Can we make KeyValue based on ByteString, instead of byte[]? If so, we can avoid copying around too.",non_debt,-
hbase,5958,comment_9,"By the way, is it an option to use reflection to access the private constructor? If so, I can have a wrap method to use the private constructor, or the original copyFrom if the private constructor is not accessible. Reflection has overhead of course.",code_debt,low_quality_code
hbase,5958,comment_10,Makes me nervous to reach in and use the private constructor... do you have some benchmarks that show that there's a noticeable speedup by doing so?,code_debt,slow_algorithm
hbase,5958,comment_11,"I don't have a benchmark yet. Based on profiling, copyFrom and toByteArray are some hotspots.",non_debt,-
hbase,5958,comment_12,Close it as Invalid.,non_debt,-
hbase,5975,summary,Failed suppression of fs shutdown hook with Hadoop 2.0.0,non_debt,-
hbase,5975,description,"Unit test failed with error: Failed suppression of fs shutdown hook failed to delete the hook since it should be runnable instead of a thread for HADOOP 2.0.0. For other HADOOP version, runnable should work too since thread implements runnable.",non_debt,-
hbase,5975,comment_0,@Jimmy: Which unit test exhibited the failure ? Thanks,non_debt,-
hbase,5975,comment_1,Many tests. I am testing with TestAcidGuarantees,non_debt,-
hbase,5975,comment_3,TestMasterObserver passed with the patch. TestAcidGuarantees passed with patch against hadoop 2.0 +1 on patch.,non_debt,-
hbase,5975,comment_4,Will integrate the patch tomorrow morning if there is no objection.,non_debt,-
hbase,5975,comment_5,"Ted, let's make an informal rule that we don't commit unless all tests pass locally with Hadoop 2.0.0, or the failures are known due to some other issue, until there is a Hudson project for it. So the JIRA comment on resolution should include the test summary.",non_debt,-
hbase,5975,comment_6,I applied patch from HBASE-5966. I saw this failure: I think Jonathan H. is working on the above issue in HBASE-5876.,non_debt,-
hbase,5975,comment_7,Here is another failure: didn't provide detail for the IOE.,non_debt,-
hbase,5975,comment_8,"@Ted, which patch from hbase-5966 did you use? You should use the patch I posted: hbase-5966.patch. Another thing, you need to make sure JAVA_HOME is set properly. I tried again and TestTableMapReduce is green for me: Running 1, 0, Errors: 0, Skipped: 0, 70.037 sec Running 2, 0, Errors: 2, Skipped: 0, 47.781 sec <<< FAILURE! Running 1, 0, Errors: 0, Skipped: 0, 102.161 sec Here is the command I used: mvn -PlocalTests -Dhadoop.profile=23 clean test I will follow up with Jon on HBASE-5876. The combined patch I used is attached too.",non_debt,-
hbase,5975,comment_9,I tried the combined patch. TestTableMapReduce passed. Do you see other failed test(s) ?,non_debt,-
hbase,5975,comment_10,and TestLogRolling failed too. I filed HBASE-5985 and HBASE-5984 for them.,non_debt,-
hbase,5975,comment_11,@Andy: Are you fine with tracking the known test failures in their respective JIRA ?,non_debt,-
hbase,5975,comment_13,1,non_debt,-
hbase,5975,comment_14,"Integrated to 0.94 and trunk. Thanks for the patch Jimmy. Thanks for the review, Andy.",non_debt,-
hbase,5975,comment_18,Resolving. Ted applied this to 0.94 and trunk.,non_debt,-
hbase,5975,comment_19,What you all want me to set up in jenkins? A build against 2.0.0? Should I put it in place of the 0.23 build we currently have?,non_debt,-
hbase,6009,summary,Changes for HBASE-5209 are technically incompatible,non_debt,-
hbase,6009,description,"The additions to add backup masters to ClusterStatus are technically incompatible between clients and servers. Older clients will basically not read the extra bits that the newer server pushes for the backup masters, thus screwing up the serialization for the next blob in the pipe. For the Writable, we can add a total size field for ClusterStatus at the beginning, or we can have start and end markers. I can make a patch for either approach; interested in whatever folks have to suggest. Would be good to get this in soon to limit the damage to 0.92.1 (don't know if we can get this in in time for 0.94.0). Either change will make us forward-compatible starting with when the change goes in, but will not fix the backwards incompatibility, which we will have to mark with a release note as there have already been releases with this change. Hopefully we can do this in a cleaner way when wire compat rolls around in 0.96.",design_debt,non-optimal_design
hbase,6009,comment_0,How do we define the markers ? Through a series of magic bytes ? Looks like total size field for ClusterStatus is better choice.,design_debt,non-optimal_design
hbase,6009,comment_1,"I looked at the total size field option for this, starting from the write case. To calculate total size written, you have to know how many bytes were written for each write() call on ClusterStatus, including any objects contained inside it. The DataOutput interface for Writables doesn't have a way to return how many bytes were written to the stream. This is not a problem for primitive types as we can figure that out trivially. Even for somewhat more complicated situations such as modified UTF-8s written with the writeUTF call, the number of written bytes for a String can at least be calculated based on the formula for modified UTF-8 conversion. However, for calls to Object's write functions (e.g. for HRegionLoad), this becomes somewhat more problematic as there is no obvious answer as to how many bytes were written. We could use reflection to grab the fields, but then there is no guarantee that all of the fields of the Object are actually written to the stream when write() is called. So you'd have to introduce some hardcoded way of knowing how much was written for each Object, which is Bad. I'm tempted to say that we shouldn't add any more fields to ClusterStatus or similar APIs until 0.96, when hopefully our wire compatibility efforts will kick in and we can do this in a compatible way without having to jump through hoops.",design_debt,non-optimal_design
hbase,6009,comment_2,@David: See the following:,non_debt,-
hbase,6009,comment_3,Lets try and avoid serializing fat ClusterStatus objects twice. We have never made guarantee that old clients could talk to new servers pre-0.96 (Too hard in the Writables world). What is the scenario David? You cannot update the clients? Wouldn't you have to update the clients anyways if you introduce clusterstatus size? Pardon me if I'm not getting this.,non_debt,-
hbase,6009,comment_4,"The immediate issue here is that HBASE-5209 was committed in 0.92.1, and that broke compatibility with 0.92.0. I suppose anyone who cares about 0.92 branch has moved to 0.92.1 so that there is no practical hit. You are right that adding a size would be another incompatible change, hence my later comment about ""let's just not make any more changes until 0.96"". :D Anyway in the absence of any changes, I can at least add a release note to 0.92.1 stating this incompatibility with 0.92.0. I'll use this JIRA to track that.",design_debt,non-optimal_design
hbase,6009,comment_5,+1 for the release note on 0.92.1.,non_debt,-
hbase,6009,comment_6,That was dumb. Did I do it? Probably. Should we back it out for 0.92.2?,non_debt,-
hbase,6009,comment_7,HBASE-5209 is an improvement. Backing it out wouldn't be too bad.,non_debt,-
hbase,6009,comment_8,"I wouldn't back it out ... I think that would just create another incompatibility event between 0.92.1 and 0.92.2. The original goal was to get this committed for 0.92.0 (which would have avoided this), but I suppose we didn't make it for 0.92.0. I should have said something about not doing this once 0.92.0 came out, so my apologies on that.",non_debt,-
hbase,6009,comment_9,"It seems like from discussion and inaction, we've decided to keep this in the acknowledge the incompatiblity between 0.92.0 and 0.92.1+ series, and keep this in 0.94.0 series allowing for compatibility between 0.92.1+ and 0.94.0",non_debt,-
hbase,6016,summary,could return false for disabling table regions,non_debt,-
hbase,6016,description,"For the disabling region, I think we needn't assign it , and processDeadRegion could return false.",non_debt,-
hbase,6016,comment_0,@chunhui returning false in in case of table in disabling state will effect process dead servers during master start up. -- return false and then assign become false. Now we cannot set offline in znode it will be in M_ZK_REGION_CLOSING state only. We need to wait until timeout monitor to trigger(30 min) unassign. But if set to offline During we will call assign and there we remove rit and znode if table is in disabling or disabled state. Better not handle this case. Please correct me if wrong.,non_debt,-
hbase,6016,comment_1,"@rajeshbabu When master startup after restart, We will do the following: nodes); the M_ZK_REGION_CLOSING or RS_ZK_REGION_CLOSED znode will be handled by - So, we will close this region at last. Thanks for the review, correct me if wrong.",non_debt,-
hbase,6016,comment_2,"Yes,you are correct. If we return false in case of disabling,we can aslo avoid creating znode and for already closed regions because 'assign' is false. By this we can only handle actual regions in transition. Its good. Thanks.",non_debt,-
hbase,6016,comment_3,1,non_debt,-
hbase,6016,comment_5,Committed to 0.94 and to trunk. Thanks for the patch Chunhui.,non_debt,-
hbase,6050,summary,"HLogSplitter renaming recovered.edits and CJ removing the parent directory race, making the HBCK think cluster is inconsistent.",design_debt,non-optimal_design
hbase,6050,description,The scenario is like this There if the regiondir doesnot exist we tend to create and then add the recovered.edits. Because of this HBCK thinks it to be an orphan region because we have the regiondir but with no regioninfo. Ideally cluster is fine but we it is misleading.,design_debt,non-optimal_design
hbase,6050,comment_0,Why are we trying to create the dstdir? What is the reason for it? Is the fix to be applied here or on the HBCK side so that he does not think that there is some inconsistency? But if we make this change in HBCK we are not sure how to delete the recovered.edits file created because master will never try to open this region?,non_debt,-
hbase,6050,comment_1,"Good one Ram. So, we are talking about the parent region? It does seem wrong that we would recreate a parent region dir in the distributed log splitter. How about we remove that dir creation code? I can see our making the recovered.edits dir because it won't always be there but creating all of its parent dirs is not right. My guess is that the mkdirs was done because it was just easier than verifying parent dir present. If parent dir not present, log the fact that there is no target region into which to put the edits and move on I'd say.",non_debt,-
hbase,6050,comment_2,Yes it is the parent region. Yes if destination does not exist we can move one and so we will consider the log splitting process successful. But the file created in the splitlog folder by the distributed log splitting will never be cleared i think.? May be i need to check the code on that. I will come up with a patch on this tomorrow.,non_debt,-
hbase,6050,comment_3,Trunk patch. Pls provide your comments.,non_debt,-
hbase,6050,comment_5,Replication related testcases are failing in the previous few QA builds. So this patch has not introduced it.,non_debt,-
hbase,6050,comment_6,Pls share your comments on this patch?,non_debt,-
hbase,6050,comment_7,Pls share your comments on this patch? If it is ok i can prepare for other versions also.,non_debt,-
hbase,6050,comment_8,Patch looks good. Minor: Please insert spaces around regionDir:,code_debt,low_quality_code
hbase,6050,comment_9,Thanks Ted. Will prepare patches for 0.92 and 0.94 and commit them later today in the evening if there is no objection.,non_debt,-
hbase,6050,comment_10,Just for clarification - this edits are actually replayed to the daughter regions and these recovered.edits files are kept around for something (the CJ?) to eventually clean up?,non_debt,-
hbase,6050,comment_11,@Jon In our case the split got completed and the RS went down due to ZK issue and that is why the Master was not able to respond to the split region completion. Because the RS went down the recovered.edits creation came into play. Ideally CJ just cleans up the entire region directory because the parent is in splitted state and offlined. Also in this case as the split is completed we are sure that the data is also flushed to store files. Daughter regions will have its own region directory. Did i answer your question? ;),non_debt,-
hbase,6050,comment_12,I will commit this tomorrow morning.,non_debt,-
hbase,6050,comment_13,"Committed to trunk, 0.94 and 0.92. Thanks for review Ted and Jon. Thanks Stack for your idea. P.S. committed a small addendum for HBASE-6002 for 0.92 only as both were part of HLogSplitter.",non_debt,-
hbase,6093,summary,Flatten timestamps during flush and compaction,non_debt,-
hbase,6093,description,"Many applications run with maxVersions=1 and do not care about timestamps, or they will specify one timestamp per row as a normal KeyValue rather than per-cell. Then, DataBlockEncoders like those in HBASE-4218 and HBASE-4676 often encode timestamps as diffs from the previous or diffs from the minimum timestamp in the block. If all timestamps in a block are the same, they will all compress to basically <= 8 bytes total per block. This can be 10% to 25% space savings for some schemas, and that savings is realized both on disk and in block cache. We could add a ColumnFamily setting If true, then all timestamps are modified during a flush/compaction to the currentTimeMillis() at the start of the flush/compaction. If all timestamps are made identical in a file, then the encoder will be able to eliminate them. The simplest use case is probably that where all inserts are type=Put, there are no overwrites, and there are no deletes. As use cases get more complex, then so does the implementation. For example, what happens when there is a Put and a Delete of the same cell in the same memstore? Maybe for a flush at t=flushStartTime, the Put gets timestamp=t, and the Delete gets timestamp=t+1. Or maybe HBASE-4241 could take care of this problem.",design_debt,non-optimal_design
hbase,6093,comment_0,"oops - for flushes you would set all timestamps to the flush start time like i said above. But for compactions you would would set all timestamps to the earliest timestamp in the compaction, and ensure that only consecutive files get compacted together",non_debt,-
hbase,6106,summary,Update documentation to reflect design and interface changes,non_debt,-
hbase,6106,description,None,non_debt,-
hbase,6106,comment_0,Superseded by subsequent documentation updates. Retired with parent.,non_debt,-
hbase,6122,summary,Backup master does not become Active master after ZK exception,non_debt,-
hbase,6122,description,"Now when the new active master goes down and the current back up master comes up, it goes down again with the zk expiry exception it got in the first step. In we try to wait till the back up master becomes active. When the back up master (it is in back up mode as he got ZK exception), once again tries to come to active we don't get the return value that comes out from We tend to return the which was previously false. Now because of this instead of again becoming active the back up master goes down in the abort() code. Thanks to Gopi,my colleague for reporting this issue.",non_debt,-
hbase,6122,comment_0,I found some changes in the trunk code. So not sure if it is applicable in trunk. Attached patches for 0.94 and 0.92.,non_debt,-
hbase,6122,comment_1,+1 patch looks good to me.,non_debt,-
hbase,6122,comment_2,Committed to 0.92 and 0.94. Thanks for the review Lars.,non_debt,-
hbase,6122,comment_5,Reopening. Backing out these patches. It seems reponsible for these failures:,non_debt,-
hbase,6122,comment_6,I reverted from 0.92 and 0.94 branches till we figure the failures.,non_debt,-
hbase,6122,comment_9,Oh... Let me check out the reason for the failure. Sorry for the mess.,non_debt,-
hbase,6122,comment_10,I checked the test case. Ideally the flow is making the master to become active but the problem as described in this JIRA still makes the master to go down. I added a log in See the below log in the logs. This means ideally the master should come up if there is no problem in again becoming active. Along with the patch this testcase should be modified to make the assertTrue to assertFalse. Pls correct me if am wrong. The fix still remains valid.,non_debt,-
hbase,6122,comment_11,Pls take a look at the patch. I have not corrected the test case for the testcase to pass. Ideally the previous test was covering up the bug. Correct me if am wrong.,non_debt,-
hbase,6122,comment_12,"@Ram Which assert should be changed? Do you want to include the assert change in your patch? Or are you suggesting a previous test case is broke? If so, which? Thanks.",non_debt,-
hbase,6122,comment_13,I have attached the patch Stack. It is changing the assert of the testcase,non_debt,-
hbase,6122,comment_14,Looks good Ram. +1,non_debt,-
hbase,6122,comment_15,Committed again with the latest patch to 0.92 and 0.94. Hope things are ok this time. Thanks Stack for your review.,non_debt,-
hbase,6122,comment_20,@ram Do you mean that the problem is not reproducible on trunk?,non_debt,-
hbase,6122,comment_21,@N The trunk code is different. Currently there is a while(true) loop and as far as i see it should be ok in trunk. I did not try to reproduce in trunk.,non_debt,-
hbase,6122,comment_22,"Thanks, I will give it a try to be sure.",non_debt,-
hbase,6151,summary,Master can die if RegionServer throws ServerNotRunningYet,non_debt,-
hbase,6151,description,"See, for example: The HRegionServer calls HBaseServer: but the server can start accepting RPCs once the threads have been started, but if they do, they throw until openServer runs. We should probably 1) Catch the remote exception and retry on the master 2) Look into whether the start() behavior of HBaseServer makes any sense. Why would you start accepting RPCs only to throw back",design_debt,non-optimal_design
hbase,6151,comment_0,should be handled in the last catch block of,non_debt,-
hbase,6151,comment_1,I may have been wrong about this affecting 0.92+. It looks like HBASE-4455 fixed this in 0.92. I'll look at backporting.,non_debt,-
hbase,6184,summary,HRegionInfo was null or empty in Meta,non_debt,-
hbase,6184,description,"insert data hadoop-0.23.2 + hbase-0.94.0 2012-06-07 13:09:38,573 WARN Encountered problems when prefetch META table: HRegionInfo was null or empty in Meta for hbase_one_col,",non_debt,-
hbase,6184,comment_0,"This change will affect the look up in the META table? When searchRow is created with passing newformat=true, it will add the encoded name also at the end[<tableName In your issue you are getting the result but in that result the HRegionInfo seems coming as null only? Do this above change really fix your issue? Do u facing some other issues?",design_debt,non-optimal_design
hbase,6184,comment_1,Can you may be check what was the scenario that happened before this? May be that will give us a clue.,non_debt,-
hbase,6184,comment_2,"public HRegionInfo(final byte[] tableName, final byte[] startKey, final byte[] endKey, final boolean split, final long regionid) throws { super(); if (tableName == null) { throw new cannot be null""); } this.tableName = tableName.clone(); this.offLine = false; this.regionId = regionid; this.regionName = startKey, regionId, true); this.regionNameStr = this.split = split; this.endKey = endKey == null? endKey.clone(); this.startKey = startKey == null? startKey.clone(); this.tableName = tableName.clone(); setHashCode(); }",non_debt,-
hbase,6184,comment_3,"This change, my program is no problem",non_debt,-
hbase,6184,comment_4,How will this fix the issue? Have you checked the data in the meta table? Does that row have region info? I think this most likely is a meta table data issue.,non_debt,-
hbase,6184,comment_5,Moving to 0.94.2 for now.,non_debt,-
hbase,6184,comment_6,And on to 0.94.3,non_debt,-
hbase,6184,comment_7,I don't quite grok the change. @jiafeng: Could explain again why this fix the issue? (sorry for us being slow on this)... Thanks.,non_debt,-
hbase,6184,comment_8,"This can be happened when region split. 0.94.x version: write memstore, write hlog, update mvcc. Client: Server : doesn't consider the readPoint, but the get will, so some value doesn't have commit, getRowKeyAtOrBefore see it, but get will ignore it, so there is possiable that will return null result.",non_debt,-
hbase,6184,comment_9,"0.94 write memstore write hlog // last a few ms.. update mvcc Current readPoint = 9;, and the new KeyValue memstoreTS = 10, then the is called. KeyValue key = will see the new KeyValue, but Get get = new Get(key.getRow()); result = get(get, null); will not see the new KeyValue.",non_debt,-
hbase,6184,comment_10,Thanks for your update. So do you think your patch will address this problem? Your explanation seems to be wrt MVCC right?,non_debt,-
hbase,6184,comment_11,"this is not my patch, and i think this is not a serious problem, doesn't solved this problem...",non_debt,-
hbase,6184,comment_12,Oh...Am sorry i thought the patch also was yours. Thanks anyway !!.,non_debt,-
hbase,6184,comment_13,Removing from 0.94,non_debt,-
hbase,6184,comment_14,"The issue also happened in hadoop-2.0.5_alpha + hbase-0.94.8 as: HRegionInfo was null or empty in Meta for hbase:namespace,",non_debt,-
hbase,6184,comment_15,Resolving old issue that we don't see anymore as 'cannot reproduce',non_debt,-
hbase,6197,summary,HRegion's append operation may lose data,non_debt,-
hbase,6197,description,"Like the HBASE-6195, when flushing the append thread will read out the old value for the larger timestamp in snapshot and smaller timestamp in memstore. We Should make the first-in-thread generates the smaller timestamp.",non_debt,-
hbase,6197,comment_0,Patch looks good to me.,non_debt,-
hbase,6197,comment_2,"I ran TestHLog and it passed locally. Patch integrated to trunk. Thanks for the patch, Xing.",non_debt,-
hbase,6197,comment_5,Ted committed this. Backport?,non_debt,-
hbase,6197,comment_6,I made HBASE-6210 for backport.,non_debt,-
hbase,6197,comment_7,Incorrectly assigned this to me.,non_debt,-
hbase,6197,comment_10,Marking closed.,non_debt,-
hbase,6201,summary,HBase integration/system tests,non_debt,-
hbase,6201,description,"Integration and general system tests have been discussed previously, and the conclusion is that we need to unify how we do ""release candidate"" testing (HBASE-6091). In this issue, I would like to discuss and agree on a general plan, and open subtickets for execution so that we can carry out most of the tests in HBASE-6091 automatically. Initially, here is what I have in mind: 1. Create hbase-it (or hbase-tests) containing forward port of HBASE-4454 (without any tests). This will allow integration test to be run with 2. Add ability to run all integration/system tests on a given cluster. Smt like: should run the test suite on the given cluster. (Right now we can launch some of the tests from command line). Most of the system tests will be client side, and interface with the cluster through public APIs. We need a tool on top of MiniHBaseCluster or improve so that tests can interface with the mini cluster or the actual cluster uniformly. 3. Port candidate unit tests to the integration tests module. Some of the candidates are: - TestAcidGuarantees / TestAtomicOperation - TestRegionBalancing (HBASE-6053) - - TestMasterFailover - TestImportExport - TestMultiVersions / TestKeepDeletes - TestFromClientSide - TestShell and src/test/ruby - TestRollingRestart - Test**OnCluster - Balancer tests These tests should continue to be run as unit tests w/o any change in semantics. However, given an actual cluster, they should use that, instead of spinning a mini cluster. 4. Add more tests, especially, long running ingestion tests (goraci, BigTop's TestLoadAndVerify, LoadTestTool), and chaos monkey style fault tests. All suggestions welcome.",non_debt,-
hbase,6201,comment_0,"Bigtop provides a framework for integration tests that is, essentially, 'mvn verify'. If we are discussing an integration test framework that must interact with a cluster, perhaps on demand, then there is potentially one already under development for that.",non_debt,-
hbase,6201,comment_1,"Thanks for bringing this up. I know that bigtop provides a test framework for integration tests. From my perspective, I see hbase and bigtop sharing responsibility on the testing side, and we can work to define best practices for this, and would love to hear Bigtop's perspective as well. I completely agree that HBase code, should not bother with deployments, cluster management services, smoke testing, nor integration with other components (hive, pig, etc). Those kind of functionality can belong in BigTop or similar projects. However, some core testing functionality, is better managed by the HBase project. Lets consider the TestMasterFailover test. Right now it is a unit test, testing the internal state transitions, when the master fails. However, we can extend this test to run from the client side, and see whether the transition is transparent when we kill the active master on an actual cluster. That kind of testing, should be managed by HBase itself, because, although they would run from the client side, these kind of tests are hbase-specific, and better managed by Hbase devs. Also, I do not expect BigTop to host a large number of test cases for all of the stack (right now 8 projects). Having said that, in this issue, we can come up with a way to interface with BigTop (and other projects, custom jenkins jobs, etc) so that, these tests can use the underlying deployment, server management, etc services, and BigTop, and others can just execute the HBase internal integration tests on the cluster. A simple way for this is that HBase to offer {{mvn verify}} to be consumed by BigTop, and those tests will use HBase's own scripts (and SSH, etc) for cluster/server management. Since BigTop configures the cluster to be usable by those, it should be ok.",non_debt,-
hbase,6201,comment_2,"Makes sense, thanks Enis.",non_debt,-
hbase,6201,comment_3,"Thanks for doing this Enis. +1 on making subjiras off this one. On IT tests and mvn verify, I concluded it a lost cause. Maybe you can figure how to make it change its behavior when you pass a -Dconf=xxxx and have it instead run on a real cluster (I found the mvn verify targets insufficient IIRC). The above is a comment on the mvn verify mechanism only, not on the intent of this issue. I'm rah-rah on your intent. Just suggesting we may need to do it atop a different vehicle (BT would be coolio. On hearing BT perspective on this, we may have blown that for a while. We've gotten Roman excited on multiple occasions but have failed to come through. I'd say lets deliver something first, then Roman might think us serious this time). I agree. Need to make it so test is same whether on mini or real cluster. +1 on #3 and #4 points above.",non_debt,-
hbase,6201,comment_4,That is disappointing. And we should also consider the potential benefits of importing and using iTest: It provides some useful functionality.,non_debt,-
hbase,6201,comment_5,"I've opened BIGTOP-632, to track pushing itest-common as an artifact. The Service, and Shell classes seem usable. I could not find any SSH wrappers.",non_debt,-
hbase,6201,comment_6,"Sorry for chiming in late, but here's how I see it after quite a bit of internal discussions with some of the HBase and HDFS devs. First of all, lets not got caught up in terminology of what is a unit test, what is a functional test, etc. If we assume this stance than all the tests, essentially, fall under the 3 main categories: # tests that muck about with the internals of the particular single project (HDFS, HBase, etc) using things like private APIs (or sometimes even things like reflections, etc to really get into the guts of the system) # tests that concern themselves with a single project (HDFS, HBase, etc) but use only public APIs AND don't use # tests that concern themselves with multiple projects at the same time (imagine a test that submits an Oozie workflow that has some Pig and Hive actions actively manipulating data in HBase) but only using public APIs It is pretty clear that #3 definitely belongs to Bigtop while #1 definitely belongs to individual projects. For quite some time I was thinking that #2 belongs to Bigtop testbase as well, but I've changed my mind. I now believe that such tests should reside in individual projects and: # be clearly marked as such not to be confused with class #1 (test suites, test lists, naming convention work, etc) # be written/refactored in such a way that doesn't tie them to a particular deployment strategy. IOW they should assume the subsystem to be deployed. # be hooked up to the project build's system in such a way that takes care of deploying the least amount of a system to make them run (e.g. MiniDFS, MiniMR, etc.) Thus if HBase can follow these rules and have a subset of tests that can be executed in different envs. both HBase core devs and bigger Bigtop dev community win, since we can leverage each other's work. Makes sense? If it does I can help with re-factoring.",code_debt,low_quality_code
hbase,6201,comment_7,"I think your categorization, and my comments above are telling the same thing, no confusion there. This umbrella issue is all about maintaining #2 kind of tests inside HBase. Now, the problem is how to best interface between HBase and Bigtop. My proposal is that depends on itest-common, and uses it to interact with the servers. My understanding is that, even if you are not deploying the cluster with bigtop, as long as /etc/init.d/ scripts are there, you should be fine. At this point, we only need starting / stopping deamons kind of functionality, assuming the cluster is already deployed. On the other side, if we provide a ""mvn verify"" in hbase-it module to run the tests on the actual cluster, I assume BigTop can leverage this to carry out the tests. For refactoring, once the module, and other bits are ready, we can move select tests from Bigtop to HBase. I'll open a subtask for that.",non_debt,-
hbase,6201,comment_8,"Great to hear -- in that case the implementation is the next level to go to. Basically, all that Bigtop needs is a maven artifact that we can plug in our infrastructure. We won't be using the 'mvn verify' as it is implemented in HBase's pom.xml but rather hooking the very same artifact to our Maven execution framework. Are you saying that you would like the tests themeselves to get involved in the lifecycle of each service? Like bringing them up and down, etc? This an area I'm really interested in providing a framework for in Bigtop. I'm about to open up a JIRA for that with the hope that we can cook something usable up rather quickly. The key point is that I don't want tests to have any logic that concerns itself with ssh/etc. It needs to be a sort of an agent-based framework that allows tests to query the topology of the cluster and also perform actions on the nodes of that topology in a most generic sense.",non_debt,-
hbase,6201,comment_9,"Feel free to chime in: BIGTOP-635 This is meant to cover HBase and HDFS use cases to begin with, but also should be generic enough to tackle just about anything. Given the level of ambition -- the more code we can reuse the better. Hence anybody familiar with real Cluster Management frameworks are welcome to chime in and give us feedback on how realistic such a reuse really is.",non_debt,-
hbase,6201,comment_10,"Yes. At the current state, most of our unit tests, which are candidates to be upgraded to be system tests does start a mini-cluster of n-nodes, load some data, kill a few nodes, verify, etc. We are them to do the same things on the actual cluster. A particular test case, for example, starts 4 region servers, put some data, kills 1 RS, checks whether the regions are balanced, kills one more, checks agains, etc. Some basic functionality we can use from itest are: - Starting / stopping / sending a signal to daemons (start a region server on host1, kill master on host2, etc). For both HBase and Hadoop processes. - Basic cluster/node discovery (give me the nodes running hmaster) - Run this command on host3 (SSH)",design_debt,non-optimal_design
hbase,6201,comment_11,"@Enis, in that case I'd strongly encourage the community to take a serious look at BIGTOP-635. This is exactly what we're hoping to achieve with it.",non_debt,-
hbase,6201,comment_15,"FYI, the guys at Wibidata have provided a [maven that looks potentially interesting for the purpose of running these integration tests locally. It may need to be jury-rigged to launch a cluster out of the local sandbox rather than one provided by an external release...",non_debt,-
hbase,6201,comment_16,Can this be closed? One subtask is open but it seems that this issue took on what the subtask was all about.,non_debt,-
hbase,6201,comment_17,"All subtasks except one are resolved. We can address that independently. Resolving this. Since most of the subtasks were fixed against 0.95.0, I set the fix version to 0.95.0. Please change it accordingly.",non_debt,-
hbase,6201,comment_18,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,6214,summary,Backport HBASE-5998 to 94.1,non_debt,-
hbase,6214,description,None,non_debt,-
hbase,6214,comment_0,Who's working on this? :),non_debt,-
hbase,6214,comment_1,@Lars Uploaded backport patch for 94.1 in HBASE-5998. Can you look at it?,non_debt,-
hbase,6214,comment_2,I committed the patch that was over on hbase-5998. Looks good to me. Thanks for the backport Rajesh,non_debt,-
hbase,6244,summary,[REST] Result generators do not need to query table schema,non_debt,-
hbase,6244,description,"Now, the RowResultGenerator and the will fit the column family if the request doesn't contain any column info. The will cost 10+ milliseconds in our hbase cluster each request. We can remove these code because the server will auto add the columns.",design_debt,non-optimal_design
hbase,6244,comment_0,"Committed trivial patch to trunk, 0.94, and 0.92 branches. All unit tests pass locally for trunk. All REST unit tests pass locally for 0.94. Many thanks for pointing out this issue, Xing! Edit: ... and REST tests pass for 0.92.",non_debt,-
hbase,6264,summary,Typos in the book documentation,documentation_debt,low_quality_documentation
hbase,6264,description,"In section 6.9: In section 9.2: There is ""are are"" twice. I'm not 100% sure what's the best way to propose a fix, so I have included the patch below. If that's fine, I will probably propose some other corrections. JM Index:  (rvision 1352979) +++ (copie de travail) @@ -828,7 +828,7 @@ Secondary Indexes and Alternate Query Paths </title <para- A common example on the dist-list is where a row-key is of the format ""user-timestamp"" but there are are reporting requirements on activity across users for certain + A common example on the dist-list is where a row-key is of the format ""user-timestamp"" but there are reporting requirements on activity across users for certain time ranges. Thus, selecting by user is easy because it is in the lead position of the key, but time is not. </para <para@@ -1324,7 +1324,7 @@ <section <title- <para+ <para of the HBase shell's <code </para <section",documentation_debt,low_quality_documentation
hbase,6264,comment_0,Patch to correct documentation typos.,documentation_debt,low_quality_documentation
hbase,6264,comment_1,what needs to be done to push this along?,non_debt,-
hbase,6264,comment_2,I made Jean-Marc's suggested changes. They'll show on site next time we push it up.,non_debt,-
hbase,6264,comment_4,Marking closed.,non_debt,-
hbase,6282,summary,"The introspection, etc. of objects in the RPC has to be handled for PB objects",non_debt,-
hbase,6282,description,"The places where the type of objects are inspected need to be updated to take into consideration PB types. I have noticed being used, and the private method also needs updating (in the PB world, all information about is contained in one PB argument).",non_debt,-
hbase,6282,comment_0,"To add to the list, QosFunction in the HRegionServer class needs to be updated (the method arguments, etc. are going to be PB types when the PB objects are used).",non_debt,-
hbase,6282,comment_1,also needs to be updated.,non_debt,-
hbase,6282,comment_2,Upgrading the Priority so that it gets due attention (maybe after a couple of more PB jira fixes).,non_debt,-
hbase,6282,comment_3,BTW the patch in HBASE-6414 addresses the QosFunction part.,non_debt,-
hbase,6282,comment_4,What do I need to do here ? Enable trace on RPC and I'll see all the problems above (the pb objects will come out all binary?) We don't need to do '...and the private method also needs updating' because WritableRpcEngine has been removed? How do I see output from Thanks DD.,non_debt,-
hbase,6282,comment_5,"Thanks for taking a look at this. Yes, I guess that's what you'd see.. I think most of the changes would be similar to the changes that I did to the QosFunction as part of HBASE-6414. I guess this is used in the UI side. I see toJSON (that internally invokes toMap) being called from within",non_debt,-
hbase,6282,comment_6,"Sorry, missed this question.. Actually I had moved logResponse to ProtobufRpcEngine as part of some jira.",non_debt,-
hbase,6282,comment_7,"If I enable IPC trace, its not bad as is: I can call toString on the pb param. This is totally obnoxious but would be a nice option to have debugging. Let me post a patch. Regards the logResponse, that needs fixup in general. I had it recently and found the message reported close to useless. Lets file a new issue to fix that distinct from pb'ing.",non_debt,-
hbase,6282,comment_8,"Lets apply this and close this issue. W/ this applied, if you enable TRACE, you see this kinda of stuff: It is ridiculous detail but could be life saver debugging. Could later work on pb toStringing so it doesn't dump it all... just start of String and a length instead but this should be good for now. Remove Objects class since no longer used.",code_debt,dead_code
hbase,6282,comment_9,Any chance of review on this?,non_debt,-
hbase,6282,comment_10,"Yeah , seems reasonable to me. +1",non_debt,-
hbase,6282,comment_11,Thanks for review Devaraj. Committed to trunk.,non_debt,-
hbase,6282,comment_13,Addendum committed to trunk that adds printing of the response we send back (Original patch just does the request),non_debt,-
hbase,6282,comment_14,Committed the addendum.,non_debt,-
hbase,6282,comment_17,Marking closed.,non_debt,-
hbase,6405,summary,Create Hadoop compatibilty modules and Metrics2 implementation of replication metrics,non_debt,-
hbase,6405,description,None,non_debt,-
hbase,6405,comment_0,Same patch as 4050-8.patch from Elliot.,non_debt,-
hbase,6405,comment_1,"Integrated to trunk. Thanks for the patch, Elliot. Thanks for the review, Stack.",non_debt,-
hbase,6405,comment_2,Thanks,non_debt,-
hbase,6405,comment_4,Fix rat. I also filed: HBASE-6415 as it looks like trunk has been broken on rat for a while and the pre-commit hooks didn't alert us when the broken checkin was committed.,non_debt,-
hbase,6405,comment_5,"Addendum checked in. Thanks for the quick turn-around, Elliot.",non_debt,-
hbase,6405,comment_8,"Eclipse is having trouble understanding the usage in It seems to not be able to find the plugin goal build-classpath with the missing version. Also, the m2e lifecycle mapping needs to be told how to handle the lifecycle. Attaching patch to fix the above as well as fix formatting on the added modules.",non_debt,-
hbase,6405,comment_9,Addendum 2 looks good.,non_debt,-
hbase,6405,comment_10,"Jesse: Your added patch fails to compile when compiling for hadoop2. It's pretty important that has an explicit dependency on hadoop1's version of things, and has an explicit dependency on hadoop2 versions. If I add <version Is there some reason that only hbase-hadoop2 has the maven m2e stuff added ?",non_debt,-
hbase,6405,comment_11,"@Elliot: yeah, I just noticed that it wasn't building - my bad. Adding back in those tags should be fine, but they need to by pulled out of the properties definition in /hbase/pom.xml's hadoop-1.0 profile and just have them reside in the general properties section in /hbase/pom.xml. Yeah, only hadoop2 has the running to specify the classpath since it is bound to an ""interesting"" phase. Specifically, from the m2e docs: where 'interesting phases' are defined here: Currently, it ignores the phase when building in eclipse (which is generally fine, considering command line maven is really source of truth). I'm working on a new version that fixes hadoop1 and has the build plugin execute.",non_debt,-
hbase,6405,comment_12,"Attaching patch that seems to work for me, but the issue raised in HBASE-6421 is breaking the build for me too, so hard to say for sure.",non_debt,-
hbase,6405,comment_13,hadoop-one.version and hadoop-two.version are defined in the global properties phase. The hadoop1 profile just sets the hadoop.version to whatever value hadoop-one.version holds. Likewise hadoop2 profile sets hadoop.version to whatever value hadoop-two.version has.,non_debt,-
hbase,6405,comment_14,"@Elliot - yeah, RTFS, i know. Just looked it up and realized I had put my foot in my mouth. What do think of the latest?",non_debt,-
hbase,6405,comment_15,+1 looks good to me. Having issues creating the last assembly like everyone else right now. However it compiles and everything looks good. Thanks. Sorry I broke eclipse :-/,non_debt,-
hbase,6405,comment_16,Eclipse is old and busted anyways (too bad everyone and their mother uses it). @Ted - good enough for commit?,non_debt,-
hbase,6405,comment_17,Will integrate later tonight.,non_debt,-
hbase,6405,comment_18,"Addendum v2 integrated to trunk. Thanks for the patch, Jesse. Thanks for the review, Elliot.",non_debt,-
hbase,6405,comment_19,Thanks Ted!,non_debt,-
hbase,6405,comment_23,Marking closed.,non_debt,-
hbase,6675,summary,takes too much time: 652.393s,code_debt,slow_algorithm
hbase,6675,description,"On trunk, as of today: Running 12, 0, Errors: 0, Skipped: 0, 652.393 sec Target beeing less than 3 minutes.",non_debt,-
hbase,6675,comment_0,Old hbase resolving.,non_debt,-
hbase,6697,summary,uses a fixed port (60020),non_debt,-
hbase,6697,description,"You can have this if the port is used. Testable by doing ""nc -l 60020"" before launching the test.",non_debt,-
hbase,6697,comment_0,"master & region server ports where set to ""random"" in the LocalHBaseCluster class; but not in the test hbase-site. So the pattern used in this test class was not working. Fixed, let's see if it breaks anything on hadoop qa...",code_debt,low_quality_code
hbase,6697,comment_2,"seems ok so. Will commit tomorrow except if I have a no go. As a side note, I ran locally the tests with ""nc -l 60020; nc -l 60000; nc -l 2181"" to take some commonly used ports, it went ok.",non_debt,-
hbase,6697,comment_3,Nice effort. +1.,non_debt,-
hbase,6697,comment_4,+1 on patch,non_debt,-
hbase,6697,comment_5,Committed revision 1379715. Thanks for the reviews!,non_debt,-
hbase,6806,summary,HBASE-4658 breaks backward compatibility / example scripts,non_debt,-
hbase,6806,description,HBASE-4658 introduces the new 'attributes' argument as a non optional parameter. This is not backward compatible and also breaks the code in the example section. Resolution: Mark as 'optional',non_debt,-
hbase,6806,comment_0,"i see, thrift does not support optional function arguments. maybe fix the examples then.",non_debt,-
hbase,6806,comment_1,Any chance of a patch Lukas to fix the example? (Thanks for noticing this).,non_debt,-
hbase,6806,comment_2,"Adding patch for fixing DemoClient.*. Tested: python, cpp, java Untested: php, perl, ruby Also refactored the python example to match the cpp and java version.",non_debt,-
hbase,6806,comment_3,corrected patch name.,non_debt,-
hbase,6806,comment_4,Woah. Thanks for fixing our broke examples Lukas. It works for you?,non_debt,-
hbase,6806,comment_5,"python, c++ and java: worksforme I had no chance to test php, perl and ruby, they might even have syntactic errors in it...",test_debt,lack_of_tests
hbase,6806,comment_6,"Passing by hadoopqa. Thanks Lukas. Let the php, perl, and ruby heads file an issue if broke. We'll take your fixes for the rest.",non_debt,-
hbase,6806,comment_8,Committed to trunk. Thanks for the fixup Lukas. Nice.,non_debt,-
hbase,6806,comment_10,"Hmm... it puts the commit in all issues referenced by the commit message, here and HBASE-4658",non_debt,-
hbase,6806,comment_12,My reading of this issue is there is no compatibility issue as such. Is this right? DemoClient needed to be changed in order to cater for the additional parameter in order to build the client. Thrift can handle the situation where an old client can make a call to a new server (as long as the service method is not renamed).,non_debt,-
hbase,6806,comment_13,Marking closed.,non_debt,-
hbase,6835,summary,HBaseAdmin.flush claims to be asynchronous but appears to be synchronous,non_debt,-
hbase,6835,description,"Relevant comment: but it looks like it's synchronous. In fact, it returns whether the flush ran or not:",non_debt,-
hbase,6835,comment_0,Related or similar to HBASE-4198?,non_debt,-
hbase,6835,comment_1,"Thanks for pointing to that, linking as related. HBASE-4198 seems like a good idea; for now, let's make the comment accurate.",documentation_debt,low_quality_documentation
hbase,6835,comment_2,1,non_debt,-
hbase,6835,comment_4,"Thanks for the review, stack. Committed to trunk.",non_debt,-
hbase,6835,comment_7,Marking closed.,non_debt,-
hbase,6969,summary,Allow for injecting instance,non_debt,-
hbase,6969,description,"While not an official part of the HBase API, it'd be nice if HBaseTestingUtility allowed for consumer to inject their own instance of the when executing tests. Currently there is no way to control how the ZK instance is started which we've had to hack around when there were ZK version conflicts (3.3 vs 3.4). Or if you want to control which specific port it starts on vs random. Allowing consumers to inject an instance (unstarted) gives them the freedom to implement their own without having to fork the entire HBaseTestingUtility class",design_debt,non-optimal_design
hbase,6969,comment_0,Simple patch file that just allows the HBaseTestingUtility to access a base MiniZKCluster instance.,non_debt,-
hbase,6969,comment_1,It should be noted I made the patch based off of trunk but I would like it patched back to 0.92.1 (the version I'm currently on). Trunk has a number of improvements (e.g configuration for the port) but I'm on CDH4.1 which is 0.92.1.,non_debt,-
hbase,6969,comment_2,"As written, at end of tests, won't your passed in zkcluster be shutdown? That is probably not what you want? Should there be a to answer your added Is baseZKCluster necessary? Why not just an internal flag which has whether or not HBaseTestingUtility started the zk cluster? If we didn't start it, we shouldn't stop it on the way out? Otherwise, looks like useful functionality to add. Thanks Micah.",design_debt,non-optimal_design
hbase,6969,comment_3,"I documented it on the method but the assumption was that the injected zkCluster is not started. Since the patch truthfully just swaps out the call to ""new the code which currently manages the startup and shutdown should manage the injected value. Also when startup is called, the member variable ""zkCluster"" is initialized to the baseZKCluster and the existing calls to getZKCluster() should return it without needing a new ""get"". The ""don't start"" flag idea could work. The assumption in that case would be the ZK Quorum would have to be started and relevant configuration set in the injected Configuration object I assume?",non_debt,-
hbase,6969,comment_4,Pardon me. I missed this. I looked at adding an override of startMiniZKCluster that took a Would the attached patch work for you?,non_debt,-
hbase,6969,comment_5,Adds startMiniZKCluster override that takes a Patch is a little bigger than it should be since it removes private methods no longer used.,non_debt,-
hbase,6969,comment_6,That should work. If I can extend HBaseTestingUtility and control how ZK is started it will make my life easier.,non_debt,-
hbase,6969,comment_7,"You don't need to extend it, right? The patch gives you a version of what your patch did only w/o the special data member and the extra setter? Thanks Micah.",non_debt,-
hbase,6969,comment_11,Committed ages ago,non_debt,-
hbase,7000,summary,Fix the WARNING in KeyValue class,non_debt,-
hbase,7000,description,None,non_debt,-
hbase,7000,comment_0,"there're two choices here. 1 attached file shows change to some value not the old Integer.MAX_VALUE 2 modify KeyValue.java remove the following code: if (vlength throw new length "" + vlength + "" }",non_debt,-
hbase,7000,comment_1,"IMHO, the maybe changed in future, so the check statement should be always be there for safety",requirement_debt,non-functional_requirements_not_fully_satisfied
hbase,7000,comment_2,"BTW, the WARING was caused by : an integer comparison(vlength > Integer.MAX_VALUE) that always returns the same value",non_debt,-
hbase,7000,comment_4,patch looks good. nit: insert space before 1:,code_debt,low_quality_code
hbase,7000,comment_5,Attached v2 patch addressed Ted's comments,non_debt,-
hbase,7000,comment_7,The two failed tests are flaky. Integrated to trunk. Thanks for the patch Liang.,test_debt,flaky_test
hbase,7000,comment_10,Marking closed.,non_debt,-
hbase,7022,summary,Use multi to batch offline regions in zookeeper,non_debt,-
hbase,7022,description,Bulk assigner needs to set regions offline in zookeeper one by one. I was wondering if we can have some performance improvement if we batch these operations using ZooKeeper#multi.,non_debt,-
hbase,7022,comment_0,"Currently, I think ZooKeeper#multi doesnt help very much since it's synchronous, transactional, and doesnt support something like upsert in SQL (i.e. create it if a node doesnt exist, otherwise, update its data). It supports batch of size less than 1MB. If we can have an asynchronous, non-transactional multi zookeeper client function, which also supports upsert, it will really help.",non_debt,-
hbase,7022,comment_1,"Patched ZooKeeper with async multi support. Tried to use it to batch offline regions, but didn't get much performance gain as expected.",non_debt,-
hbase,7074,summary,Document Metrics 2,non_debt,-
hbase,7074,description,"* Explain why this work was done. * Explain how the data flows from the core classes into hadoop metrics2, and on to jmx. * Explain naming metrics.",non_debt,-
hbase,7074,comment_0,"Yeah, that'd be great Elliott. Doesn't have to be much but you need to explain why the gymnastics are going on. Minor: explain that metrics2 is not a h2 only thing....",non_debt,-
hbase,7074,comment_1,Making critical,non_debt,-
hbase,7074,comment_2,Making major rather than critical (its parent task is critical... this is only think in way of closing parent issue),non_debt,-
hbase,7074,comment_3,"Here's a patch that adds how to add a metric to the documentation. This pathc, the work Doug did, and the blog post on Apache blogs and Cloudera's blog should be enough to give everyone some idea of how metrics 2 was used.",non_debt,-
hbase,7074,comment_4,1,non_debt,-
hbase,7172,summary,fails when run individually and is flaky,test_debt,flaky_test
hbase,7172,description,"fails when run individually (run just that test case from eclipse). I've also noticed that it is flaky on windows. The reason is a rare race condition, which somehow does not happen that much when the whole class is run. The sequence of events is smt like this: - we create 1 log file to split - we call in its own thread. - is waiting in since there are no splitlogworkers, it keep waiting. - we delete the task znode from zk - SplitLogManager receives the zk callback from which will call setDone() and mark the task as success. - However, meanwhile the loops sees that remainingInZK == 0, and calls return concurrently to the above. - on return from fails because the znode delete callback has not completed yet. This race only happens when the last task is deleted from zk, and normally only the SplitLogManager deletes the task znodes after processing it, so I don't think this is a production issue.",test_debt,flaky_test
hbase,7172,comment_0,"Attaching a simple patch. We should only return from the wait loop if there are no remaining tasks and no znodes. If remainingInZK == 0 && and actual > 0, then that task will eventually be resubmitted. I think this can only happen if we somehow miss to setup the zk watchers.",non_debt,-
hbase,7172,comment_1,"Looks fine to me. If it passes hadoopqa, go ahead commit.",non_debt,-
hbase,7172,comment_3,1,non_debt,-
hbase,7172,comment_4,"Ops, I've found some more flaky tests: I think we can just increase the timeouts a la HBASE-7165. I'll do a v2 patch.",test_debt,flaky_test
hbase,7172,comment_5,I should have probably done that in HBASE-7165 (on the other hand I did not see this test failing in recent 0.94 builds). +1 on increasing TOs there as well.,non_debt,-
hbase,7172,comment_6,"Attaching v2 patches, which hopefully fixes the remaining flaky tests.",test_debt,flaky_test
hbase,7172,comment_7,+1 on patch,non_debt,-
hbase,7172,comment_8,1,non_debt,-
hbase,7172,comment_9,I've committed this to trunk and 0.94. Thanks Lars and Stack for reviews.,non_debt,-
hbase,7212,summary,Globally Barriered Procedure mechanism,non_debt,-
hbase,7212,description,"This is a simplified version of what was proposed in HBASE-6573. Instead of claiming to be a 2pc or 3pc implementation (which implies logging at each actor, and recovery operations) this is just provides a best effort global barrier mechanism called a Procedure. Users need only to implement a methods to acquireBarrier, to act when insideBarrier, and to releaseBarrier that use the ExternalException cooperative error checking mechanism. Globally consistent snapshots require the ability to quiesce writes to a set of region servers before a the snapshot operation is executed. Also if any node fails, it needs to be able to notify them so that they abort. The first cut of other online snapshots don't need the fully barrier but may still use this for its error propagation mechanisms. This version removes the extra layer incurred in the previous implementation due to the use of generics, separates the coordinator and members, and reduces the amount of inheritance used in favor of composition.",design_debt,non-optimal_design
hbase,7212,comment_0,Attached a quick deck with a summary of the design and semantics of this mechanism.,non_debt,-
hbase,7212,comment_2,"Include a patch that includes all work on top of the jesse's snapshots branch, and then the actual patch under review.",non_debt,-
hbase,7212,comment_4,"Doc looks great. Do you have to write it yourself? Anything already available that you could use? Say we used the zk curator client, it has a few barriers implemented already: If we were using curator say, could you use these receipes as building blocks so you didn't have to write this yourself? (This feature has to be backportable to 0.94?) Reading the diagram, I""m not sure what receivedreached is. Or sendReached. sendReached is the coordinator saying all participants responded/are participating? On your barrier you say ""...but does not support ACID semantics"" and thats ok because the 'transactions' we'll be running over this mechanism do not require it? Because they can proceed and complete in any order and result will come off the same? You say ""....Does not recover on failures"" ... because the operation just FAILs. Right? Only one of these precedures can be ongoingn at any one time? Is that right? How do I read these set of slides? There is a 'Barrier Procedure Coordination' and then there is 'Procedure Coordination'? So, the PC makes use of a BPC? BPC is the skeleton you hang PC on? Why you say this 'If we arent doing proper 2PC do we need all this infrastructure?'? Are you making a case for our not needing 2PC given what is being implemented? Coordinator can be any client? Does not have to be master? What is Does this barrier acquistion have any relation to zk barrier receipe? What is 'class' in the zk node hierarchy? Class of procedure? Procedure looks good to me.",documentation_debt,low_quality_documentation
hbase,7212,comment_5,"The online-snapshots is a 'class' (e.g. all online snapshots) while a procedure name is an actual name for a particular snapshotting request (snapshot121201, snapshot121202 etc). Off the top of my head I can't think of any other HBase processes that are ok with the procedure mechanism's semantics (other operations like enabling, disabling, schema change, splitting, merging probably want 2pc and its recovery requirements). I think this extra znode dir could probably get removed.",documentation_debt,low_quality_documentation
hbase,7212,comment_6,"+1 This makes a good case. I like the ""keep it as simple as possible and only do as much as we actually need to"" approach. Edit: Moved unrelated comment to HBASE-7254",non_debt,-
hbase,7212,comment_7,"On curator double-barrier, it would seem there is no 'abort' as you say. They do have timeouts on barrier enter and leave. Would that be enough See Rather than 'abort', you could just timeout? That might be simpler still? i.e. your ""Need to be able to force failure after a specified timeout elapses"" double-barrier does not seem to be enough though. There needs to be a means of telling cluster members to go for a particular snapshot barrier. To this end, I suppose all members need to be watching a snapshot dir and when a new snapshot appears, all try to 'enter' its barrier? Is it true that you do not want members to start 'snapshotting' until ALL participants have 'entered' the barrier? Does it matter if they start doing their work soon as they 'enter' the barrier (using curator/zk receipe terms). Reading on, it seems like its fine if members just go about their merry way....working on their part of snapshot. If not all members complete, the coordinator will clean up the incomplete. What do you think of the terms in the zk receipe: i.e. rather than 'reach' a barrier, 'enter' it? Some of the answers you give above should go into doc of this feature. They are quality. I buy your argument for going w/ the more basic barrier rather than 2pc function for snapshots (Yeah, 2pc would be useful for other distributed ops like table enable/disable w/ us 'failing forward' an interrupted table enable or disable) On 'Comms', it was just unclear to me what it was. Makes sense now.",documentation_debt,outdated_documentation
hbase,7212,comment_8,What happens when the coordinator dies (in this case hmaster). Does the new HMaster discover the prev procedure and abort?,non_debt,-
hbase,7212,comment_9,"I need to take a look at the source implementation of the curator double barrier and examples of its use to do a better job of comparing. Based on the api and the zk recipes, I'm going to make some assumptions here. As another analogy, it seems that our procedure mechanism is similar to a monitor (synchronized in java) that guarantees enter/acquire and leave/release of the barrier parts, while the curator one is lower level and leaves it to the implementer to enforce that invariant. So in this patch, the time-based abort trigger and a potential user-induced cancellation uses the same mechanism to notify all members (and the coordinator) that the procedure has aborted. I'm speculating but with think one assumption with this mechanism has vs the double barrier's is that we assume that the actions on the members may be slow (one implementation waits for a memstore flush per region) and may need to be interrupted before completion. The curator double barrier api doesn't have such a mechanism and we may have to wait for all operations to complete before we can abort them. I believe that would be the case if we used curator. I don't think we can't use it -- and the factoring out of the *Comms/*Rpcs would potentially allow us to move that in a future rev. At the end of the day, the full barrier is only required for the snapshot that completely blocks all writes to get a truly consistent snapshot. The weaker snapshots (either the timestamp based or log roll based) won't give those guarantees and doesn't actually need the full barrier. For the first cut however, I'm probably going to use it since it handles the error propagation and cross process cancellation. I'm fine with it -- I'll change the terms acquire - I'll do another rev of the docs to make it consistent with the changes being made.",documentation_debt,outdated_documentation
hbase,7212,comment_10,"The new HMaster will delete all znodes associated with the procedure class (all znodes associated with snapshotting procedures), all members still using them should timeout and fail, and new operations need to be issued. For snapshots in particular, there isn't really a chance for a partial snapshot being present when taking one because all the snapshot work is done in a temp dir and atomically put into place with a dir rename op after the coordinator realizes all the members have released/leave'd successfully. There will be junk in these tmp dirs left over but they get cleaned up on the next take snapshot attempt, or when the new master starts.",non_debt,-
hbase,7212,comment_11,pre patch and patch applied to the 12/18 snapshots branch.,non_debt,-
hbase,7212,comment_13,I encountered some error submitting review comments on review board. The following two new methods can be declared package private: Can we use MapMaker from Guava so that we don't need to introduce WeakValueMapping class ? See for an example.,non_debt,-
hbase,7212,comment_14,"waitForLatch got moved out of the ErrorHandling patch over to here -- but this is used in follow on patches with code that resides in different packages. I agree with createProcedure, I've made it package protected. (there was some concern about mokito limitations) Thanks for the pointer to guava's MapMaker and the example in CoprocessorHost. I've moved over to that api and removed WeakValueMapping.",non_debt,-
hbase,7212,comment_15,v8 is what I plan on committing to the online-snapshot branch.,non_debt,-
hbase,7212,comment_17,I've committed this to the online-snapshot branch. As a reminder this branch is here. The offline branch is here:,non_debt,-
hbase,7212,comment_18,"Thanks for the reviews nick, matteo, stack, jesse, and ted. If there are other concerns I'll file them as follow-ons that will block merge to trunk.",non_debt,-
hbase,7212,comment_19,Marking closed.,non_debt,-
hbase,7330,summary,Security hooks missing in region server and master APIs.,non_debt,-
hbase,7330,description,"Some of the APIs in Master and Region server are missing hooks to the coprocessors. So even if security is enabled, an unauthorized user can perform certain operations. The following is the list of operations: 1. HMaster.offline() 2. 3. 5. 6. regionName, byte[] columnFamily) 7. regionName, byte[][] columnFamilies) 8. regionName 9. byte [] regionName, final byte [] row) 10. byte [] regionName, final long lockId) 11. 12. 13. entries) 14. 15. 16.",non_debt,-
hbase,7330,comment_0,"This issue is a partial duplicate, but thanks for listing the particulars, I'm not sure they are all laid out in one place. See HBASE-6096 and HBASE-6101. It looks like the ""Convert to Subtask"" JIRA option is now missing, or I'd convert it. Should we close HBASE-7330 and HBASE-7331 as duplicates and move this to HBASE-6101?",non_debt,-
hbase,7330,comment_1,Can I copy the details in description to HBase-6101 and make HBASE-7331 as subtask of HBASE-6101 ? We can close this JIRA as duplicate.,non_debt,-
hbase,7330,comment_2,Or we can make HBASE-7331 a subtask of HBASE-5352.,non_debt,-
hbase,7330,comment_3,How does this look?:,non_debt,-
hbase,7330,comment_4,Looks good. So are you okay with close this jira as duplicate and then making HBASE-7331 as subtask of 6101 or 5352 ?,non_debt,-
hbase,7330,comment_5,Let me try that.,non_debt,-
hbase,7604,summary,Remove duplicated code from HFileLink,code_debt,duplicated_code
hbase,7604,description,"the static method getReferencedPath() contains almost the same code done internally by the FileLink, and is only used by ExportSnapshot. Remove that code and use the class directly",code_debt,duplicated_code
hbase,7604,comment_0,1,non_debt,-
hbase,7604,comment_1,merged to the snapshots branch,non_debt,-
hbase,7604,comment_2,Marking closed.,non_debt,-
hbase,7628,summary,Port HBASE-6509 fast-forwarding FuzzyRowFilter to 0.94,non_debt,-
hbase,7628,description,"This is to port HBASE-6509: 'Implement fast-forwarding FuzzyRowFilter to allow filtering rows e.g. by ""???alex?b""' to 0.94",non_debt,-
hbase,7628,comment_0,Do we really want this in 0.94? I'm not really a fan of this filter being in hbase core.,non_debt,-
hbase,7628,comment_1,I read through patch for HBASE-6509 again. The addition of fuzzy filter is not intrusive. I think the risk of this addition is very low.,non_debt,-
hbase,7628,comment_2,Why not? Where else should we have it? It seems useful on its own.,non_debt,-
hbase,7628,comment_3,I do agree that Filters need to be supported better. At least with some classloading that is similar to coprocessors.,non_debt,-
hbase,7628,comment_4,"My main concern is that this is actually fairly complicated, and that a filter like this might be better as some sort of plugin. As with the original patch, I'm a +0 on this -- low risk and potentially useful I agree, but complicated.",non_debt,-
hbase,7628,comment_5,I was trying to back port for our version,non_debt,-
hbase,7628,comment_6,The following tests passed locally: 1058 mt 1060 mt,non_debt,-
hbase,7628,comment_7,"+1 ( your point is duly noted, we should make a followup jira - or I think there might be one already - to support filters better)",non_debt,-
hbase,7628,comment_8,"Integrated to 0.94 branch Thanks for the patch, Anoop. Thanks for the review, Lars.",non_debt,-
hbase,7628,comment_9,Added link to related HBASE-6527 (pluggable filters),non_debt,-
hbase,7785,summary,rolling-restart.sh script unable to check expiration of master znode,non_debt,-
hbase,7785,description,"When rolling-restart.sh script stop master it enters loop trying to detect that master znode is deleted. Since it is unable to execute check command script hangs in infinite loop. Problematic line of script is: while ! bin/hbase zkcli stat $zmaster 2 ""bin/hbase zkcli stat"" can not be executed since script is run from bin directory. My suggestion is that this line should be like this in order to work: while ! ""$bin""/hbase zkcli stat $zmaster 2 After i made this change i was able to execute rolling restart.",non_debt,-
hbase,7785,comment_0,Here is diff for change,non_debt,-
hbase,7785,comment_1,Yep. We do this everywhere else in this script. This is the only exception. Going to commit in a few.,non_debt,-
hbase,7785,comment_2,Committed to 0.94 and 0.96. Thanks for the patch.,non_debt,-
hbase,7901,summary,has a hidden issue,non_debt,-
hbase,7901,description,"has what looks like a mistake in the j-loop, where i is checked in the 2nd part of for. It looks like the loop never executes, as if it did it would never terminate. If that is fixed, test fails.",non_debt,-
hbase,7901,comment_0,+1 on patch.,non_debt,-
hbase,7901,comment_2,"Integrated to trunk. Thanks for the patch, Sergey.",non_debt,-
hbase,7901,comment_5,Marking closed.,non_debt,-
hbase,7933,summary,NPE in TableLockManager,non_debt,-
hbase,7933,description,We are getting NPE in TableLockManager sometimes in tests.,non_debt,-
hbase,7933,comment_1,"I think the bug is a race condition for the parent znode for the table. deletes parent znode, so that we do not leak znodes for deleted tables.",code_debt,low_quality_code
hbase,7933,comment_2,Attaching a patch which theoretically should fix the problem.,non_debt,-
hbase,7933,comment_3,Only one specific KeeperException is caught. You may want to use a second catch block for other KeeperException's.,non_debt,-
hbase,7933,comment_4,"We catch the noNode exception in case, the parent dir is not created yet. Other KeeperExceptions are thrown from the method, and converted to IOE at the callee.",non_debt,-
hbase,7933,comment_6,TestConstraint is a medium test. Meaning lock manager tests were not run.,test_debt,low_coverage
hbase,7933,comment_7,Retrying against hadoopqa,non_debt,-
hbase,7933,comment_8,Making critical. Its a test that fails sometimes.,non_debt,-
hbase,7933,comment_10,"I've run the tests with this once again. The ACL tests seem to be flaky. Master does not wait for _acl_ table to be created before accepting other create table statements. Will open another issue. I want to get this resolved sooner rather than later, since it affects the tests and pre-commit tests.",test_debt,flaky_test
hbase,7933,comment_11,+1 from me.,non_debt,-
hbase,7933,comment_12,1,non_debt,-
hbase,7933,comment_13,Committed this. Thanks for review.,non_debt,-
hbase,7933,comment_18,Marking closed.,non_debt,-
hbase,7940,summary,Upgrade version to 0.97-SNAPSHOT in pom.xml files,non_debt,-
hbase,7940,description,In trunk build #3898: This was due to version in pom.xml files for various modules not being updated.,non_debt,-
hbase,7940,comment_0,"Patch for trunk. With patch, I got: The above is resolved by Rajesh's addendum for HBASE-7790",non_debt,-
hbase,7940,comment_1,Thanks for diggingin on this Ted. I changed the top-level pom but did not realize we had hard-coded versions in all submodules. How about this patch. It has us set version once in one place.,code_debt,low_quality_code
hbase,7940,comment_2,That should help with version upgrade in the future. +1,non_debt,-
hbase,7940,comment_3,Patch for 0.95.,non_debt,-
hbase,7940,comment_4,Committed because build is broke. Thanks Ted for figuring this one.,non_debt,-
hbase,7940,comment_9,Marking closed.,non_debt,-
hbase,8026,summary,HBase Shell docs for scan command does not reference VERSIONS,documentation_debt,low_quality_documentation
hbase,8026,description,"a table; pass table name and optionally a dictionary of scanner specifications. Scanner specifications may include one or more of: TIMERANGE, FILTER, LIMIT, STARTROW, STOPROW, TIMESTAMP, MAXLENGTH, or COLUMNS, CACHE VERSIONS should be mentioned somewhere here.",non_debt,-
hbase,8026,comment_0,"Agree. Versions go along with RAW scanner attribute. Eg : scan 't1', {RAW =So we need to mention both RAW and VERSIONS.",non_debt,-
hbase,8026,comment_2,"Manual steps : 1) Following works : scan 't1', {RAW =2) help 'scan' shows both RAW and VERSIONS info.",non_debt,-
hbase,8026,comment_3,Could you expand the help to include mention of how one uses RAW/VERSIONS? Also please add a test in should be straight forward.,test_debt,lack_of_tests
hbase,8026,comment_4,"1) Usage : Its already present and can be seen once we do help scan on shell. Excerpt that include info on versions and raw : "" Also for experts, there is an advanced option -- RAW -- which instructs the scanner to return all cells (including delete markers and uncollected deleted cells). This option cannot be combined with requesting specific COLUMNS. Disabled by default. Example: hbase"" 2) Test - This jira is not about functionality of RAW and VERSIONS command. Its just the mention of these commands missing in the help. If you meant writing a new unit test for versions and raw command, then I will go through the shell tests to see if there is any corresponding test there and if not present, I can raise a new Jira and can work on it.",test_debt,lack_of_tests
hbase,8026,comment_5,"1) Okay, that looks good. 2) yes please raise a follow on jira for the test if there isn't one.",non_debt,-
hbase,8026,comment_6,+1 Pushed to 0.98+,non_debt,-
hbase,8026,comment_12,",  : created new jira for test case :",non_debt,-
hbase,8026,comment_13,Closing this issue after 1.0.0 release.,non_debt,-
hbase,8056,summary,allow StoreScanner to drop deletes from some part of the compaction range,non_debt,-
hbase,8056,description,"Allow StoreScanner to drop deletes from some part of the compaction range. Needed for stripe compactor, and maybe level compactor (although at present I am not sure how level compactor will drop deletes at all).",non_debt,-
hbase,8056,comment_0,A fairly simple patch with lots of comments. I am trying to see if there's a viable way to test the code in this area.,test_debt,low_coverage
hbase,8056,comment_1,"The comment in SQM should say ""set to false"", not ""set to true"", I will fix that.",documentation_debt,low_quality_documentation
hbase,8056,comment_3,Change is reasonable to me. That would be nice. : Can you take a look ?,non_debt,-
hbase,8056,comment_4,Could make a new function to place the added code in StoreScanner and mark that it is used in stripe compactions. I think it would be clear for reader. The change is avaiable to me,design_debt,non-optimal_design
hbase,8056,comment_5,Can you store dropDeletesFromRow and dropDeletesToRow in ScanQueryMatcher and keep the logic local there?,design_debt,non-optimal_design
hbase,8056,comment_6,"Moved code, added tests",architecture_debt,violation_of_modularity
hbase,8056,comment_8,Latest patch looks good.,non_debt,-
hbase,8056,comment_10,Any more feedback? Thanks,non_debt,-
hbase,8056,comment_11,1,non_debt,-
hbase,8056,comment_12,I will commit tomorrow after running tests,non_debt,-
hbase,8056,comment_17,the patch committed long time ago,non_debt,-
hbase,8089,summary,Add type support,non_debt,-
hbase,8089,description,"This proposal outlines an improvement to HBase that provides for a set of types, above and beyond the existing ""byte-bucket"" strategy. This is intended to reduce user-level duplication of effort, provide better support for 3rd-party integration, and provide an overall improved experience for developers using HBase.",design_debt,non-optimal_design
hbase,8089,comment_0,"I'm beginning to think variable-length encoding for anything but char,byte arrays is an unnecessary micro-optimization. Instead of helping a user pack data via encoding, we should encourage the use of compression.",design_debt,non-optimal_design
hbase,8089,comment_1,"ImportTSV is a very good tool used for bulk loading. We can add a type support for this tool also. When the MR reads the file lines and convert it into bytes to store into HBase, this type can be considered. We can have a sub issue for that also?",non_debt,-
hbase,8089,comment_2,"Updated spec document. Supported types changed a little, and a spec is outlined for the basics. Biggest open questions include: - should null values be required for each type or is it enough to support reading a null marker via the STRUCT/UNION implementation? - how to handle String and byte[] types. Orderly goes to great pains to encode values, Phoenix restricts the context in which they can be used.",non_debt,-
hbase,8089,comment_3,"Updated spec document with definitions for VARCHAR and CHAR. After discussion and deliberation, I decided to roughly follow Orderly's approach. The reasoning being: the additional computation imposed by incrementing values and (slight) storage overhead of explicit termination is worth the cost. That is, this approach places no limitation on where the user can use a \{VAR,\}CHAR type.",non_debt,-
hbase,8089,comment_4,I read the description for VARCHAR and CHAR. Looks good.,non_debt,-
hbase,8089,comment_5,"Nick, ORC gets a lot of mileage by doing type-specific compression. In particular, the integer columns use a vint representation (protobuf vint encoding) and run length encoding. The string columns use an adaptive dictionary (the writer switches between dictionary or direct encoding based on the 100k initial values) approach. That allows both tighter representation before turning on the relatively expensive zlib or even tighter encodings when combined with zlib.",non_debt,-
hbase,8089,comment_6,"You should also look at the other types from Hive: * Byte * Timestamp * List * Map * Union Hive includes a standard serialization library that produces serializations that memcmp into the natural sort order, which it uses for MapReduce key serialization.",non_debt,-
hbase,8089,comment_7,"I didn't know about this feature in Hive, I'll check it out. Thanks for the reference, . The memcmp feature is critical for our needs; this is why most existing tools (ie, protobuf) don't work in this context. Do these Hive formats support NULLs -- I'm curious how the trade-off for fixed-width types was handled. How does it handle compound keys? It looks like I have more homework to do :)",non_debt,-
hbase,8089,comment_8,"Updates the definition of {{BOOLEAN}} type to support NULL. Updates serialized definition for {{\{VAR,\}CHAR}} to also invert the termination byte, thus preserving sort order. Add a working definition for {{STRUCT}}.",non_debt,-
hbase,8089,comment_9,"Nick, The documentation BinarySortableSerde is here: In Hive, it is only used in MapReduce to cut down the cost of the sort during the shuffle.",non_debt,-
hbase,8089,comment_10,"I like the looks of SQLite4's encoding structure [0]. Specifically, numeric values [1], regardless of type, are directly comparable. I think it could be easily extended to support maps and lists. Thoughts? [0]: [0]:",non_debt,-
hbase,8089,comment_11,"The advantages I see for following SQLite4 include: - Serialized values are marked with their type as an initial byte. This is advantageous as serialized values can be sniffed and deserialized by tools ignorant of the application schema. - Numeric types (integral and real numbers) are all normalized to identical encoding. This allows them to be compared directly and provides more flexibility to users. - A C language implementation and tools are readily available for validation, providing test scenarios and as near-complete implementation when we're ready to work on the non-JVM client. The primary detriment I see with using their encoding is the limitation on disallowing null bytes in Strings. The same restriction applies to blobs except for those used as the last value in a compound key. IIRC, this restriction is identical to that imposed by Phoenix.",non_debt,-
hbase,8089,comment_12,It sounds well thought out. Are you thinking we'd be able to add custom types on top of their base types? Could we allow nulls in values? I personally don't mind disallowing them in row keys.,non_debt,-
hbase,8089,comment_13,I don't see why not. We could also cherry-pick the null-safe String and Blob implementations from Orderly if it's a critical feature.,non_debt,-
hbase,8089,comment_14,I'm leaving a comment here since there's way more watchers on this parent ticket. Check out the patch on HBASE-8201 for an implementation of serialization primitives based on the SQLite4 spec.,non_debt,-
hbase,8089,comment_15,Attaching my slides from the Hadoop Summit BoF talk per 's suggestion.,non_debt,-
hbase,8089,comment_16,Nick - what other dependencies does your whole new type library have on HBase besides ByteRange? It would be grand if it were a standalone jar that could be used by other projects without importing hbase-specific libs (which then drag in other dependencies). All of this functionality is really cool and is more likely to gain adoption if it's as easy as possible to drop in existing projects.,non_debt,-
hbase,8089,comment_17,"Matt - the only other thing is a dependency on HBase's Bytes in a couple places. I think this can easily be removed. Part of the point of this effort is to have HBase ship a standard implementation that other Hadoop ecosystem components can rely on. If I just wanted something for my own application, I'd use Orderly and be done with it.",build_debt,over-declared_dependencies
hbase,8089,comment_18,"Hey Nick, It might be worth updating this jira to reflect the latest state of the work. IIUC this work is about proving a client-side library that does the order-preserving serialization, that higher level projects (eg Phoenix & Kiji) can use for row keys and column qualifiers. Per the other jiras, cell serialization, defining types, and schema are out of scope. These are left to higher-level systems which may make different choices (eg in terms of how to create compound keys) and may have different type models, but at least will be able to share serialization. IMO it's worth considering creating a separate project for this as this is genuinely useful outside HBase (eg container formats) and would benefit from multiple language implementations (the serialization here is language agnostic right?) and so the HBase project may end up being a clunky place to maintain things. Thanks, Eli",design_debt,non-optimal_design
hbase,8089,comment_19,"Hi Eli, You're right, I've left this ticket untouched while working through the initial subtasks. The order-preserving serialization is a critical component of the work. I think this is a feature that HBase absolutely must provide if there's to be any hope for interoperability. I also think the serialization format is necessary but not sufficient. An HBase that ships with an API for describing data types and implementations of a set of common definitions takes the next step in interoperability. By defining the type interface, type implementations provided by 3rd parties become pluggable -- it becomes feasible for a user to plug a type from Phoenix into their Kiji application. Systems like Phoenix, Kiji, and HCatalog are all choices for defining and managing schema. It may be the case that HBase should define the schema interfaces as well, but that's definitely beyond the scope here. But if those tools are going to interoperate, they need a common language of types with which to do so. Serialization, IMHO, is insufficient. I don't know if there's a new project to be built out of this work. I see no need to create such a thing when the needs and use are not yet proven. The introduction of types in HBase will shake things up enough as it is, let's see how people and projects use them before promoting this stuff to its own project. Yes, the serialization formats defined in HBASE-8201 are designed to be language agnostic. It's highly likely that I've missed some critical details here or there in the specification. Time will tell :) -n",non_debt,-
hbase,8089,comment_20,So you are thinking more than just a client-side utility lib but an actual facade that does typing (though it is all client-side) as in for example TypedHTable that does something like typedHTable.put(new and int i = typedHTable.get(new HBase internally is still all byte arrays but it'd have this new class that made it look like we could exploit this typing info server-side (e.g. better compression)? I suppose I'd have to register a serializer w/ this new TypedHTable too? Would the serializer be per table?,non_debt,-
hbase,8089,comment_21,"This is a possible direction which I have not thoroughly explored. Some discussion around client-side ease-of-use has started on HBASE-7941. I've had a couple hallway conversations about bringing type awareness into the RegionServer, but none of it concrete.",non_debt,-
hbase,8089,comment_22,"Thanks Nick. Probably worth a broader discussion. The view in HBASE-7941 that HBase is a database and should therefore provide types is pretty different from Bigtable's design: ""In many ways, Bigtable resembles a database: it shares many implementation strategies with databases... but Bigtable provides a different interface than such systems. Bigtable does not support a full relational data model; instead, it provides clients with a simple data model that supports dynamic control over data layout and format, and allows clients to reason about the locality properties of the data represented in the underlying storage... Bigtable also treats data as uninterpreted strings, although clients often serialize various forms of structured and semi-structured data into these strings. Clients can control the locality of their data through careful choices in their schemas."" While you could preserve the flexibility here while providing one implementation of a type model in HBase I think it's an explicit, existing design decision to have HBase support multiple distinct type models in higher level systems. And if those systems want to share code and type models that's great, but IMO HBase is a storage system w/o an explicit type model by design, and we start to lose the above benefits as we bring type awareness into core HBase components like the RS.",non_debt,-
hbase,8089,comment_23,"What is currently actionable for 0.98, a timeframe of a few weeks... ?",non_debt,-
hbase,8089,comment_24,"Of these subtasks, probably performance improvements (HBASE-8694) and type comparisons (HBASE-8863) could be tackled by a willing individual. Client-side API enhancements will take some time for discussion. I think the ImportTSV stuff should be tackled after we've defined a language for type declaration (similar to what we have for Filters in {{ParseFilter}}).",non_debt,-
hbase,8089,comment_25,Unscheduling from 0.98,non_debt,-
hbase,8103,summary,Fix pom so 0.94 can generate site reports,non_debt,-
hbase,8103,description,Site and info plugins are too old. The site plugin is in the wrong place. Can't generate reports w/o update and move of location.,architecture_debt,using_obsolete_technology
hbase,8103,comment_0,Small change to update site and info plugins and moved site out of reporting to be actual plugin.,non_debt,-
hbase,8103,comment_1,Committed to 0.94 branch.,non_debt,-
hbase,8193,summary,should also check if the table is Enabled after successful creation,non_debt,-
hbase,8193,description,Currently only checks in META to say if a table is available or not. It should also check with the zkTable state if it is ENABLED before returning true.,non_debt,-
hbase,8193,comment_0,"A round-trip to ZK is expensive... Should we put this call as optionnal with a parameter? For long running clusters, thise case might never happend. But I also agree that this call might not be done so often so perfs impacts might be small...",code_debt,slow_algorithm
hbase,8193,comment_1,I agree. But this may be needed so that the client is sure that the table is created. I have heard of a use case where every day a table is created and the same is disable and dropped on the next day before the next creation of table. So i thought our apis should enable them to know if a table is created properly or not. Making it as an optional parameter is also fine with me.,non_debt,-
hbase,8241,summary,Fix the bad dependency on netty from HDFS,non_debt,-
hbase,8241,description,"Even if it's fixed on trunk & branch-2, the current version of hdfs still has a previous version of netty, with a different group id. Let's fix this.",non_debt,-
hbase,8241,comment_1,I will commit on trunk and 0.95 tomorrow if there is no objection (or sooner if I have +1).,non_debt,-
hbase,8241,comment_2,1,non_debt,-
hbase,8241,comment_3,"+1 You know how to check it worked N I presume? If need help, new build commands are in release note for HBASE-8187",non_debt,-
hbase,8241,comment_4,"I used mvn dependency:tree. But it this case, the bad dependency breaks one of the integration tests I'm finishing in HBASE-7840, so it was easier :-) To be sure I will try ""mvn clean install -DskipTests javadoc:aggregate site assembly:single"", as with maven triple check is often the right way!",non_debt,-
hbase,8241,comment_5,"Checked, we had two netty versions in the assembly, and now we have only one. Committed, thanks for the review!",non_debt,-
hbase,8256,summary,Add category Flaky for tests which are flaky,test_debt,flaky_test
hbase,8256,description,"To make the Jenkin build more useful, it is good to keep it blue/green. We can mark those flaky tests flaky, and don't run them by default. However, people can still run them. We can also set up a Jekin build just for those flaky tests.",test_debt,flaky_test
hbase,8256,comment_0,From Andy on dev@hbase:,non_debt,-
hbase,8256,comment_1,"I tried to use excludedGroups to exclude the Flaky methods. However, it excludes the whole test class, not just those flaky methods. Per we need to use test suite?",test_debt,flaky_test
hbase,8256,comment_2,"fwiw, I created HBASE-8267 to see if we can link flakyness to some lack of resources.",non_debt,-
hbase,8256,comment_3,"This patch is a work-in-progress. I'd like to make sure the approach looks right. I have tried the hbase-common module, things look good. However, I have removed the group based testing to make this work.",non_debt,-
hbase,8256,comment_4,"In the hbase-common module, the following works as expected: mvn clean test -PrunAllTests mvn clean test -PrunSmallTests mvn clean test -PrunMediumTests mvn clean test -PrunLargeTests mvn clean test -PrunFlakyTests",non_debt,-
hbase,8256,comment_5,"Other than the group based testing issue, one more issue is that it can't pick up those old test cases extending TestCase. We need to convert them.",non_debt,-
hbase,8256,comment_6,"The patch touches the same code as HBASE-4955. Not a huge deal, I can redo it. - I fear your may end up with something that will work only with our version of surefire, and we will get into new surefire regressions when we will use the official version. - The last version of surefire may contains fixes that you could use (I've seen some stuff around categories). A quick word of what I had in mind myself (that's thtake agese content of the patch v2 in HBASE-4955) - migrate to surefire 2.14.1 or 2.15 if the regressions we get with these versions are acceptable. - remove the profiles no one use such as runSmallTests - for small tests, use the new feature 'reuseFork' of surefire: this is the only way (in the official release) to have separate logs per test without paying the cost of a fork per single test. - for medium and large, as today: multiple fork in parallel. If what you do does not break this, I will be happy :-) The ideal solution would be to migrate to the official version before doing this, but I must agree but it can take longer than expected.",code_debt,dead_code
hbase,8256,comment_7,can you create an r?,non_debt,-
hbase,8256,comment_8,", could you please feel free to modify the patch as needed and post a new one? I tried to monitor the JVM/threads used by surefire, and didn't see the number changes much. I am sure I have missed some surefire settings. , what's an r?",non_debt,-
hbase,8256,comment_9,"I've documented the settings in the reference guide. Unfortunately, the surefire settings changed in 2.14, and it's not perfectly documented (see HBASE-4955 for how to use them), hence the risk of doing something that will have to be redone very soon...",documentation_debt,outdated_documentation
hbase,8256,comment_10,I think this is ok because we went the flakey test category to be an empty set ideally. So lets get the builds green by segregating them and then fix them before we have to worry about new versions of surefire. Easy. :-),test_debt,flaky_test
hbase,8256,comment_11,r is review board review :) It has /r/ in url,non_debt,-
hbase,8256,comment_12,I see. Posted on RB:,non_debt,-
hbase,8256,comment_13,+1 as long as it works :),non_debt,-
hbase,8256,comment_14,How can you get away w/ this Jimmy? Removing from hbase-client? You inherit from parent pom? Or removing this -- skipClientTests? It looks useful? Ditto for hbase-common? This is a radical patch Jimmy. What is going on in the top level pom revoving stuff like firstPartForkMode? Defer to  He knows this carry-on best.,non_debt,-
hbase,8256,comment_15,"I would prefer to keep a real 'small tests' category. Sharing the JVM for small tests saves us from a lot of fork and makes tests faster. As we're supposed to have more and more tests, I would love to have more and more *small* tests. The setting to use with newer surefire version is 'reuseFork'. last time I checked, test methods and categories were buggy, but may be it has been fixed. Would it work to: - use an exclusion group fo 'Flaky' Category - add a profile to run only the 'Flaky' You said previously that you need as well to convert the old test cases extending TestCase. May be this could be done in a separate JIRA?",test_debt,flaky_test
hbase,8256,comment_16,", Yes, I added a profile to run only flaky tests. I used categories to exclude flaky category. It works actually so far. As to converting old test cases extending TestCase, we can do it in a separate Jira. As Andy said, we can have this patch in, then you fix those surefire setting issue separately. It is also fine with me that you take this over and fix as you like before let the patch in. Yes, the patch is not as small as I expected. Sub-modules can inherit from parent pom. As to the pom related changes, I can defer to  for further fixes/improvements.",test_debt,flaky_test
hbase,8256,comment_17,"Sorry I wasn't clear: I was thinking: with this category, why do you need to remove the two steps (small / medium & large) build?",non_debt,-
hbase,8256,comment_18,"For surefire, I'm currently stuck. I have an issue with the 2.14.1 I tried to workaround it without success. I also failed to reproduce it outside of HBase. I will need to create a JIRA in surefire, but the repro scenario will be with HBase so it will be more difficult to get a fix...",non_debt,-
hbase,8256,comment_19,"The group based executions don't work very well with categories. For categories, I can make it work with includes, not groups. Could it be an surefire thing?",non_debt,-
hbase,8256,comment_20,Likely. And may be fixed in the latest versions (the ones I can't make working).,non_debt,-
hbase,8256,comment_21,"Posted a new patch which is not as radical. The issue with this patch is that if any method is marked as flaky, the whole test class is ignored in runAllTests, although runFlakyTests works fine. This could be related to SUREFIRE-862. Does our surefire have this fix?",test_debt,flaky_test
hbase,8256,comment_23,"No, the fix is from august '12, and our fork from january '12.",non_debt,-
hbase,8256,comment_24,"Ok, so this one depends on HBASE-4955, using a newer surefire?",non_debt,-
hbase,8256,comment_25,It appears collectively we have decided not to do this. Reopen if you want to try this again some time in the future.,non_debt,-
hbase,8300,summary,fails to delete files due to open handles left when region is split,non_debt,-
hbase,8300,description,This issue is related to HBASE-6823. logs below. Failed delete of Method) Failed delete of Method) Failed delete of Method),non_debt,-
hbase,8300,comment_0,The attached patch adds a line to close the reader of the parent region after it's used to compare top/bottom keys. The reader was left active after the parent region is closed.,non_debt,-
hbase,8300,comment_1,Thanks Malie. Did you test the patch by running the test on windows?,non_debt,-
hbase,8300,comment_2,Yes. It works for me on windows.,non_debt,-
hbase,8300,comment_3,Patch looks good. Will commit if hadoopqa passes.,non_debt,-
hbase,8300,comment_4,Reattching the patch to trigger hadoopqa. Somehow it is not picking this up.,non_debt,-
hbase,8300,comment_6,"No new tests are included because three existing tests were broken, and the patch fixes the broken tests.",non_debt,-
hbase,8300,comment_7,any idea why The patch does not appear to apply with p0 to p2?,non_debt,-
hbase,8300,comment_8,"The patch seems taken from hbase-server instead of the root so you get instead of I'm +1 on the patch, just add a comment that says that the method will close the StoreFile.",non_debt,-
hbase,8300,comment_9,do you mean to add a comment in the code or just here?,non_debt,-
hbase,8300,comment_10,"Malie, could you please regenerate the patch from the top level, and reattach it here. Thanks.",non_debt,-
hbase,8300,comment_11,Regenerated and attached as,non_debt,-
hbase,8300,comment_13,oops. now I use command line svn client to regenerate. Attached as,non_debt,-
hbase,8300,comment_15,Committed this to trunk and 0.95. Thanks Malie.,non_debt,-
hbase,8300,comment_16,"Enis, thanks for reviewing it.",non_debt,-
hbase,8300,comment_20,"The fix in this bug is incomplete, HBASE-13585 will provide a complete fix.",non_debt,-
hbase,8324,summary,fails against hadoop2 profile,non_debt,-
hbase,8324,description,Two tests cases are failing:,non_debt,-
hbase,8324,comment_0,jps says the MRAppMaster and YarnChild processes are spawned but these processes are essentially black boxes.,non_debt,-
hbase,8324,comment_1,"Talked with , and we found this in the MRAppMaster's logs: This is related to MAPREDUCE-4880 which is in turn fixed by MAPREDUCE-4607 (a race in speculative task execution, fixed in 2.0.3-alpha). Compiling and running against hadoop-2.0.3-alpha fails out even earlier so instead of going that route, I'm going to try an alternate workaround -- disabling mapper and reducer speculative execution.",design_debt,non-optimal_design
hbase,8324,comment_2,And it passes consistently now.,non_debt,-
hbase,8324,comment_3,patch uses the hadoop.profile=2.0 trick to do the precommit run against hadoop2.,non_debt,-
hbase,8324,comment_5,v2 improves comments on patch.,documentation_debt,low_quality_documentation
hbase,8324,comment_6,I didn't modify any javadoc so I don't believe this patch was responsible for that warning. Committed to 95/trunk,non_debt,-
hbase,8324,comment_8,Please TODO in this comment or file a new ticket to remove this configuration tweak after the issue is resolved upstream. I don't want one-offs like this to become lost and forgotten.,requirement_debt,requirement_partially_implemented
hbase,8324,comment_9,I don't think this is a one-off -- this would affect all of our MR tests that use more than one mapper/reducer. I don't think speculative execution should be on for our MR tests in general.,design_debt,non-optimal_design
hbase,8324,comment_10,"Fair enough. However, that means we are leaving HBase's interaction with this feature untested. Does that mean we will advise users against using it with HBase? If we're not going to test it, that means we're not taking responsibility for it; thus I conclude that we must ""officially drop support"" for the feature, however that's done.",test_debt,lack_of_tests
hbase,8324,comment_11,"This unit test is to test the HFileOutputFormat; it should essentially be proof that the HFileOutputFormat works as expected. Making sure the MR implementation is correct is out of scope and not hbase's resonsibility. I think it does make sense to say that hadoop2 needs to have a release notes saying not to use speculative execution (or jobs can fail if speculative execution is on). Taken to a silly extreme, we'd need to run this and all the other mini mr cluster tests against a wide array of different MR configurations. For the sake of a unit test, I think two (hadoop1 and hadoop2) is plenty enough.",non_debt,-
hbase,8324,comment_12,"I maintain that it's dangerous to users if we make a habit of disabling code-paths that exercise known issues w/o notifying users of the same. In this specific instance, I defer to your judgement.",non_debt,-
hbase,8510,summary,HBASE-8469 added a hdfs-site.xml file for tests but it gets included in the test jar,non_debt,-
hbase,8510,description,found in his tests that HBase recently started ignoring hdfs-site.xml when it's on the classpath. I found that HBASE-8469 added an hdfs-site.xml for the unit tests but it's not excluded when building the jar.,non_debt,-
hbase,8510,comment_0,Attaching the one-line exclude from hbase-server's pom file. I verified that the hdfs-site.xml is not in the jar after the change.,non_debt,-
hbase,8510,comment_1,+1 if the unit tests from HBASE-8469 don't get rebroken.,non_debt,-
hbase,8510,comment_2,"God I hate how submit patch and start progress are next to each other and keep switching place. Anyways, I'm marking patch available to have a run.",non_debt,-
hbase,8510,comment_3,Please also run some of the tests broken in HBASE-8469 against hadoop 2.0.5-SNAPSHOT. If passes along with the pre commit run I'd be satisfied.,non_debt,-
hbase,8510,comment_4,TestFSUtils passes fine.,non_debt,-
hbase,8510,comment_6,Pretty sure TestAdmin is unrelated.,non_debt,-
hbase,8510,comment_7,Committed to trunk and 0.95,non_debt,-
hbase,8527,summary,clean up code around compaction completion in HStore,code_debt,low_quality_code
hbase,8527,description,The methods completeCompaction and it's caller are too long. Something I changed while doing something else there; putting in a separate easy-to-review JIRA to make the other future change smaller.,non_debt,-
hbase,8527,comment_0,"Move some stuff into methods, mostly",non_debt,-
hbase,8527,comment_2,I see the call is moved. Better move the comment as well.,non_debt,-
hbase,8527,comment_3,"I shortened the comment, and did move it",non_debt,-
hbase,8527,comment_4,Good. +1,non_debt,-
hbase,8527,comment_5,lgtm. Minor nit: Should there be a comment on when the point of no return is?,code_debt,low_quality_code
hbase,8527,comment_6,Will add comment on commit,non_debt,-
hbase,8527,comment_7,attaching patch,non_debt,-
hbase,8608,summary,Do an edit of logs.. we log too much.,design_debt,non-optimal_design
hbase,8608,description,"Since we enabled blooms, there is a bunch of bloom spew.... We also do a bunch of logging around new file open....",design_debt,non-optimal_design
hbase,8608,comment_0,Minor edit moving stuff to trace level.,non_debt,-
hbase,8608,comment_1,What I committed to trunk and 0.95.,non_debt,-
hbase,8608,comment_2,"This check is probably not useful, string is already built",non_debt,-
hbase,8608,comment_3,agree,non_debt,-
hbase,8608,comment_8,Should this be resolved?,non_debt,-
hbase,8608,comment_9,Committed a while back (Thanks ),non_debt,-
hbase,8618,summary,Master is providing dead RegionServer ServerName's to the balancer,non_debt,-
hbase,8618,description,The balancer should not be passed any ServerName's for RS's that are dead.,non_debt,-
hbase,8618,comment_0,Remove the server name from RegionStates when a region server is shut down (failed or graceful). Previously the regions were just removed; now the whole hash entry is removed.,non_debt,-
hbase,8618,comment_1,Good on you ... running by hadoopqa,non_debt,-
hbase,8618,comment_3,Committed to trunk and 0.95. Thanks Elliott.,non_debt,-
hbase,8618,comment_8,"technically a dup of HBASE-8250, closed that one",non_debt,-
hbase,8665,summary,bad compaction priority behavior in queue can cause store to be blocked,non_debt,-
hbase,8665,description,"Note that this can be solved by bumping up the number of compaction threads but still it seems like this priority ""inversion"" should be dealt with. There's a store with 1 big file and 3 flushes (1 2 3 4) sitting around and minding its own business when it decides to compact. Compaction (2 3 4) is created and put in queue, it's low priority, so it doesn't get out of the queue for some time - other stores are compacting. Meanwhile more files are flushed and at (1 2 3 4 5 6 7) it decides to compact (5 6 7). This compaction now has higher priority than the first one. After that if the load is high it enters vicious cycle of compacting and compacting files as they arrive, with store being blocked on and off, with the (2 3 4) compaction staying in queue for up to ~20 minutes (that I've seen). I wonder why we do thing thing where we queue compaction and compact separately. Perhaps we should take snapshot of all store priorities, then do select in order and execute the first compaction we find. This will need starvation safeguard too but should probably be better. Btw, exploring compaction policy may be more prone to this, as it can select files from the middle, not just beginning, which, given the treatment of already selected files that was not changed from the old ratio-based one (all files with lower seqNums than the ones selected are also ineligible for further selection), will make more files ineligible (e.g. imagine with 10 blocking files, with 8 present (1-8), (6 7 8) being selected and getting stuck). Today I see the case that would also apply to old policy, but yesterday I saw file distribution something like this: 4,5g, 2,1g, 295,9m, 113,3m, 68,0m, 67,8m, 1,1g, 295,1m, 100,4m, unfortunately w/o enough logs to figure out how it resulted.",design_debt,non-optimal_design
hbase,8665,comment_0,"Nice catch, and great explanation. Thanks",non_debt,-
hbase,8665,comment_1,What do you think? I have no historical context on why selection and compaction were separated.,non_debt,-
hbase,8665,comment_3,"We discussed various options and their relative advantages and disadvantages here. The meta-cause of this issue is that priority is not really a property a particular compaction, it's property of the store, and the mismatch between priority attached to compaction based on some store state that becomes stale, and the current store priority, causes the inversion-like behavior. The conclusion is that the simplest approach appears to be to put compaction in the queue w/o selection. That way whenever we get the store from the queue we can get best up-to-date selection. We can discard records that are no longer relevant (if we get one from queue and cannot select). Other schemes like dynamically updating queue on every change to store, or enumerating stores, seem to be more the latter especially in case if there are multiple threads.",non_debt,-
hbase,8665,comment_4,"Strongly disagree. A major compaction for an important store is not nearly as important as a compaction to get under the blocking storefile count. Talked with JD and the compaction priority was put in so that compactions queued to un-block a flush can be given a higher priority. With that in mind I think the best solution that I've heard so far, with the least code churn, is to raise the priority on any non major compactions in the queue when we are thinking about queueing another. The other harder more invasive solution that we (JD, Jon, and I) talked about was to put in real scheduling. Something like the linux O(1) scheduler (with an active and expired set of queues) would work. The idea is pretty well tested and shouldn't be too hard to code. Though I don't think that we really need that yet.",non_debt,-
hbase,8665,comment_5,"W.r.t. property of the store: 1) Current priority is derived entirely from store state. In fact it only depends on number of files and blocking limit, so there's no such thing as important store other than store. 2) It seems to be the obvious cause of this particular issue, see above. When selection is added to queue, its priority is low. As store fills up it's ""real"" priority changes, but we have no way to influence it. Bumping its priority when we queue another one is just a hack around that imho; if it even helps. If blocked store comes into a queue full of non-blocked stores why should we bump them? And how much. 3) Additionally if we pre-select in advance we simply end up with inefficient compactions, for example in this case we could have compacted 6 small files in one go, but would have ended up compacting 3 and 3 if it spent less time in the queue.",code_debt,low_quality_code
hbase,8665,comment_6,I was suggesting bumping the priority instead of queueing another. Bumping the compaction request to what would currently be computed seems reasonable. In this case pre-selecting doesn't seem to be a bad thing. Prioritizing a fast compaction over being a little more efficient seems like a trade off that most people would want. So since current implementation biases towards what I would expect users to want maybe we shouldn't change that until we can put those smarts into the compaction selection (plumbing reason the compaction was requested into selection).,design_debt,non-optimal_design
hbase,8665,comment_7,"Well, the effect of getting a faster compaction in this case is a pure accident, if there was not a smaller one already queued, it would still compact 6 according to policy. Also, out of many possible faster compactions in this case, bad one (later files) is chosen, so it's not really what user would expect. Policy should make such decisions - if we prefer faster compactions for blocked store, we should have it in the policy, and so last-moment selection would still choose the best one. As for bumping the priority of current to what it would have been, it is actually equivalent to just sorting them by current store priority... I wonder if there's any fundamental reason to divorce selection from compaction? If we introduce compaction-based priority modifiers, not just store based, we could still apply them by doing selection in multiple stores and comparing priorities. Selecting is not that expensive, given how frequently we compact.",design_debt,non-optimal_design
hbase,8665,comment_8,We already do that. If we think we're blocked then the exploring compaction policy chooses the smallest set of files.,non_debt,-
hbase,8665,comment_9,It only does that if nothing is in ratio. I filed a JIRA for that...,non_debt,-
hbase,8665,comment_10,"Will submit some simple patch tomorrow, we are hitting this a lot under high load. As soon as I can make the test slightly less ugly.",code_debt,low_quality_code
hbase,8665,comment_11,"Here's the patch. It's rather simple, most complexity is in tests. It allows for both pre-selected compactions (from coprocessors and users) and non-pre-selected. When non-preselected compaction makes it to run() it is selected and executed. There are two special cases: 1) Store priority might have decreased since the compaction was queued; in that case it's re-queued with new priority to avoid inversion. 2) Without selecting, we don't know which pool to go to. We go to small pool by default, and if the compaction is large after selection, queue it to the large pool; it can bounce back if it becomes small while stuck in the large pool. Both of these cases can cause compaction to get continuously requeued, or bounced between pools, however it can only happen if store priority decreases (i.e. other compactions happen), or files are removed from store (same); or files are added in some very special pattern that causes policy to select continuously).",design_debt,non-optimal_design
hbase,8665,comment_12,Mind adding javadoc for the selectNow parameter ? Add debug log for the above case ? Please add license header for Can be private ?,documentation_debt,outdated_documentation
hbase,8665,comment_14,This test passes locally,non_debt,-
hbase,8665,comment_15,ping?,non_debt,-
hbase,8665,comment_16,"Sorry, looking now.",non_debt,-
hbase,8665,comment_17,needs a header (on commit please). Other than that I'm +1,non_debt,-
hbase,8665,comment_18,ok for 95?,non_debt,-
hbase,8665,comment_20,ping?,non_debt,-
hbase,8665,comment_21,"I've changed the criticality to blocker, this happens in all my tests when I try to insert more than ~220m lines with ycsb on a 5 nodes cluster with 2 clients (once it's stuck it remains stuck forever; even if you stop the clients).",non_debt,-
hbase,8665,comment_22,+1 for 0.95 (did a quick scan). Needs license added on commit as per Elliott. Thanks,non_debt,-
hbase,8800,summary,Return non-zero exit codes when a region server aborts,non_debt,-
hbase,8800,description,"There's a few exit code-related jiras flying around, but it seems that at least for the region server we have a bigger problem: it always returns 0 when exiting once it's started. I also saw that we have a couple -1 as exit codes, AFAIK this should be 1 (or at least a positive number).",non_debt,-
hbase,8800,comment_0,"Adding the same sort of mechanism that is used in HMasterCommandLine to get the state of the RS. If it aborts, we return 1. I also changed all the -1 to 1. Let's see if I break any unit test...",non_debt,-
hbase,8800,comment_1,"+1 on patch. Poking around, postive on error is more usual.",non_debt,-
hbase,8800,comment_3,"failure is something else, so I'll go ahead and commit to trunk and 0.95 , any interest in 0.94?",non_debt,-
hbase,8800,comment_4,+1 lgtm.,non_debt,-
hbase,8800,comment_9,Resolving as committed. Probably too radical a change for 0.94 but will let  make call.,non_debt,-
hbase,8816,summary,Add support of loading multiple tables into LoadTestTool,non_debt,-
hbase,8816,description,"Introducing an optional parameter 'num_tables' into LoadTestTool. When it's specified with positive integer n, LoadTestTool will load n tables parallely. -tn parameter value becomes table name prefix. Tables are created with name in format <tn The motivation is to add a handy way to load multiple tables concurrently. In addition, we could use this option to test resource leakage of long running clients.",design_debt,non-optimal_design
hbase,8816,comment_1,"Should we change argument name concurrent_factor to num_tables? concurrent_factor is not that descriptive. You have changed the parsing of args from being in run(String[]) method to being in main. This breaks the usage model for the Tool interface. We should not do parsing in static main. Maybe you can use the instance which parsed the command line opts as the controller, and spawn all the other LTT instances from that, and join in the end. Can you please update the patch for trunk as well as 0.94. I think this is only for 0.94.",design_debt,non-optimal_design
hbase,8816,comment_2,Thanks  for the comments! I incorporated your feedbacks into v1 patch.,non_debt,-
hbase,8816,comment_4,"Why 64 for upper limit on num_tables? We can do Short.MAX_VALUE. We need a trunk version of the patch as well. Lastly, instead of the command line parsing wizardry for passing a modified version of the args to child classes, maybe you can just construct LoadTestTool instances with the same args in WorkerThread (calling processOptions manually), and then call setNumTables(1), and + ""_"" + i).",design_debt,non-optimal_design
hbase,8816,comment_5,Incorporated 's latest comments. Will submit a trunk patch soon. Thanks.,non_debt,-
hbase,8816,comment_7,"Ok, marked fixed versions. Let's get this in for trunk and 0.94.10.",non_debt,-
hbase,8816,comment_8,How about passing a comma-separated list of table names to the -tn parameter? That way the one table is simply a special case.,design_debt,non-optimal_design
hbase,8816,comment_9,"Sounds good, but if you want to run with 50 tables, 10 writer/readers each, then passing that parameter will become ugly, no?",design_debt,non-optimal_design
hbase,8816,comment_10,Fair enough. Whatever is best for your scenario.,non_debt,-
hbase,8816,comment_11,Wanna commit? Otherwise I can push to 0.94.12.,non_debt,-
hbase,8816,comment_12,The patch looks good. There is a typo in Can be addressed at commit. Jeff please commit the trunk first though.,documentation_debt,low_quality_documentation
hbase,8816,comment_13,Upload trunk patch and will check in soon after 0.94.11 is out.,non_debt,-
hbase,8816,comment_15,Let's get this into 0.94.11.,non_debt,-
hbase,8816,comment_16,"Integrated the v3 patch into trunk, 0.95 and 0.94.11 branch. Thanks.",non_debt,-
hbase,8860,summary,times out sometimes,non_debt,-
hbase,8860,description,"It times out after running for 10 minutes. From the log, we can see there are already 5 regions. Just need one more to pass the test. I think we can make the test pass as long as there are 5 regions.",non_debt,-
hbase,8860,comment_0,The patch makes the test pass as long as there are 5 regions instead of 6 regions.,non_debt,-
hbase,8860,comment_2,"I was going to say commit this but do not close the issue as fixed and then I saw this comment: // This is not an exact test So, +1 on patch and on closing. Can open new issue if we fail again. Thanks .",non_debt,-
hbase,8860,comment_3,Integrated into trunk and 0.95. Thanks a lot for reviewing it.,non_debt,-
hbase,8904,summary,Clean up IntegrationTestMTTR and add small features.,non_debt,-
hbase,8904,description,* Clean up the Long overflow (oops) * Clean up the usage of tableName vs tableNameBytes * Add in more than just max time. * Add in tracing.,code_debt,low_quality_code
hbase,8904,comment_0,Here's a patch that adds tracing and better timing. results now look like: Which with some formatting looks like:,non_debt,-
hbase,8904,comment_2,lgtm if it runs on your cluster.,non_debt,-
hbase,8993,summary,fails,non_debt,-
hbase,8993,description,Test looked to be making progress though perhaps a stall here: May have just been taking time loading data. Let me up the timeout and add debug because hard tracking where we are in this issue going by logs. {code},non_debt,-
hbase,8993,comment_0,Will commit this extra logging and upped timeout.,non_debt,-
hbase,8993,comment_1,Committed 8993.txt to trunk and 0.95. Leaving the issue open in case fails again soon.,non_debt,-
hbase,8993,comment_4,Hasn't failed in a good while. Resolving.,non_debt,-
hbase,9010,summary,Reenable,non_debt,-
hbase,9010,description,See HBASE-9009 where we disable it because it fails in mysterious ways.,non_debt,-
hbase,9010,comment_0,HBASE-10774,non_debt,-
hbase,9052,summary,Prevent split/merged region from assigning again,non_debt,-
hbase,9052,description,"If a region is split/merged, before it's removed from meta, you can still assign it from the HBase shell. It's better to prevent this from happening.",design_debt,non-optimal_design
hbase,9052,comment_0,"Attached a patch which moves split region to SPLIT state, merged region to MERGED state, so they won't be assigned any more unless its state is changed.",non_debt,-
hbase,9052,comment_1,nit: regionOffline should be renamed setRegionOffline or offlineRegion? I suppose you are following the precedent where this operation is done in a method named regionOffline (reads oddly). I see tightening up of allowed states but how are we prevening assign of MERGE and SPLIT?,code_debt,low_quality_code
hbase,9052,comment_3,"Sure, I will change to That's prevented by the AM#assign method already. For regular assignment, it requires the region to be in offline/closed state. For forcing assignment, it requires the region to be in a closing(or pending close), opening(or pending open). Now, since we move the region to SPLIT/MERGE state, they won't be assigned any more unless the state is changed to offline/closed. If master fails over, we lose these states. However, the merged regions are not known the new master any more since they are deleted from meta. So they won't be assigned either. For split region, the isSplit/isOffline flag is used in rebuilding user regions so they won't be picked up either.",non_debt,-
hbase,9052,comment_4,1,non_debt,-
hbase,9052,comment_5,"is already used in many places, even in the access control coprocessor. Probably we will leave the naming like this?",non_debt,-
hbase,9052,comment_6,Yeah. Its a bad name but should be consistent.,code_debt,low_quality_code
hbase,9052,comment_7,"Thanks. Integrated into trunk and 0.95. For 0.95, the unit test was revised a little due to code difference.",non_debt,-
hbase,9125,summary,TestJoinedScanners fails,non_debt,-
hbase,9125,comment_0,That looks dangerous.,non_debt,-
hbase,9125,comment_1,see HBASE-7606,non_debt,-
hbase,9131,summary,Add admin-level documention about configuration and usage of the Bucket Cache,non_debt,-
hbase,9131,description,HBASE-7404 added the bucket cache but its configuration settings are currently undocumented. Without documentation developers would be the only ones aware of the feature. Specifically documentation about slide 23 from would be great to add!,documentation_debt,outdated_documentation
hbase,9131,comment_0,Upload the patch of BucketCache documentation to book.xml.  Any suggestions?,non_debt,-
hbase,9131,comment_1,"Thanks. I think we are somewhere between too little detail and too much detail. First, can we add the config variables to hbase-default.xml (with full descriptions and with units). Now to the meat: The patch doesn't tell the admin why or when they'd want to consider using this. The link/pdf requires having to search for the bucket cache sections in the 2nd page and then goes on into too much design detail for an average admin. (It also lacks the config variables / instructions). My suggestion: Take let's take the high-level parts from section 3 of the pdf, polish it and add it to the official docs. Here's a stab at the sections that I think would be good for the ref guide with the prose improved a little bit: Let me know what you think, and feel free to update/correct the draft.",documentation_debt,low_quality_documentation
hbase,9131,comment_2,Resolving as done by changes to doc by Misty recently and by HBASE-11364,non_debt,-
hbase,9147,summary,Mechanism to limit system table deletion and creation in non-secure mode,non_debt,-
hbase,9147,description,None,non_debt,-
hbase,9147,comment_0,HBASE-10619 will handle this. what say  ?,non_debt,-
hbase,9147,comment_1,Yep. There is a patch in HBASE-10619 and I will commit shortly. I will close this as dup.,non_debt,-
hbase,9158,summary,Serious bug in cyclic replication,non_debt,-
hbase,9158,description,"While studying the code for HBASE-7709, I found a serious bug in the current cyclic replication code. The problem is here in Now note that edits replicated from remote cluster and local edits might interleave in the WAL, we might also receive edit from multiple remote clusters. Hence that <walEdit Fixing this in doMiniBatchMutation seems tricky to do efficiently (imagine we get a batch with cluster1, cluster2, cluster1, cluster2, ..., in that case each edit would have to be its own batch). The coprocessor handling would also be difficult. The other option is create batches of Puts grouped by the cluster id in this is not as general, but equally correct. This is the approach I would favor. Lastly this is very hard to verify in a unittest.",non_debt,-
hbase,9158,comment_0,Maybe it can be added to doMiniBatchMutation after all. Will work on a patch tomorrow.,non_debt,-
hbase,9158,comment_1,"Here's a possibility fixing it in This is probably not correct, as we now have multiple calls to and hence log-appending is no longer atomic; what if the 3rd call fails?",non_debt,-
hbase,9158,comment_2,"Here is why this is serious. Say we have the following cycle setup: A -Now image on cluster C we get a batch of edits from B. Now also say it just so happens that the first edit in the batch is from A and the following ones are from B. When we execute the Put all edits will be tagged with A as he source, and thus none of B's edit will make it to A when it is C's turn to replicate to A.",non_debt,-
hbase,9158,comment_3,"Patch that does the grouping at the ReplicationSink. Since replication sink is the only part that ever gets edits from different clusters, this is correct. (Also made uses the new API from HBASE-6580) Please let know what you think. I'll make a trunk patch and would to commit this in time for 0.94.11.",non_debt,-
hbase,9158,comment_4,Cleaned up 0.94 patch.,code_debt,low_quality_code
hbase,9158,comment_5,Same patch for trunk. (Needed addendum in HBASE-8408),non_debt,-
hbase,9158,comment_6,Let's get a hadoop qa run.,non_debt,-
hbase,9158,comment_7,This is the right one.,non_debt,-
hbase,9158,comment_9,Yeah... That does not look good.,non_debt,-
hbase,9158,comment_10,Should fix the tests (0.94),non_debt,-
hbase,9158,comment_11,And for trunk. (just one character change :) ),non_debt,-
hbase,9158,comment_12,"Very nice catch Lars, this is really important! Do you think this could be easy to unit test? In TestReplicationSink you could pass a mixed bag of edits, not sure how you'd verify them after though.",non_debt,-
hbase,9158,comment_13,"Just adds a tests. I just used the setup There we three cluster 1,2,and 3 in a cycle. What I do is this: # disable peer 3 on 2 # Put a row1 into 1, wait until that replicates to 2 # Put a row2 into 2. So now we have the row1 and row2 in the WAL at 2, in that order # reenable peer 3 on 2 # without the fix the row2 would not be replicated back to 1, because on the way from 2 to 3 it would be retagged with 1's cluster id. (takes more writing to explain the test than writing the actual test)",non_debt,-
hbase,9158,comment_14,"Ah, clever test. I'll try it out then you have my +1.",non_debt,-
hbase,9158,comment_15,"Thanks :) BTW. the commented configs in the test are not needed, will remove them upon commit (if everything else works out)",non_debt,-
hbase,9158,comment_16,"So I tried the test with and without the patch to ReplicationSink, and it behaves as expected in both cases 100% of the time. +1",non_debt,-
hbase,9158,comment_18,"Some hanging tests, not related (the replication related tests are passing). I'm not changing any Javadoc, so also unrelated. Committed to 0.94, 0.95, and trunk. Pfeeww. Thanks for the review, J_D.",non_debt,-
hbase,9222,summary,Thrift DemoClient failed with error length is 0),non_debt,-
hbase,9222,description,We make it illegal passing null row to Put/Delete from hbase-8101. While Thrift demo client still verify empty row situation as following:,non_debt,-
hbase,9222,comment_0,Removed the empty row scenario from DemoClient because it's illegal situation from now on.,non_debt,-
hbase,9222,comment_2,1,non_debt,-
hbase,9222,comment_3,Thanks for the reviews. I integrated the patch into 0.95 and trunk branch. Thanks.,non_debt,-
hbase,9231,summary,Multipage book is generated to the wrong location,non_debt,-
hbase,9231,description,None,non_debt,-
hbase,9231,comment_0,Trivial change. Fixes the issue for me. Not sure if that other negative side effect. ?,non_debt,-
hbase,9231,comment_1,"Go for it. Check the generated website. See along left tab. Can you get single page and multipage version? If so, commit.",non_debt,-
hbase,9231,comment_2,"Doubled checked. All good. Committed to 0.94. (I don't think this is an issue in 0.95+, but need to check)",non_debt,-
hbase,9303,summary,Snapshot restore of table which splits after snapshot was taken encounters 'Region is not online',non_debt,-
hbase,9303,description,Take snapshot of a table ('tablethree' in the log). Put some data in the table and split the table. Restore snapshot. Table cannot be enabled due to:,non_debt,-
hbase,9303,comment_0,"Logs files. Snapshot for tablethree was taken. When restoring, region couldn't get online.",non_debt,-
hbase,9303,comment_1,"added a safe patch, to avoid all the problems.. reset all the region state to OFFLINE and clears completely .META. for the restored table. Even if 94 doesn't seems to be affected, probably because the AssignmentManager is less strict, I'm going to apply the patch to 94 too. the patch should be the same.",non_debt,-
hbase,9303,comment_2,What is special about this c32e63 region? is this a split parent or daughter?,non_debt,-
hbase,9303,comment_3,"split parent, the assignment manger will not try to assign a SPLIT and the restore will not rewrite the region as ""normal region"" but it will leave the SPLIT attribute",non_debt,-
hbase,9303,comment_4,It was the parent:,non_debt,-
hbase,9303,comment_5,"can you explain the internal chain of events / root cause of the problem is? all I know currently is something with restore, split, and the content of meta.",non_debt,-
hbase,9303,comment_6,"Ok, so we have a split parent with the SPLIT marker in the hri in meta. Is this the hri saved off as part of the snapshot manifest, or is it only in the meta hri? Is the problem only that the meta entry has SPLIT as an attribute? Could we have a unit test where we force meta to have the split marker and the restore?",test_debt,lack_of_tests
hbase,9303,comment_7,"let me try to explain the problem... the restore tries to do a diff between the current state and the snapshot state snapshot regions already present in the current state will not be removed from META this means that your meta will contain extra information from the current state (not present at the time of the snapshot) In this specific case, the parent split region is now marked as SPLIT. so the assignment manager knows that this region should never be assigned again. At this point we restore... the current split parent goes back to be a normal region.. but in meta and in the assignment manager memory state is still marked as split.. so not assigned... At this point your table has a missing region that will never go online.",non_debt,-
hbase,9303,comment_8,+1 on patch v1.,non_debt,-
hbase,9303,comment_9,"ok, this explanation is really helpful, and I buy it. There are some subtle things going on here, so can we add comments in the code about why there are two mutate calls and why we must first delete and then rewrite (instead of reusing like we did before)?",code_debt,low_quality_code
hbase,9303,comment_11,v2 is lovely. +1.,non_debt,-
hbase,9303,comment_12,Looks good. Need this in 0.94 as well. (It's a bit disconcerting that we have missed this up to now),non_debt,-
hbase,9315,summary,fails on suse,non_debt,-
hbase,9315,description,One of our build machines is consistently having trouble with this test.,non_debt,-
hbase,9315,comment_0,"The test calculates a cache size and block size such that 9 blocks will fit in cache, and then inserts one extra block. Sometimes two single-block evictions happen instead of one single-block or double-block eviction. My best guess for the reason is the delta between minSize and acceptableSize is interacting with the few bytes of free space between the total size of all cached blocks and the cache size. There also appears to be a race between when System.out.println is called and the assert on the following line -- often ""Background Evictions run: 1"" is printed even when the assertion fails.",non_debt,-
hbase,9315,comment_1,This patch alters the test to wait for the cache to stabilize before asserting on the number of evictions run. This may be overkill.,design_debt,non-optimal_design
hbase,9315,comment_2,Patch looks good. It passes on your suse box now Nick? I triggered hadoopqa build.,non_debt,-
hbase,9315,comment_3,"It passes when I run the test manually. Just about to kick of a build now, as it happens.",non_debt,-
hbase,9315,comment_4,"Test passes on this side. I'm curious for someone with more experience in the LRU cache to have a look at my comment, is my reasoning for why this fails sound? Should I be fixing something different in the test setup?",non_debt,-
hbase,9315,comment_7,Committed to 0.95 and trunk. Thanks N.,non_debt,-
hbase,9315,comment_12,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,9340,summary,revoke 'user' throws,non_debt,-
hbase,9340,description,"Trying to revoke a global rights throws The problem is that jruby is not able to do the bind with revoke(..., actions)",non_debt,-
hbase,9340,comment_0,+1 if works for you M,non_debt,-
hbase,9340,comment_1,nit: no need for that extra newline.,code_debt,low_quality_code
hbase,9340,comment_6,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,9355,summary,doesn't close the FileSystem,non_debt,-
hbase,9355,description,Here is related code: The FileSystem returned by getTestFileSystem() is not closed.,non_debt,-
hbase,9355,comment_0,"Attached patch. Think the configuration parameter - fs.automatic.close set to true seems better, rather than the boilerplate.What do you think?",non_debt,-
hbase,9355,comment_1,"Attached patch. Think the configuration parameter - fs.automatic.close set to true seems better, rather than the boilerplate.What do you think?",non_debt,-
hbase,9355,comment_3,"Is there any reason to close the file system From the method name, it just need clean up data.",non_debt,-
hbase,9355,comment_4,"When returns true, dataTestDirOnTestFS is set to null. Test file system should be closed in the above case.",non_debt,-
hbase,9355,comment_5,lgtm +1,non_debt,-
hbase,9371,summary,Eliminate log spam when tailing files,code_debt,low_quality_code
hbase,9371,description,"Tailing a file involves reopening it for each seek, so for example when running replication it looks like this every time: The frequency makes it more relevant for TRACEing.",non_debt,-
hbase,9371,comment_0,Attaching the patch that make them traces.,non_debt,-
hbase,9371,comment_1,1,non_debt,-
hbase,9371,comment_2,"Committed to branch and trunk, thanks Stack.",non_debt,-
hbase,9371,comment_7,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,9382,summary,replicateWALEntry doesn't use the replication handlers,non_debt,-
hbase,9382,description,"By default we assign 3 handlers for replication, but as far as I can tell the replication traffic uses the normal handlers in 0.96",non_debt,-
hbase,9382,comment_0,"Broken by: The priority checker thingy is brittle. It looks at name of incoming method to figure priority (we should do Elliott idea of having client just say what priority they want). The above change did this kinda thing: When we ask the regionserver its method via reflection it returns the proper java method name but incoming, pb has method name as",non_debt,-
hbase,9382,comment_1,All priority was broke. Here is simple fix and test. This patch is for 0.95.,non_debt,-
hbase,9382,comment_2,Trunk version.,non_debt,-
hbase,9382,comment_3,I'll test this one out this morning.,non_debt,-
hbase,9382,comment_4,"It all checks out, +1",non_debt,-
hbase,9382,comment_6,Committed to 0.95 and trunk. Thanks for review and trying it,non_debt,-
hbase,9382,comment_11,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,9415,summary,"In rpcServer, replicationQueue is initialized with the max queue size instead of the max queue lenght",non_debt,-
hbase,9415,description,"One is about the size of the content, one about the size the the list.",non_debt,-
hbase,9415,comment_1,"I couldn't locate this code in trunk. I do see related code in SimpleRpcScheduler, but that is correctly setting the queue size:",non_debt,-
hbase,9415,comment_2,What am I missing? Thanks.,non_debt,-
hbase,9415,comment_3,"Oh, sorry. It's only on 0.96 so (I should have checked).",non_debt,-
hbase,9415,comment_4,+1. A 0.94 patch would be awesome. Thanks.,non_debt,-
hbase,9415,comment_5,Please commit on 0.96 branch,non_debt,-
hbase,9415,comment_6,Done for 0.96 & 0.94,non_debt,-
hbase,9425,summary,"Starting a LocalHBaseCluster when 2181 is occupied results in ""Too many open files""",non_debt,-
hbase,9425,description,"This bug was introduced via HBASE-6677 ""Random ZooKeeper port in test can overrun max port"". If 2181 is occupied but you start a LocalHBaseCluster (let's say you untar hbase and start it right away) you'll get this: The reason is that returns 2181 if defaultClientPort is greater than 0, which it always is when starting a LocalHBaseCluster.",non_debt,-
hbase,9425,comment_0,Added code to detect if we're supposed to try picking a new port or not and return -1 in the latter case. Mind taking a look-see ?,non_debt,-
hbase,9425,comment_1,"LGTM, thanks !",non_debt,-
hbase,9425,comment_2,Now e just fail if something on 2181? I suppose that better than current situation.,non_debt,-
hbase,9425,comment_3,"Right, we're back to what we had before HBASE-6677.",non_debt,-
hbase,9425,comment_4,"Committed to branch and trunk, thanks guys.",non_debt,-
hbase,9425,comment_9,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,9427,summary,Copy constructor of needs to consider the offset,non_debt,-
hbase,9427,description,A simple test below would fail with The reason for this is the constructor would always assume 0 as the offset while it can get it from ibw.getOffset() method.,non_debt,-
hbase,9427,comment_0,This issue is a duplicate of HBASE-8781 and i was looking at 0.94 code base. This can be closed as duplicate.,non_debt,-
hbase,9427,comment_1,Dup of HBASE-8781 according to Vasu,non_debt,-
hbase,9433,summary,OpenRegionHandler uses different assignment timeout,non_debt,-
hbase,9433,description,OpenRegionHandler uses the timeout monitor checking period as the timeout. It should share the same default as in AssignmentManager.,code_debt,low_quality_code
hbase,9433,comment_0,Defined some constants so that they are shared in both AM and OpenRegionHandler,non_debt,-
hbase,9433,comment_2,Rebased the patch.,non_debt,-
hbase,9433,comment_3,"+1, looks good to me.",non_debt,-
hbase,9433,comment_4,"Attached v2, one more place, same fix.",non_debt,-
hbase,9433,comment_6,+1 for trunk and 0.96,non_debt,-
hbase,9433,comment_7,Integrated into trunk and 0.96. Thanks.,non_debt,-
hbase,9433,comment_12,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,9461,summary,Some doc and cleanup in RPCServer,code_debt,low_quality_code
hbase,9461,description,RPC is a dog to follow. I want to do buffer pooling for reading requests but its tough drawing the diagram of who is doing what when. HBASE-8884 seems to have made it more involved still. This issue is about doing a bit of untangling.,non_debt,-
hbase,9461,comment_0,"Here is a start. Adds start of a description of who is doing what to who. Removes thread local that made the RcpServer instance available -- not used (seemingly), ugly.",code_debt,low_quality_code
hbase,9461,comment_2,The ipc package has three different 'Context' classes going on: + Scheduler Context (Context base class is inner class of RpcScheduler but is inner class of RpcServer -- fun). + We have a RequestContext which is set by CallRunner on each invocation -- nothing to do w/ scheduler context + Calls have an RpcCallContext which is something that is different again.,non_debt,-
hbase,9461,comment_4,"More doc and untangling. More to do but this ok for now. RpcServer had internal CallRunner class. The scheduler was reaching over to use this inner class. Broke it out of RpcServer and made it Standalone. Made it take an RpcServerInterface instead of RpcServer. Undid CallRunner implementing Runnable (confusing -- how it is run is internal function of Scheduler implementation) Cleaned up RpcServerInterface explaining why a start and a startThreads and an openServer. Added a few methods to support CallRunner being outside of RpcServer. Added into CallRunner the managment of the call queue size. As it was, callQueueSize was incremented before we created a CallRunner but internal to CallRunner it was managing the decrement. Shutdown access on CallRunner constructor and CR#getStatus and other methods only for use in this package. Added javadoc all around to explain the cryptic. was like CallRunner, a class used by the Scheduler only it was an inner class of RpcServer. Moved it out. Removed unused thread local SERVER. Add simple test to demo being able to instantiate a CallRunner outside of RpcServer context.a",code_debt,low_quality_code
hbase,9461,comment_6,"Just asking, if you find the time. Some comments describing the intention (what this class is supposed to do) would be great. For example, the delayed calls, undelayed calls with delayed response are not trivial (are they used?). It's very difficult to understand what these classes are supposed to do and why. Or this low level, with a specific case with '==1' Basically, I wanted to try a Netty based implementation, but there are so many corner cases that even estimating the time needed for a hacked implementation is difficult....",code_debt,low_quality_code
hbase,9461,comment_7,"Smile. I am in same boat as you. I want to untangle this rats nest so I can understand whats going on and try stuff (I want to try pool of bytebuffers -- direct bytebuffers even -- for incoming requests. Putting in netty too would be sweet). Unless you object, was going to commit this since it some progress. Was going to do more along this line soon but in other issues; I'm afraid the refactors will rot between getting time to work in here. The Delay stuff is unused I think. It was an experiment. Maybe I'll look at that next and purge it if I can.",code_debt,dead_code
hbase,9461,comment_8,"sure. I was mainly hijacking the jira :-) It's my impression as well (the code is HBASE-3899). The idea seems very good, but if it's not used the ratio complexity vs. usefulness can't be good.",non_debt,-
hbase,9461,comment_9,Committed to trunk.,non_debt,-
hbase,9461,comment_12,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,9479,summary,Using HBase Jars in Webapp Causes Many Issues,non_debt,-
hbase,9479,description,"Right now, HBase contains so many dependencies, that using the most basic HBase functionality such as HConnection in a larger application is unreasonably hard. For example, trying to include HBase connectivity in a Spring web app leads to hundreds of JarClassLoader errors such as: Why is this all bundled together? Why not have an ""hbase-client"" or ""hbase-client-dev"" package which is friendly for creating applications? I have spent 2+ days attempting to run a web service which is backed by HBase with no luck. I have created several stack overflow questions: The use of BeanUtils is also known to have a very bad issue: ""The three jars contain wrong classes"" Why is this so difficult? How do I include what I need to make an HBase app. So far I have tried using Maven, but this approach is draconian, and I have not succeeded. Am I Pwned?",build_debt,build_others
hbase,9479,comment_0,"Excuse me, this issue is NOT invalid. Perhaps you want it to have a more serisuo title? This is a major waste of time, caused by known bugs. You dont consider this valid?",non_debt,-
hbase,9479,comment_1,"Please ask on the mailing list before creating jira. This tool is not for support but for change tracking. When you ask something on the mialing list, mention the hbase version. There is a hbase-client in the hbase version currently released (0.96). This won't remove all the dependencies. And yes, in maven is the way to do it.",non_debt,-
hbase,9479,comment_2,I emailed the dev mailing list over the weekend. This is an improvement or a change you should track. Do you expect people to develop with HBase? Is this a rite of passage afforded to all would be hbase coders? Can you show me one place that mentions how to put an HConnection in a web service?,non_debt,-
hbase,9479,comment_3,"I wrote my comment before seeing yours. Please just ask on the mailing list before creating jiras. If there is something we can fix we will, but jiras needs to reference something with a simple enough scope to get implemented.",non_debt,-
hbase,9479,comment_4,I consider this a bug.,non_debt,-
hbase,9479,comment_5,"How about changing it to ""Create an HBase client lib""?",non_debt,-
hbase,9479,comment_6,I agree this is a pain in *ss. At the same time this is how Apache/Java has evolved now. What would you have us do? Do you have solution in mind?,non_debt,-
hbase,9479,comment_7,Then this lib would have all the dependencies. How do you propose we deal with the external dependencies?,non_debt,-
hbase,9479,comment_8,"Hi David, I agree with Nicolas. This should be discussed on the mailing list first. I have a web server server HTTP requested and connecting to HBase to get the content and do some processing, and I have not faced any issue to do that. I have just uploaded the jar, done my code, and that's all. Took me 1h. So I guess you might want to get some feedback from the others on the user mailing list (not the dev one).",non_debt,-
hbase,9479,comment_9,"I understand your pain, but please read my comment as well (the first one): ??There is a hbase-client in the hbase version currently released (0.96)?? Then this libs still have a lot of dependencies, netty, log4j, and so one are found all over the place in Java. Then maven and its dependency management is the common solution.",non_debt,-
hbase,9479,comment_10,"Hi Nicolas, thanks, I will try the 96 client libs. I haven't had time to get in to 96 and setting it up. So I was able to silence the JarClassLoader errors with a beefy pom. But for some reason my is failing at runtime do to some kind of weird file not found issue:",non_debt,-
hbase,9479,comment_11,"I agree with Nicholas. , we're not trying to be nasty (really, we're nice folks). But this just isn't something specific to HBase. If you have a specific solution in mind (maybe a patch even? :) ), we can reopen and commit the fix.",non_debt,-
hbase,9479,comment_12,"Hi Lars, thanks. I think it is specific to hbase in the sense that hbase the hbase jars hosted on maven central have dependencies that just dont ""play nice"" with other libraries. I think that is project specific. I know you are all nice guys. Im just looking for a solution, because I have a demo to give in a finite amount of time. And no one seems interested on SO.",non_debt,-
hbase,9479,comment_13,"Sorry about the pain. In 0.94, client and server were all bundled up in the one ball w/ client dependencies those of the servers so yeah, it is ugly. We've been trying to improve our story in 0.96. Some pruning/edit has been done to 'shield' downstreamers in 0.96 from the lorry-load of dependencies pulled in by our dependencies but for sure we could do better.",build_debt,build_others
hbase,9479,comment_14,You know anything about the provenance of ? Not one of ours. Can you get better stack trace that the one pasted in here?,non_debt,-
hbase,9479,comment_15,"Hi Stack, thanks for the update. Thats great news. Sorry about the jar name, I meant to change that to /myapp.one-jar.jar to genericize the name of the app. Im currently scratching my head over why something is trying to open /target/app.jar. The slash makes me think it is the root? Also I think this is Hadoop looking for config files. Im worried its complected with the client/server includes excludes above, or the one jar packaging with the Maven one jar plugin. I just opened <a SO</a>. My Hadoop and Hbase config files are both in the classpath, so I'm not sure what to try next. Any ideas?",non_debt,-
hbase,9479,comment_16,", where do I download .96?",non_debt,-
hbase,9479,comment_17,"For example, only 95 and 94.11 stable are available currently on mirrors",non_debt,-
hbase,9479,comment_18,"It's not release, yet, but you can download a developer release (0.95.2) from the HBase download site: (This is a developer release, not for production, but it has the client module - my guess you'll find the same problem there with many dependencies that you need to resolve if you cannot or do not want to use maven)",non_debt,-
hbase,9479,comment_19,"Hi Lars, I am happy to use Maven. As it is I still cant get this api up and running. Unit tests work but when I try to connect in a running jetty server I get this strange message about not being able to find my own jar file Do you know off hand what this might be about. Any ideas on why Java is trying to open Also if I share this code with you after removing proprietary info is this something youd be willing to take a 10 minute look at? :)",non_debt,-
hbase,9479,comment_20,"As Stack said, (or /target/app.jar) is not one of ours nor any of Hadoops. I'd check the setting of your web container. Let's continue on the mailing list.",non_debt,-
hbase,9484,summary,"Backport 8534 ""Fix coverage for to 0.96",non_debt,-
hbase,9484,description,None,non_debt,-
hbase,9484,comment_0,patch for 0.96. Includes addendum.,non_debt,-
hbase,9484,comment_2,Integration tests fail consistently on hadoop2 when run in local mode. Investigating.,non_debt,-
hbase,9484,comment_3,Dropping assignment since our track is shut down. I'm sorry. Please see HBASE-9459 for updated patch for branch 0.94.,non_debt,-
hbase,9484,comment_4,See the patch committed to trunk on HBASE-9165. It contains fixes to which are yet to be backported by this ticket.,non_debt,-
hbase,9484,comment_7,Looks like this issue should be resolved? It was committed to 0.96?,non_debt,-
hbase,9484,comment_8,"this has not been committed yet. There's another issue that mentioned this one in its commit message, which caused a bit of spam.",non_debt,-
hbase,9484,comment_9,0.96 is EOL'd.,non_debt,-
hbase,9493,summary,Rename CellUtil#get*Array to CellUtil#clone*,non_debt,-
hbase,9493,description,"To make reading code easier, we should rename the CellUtil#get*Array methods to to CellUtil#clone*. This eliminates the possibly confusion between the semantics of these methods (which allocate-copy) and that of Cell#get*Array (which essentially just returns a pointer).",code_debt,low_quality_code
hbase,9493,comment_0,Simple braindead patch. New name makes it clear that allocation is going on (and also clearly points out that we are doing a lot of small array allocations in tests).,code_debt,low_quality_code
hbase,9493,comment_1,+1 This is an improvement.,non_debt,-
hbase,9493,comment_3,Thanks for the looksee stack. Committed to 0.96 and 0.98. The renamed api was only available in 0.95 so no deprecation work required.,non_debt,-
hbase,9493,comment_8,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,9545,summary,NPE when trying to get cluster status on an hbase cluster that isn't there,non_debt,-
hbase,9545,description,"As part of some fault injection testing, I'm trying to talk to an HBaseCluster that isn't there, opening a connection and expecting things to fail. It turns out you can create an {{HBaseAdmin}} instance, but when you ask for its cluster status the NPE surfaces",non_debt,-
hbase,9545,comment_0,"Stack trace It looks like assumes its {{masterMonitor}} field is never null, but if there is no connection,that isn't true. The close() operation should be made a bit more robust, so as not to hide the underlying RPC failures I expect to see",code_debt,low_quality_code
hbase,9545,comment_1,You're running on an older HBase? HBASE-9498?,non_debt,-
hbase,9545,comment_2,you are right -it goes away on trunk. Marking as duplicate,non_debt,-
hbase,9651,summary,Backport HBASE-3890 'Scheduled tasks in distributed log splitting not in sync with ZK' to 0.94,non_debt,-
hbase,9651,description,"HBASE-3890 was fixed in 0.96 and trunk. This issue is to backport to 0.94 Note that there must be more slightly off here. Although the splitlogs znode is now empty the master is still stuck here: There seems to be an issue with what is in ZK and what the TaskBatch holds. In my case it could be related to the fact that the task was already in ZK after many faulty restarts because of the NPE. Maybe it was added once (since that is keyed by path, and that is unique on my machine), but the reference count upped twice? Now that the real one is done, the done counter has been increased, but will never match the scheduled.",non_debt,-
hbase,9651,comment_0,Tests passed:,non_debt,-
hbase,9651,comment_1,1,non_debt,-
hbase,9651,comment_2,Integrated to 0.94 Thanks for the review.,non_debt,-
hbase,9651,comment_5,Was committed some time back. Marking fixed.,non_debt,-
hbase,9668,summary,Apply small scan to Scan's used by AggregationClient,non_debt,-
hbase,9668,description,The Scan objects used to construct AggregateRequest's for aggregates supported by AggregationClient qualify as small scan because response from each region is small. We should utilize small scan for better performance.,design_debt,non-optimal_design
hbase,9668,comment_0,"I don't think we should apply small scan for AggregationClient. The function of small scan: 1.Reduce RPC, but it won't take affect in AggregationClient since we call it through coprocessor 2.Using position read when reading data block from HDFS. Position read is better if scan range is small, and seek+read is better if scan range is large. Although the response is small, the scan range may be large(It means it will scan a lot of data)",non_debt,-
hbase,9668,comment_1,"In Phoenix, there is The concept of small scan is applicable to region scanner that does TopN.",non_debt,-
hbase,9671,summary,should check whether table is enabled,non_debt,-
hbase,9671,description,In our integration test we saw the following: didn't check that table was enabled before issuing compaction request.,non_debt,-
hbase,9671,comment_0,I would disagree. The actions should't try and behave. They should always try and do the most destructive thing possible. That will find more issues. For example it's always possible that we have a problem with compaction a region while we're still opening it.,non_debt,-
hbase,9671,comment_1,Can you clarify what would be the expected behavior ?,non_debt,-
hbase,9671,comment_3,I would expect it to throw an exception like it currently is. The point is that chaos monkey actions shouldn't be trying to avoid exceptions. It should be issuing commands at will and not trying to protect the cluster at all. Anything else can hide bugs.,design_debt,non-optimal_design
hbase,9671,comment_4,I think this is a bug in CompactSplitThread. I see we already have I suppose it should also verify the table is enabled and the region is online.,non_debt,-
hbase,9671,comment_5,I'm not sure how useful that would be. At some point it will always be possible to get a nsre.,non_debt,-
hbase,9671,comment_6,"This issue is invalid then, at least as it is cast (""Make the Chaos Monkey Nice"")?",non_debt,-
hbase,9671,comment_7,"If that's the case, the compaction management code should handle that scenario more gracefully.",non_debt,-
hbase,9671,comment_8,"If there is no consensus, I can close this JIRA.",non_debt,-
hbase,9671,comment_9,Resolved as invalid per discussion on issue.,non_debt,-
hbase,9683,summary,Port HTrace support to 0.94,non_debt,-
hbase,9683,description,Port HTrace (HBASE-6524) to 0.94. This patch includes a wire-format change on writable RPC. It serializes the traceID and spanID in class Invocation. This should be compatible with old versions.,non_debt,-
hbase,9683,comment_0,"This breaks wire compat correct ? Also there are much newer versions of HTrace that have change the user facing api, and added some perf gains. So my preference would be to use 2.0.1 if we even want this in 0.94",non_debt,-
hbase,9683,comment_1,"No. The goal of this patch is to add trace support without breaking wire compatibility. This is a little bit tricky. It appends 1 boolean + 2 longs to Invocation. There are two cases (I've tested manually): 1) old client + new server: There is no appended trace info, so we will get EOFException in readTraceInfo(). It saliently ignore it and return null, effectively performing no tracing at server side. 2) new client + old server: The client sends request with trace info. The server has no knowledge on this. It will stop reading just after parameters. Not that this unread bytes should have no side effects, as in the data buffer is preceded by its length (in fact, the input stream is a and any remaining bytes are discarded, see Oh, I didn't notice trunk has update on it. Let me make another patch. As maintaining wire compatibility is so important here. I'd like to ask you guys to review if there is anything missing.",non_debt,-
hbase,9683,comment_2,"It will not break wire compat if the serialized class is not followed by anything else, I assume it's the case? Otherwise trying to read next object will read these bytes",non_debt,-
hbase,9683,comment_3,"Hi . Sorry for the late reply. To my understanding, I think this would be the only possible case.",non_debt,-
hbase,9683,comment_4,"Here is port of HBASE-9121 and HBASE-9366 to 0.94 branch. , please have a look at this one. I haven't get zipkin work on my box yet, but TestHTraceHooks can pass thus it should basically work.",non_debt,-
hbase,9683,comment_5,"I finally get this worked. There is something tricky about thrift version. As HTrace is using thrift 0.9.0 and hbase 0.94 is using thrift 0.8.0. HTrace's code is not able to run with thrift 0.8.0. So I have to configure HBASE_CLASSPATH and add thrift 0.9.0 jar. (I don't know why this dependency is not propagated to hbase.) HBase trunk has been updated to thrift 0.9.0 (HBASE-7005). Because of compatibility reasons, we can't get 0.94 upgrade too. Is it possible to make htrace depend on thrift 0.8.0?  , what do you think of this?",non_debt,-
hbase,9683,comment_6,"v3 fixes a missing of ""Trace.wrap"" where put operation was not traced due to thread switch.",non_debt,-
hbase,9683,comment_8,I'm a pretty strong -0 on this. While I think tracing is pretty awesome and very useful. I'm very wary of doing anything to the RPC of 94. It's on 94.13. That's pretty far into what should be stabilization.,non_debt,-
hbase,9683,comment_9,"Hi , I agree that the 0.94 RPC is ugly in terms of expansion and it needs much more carefulness and efforts than the PB based one. I believe that the 0.94 series will remain in production for a while (at least for us and for several other users that I know of). Having this back ported to 0.94 will definitely benefit a lot of existing users.",design_debt,non-optimal_design
hbase,9683,comment_10,", what do you think of this jira as 0.94 release manager? than ks",non_debt,-
hbase,9683,comment_11,I'll defer to Elliott on this. Personally I'm +0.,non_debt,-
hbase,9683,comment_12,Resolving as Not A Problem based on sentiment on issue,non_debt,-
hbase,9689,summary,Support using OperationAttributes through shell script,non_debt,-
hbase,9689,description,Currently the ruby scripts for Put does not support setting of Operation Attributes through shell. It may be useful in some cases and also for testing. And that would give a full fledged support using shell.,non_debt,-
hbase,9689,comment_0,Can support specifying attrs while get/scan also. +1 for the requirement,non_debt,-
hbase,9689,comment_1,A patch that supports for Put. One drawback here is that if the ts is not specified then this attributes cannot be specified. I tried using the args way for the Attributes alone but was not able to do that. In scan except for table all others are specified as args so it would be easier to do there. In put I don't think we can it now to args way. Tested this and it works. I can provide a patch for scan too.,non_debt,-
hbase,9689,comment_3,Updated patch. Things work fine with Put with and without timestamp. Also gets and scans support setting of Operation Attributes. Tested patch. Pls provide your comments on this.,non_debt,-
hbase,9689,comment_5,Testcase failure not related to this patch.,non_debt,-
hbase,9689,comment_6,"+1 patch looks ok, would be good to have coverage in TestShell for this",test_debt,low_coverage
hbase,9689,comment_7,"Looks good Correct the comment pls. Seems copy paste :) set_get_attributes, set_put_attributes seems same code repeating. We can make this one def?",code_debt,duplicated_code
hbase,9689,comment_8,Latest patch. Observed one problem with Attributes specified when the timestamp is not specified in the put args. Now timestamp should be specified So this change is a new behaviour for specifying puts. TestShell passes with this. If this change is fine I can commit this tomorrow.,non_debt,-
hbase,9689,comment_9,"There is no way to have a backwards compatible put command that does not take an attributes hash, just a timestamp? It's fine if timestamp has to be specified as part of the attribute hash if one is present.",design_debt,non-optimal_design
hbase,9689,comment_10,You can do that. What i meant was Timstamp cannot be specified as per the older way The problem is put accepts a set of parameters. So if we avoid the ts parameter and specify the ATTRIBUTE instead the timestamp is taken as 'mykeymyvalue'.,non_debt,-
hbase,9689,comment_12,"Yes, that was my question exactly. So if anyone wants to use the shell to test puts, including messing with timestamps, and has learned the old way, after this change now what they know will not work. I am wondering if there is a way to keep the old way of specifying timestamps as an alternative. Of course it's fine that if someone wants to set operation attributes, that the old way won't work _for that case_.",non_debt,-
hbase,9689,comment_13,"What Andrew asks; aren't there places in our ruby code where we look at the type? If a number, consider it a timestamp and if a map, treat it as attributes?",non_debt,-
hbase,9689,comment_14,I did this.. but the attributes became a single string mykeymyvalue rather than a map type. Ok I will check for someother workaround. Thanks a lot for the reviews.,non_debt,-
hbase,9689,comment_15,This patch works as expected. No backward compatibility changes. TestShell passes with this. If it is fine I can commit this later today.,non_debt,-
hbase,9689,comment_17,I skimmed the patch. Looks good. Thanks for going back and making it so we don't break BC.,non_debt,-
hbase,9689,comment_18,"+1 on latest patch, thanks Ram",non_debt,-
hbase,9689,comment_19,LGTM,non_debt,-
hbase,9689,comment_20,"Committed to trunk. Thanks Andy, Anoop and Stack for the reviews.",non_debt,-
hbase,9689,comment_23,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,9716,summary,LoadTestTool should provide default min and max settings to the data generator,non_debt,-
hbase,9716,description,"Allows use with skip_init and reader threads only, otherwise we see this:",non_debt,-
hbase,9716,comment_0,1,non_debt,-
hbase,9716,comment_1,"Not sure what happened to the trunk patch, attaching again.",non_debt,-
hbase,9716,comment_2,"Thanks for the +1 Ted. Going to commit to trunk, 0.96, and 0.94 branches soon unless objection.",non_debt,-
hbase,9716,comment_3,Bit by this again today. Committed.,non_debt,-
hbase,9742,summary,Add Documentation For Simple User Access,non_debt,-
hbase,9742,description,"The [security in the HBase book only talks about using Kerberos. There is a simple user access mode too that is not documented. This should be documented for development systems and HBase installs where security is not an issue, but they want to prevent user mistakes. I've added a section to the security chapter that talks about simple user access. The new section makes it very clear it is not secure.",documentation_debt,low_quality_documentation
hbase,9742,comment_0,Added new section about simple user access.,non_debt,-
hbase,9742,comment_1,"Some of the changes in HBASE-5732 aren't documented. In the configuration section, I read through the code to see what the changes are now.",documentation_debt,low_quality_documentation
hbase,9742,comment_3,The patch for the 0.94 branch.,non_debt,-
hbase,9742,comment_5,Committed to trunk. Thanks for the patch jesse. We usually just copy trunk to the 0.94 branch when we make up doc. Nice addition. Thanks.,non_debt,-
hbase,9742,comment_8,Added 0.94 config sections to the files.,non_debt,-
hbase,9742,comment_9,You want me to apply this stuff to 0.94 Jesse? Usually we just let the trunk doc do the job for users. I tried applying the above but got this: It works for you? Thanks boss.,non_debt,-
hbase,9742,comment_10,"No, simplewith0_94.diff is a patch for the 0.96 branch. It adds the 0.94 specific configuration docs to the 0.96 branch so everything is together.",non_debt,-
hbase,9742,comment_11,Applied Jesse's 0.94 patch. Thanks Jesse.,non_debt,-
hbase,9742,comment_14,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,9806,summary,Add PerfEval tool for BlockCache,non_debt,-
hbase,9806,description,We have at least three different block caching layers with myriad configuration settings. Let's add a tool for evaluating memory allocations and configuration combinations with different access patterns.,non_debt,-
hbase,9806,comment_0,"Here's an initial version, based heavily on HFilerPerfEval code. Would be nice to reduce duplication between the two. Sample output, for reference.",code_debt,duplicated_code
hbase,9806,comment_1,Can we not do this via PE? Having a PerfTest driver might be good. See . That way we know easily where to look at for perf testing? Instead of looking at 20 different classes?,non_debt,-
hbase,9806,comment_2,"goes through the HTableInterface; I'm looking for a component-level benchmark. Agreed, this would qualify as a tool to register in whatever comes out of HBASE-9599.",non_debt,-
hbase,9806,comment_3,"Updated patch after running with a couple different configurations. Hopefully I'll get to run on a larger memory machine tomorrow. I have a couple more things to expose as configuration and I'd like to get it to the point where it can automatically determine the amount of data to write based on different cache:data ratios. I also tried running multiple concurrent the cache consumers, but saw warnings about double-storing blocks. Will investigate that further before adding.  how much RAM do your test machines have?",code_debt,low_quality_code
hbase,9806,comment_4,"They have 16GB by default but I can get 16GB from one and put it on another one. So. Options are: 1) I can run on a 4 nodes cluster where all nodes will have 16GB 2) I can run on 2 servers (1 SSD, 1SATA) with 32GB. Of course, I also have some nodes where I have only 4GB ;) But I don't think you are interested but those one ;)",non_debt,-
hbase,9806,comment_6,"This test runs on a single host, not a cluster. I've been running it against local fs; it's only profiling the behavior of the BlockCache implementation, so nothing else is required. I just got my 24GB machines back, so I'll start with one of those. If you'd like to create a frankenbox with 32GB and run some tests, that would be cool too. Since you have SSD, you could also profile the bucketcache, which would be cool. I can attach some reference configurations I've been playing with to get you started.",non_debt,-
hbase,9806,comment_7,"Any chance one of you has the configs laying around from the benchmarks run on the slabcache [announcement I'm attempting to reproduce and slabcache looks pretty unstable. I wonder if I'm exercising some edge-case based on my configuration or if there's some bit-rot happened. Specifically, I'm looking for values for HBASE_HEAPSIZE, and anything else that was using a non-default value. Thanks.",design_debt,non-optimal_design
hbase,9806,comment_8,"Sure! I will create a ""frankenbox"" and try that. I will have physical access to the computers only Friday evening. So I might be able to configure that this week-end. Also, I found a motherboard where I can put 64GB!!!! A bit expensive, but I will most probably buy it so we can test with high memory values and see how it reacts! Donations are welcome ;) Hahaha. I will need some details on what you want me to run 0.96.0 vs 0.96.0+9806 ? Anything else? Specific settings?",non_debt,-
hbase,9806,comment_9,"Ok. Server is ready. 32GB + 1 SDD + 5 SATA. Do you want that to run in standalone in the SSD? Or pseudo-dist with all the disks and ""namenode"" directory in SATA? So far I'm going to run 0.96.0 and Nothing else? Please advice.",non_debt,-
hbase,9806,comment_10,Any results so far?,non_debt,-
hbase,9806,comment_11,"Hi , I was waiting to know what I should test... Server is ready, just waiting for guidelines. question was ""Do you want that to run in standalone in the SSD? Or pseudo-dist with all the disks and ""namenode"" directory in SATA?"" And also regarding what to run with: ""So far I'm going to run 0.96.0 and Nothing else?"" As soon as I know that I start the tests. As I said, server is ready and off for now. Just waiting for the work ;)",non_debt,-
hbase,9806,comment_12,"Sorry for the delays,  -- Strata and all that. I wanted to add a kind of self-tuning to the patch, give it the ability to write records until it fills up the cache and no more. No luck as of yet, which means determining how many rows to write requires a little guess-work and checking the logs. I've run the test for 3g (-r 1700000) and 20g (-r 12000000) heaps using the attached configs and 100 iterations. Attached also are the charts I generated from the logs. I tried 1000 iterations to see if anything happens over a longer interval but nothing exciting. I'm also updating my log-parsing script to overly the GC events. More to follow. On your fancy rig, can you run for maybe 8g, 16g, 20g, and 28g? You'll have to apply one of these conf patches and then adjust the heap size yourself. You'll also need to play with the -r param to work out how many rows you can fit into the cache w.o eviction (please let me know what you end up using!) -i 100 should be a good starting point, but if you have time I'd take logs from 500 or 1000. Thank you much!",design_debt,non-optimal_design
hbase,9806,comment_14,"Heya . Any chance you had a moment to run this? I'm leaving on holiday next week and I'd like to tie up some loose ends. Even just running it, verifying it works for you, so I can get it committed and start improving and refactoring these perf tools when I get back.",non_debt,-
hbase,9806,comment_15,Sure! I might be able to give it a try tonight of tomorrow... I keep you posted.,non_debt,-
hbase,9806,comment_16,Working on that right now...,non_debt,-
hbase,9806,comment_17,Hum. Seems to not compile in Trunk: [ERROR] COMPILATION ERROR : [INFO] [ERROR] error: no suitable method found for I will take a look,non_debt,-
hbase,9806,comment_18,Need to be,non_debt,-
hbase,9806,comment_19,"Yep, there has been drift. Here's an updated patch.",non_debt,-
hbase,9806,comment_21,"Should this be in the io.hfile package since that is where BC's normally fester. PE has more than PerfEval at the moment (histograms, rows by data size). This does gaussian rows which PE doesn't have which would be nice (but for BlockCache testing we want random-sized data I'd say). Should this go in ? Or want me to adopt to PE? Add a random data size? Will help yeah w/ looking at BC options quickly on one machine.",non_debt,-
hbase,9806,comment_22,"Moving to the io.hfile package makes sense. Maybe we merge this one into HFilePerfEval, as these are closely related? I'd like to add support for more schema varieties and access distributions on PerfEval, similar to what these tools have. The whole business could use some refactor toward more code sharing. It all depends on the goal of the tool though. I like PerfEval because it's closer to what a user will see. OTOH, it makes it difficult for an hbase dev to get a sense for the IO subsystem pieces in isolation.",non_debt,-
hbase,9806,comment_24,Linking an issue that has simple BC toolset superceded by work here.,non_debt,-
hbase,9893,summary,Incorrect assert condition in OrderedBytes decoding,non_debt,-
hbase,9893,description,"The following assert condition is incorrect when decoding blob var byte array. When the number of bytes to decode is multiples of 8 (i.e the original number of bytes is multiples of 7), this assert may fail.",non_debt,-
hbase,9893,comment_0,"minor issue, a quick fix.",non_debt,-
hbase,9893,comment_1,Patch lgtm (or remove the assert since this seems to be working around a mis-assertion?) What you think ?,non_debt,-
hbase,9893,comment_2,"I just catch this in my application unit test code, and this assert is valid and good for robustness.",non_debt,-
hbase,9893,comment_3,"Can you describe your test a little more precisely? The unit test exercises input case of length 7 and 8. When I run with assertions enabled, I don't get a failure.",non_debt,-
hbase,9893,comment_4,"When the length is 7 (or multiples of 7), and the 1st bit of the last byte is 1 the assertion will fail, because t will be ended with 128 instead of 0. For example:",non_debt,-
hbase,9893,comment_5,"Excellent catch. Attached is a patch that also includes updated test cases. Please let me know if you have other value permutations you'd like to see tested. It'd be nice to have a more thorough test suite around this code, a la the suite Orderly has. From the commit message",test_debt,lack_of_tests
hbase,9893,comment_6,"FYI, I retained the assertion and preserve the value of 't' in the general case for fear of some other bug cropping up and producing invalid data. It is my intention to retain these assertions for a few more releases while the new code is exercised more thoroughly.",non_debt,-
hbase,9893,comment_8,"lgtm, thanks.",non_debt,-
hbase,9893,comment_9,Reattaching patch for buildbot. Javadoc warnings aren't this patch.,non_debt,-
hbase,9893,comment_11,Broken test is unrelated. Filed HBASE-10008.,non_debt,-
hbase,9893,comment_12,Committed to 0.96 and trunk. Thanks  for the report and patch.,non_debt,-
hbase,9893,comment_17,Released in 0.96.1. Issue closed.,non_debt,-
hbase,9894,summary,remove the inappropriate assert statement in,non_debt,-
hbase,9894,description,One of my friend encountered a RS abort issue frequently during loading data. Here is the log stack: FATAL ABORTING region server 3883151: Uncaught exception in service thread getSplitPoint() called on a region that can't split!,non_debt,-
hbase,9894,comment_0,"HBase version: 0.94.6-cdh4.3.0 java -version java version ""1.6.0_26"" Java(TM) SE Runtime Environment (build 1.6.0_26-b03) Java HotSpot(TM) 64-Bit Server VM (build 20.1-b02, mixed mode) export HBASE_OPTS=""-ea To me, that assert statement in getSplitPoint() is inappropriate, and in trunk code, it has been removed already, let's just make a one line remove here, is that OK?",non_debt,-
hbase,9894,comment_2,"The assert looks a little silly. Without it, we return null and just do not split the region?",non_debt,-
hbase,9894,comment_3,Nobody should run in production with asserts enabled.,non_debt,-
hbase,9894,comment_4,"yes , totally agreed with you, i also doesn't understand why he enabled it:) anyway, let's remove it, it should be encounterd even in a debug/dev env with ""-ea"" possible right? we shouldn't abort the whole server instance in this case.",non_debt,-
hbase,9894,comment_5,"Committed to 0.94. Thanks for the patch. (I did take a quick look to see that we're not relying on this in a test, didn't find anything)",non_debt,-
hbase,9901,summary,"Add a toString in HTable, fix a log in AssignmentManager",non_debt,-
hbase,9901,description,None,non_debt,-
hbase,9901,comment_1,"There is no javadoc in the patch, and the findbugs in hbase-client or hbase-servers seems unrelated...",documentation_debt,low_quality_documentation
hbase,9901,comment_2,"The toString is ugly. Fix on commit: + return ""HTable{"" + ""connection="" + connection + "", tableName="" + tableName + '}'; Make it just: connection + "","" + tableName It will look like: Or Our logs are too profuse already -- they need paring. Let the above be the convention for tablename string. No need of having the '{' and the HTable preamble? If you do above, +1 on patch for branch and trunk",code_debt,low_quality_code
hbase,9901,comment_3,"In this case, this was generated by IntelliJ, so it's accepted as common somewhere, likely :-). But np to do something different, of course. I'm going to commit the one.",non_debt,-
hbase,9901,comment_4,Released in 0.96.1. Issue closed.,non_debt,-
hbase,9931,summary,Optional setBatch for CopyTable to copy large rows in batches,non_debt,-
hbase,9931,description,We've had CopyTable jobs fail because a small number of rows are wide enough to not fit into memory. If we could specify the batch size for CopyTable scans that shoud be able to break those large rows up into multiple iterations to save the heap.,non_debt,-
hbase,9931,comment_0,You know for sure the issue is in the cans? Are you sure this isn't a dupe of HBASE-7743?,non_debt,-
hbase,9931,comment_1,Yeah - it's in the scans. CopyTable doesn't use reducers at all.,non_debt,-
hbase,9931,comment_2,Is this just a copytable config then ?,non_debt,-
hbase,9931,comment_3,"I think a new config option to call Scan.setBatch would do it. I'm always a bit puzzled about how scanner batching and caching interact with mixed length rows, but I imagine it would work out ok. (4 years later, and still using a version where HBASE-1996 didn't make it.)",non_debt,-
hbase,9931,comment_4,"I see TableInputFormat already supports overriding many scanner properties via config. Would it be sufficient to add another configuration point, perhaps?",non_debt,-
hbase,9931,comment_5,will this patch work for you?,non_debt,-
hbase,9931,comment_7,The patch adds setBatch and removes setCaching. I think setCaching should stay in there too - probably a copy/replace bug?,non_debt,-
hbase,9931,comment_8,"Yes, you're right. Take 2.",non_debt,-
hbase,9931,comment_9,"Looks good, Nick. +1 Would love to see it hit 0.96 and 0.94 as well.",non_debt,-
hbase,9931,comment_10,", ,  Any objections on this one?",non_debt,-
hbase,9931,comment_11,1,non_debt,-
hbase,9931,comment_12,1,non_debt,-
hbase,9931,comment_13,"Patch applied to trunk, 0.96, 0.94 branches. Thanks for the report, Dave; the reviews Andrew and Lars.",non_debt,-
hbase,9931,comment_14,+1 Thanks for adding to 0.96.,non_debt,-
hbase,9931,comment_15,Released in 0.96.1. Issue closed.,non_debt,-
hbase,9950,summary,Row level replication,non_debt,-
hbase,9950,description,"We have a replication setup with the same table and column family being present in multiple data centers. Currently, all of them have exactly the same data, but each cluster doesn't need all the data. Rows need to be present in only x out of the total y clusters. This information varies at the row level and thus more granular replication cannot be achieved by setting up cluster level replication. Adding row level replication should solve this.",design_debt,non-optimal_design
hbase,9950,comment_0,How would you propose marking a row that is to be replicated ? Thanks.,non_debt,-
hbase,9950,comment_1,Could you handle this by plugging in a custom replication policy? Replication scope is an integer so can encode a fair amount of information.,non_debt,-
hbase,9950,comment_2,"not sure yet. Since there is no notion of row level data in hbase storage, I would have to create some special KVs that are stored for the row, which sounds very hacky.  Replication scope is defined at the CF level, so I don't think Ill be able to use it. I do need to plug in custom replication policy though if this is not a core feature. There are no observers for replication, are there?",design_debt,non-optimal_design
hbase,9950,comment_3,"{{HBase}} connector for {{Kafka}} can help with this scenario even though it is not inbuilt into replication. Can this be closed? ,",non_debt,-
hbase,9950,comment_4,"We also use custom replication endpoints to do things like this, so yeah let's close it. There are multiple ways to achieve this result.",non_debt,-
hbase,9953,summary,Decouple data size from client concurrency,non_debt,-
hbase,9953,description,"PerfEval tool provides a {{--rows=R}} for specifying the number of records to work with and requires the user provide a value of N, used as the concurrency level. From what I can tell, every concurrent process will interact with R rows. In order to perform an apples-to-apples test, the user must re-calculate the value R for every new value of N. Instead, I propose accepting a {{--size=S}} for the amount of data to interact with and let PerfEval divide that amongst the N clients on the user's behalf.",non_debt,-
hbase,9953,comment_0,-1 for --size instead of --rows +1 for --size in addition of --rows ;) I think the 2 can (and need to) leave together so we can still easily compare previous release to new release.,non_debt,-
hbase,9953,comment_1,"Yes, my intention isn't to remove existing functionality, at least not right away. Let's prove a new idea useful first.",non_debt,-
hbase,9953,comment_2,Hi  any update on this one?,non_debt,-
hbase,9953,comment_3,"Yes, but not on the tool itself. I ended up writing a driver script that does the calculation for me. Nothing complicated. Let me throw together a patch.",non_debt,-
hbase,9953,comment_4,Attached patch is post-refactor.,non_debt,-
hbase,9953,comment_5,2 comments. 1) You might want to update printUsage too to provide usage. 2) If both (opts.size == DEFAULT_OPTS.size) and == then opts.totalRows is never set?,non_debt,-
hbase,9953,comment_6,Refreshing patch after a short hiatus. Added description for new --size command to help text.,non_debt,-
hbase,9953,comment_7,"Yes, that's fine; in this case the default totalRows value of 1024 * 1024 is used.",non_debt,-
hbase,9953,comment_8,"ping , .",non_debt,-
hbase,9953,comment_9,"+1 Commit and I'll try it. Let me explain over in the modulo issue how it does similar but I like your take better (After trying this, may back out the modulo arg)",non_debt,-
hbase,9953,comment_11,Committed to trunk.,non_debt,-
hbase,9953,comment_13,"This seems to work nicely for me. Funny how when the block cache is 4G, if you set the size to get to 4G, you fall out of BC. If you set it to 3G, you still do. 2.8G gets the BC 100% used w/ no misses. We've lots of overhead it seems (smile). Let me remove my modulo malarky.",non_debt,-
hbase,9953,comment_14,"Yes, I've observed our population doesn't quite match the space available. Something is lying, or at least off by some small amount that's repeated many times over, but I don't yet know what.",non_debt,-
hbase,9953,comment_15,Closing this issue after 0.99.0 release.,non_debt,-
hbase,9997,summary,Add per KV security details to HBase book,non_debt,-
hbase,9997,description,Per KV visibility labels Per KV ACLs,non_debt,-
hbase,9997,comment_0,Made blocker,non_debt,-
hbase,9997,comment_1,1,non_debt,-
hbase,9997,comment_2,1,non_debt,-
hbase,9997,comment_3,Committed to Trunk. Thanks for the reviews Andy & Ram.,non_debt,-
hbase,9997,comment_6,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,10001,summary,Add a coprocessor to help testing the performances without taking into account the i/o,non_debt,-
hbase,10001,description,"We have a mockup to test only the client. If we want to include the network, without beeing limited by the i/o, we don't have much tools. This coprocessor helps to test this. I put it in the main code as to make it usable without adding a jar... I don't think it's possible avoid the WAL writes in the coprocessors. It would be great to have it to simplify the test with any kind of client (i.e. w/o changing the durability).",code_debt,complex_code
hbase,10001,comment_0,You want ? You can use prePut() hook and do a bypass from there so that even WAL write also can be avoided?,non_debt,-
hbase,10001,comment_1,"Yes :-) Great, let me update the patch.",non_debt,-
hbase,10001,comment_2,IIRC you can also modify the put in the pre hook:,non_debt,-
hbase,10001,comment_3,"Actually, if I read correctly the code in HRegion, just marking the operations as success as I do in the v1 is enough:",non_debt,-
hbase,10001,comment_4,Oh yes.. preBatchMutation is called before WAL write. So this is just fine.,non_debt,-
hbase,10001,comment_5,"Thanks for the confirmation, I'm going to update the comments & upload the v2 then.",non_debt,-
hbase,10001,comment_7,"I forgot the license. I can add it on commit. The test failure is unrelated. Any feedback on this? If it's ok, I would like to include it in 0.94 as well to help comparing the performances w/o being impacted by the disk i/o.",non_debt,-
hbase,10001,comment_8,+1 Pls add description (release notes) on how this can be used.,non_debt,-
hbase,10001,comment_9,"Committed, thanks for the review and the help, Anoop and Andrew.",non_debt,-
hbase,10001,comment_11,"This is targeted to 0.94, 0.96, and 0.98, but I see only the 0.98 commit.",non_debt,-
hbase,10001,comment_12,"Yes, I'm going to do it for 0.94 as well (today)",non_debt,-
hbase,10001,comment_14,"There is a bug linked to this: in we start with: if the preBatch hook call byPass, we then return immediately, because of this: then the finally clause is executed with walSyncSuccessful set to false, and then we try to rollback the memstore. In my load test, we're spending 15% of our time in the rollback. I can fix this in the coprocessor (by not setting bypass), but I think we should set the value of walSyncSuccessful later, just before we try to write something in the memstore: Any opinion? I can do the fix in or in the coprocessor.",non_debt,-
hbase,10001,comment_15,"We should fix it in HRegion, I think. Are you planning to change it into the Boolean (capital B) to have a way to indicate that we do not care, yet? Or have another flag (skippedByCoProc or something)? Or we could set walSyncSuccessful right before we return due to a coproc bypass.",non_debt,-
hbase,10001,comment_16,"I was thinking about something like this: What do you think? In any case, since we agree that it should be on HRegion, let me create another jira (as I need to commit the coprocessor for 0.94 here :-) )",non_debt,-
hbase,10001,comment_17,+1 on that change. And +1 on a different jira :),non_debt,-
hbase,10001,comment_22,Released in 0.96.1. Issue closed.,non_debt,-
hbase,10001,comment_23,"So, was there an addendum on this Nicolas? Or was the trunk patch what went into 0.96? Thanks.",non_debt,-
hbase,10001,comment_24,"Yes, it was the same patch for 0.96 & trunk.",non_debt,-
hbase,10001,comment_25,1,non_debt,-
hbase,10008,summary,is flakey on jenkins,test_debt,flaky_test
hbase,10008,description,"is flakey on Jenkins, this test has failed due to timeout a couple times for me on precommit. Here's the stacktrace.",test_debt,flaky_test
hbase,10008,comment_3,lgtm,non_debt,-
hbase,10008,comment_4,"Three consecutive buildbot runs without failure. I'll commit this later this afternoon unless objection. Thanks for having a look,",non_debt,-
hbase,10008,comment_8,"Committed to trunk. Thanks for the review, Ted.",non_debt,-
hbase,10008,comment_9,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,10065,summary,Stronger validation of key unwrapping,non_debt,-
hbase,10065,description,"In we use a CRC32 to validate the successful unwrapping of a data key. I chose a CRC32 to limit overhead. There is only a 1 in 2^32 chance of a random collision, low enough to be extremely unlikely. However, I was talking with my colleague Jerry Chen today about this. A cryptographic hash would lower the probability to essentially zero and we are only wrapping data keys once per HColumnDescriptor and once per HFile, saving a few bytes here and there only really. Might as well use the SHA of the data key and in addition consider running AES in GCM mode to cover that hash as additional authenticated data.",non_debt,-
hbase,10065,comment_0,"Attached patch uses SHA-1 instead of CRC32. Also, I rediscovered why I didn't use AES-GCM for this previously, AEAD modes are only supported by JDK 7+.",non_debt,-
hbase,10065,comment_1,Patch passes local tests and,non_debt,-
hbase,10065,comment_2,1,non_debt,-
hbase,10065,comment_4,HadoopQA is having issues.,non_debt,-
hbase,10065,comment_5,Committed to trunk. Thanks for the review Anoop.,non_debt,-
hbase,10065,comment_8,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,10074,summary,consolidate and improve capacity/sizing documentation,documentation_debt,low_quality_documentation
hbase,10074,description,"Region count description is in config section; region size description is in architecture sections; both of these have a lot of good technical details, but imho we could do better in terms of admin-centric advice. Currently, there's a nearly-empty capacity section; I'd like to rewrite it to consolidate capacity sizing information, and some basic configuration pertaining to it.",documentation_debt,low_quality_documentation
hbase,10074,comment_0,Here's the patch. Please advise if you think more information could be added.,non_debt,-
hbase,10074,comment_2,"I don't read docbook, so I cannot comment about the markup. However, the content is great! Here are some nits. Mind adding some JIRA references here? ""store file index to rise rising"" ? This sentence is confusing me. How about ""Hosting only 5 regions per RS will not be enough task splits for a mapreduce job, while 1000 regions will generate far too many map tasks."" In section {{<section +1",documentation_debt,low_quality_documentation
hbase,10074,comment_3,"all except the last one are migrated bits... let me fix them as part of 2nd iteration. I build docbook, it seems to look reasonable wrt formatting",non_debt,-
hbase,10074,comment_4,+1 I see a few spelling issues and would maybe change some of the numbers and statements but can be done in another issue. This reorg is excellent. Thanks .,documentation_debt,low_quality_documentation
hbase,10074,comment_5,"Actually, do you have particular JIRAs in mind?  thanks!",non_debt,-
hbase,10074,comment_6,"incorporated feedback, some spelling fixes and rephrases. I'd assume +1 stands, will commit in the afternoon",documentation_debt,low_quality_documentation
hbase,10074,comment_7,ok for 96?,non_debt,-
hbase,10074,comment_9,Looks good overall. I think we can collect a few more feedback for these recommendations. I LOVE the following quote (which is unfortunately true ATM),non_debt,-
hbase,10074,comment_10,"This is already committed to trunk, setting fix version accordingly. Consider committing to 0.98 branch also, updated affects version accordingly.",non_debt,-
hbase,10074,comment_11,The way I've been doing doc. is just commiting on trunk and then copying it back whenever I make a release. This works for now while the doc. is x-version. One day it won't. Then we can commit each doc change to each branch (version-specific recommendations are called out in the doc usually...),non_debt,-
hbase,10074,comment_12,"Ok, sure I can do a copy of trunk docbook to a 0.98 RC.",non_debt,-
hbase,10074,comment_15,"96.1 RC is being cut, will you copy the book to 96? I can push changes there, I have it ready, just need to tab and hit ""enter"" :)",non_debt,-
hbase,10074,comment_16,Go ahead and commit if you prefer to be sure. Hopefully I will just overwrite your change when I put up the next RC but I could forget!,non_debt,-
hbase,10074,comment_17,committed also to 96,non_debt,-
hbase,10074,comment_20,Released in 0.96.1. Issue closed.,non_debt,-
hbase,10075,summary,add a locality-aware balancer,non_debt,-
hbase,10075,description,"basic idea: during rebalance. For each region server, iterate regions, give each region a balance score, remove the lowest one until the region number of region server reach avg floor. during assignment. match to-be-assigned regions with each active region server as pairs, give each pair a balance score, the highest win this region. here is the mark formula: (1  * tableBalancerWeight + (1  * + * localityWeight + * stickinessWeight there are 4 factors which would influence the final balance score: 1. region balance 2. table region balance 3. region locality 4. region stickiness through adjust the weight of these 4 factors, we can balance the cluster in different strategy.",non_debt,-
hbase,10075,comment_0,"Consider attaching a patch against HBase trunk, the balancer related code has changed significantly since 0.94 and we bring in new features through trunk first.",non_debt,-
hbase,10075,comment_1,"Can you provide clarification on and Every table's region size is configurable and block size of a column family is configurable(so each each region server will have regions of different sizes), so does the number in the above parameters represent the number of regions on server, or something else?",non_debt,-
hbase,10075,comment_2,"With SLB taking into account data locality to calculate the cost and come-up with target balanced cluster, looks like the idea behind this request is taken care. no? Can this be closed?",non_debt,-
hbase,10081,summary,"Since HBASE-7091, HBASE_OPTS cannot be set on the command line",non_debt,-
hbase,10081,description,"Discussed in HBASE-7091. It's not critical, but a little bit surprising, as the comments in bin/hbase doesn't say anything about this. If you create your own hbase-env then it's not an issue...",documentation_debt,outdated_documentation
hbase,10081,comment_0,Confused by the same problem. I think we should keep the 'HBASE_OPTS' property user set working.,non_debt,-
hbase,10081,comment_1,Patch for trunk,non_debt,-
hbase,10081,comment_2,ping  It's something that we discussed a while ago in HBASE-7091. Do you mind having a look?,non_debt,-
hbase,10081,comment_3,"Ok, guess others are using it :) Looking through hbase-config.sh there are these lines: Which should ensure it doesn't get sourced multiple times - my concern for HBASE-7091. Should be fine... you okay with it ?",non_debt,-
hbase,10081,comment_4,"Hum. Doesn't that mean that the first time it will erase the value set in the shell, but not the next time? If yes, would it make sense to do something to have something like",non_debt,-
hbase,10081,comment_5,"It wouldn't erase it in the shell, I _think_, but rather just update it for the the scripts and all child processes. So, the calling shell would still have the variable you set. As an example, say I have (checked locally on my Mac): and in my hbase-env.sh have then after running bin/start-hbase.sh, I would still just see Should be fine, yes?",non_debt,-
hbase,10081,comment_8,"I didn't see the new messages - sorry about this. It's something I typically do on my dev environment. But I've seen it as well as a way to be sure that nothing is defined in the product itself, to be sure that we don't modify anything that is a part of the product (and hence modified when you upgrade). I'm not sure :-). , what do you think?",non_debt,-
hbase,10081,comment_9,The proposed change is in current master.,non_debt,-
hbase,10083,summary,[89-fb] Better error handling for the compound bloom filter,non_debt,-
hbase,10083,description,"When RegionServer failed to load a bloom block from HDFS due to any timeout or other reasons, it threw out the exception and disable the entire bloom filter for this HFile. This behavior does not make too much sense, especially for the compound bloom filter. Instead of disabling the bloom filter for the entire file, it could just return a potentially false positive result (true) and keep the bloom filter available.",design_debt,non-optimal_design
hbase,10083,comment_0,Bulk close issues per discussion at,non_debt,-
hbase,10115,summary,Add a hook for compaction deletes,non_debt,-
hbase,10115,description,The idea is to add a hook to coprocessors to be able to get triggered when compaction delete a cell before of the number of versions or the TTL.,non_debt,-
hbase,10115,comment_0,Can u tell the exact use case in your mind? Can a Scanner wrapper do the work? We have used wrapper to do some extra work during compaction. I forgot now the details of that.,non_debt,-
hbase,10115,comment_1,"I had few usecases in mind when I wrote that. 1) You have one table with some with a VERSION=5 limit because you want to keep if ""small"" and efficient. However, when a version is removed, you don't want to loose it because you need it for other purposes. So you want to ""transfer"" it to another table where you will have VERSION=MaxInt. 2) You have a TTL of 30 days in a column because you only need quick access to the last 30 days values. However, you want to keep he full history in HDFS. So each time a value expire from the table, you want to be able to store it into HDFS. You can steam it to flume, or push it on a queue, or write it into HDFS then aggregate the values, or anything else. 3) You have some mandatory requirements to keep EVERYTHING. Even what has been deleted. So when a delete is applied while compacting, you want to store somewhere else what has been deleted, just in case. Like a "".trash"" option, but for cells. There might be others, but that was all I had in mind at that time...",non_debt,-
hbase,10115,comment_2,I'm not sure I understand how these cases can't be addressed through appropriate schema options.,non_debt,-
hbase,10115,comment_3,"Hi , how will you build a schema for that? Like, let's take this usecase. User push data into HBase with versions=10. And want to make sure data after 10 versions is not lost. They can of course run MR jobs, count the versions, and make sure that version 10 is stored somewhere else in case a new version is coming and pushing it out, but will be a pain. Why can't they just hook on this deletion to do what they want with the version which is going to be purged? I'm not sure how a schema option can help with that.",non_debt,-
hbase,10115,comment_4,More like INT_MAX versions and housekeeping via MR tasks.,non_debt,-
hbase,10115,comment_5,"Why not another coproc endpoint or improve the existing one? As long as it doesn't affect perf too much... we can already wrap (or replace) compaction scanner thru coproc. ScanQueryMatcher is what decides what KVs get skipped, so if a wrapping scanner could influence that (or even just be able to react to .match call), it should be almost sufficient. It has KV, it has the default verdict. The only problem is with seek to next something results, which can skip KVs. This will no longer be allowed if all deleted KVs are to be examined if I understand that code correctly, so coproc will have to also prevent matcher from doing that?",design_debt,non-optimal_design
hbase,10115,comment_6,"Um, what I also meant to spell out more clearly is that there's no place now where we look at KV and decide that it's discarded that will be necessarily called for all KVs. Afaiu.",non_debt,-
hbase,10115,comment_7,Duplicate of HBASE-11054,non_debt,-
hbase,10213,summary,Add read log size per second metrics for replication source,non_debt,-
hbase,10213,description,"The current metrics of replication source contain logEditsReadRate, shippedBatchesRate, etc, which could indicate how fast the data replicated to peer cluster to some extent. However, it is not clear enough to know how many bytes replicating to peer cluster from these metrics. In production environment, it may be important to know the size of replicating data per second because the services may be affected if the network become busy.",non_debt,-
hbase,10213,comment_0,This patch adds a metric 'logReadRateInByte' to show how many bytes read by the source per second.,non_debt,-
hbase,10213,comment_2,"To get a good HadoopQA result, it will need a patch against trunk. should be The usual convention for names with units is to pluralize the unit, so",non_debt,-
hbase,10213,comment_3,Thanks for your comment . I update the patch for 0.94 and add a patch for trunk.,non_debt,-
hbase,10213,comment_5,"lgtm, ?",non_debt,-
hbase,10213,comment_6,+1 I need to go and make these not use the HashMap but this follows what's there now.,non_debt,-
hbase,10213,comment_7,"Integrated to trunk. Thanks for the patch, Jianwei. : Do you want this in 0.98 ?",non_debt,-
hbase,10213,comment_8,"Committed to 0.98. Thanks Ted. I fixed a spelling error on commit. Attached is an addendum for trunk to match what went into 0.98. I committed this trivial change to trunk using CTR as r1554361.  and : We can close this after a decision on 0.96 and 0.94. Almost feel bad pinging you, should be no big deal.",documentation_debt,low_quality_documentation
hbase,10213,comment_9,"Thanks for the catch, Andy. It would be not so good if a public method has spelling mistake.",documentation_debt,low_quality_documentation
hbase,10213,comment_10,Thanks for the comment  and I should pay more attention on spelling.,documentation_debt,low_quality_documentation
hbase,10213,comment_16,This has been committed to trunk and 0.98 and this issue is now just sitting here unresolved. Resolving. Reopen for commit to another branch or open a new JIRA for a backport at your option.,non_debt,-
hbase,10213,comment_17,"the intention of this jira is good:-), but by examining the patch: the metric above only reflects the log read/parse rate, not the desired replicating data to peer cluster rate, since the read/parsed log files may contain many kvs from column-families with replication scope=0 which will be filtered out and removed from the entries list before the real replicating to peer cluster occurs... why not use currentSize, the size of all entries which will be really replicated to the peer cluster?",design_debt,non-optimal_design
hbase,10213,comment_18,", thanks for your comment. We need to consider the filtered kvs when computing how much data replicate to peer cluster. The 'logReadRateInByte' only computes how much we read from HLog in source cluster. Maybe, we only another metrics such as",non_debt,-
hbase,10213,comment_19,Closing this issue after 0.99.0 release.,non_debt,-
hbase,10334,summary,RegionServer links in table.jsp is broken,documentation_debt,low_quality_documentation
hbase,10334,description,The links to RS's seems to be broken in table.jsp after HBASE-9892.,documentation_debt,low_quality_documentation
hbase,10334,comment_0,trivial patch. HBASE-9892 is committed to 0.98 and trunk for now. We should get this if as well if 0.96 and 0.94 is to be committed as well.,non_debt,-
hbase,10334,comment_1,1,non_debt,-
hbase,10334,comment_2,"Removed the 0.94 and 0.96 tags, as it is not an issue in those.",non_debt,-
hbase,10334,comment_3,"Although, then again, this won't hurt in either 0.94 or 0.96. Your call :)",non_debt,-
hbase,10334,comment_4,Seems fine for 0.96 if it works.,non_debt,-
hbase,10334,comment_5,"Yeah, +1 on 0.94 as well.",non_debt,-
hbase,10334,comment_6,"And then maybe we cal pull HBASE-9892. Memory on ""commodity"" machines is growing. Soon 128GB will be the default. The only answer HBase has is running multiple RegionServers on a single box.",non_debt,-
hbase,10334,comment_7,LGTM. Thanks .,non_debt,-
hbase,10334,comment_8,1,non_debt,-
hbase,10334,comment_9,"I've committed this to trunk and 98. Thanks for looking. I though we will have HBASE-9892 in 96 and 94, which is why I had added those fix versions. Once we get 9892 in, we can commit this as well.",non_debt,-
hbase,10334,comment_16,Closing this issue after 0.99.0 release.,non_debt,-
hbase,10390,summary,expose checkAndPut/Delete custom comparators in HTable,non_debt,-
hbase,10390,description,"checkAndPut/Delete appear to support custom comparators. However, thru HTable, there's no way to pass one, it always creates BinaryComparator from value. It would be good to expose the custom ones in the API.",non_debt,-
hbase,10390,comment_0,Patch v1.,non_debt,-
hbase,10390,comment_2,v2 patch.,non_debt,-
hbase,10390,comment_4,#NAME?,non_debt,-
hbase,10390,comment_5,"It is strange that some of the ""Comparators"" are not comparators, but actual {{Comparable}}s taking the value to compare. Of course, this issue is not for cleaning that up. An alternate would be to introduce: Notice the lack of explicit value, which is carried over with the Comparator. I am not sure this or the one in the patch is cleaner. I am fine with both.",non_debt,-
hbase,10390,comment_7,"Why does a fundamental Type -- i.e. Table -- reach down into a specialized package, 'filter', to obtain the type for compare. Seems broke. The Table Interface has no dependency on filter currently. Which of the list of subclasses makes sense in these new operations? All? Should the newly opened up constructor be marked as private since it is for internal use only, right?",non_debt,-
hbase,10390,comment_8,Was thinking the same thing some time ago. We already depend on {{CompareOp}} from filter. We should move these to parent package.,non_debt,-
hbase,10390,comment_9,Created HBASE-14997 for the compareOp change.,non_debt,-
hbase,10390,comment_10,Unscheduling from 2.0.0. No work done since 2015.,non_debt,-
hbase,10390,comment_12,Resolving as 'done'; we have this in hbase now. Reopen if i have this wrong.,non_debt,-
hbase,10391,summary,Deprecate KeyValue#getBuffer,non_debt,-
hbase,10391,description,Make the deprecation a subtask of the parent. Let the parent stand as an umbrella issue.,non_debt,-
hbase,10391,comment_0,Carried over from parent issue.,non_debt,-
hbase,10391,comment_1,Or let me wait on @apuretll ok before committing....,non_debt,-
hbase,10391,comment_2,+1 on patch.,non_debt,-
hbase,10391,comment_3,Committed to 0.98 and 0.99. Hope that ok,non_debt,-
hbase,10391,comment_7,"Sure, belated +1",non_debt,-
hbase,10391,comment_10,Closing this issue after 0.99.0 release.,non_debt,-
hbase,10417,summary,index is not incremented in,non_debt,-
hbase,10417,description,"Starting at line 76: index is a variable inside while loop that is never incremented. The condition ""index > 0"" cannot be true.",non_debt,-
hbase,10417,comment_0,lgtm,non_debt,-
hbase,10417,comment_2,"Thanks for review, Ted :)",non_debt,-
hbase,10417,comment_5,Not at a computer right now... Is this an issue for 0.94 as well?,non_debt,-
hbase,10417,comment_6,The problem is in 0.94 as well. Opened HBASE-11225 for backport.,non_debt,-
hbase,10417,comment_7,Closing this issue after 0.99.0 release.,non_debt,-
hbase,10456,summary,Balancer should not run if it's just turned off.,non_debt,-
hbase,10456,description,TestHBaseFsck failed recently because the balancer ran one more time even after it's turned off. Balancer should double-check if it is on before each run.,non_debt,-
hbase,10456,comment_0,Here is the log:,non_debt,-
hbase,10456,comment_1,1,non_debt,-
hbase,10456,comment_2,+1 for trunk and 0.98,non_debt,-
hbase,10456,comment_3,1,non_debt,-
hbase,10456,comment_5,Integrated into trunk and 0.98. Thanks.,non_debt,-
hbase,10456,comment_10,Closing this issue after 0.99.0 release.,non_debt,-
hbase,10496,summary,Add MurmurHash3 to 0.89-fb,non_debt,-
hbase,10496,description,Use MurmurHash3 in 0.89-fb.,non_debt,-
hbase,10496,comment_0,0.89-fb is no longer being actively maintained. If issues persist open an issue against the current master or stable versions.,non_debt,-
hbase,10555,summary,"Backport HBASE-8519 to 0.94, Backup master will never come up if primary master dies during initialization",non_debt,-
hbase,10555,description,"Backport HBASE-8519 to 0.94, Backup master will never come up if primary master dies during initialization. The issue described in the HBASE-8519 exits in 0.94, we need to backport the HBASE-8519 to the 0.94 too.",non_debt,-
hbase,10555,comment_0,"Looks reasonable, what do you think ?",non_debt,-
hbase,10555,comment_1,"My first reaction was ""Oh no, not more partial state in the master"", but in this case it seems necessary. +1 Will commit a bit later today.",non_debt,-
hbase,10555,comment_2,Committed to 0.94. Thanks for the patch.,non_debt,-
hbase,10561,summary,Forward port: HBASE-10212 New rpc metric: number of active handler,non_debt,-
hbase,10561,description,The metrics implementation has changed a lot in 0.96. Forward port HBASE-10212 to 0.96 and later.,non_debt,-
hbase,10561,comment_0,"How about this one.  , it would better if you have chance to take a look at the attached patch as well, since you're the original rpc scheduler and HBASE-10212's author:)",non_debt,-
hbase,10561,comment_2,+1 looks good to me,non_debt,-
hbase,10561,comment_3,"Hi Liang, +1 and thanks for porting this to 0.96. We're moving to 0.96 and will benefit from your effort.",non_debt,-
hbase,10561,comment_4,I'll commit it tomorrow unless any objection.,non_debt,-
hbase,10561,comment_5,"Integreted into trunk, 0.98 and 0.96 branches. There is a little difference in each branch, so the final committed stuff was not exactly same with the original patch, just FYI",non_debt,-
hbase,10561,comment_6,"Thank you ,  for your review.",non_debt,-
hbase,10561,comment_11,Closing this issue after 0.99.0 release.,non_debt,-
hbase,10562,summary,Fix for Hadoop 2 in 0.94,non_debt,-
hbase,10562,description,"See parent. This test was removed from 0.95.2. While investigating while it fails in 0.94 with Hadoop2 I found it is extremely slow and uses up a *lot* of threads. , any input?",code_debt,multi-thread_correctness
hbase,10562,comment_0,"NM. Needed: Allows tests to pass on Hadoop2. None-contentious change going to commit. We might want to consider reenabling the test for 0.96 and above. (, , FYI)",non_debt,-
hbase,10562,comment_1,Committed to 0.94.,non_debt,-
hbase,10562,comment_2,true),non_debt,-
hbase,10562,comment_3,":) Do you think we should resurrect this test in 0.96, 0.98, or trunk?",non_debt,-
hbase,10562,comment_4,"Put up a trunk patch, let's see.",non_debt,-
hbase,10562,comment_7,"Meh. Still failed in latest hadoop2 run. It does pass locally, though.",non_debt,-
hbase,10562,comment_10,It was disabled because couldn't get it to pass reliably.,non_debt,-
hbase,10562,comment_11,"Yeah, I am seeing the same in 0.94 and Hadoop 2. Passes fine locally, but fails frequently on the Jenkins build. It seems it actually starts a lot of processes/threads, so it might just be too much for the poor jenkins boxes.",non_debt,-
hbase,10631,summary,Avoid extra seek on FileLink open,code_debt,low_quality_code
hbase,10631,description,"There is an extra seek(0) on FileLink open, that we can skip",code_debt,low_quality_code
hbase,10631,comment_0,lgtm. +1,non_debt,-
hbase,10631,comment_1,Nice. +1 (also for 0.94 ),non_debt,-
hbase,10694,summary,casts integral division result to double,non_debt,-
hbase,10694,description,Integral division result is cast to double.,non_debt,-
hbase,10694,comment_0,Looks good to me. +1.,non_debt,-
hbase,10694,comment_2,"Thanks for the review, Jeff.",non_debt,-
hbase,10694,comment_7,Closing this issue after 0.99.0 release.,non_debt,-
hbase,10702,summary,HBase fails to respect Deletes,non_debt,-
hbase,10702,description,"One of our user contacted me about an issue with Deletes. Some of the deletes they do are not totally processed. Therefore, after the Delete, if they do a Get, from time to time, the Get return the row when it should have been deleted and should have returned nothing. After multiple Deletes, the row is finally deleted. If we don't retry after the 1st attempt, the row stays there. Even after a flush, a major_compact, etc. I have been able to reproduce the issue in 0.94.2 (CDH4.2.0 EC2), 0.94.15(CDH4.6.0 EC2) and 0.94.17 (Apache version bare metal) Here is a simple output from my test app. 1736509 Doing a delete for 0000099676 failed. Start to count puts=311 deletes=64 retries=2 2281712 Doing a delete for 0000027606 failed. Start to count puts=3679 deletes=247 retries=2 2388305 Doing a delete for 0000018306 failed. Start to count puts=4744 deletes=290 retries=2 2532943 Doing a delete for 0000030446 failed. Start to count puts=5678 deletes=337 retries=2 2551421 Doing a delete for 0000046304 failed. Start to count puts=5845 deletes=345 retries=2 2561099 Doing a delete for 0000019619 failed. Start to count puts=5869 deletes=347 retries=3 First field is the time in ms since the test started. So first error occurs after about 30 minutes. Below are the number of puts and deletes done, and the numbers of required retries to get the value deleted. Key is random number between 0000000000 and 0000100000. Very simple test. Just doing more puts than deletes. Tests are running on 0.96.1.1 for almost 1h now so it seems to be fine, but it's not on the same cluster, so I will keep that running for hours/days first.",non_debt,-
hbase,10702,comment_0,Can you share the test code? Might be a discrepancy of timestamps.,non_debt,-
hbase,10702,comment_1,"As you can see, it's a very very simple test. Just deletes and gets. Not playing with the timestamps nor the parameters nor the wal nor anything else like that. To respect the real usecase, *1 should be replaces by * 1000 but I changed that to get the results faster. Table is initially loaded with sequencialWrites 30K rows. So we are talking here to about 1 500 000 puts/day and 40 000 deletes/day. Which is still very small.",non_debt,-
hbase,10702,comment_2,"I forgot to provide some details on the user cluster. They are running with 12 racks, for a total or 110 region servers. All the servers are perfectly synchronized for the clock. They are running with 0.94.2, that's why I tried to reproduce with this version first. I ran my tests on a 4 node EC2 cluster, and a 8 node baremetal cluster.",non_debt,-
hbase,10702,comment_3,"You sure you want to use deleteColumn (vs. deleteColumns)? Delete.deleteColumn will only target the current latest version of that column for deletion (as determined by the time the delete arrives at the server). Not only is this very expensive (a Get on the server just to find the last ts), it also only targets the very latest version. (I mentioned before that the Delete API is confusing. This reminds me, maybe there's time to fix it before 1.0.)",design_debt,non-optimal_design
hbase,10702,comment_4,"deleteColums, deleteColumn, in both cases there is a get just before so it's already all in memory anyway. And this should not really change the delete feature, did it? If with deleteColumn, we should not be able to get the value we just delete, even more when we have to do that 6 time: No?",non_debt,-
hbase,10702,comment_5,"Check the timestamps. You could have Put a newer version after you issued the Delete, or you might see an older version. Try with deleteColumns as that is what you want anyway.",non_debt,-
hbase,10702,comment_6,"The puts and the gets are not done in parallel. They are done one after the other. So when I have to do 6 deletes to get the cell really deleted, there is nothing else writing to HBase at the same time. Only the puts and the gets. For me, even if it work with deleteColumns, I think there is still something wrong with deleteColumn. Oh! Except if there is multiple times the same cell written, so there is multiple version, and then when I try to delete it, I have to loop as many times as I have versions! Make total sense! I will run some more tests. But I think you are right... More to come.",non_debt,-
hbase,10702,comment_7,"Bingo :) You can do a ""raw"" scan from the shell (or Scan.setRaw(true)) to see what's really there, including the delete markers.",non_debt,-
hbase,10702,comment_8,"Now, why is it working in 0.96.1.1 with deleteColumn? I will try to write a testcase for that to validate that it's behaving as expected... Will close this one. Thanks!",test_debt,low_coverage
hbase,10702,comment_9,"0.96 and later default to VERSIONS=1, 0.94 and before to VERSIONS=3. That is probably the reason.",non_debt,-
hbase,10702,comment_10,"Again, make sense ;)",non_debt,-
hbase,10741,summary,Deprecate HTablePool and HTableFactory,non_debt,-
hbase,10741,description,"Per parent ticket, deprecate these ways to retrieve an HTable instance.",non_debt,-
hbase,10741,comment_0,I wouldn't mind sneaking this into 0.98.1 if you haven't spun the RC yet.,non_debt,-
hbase,10741,comment_1,lgtm On commit add 'since 0.98.1' to the javadoc deprecation tag.,non_debt,-
hbase,10741,comment_2,Go for it!,non_debt,-
hbase,10741,comment_4,The failures cannot be related...,non_debt,-
hbase,10741,comment_5,I don't want to confuse with the @since annotation Will instead use:,non_debt,-
hbase,10741,comment_6,Committed to three branches. Thanks for the review.,non_debt,-
hbase,10741,comment_7,Attaching what was committed.,non_debt,-
hbase,10741,comment_14,Closing this issue after 0.99.0 release.,non_debt,-
hbase,10779,summary,Doc hadoop1 deprecated in 0.98 and NOT supported in hbase 1.0,non_debt,-
hbase,10779,description,Do first two bullet items from parent issue adding doc to our hadoop support matrix.,non_debt,-
hbase,10779,comment_0,Updates the matrix with hadoop support in hbase 1.0. Adds footnotes to 0.98 and 1.0. In the hadoop 2 vs hadoop 1 notes hadoop 1 deprecated.. etc.,non_debt,-
hbase,10779,comment_1,Here is HTML of the configuration page for easier review,non_debt,-
hbase,10779,comment_2,"""You should run Hadoop 2. rather than Hadoop 1. if you can. "" Fix periods? Either remove or make ""2.x rather than 1.x if you can"" 2.3 not tested? Isn't hbase1.x defaulting to 2.3.x currently? -- // Jonathan Hsieh (shay) // HBase Tech Lead, Software Engineer, Cloudera // jon@cloudera.com // @jmhsieh",non_debt,-
hbase,10779,comment_3,"Thanks for review Jon. I made the changes you suggested and committed. Yes, hbase 1.0 is not tested on h2.3. Can change when we come close to release (we'll probably ship h2.4).",non_debt,-
hbase,10779,comment_5,"Coming in late. The changes look good. hadoop-2.4 is coming soon, so +1 for shipping with that.",non_debt,-
hbase,10779,comment_7,Closing this issue after 0.99.0 release.,non_debt,-
hbase,10864,summary,Spelling nit,documentation_debt,low_quality_documentation
hbase,10864,description,We should really be more careful about spelling qualifier,documentation_debt,low_quality_documentation
hbase,10864,comment_0,"Alex, this and the next issue are too trivial to warrant issues. Contrib something more substantial like moving our pb to protostuff in a critical location.",non_debt,-
hbase,10864,comment_1,"Or, do a bunch at a time rather than piecemeal them in?",non_debt,-
hbase,10864,comment_2,Ah no problem! Close her out. Sorry for the spam.,non_debt,-
hbase,10864,comment_3,"Well, it is something to fix....",non_debt,-
hbase,10864,comment_5,"Oh dear, i realized what the problem was, I plopped the wrong patch file in. The other one fixes something like 30 spelling mistakes. It may still be too minor",documentation_debt,low_quality_documentation
hbase,10864,comment_6,Committed to trunk. Thanks Alex.,non_debt,-
hbase,10864,comment_8,Closing this issue after 0.99.0 release.,non_debt,-
hbase,10892,summary,[Shell] Add support for globs in user_permission,non_debt,-
hbase,10892,description,It would be nice for {{user_permission}} to show all the permissions for all the tables or a subset of tables if a glob (regex) is provided.,non_debt,-
hbase,10892,comment_0,"looks ok to me, (I've also tried it a bit) but there are a couple of things I'm not sure about I think that the glob only for ""all tables"" may be confusing, because then you can't do ""tableName*"" you must use ""tableName.*"" but I'm ok having it as a special ""all tables"" case. is and was committed as Deprecated. I know that is used in other places, but we should decide if we want to use that for new code or not and later replace it where is used.",design_debt,non-optimal_design
hbase,10892,comment_1,I can add the missing . to the regex so it can work both ways. I can add another implementation without will that work ?,non_debt,-
hbase,10892,comment_3,if you do want to address those two points from Matteo then this would be good to go I'd say.,non_debt,-
hbase,10892,comment_4,New patch with the changes suggested by,non_debt,-
hbase,10892,comment_6,Yeah we need to fix the source of those zombies in precommit builds. It's not this patch. Will commit shortly unless objection. I will fix the long lines at commit time.,build_debt,build_others
hbase,10892,comment_7,+1 for me,non_debt,-
hbase,10892,comment_8,Committed to trunk and 0.98. Tested with 0.98. Thanks for the patch !,non_debt,-
hbase,10892,comment_12,Closing this issue after 0.99.0 release.,non_debt,-
hbase,10899,summary,[AccessController] Apply MAX_VERSIONS from schema or request when scanning,non_debt,-
hbase,10899,description,Similar to HBASE-10854 we need to handle different versions while the versions are in memstore and/or before compaction happens when the max versions for the CF == 1.,non_debt,-
hbase,10899,comment_0,Have a patch for this. Will wait for HBASE-10854 to get in.,non_debt,-
hbase,10899,comment_2,Ping to commit?,non_debt,-
hbase,10899,comment_3,+1 looks good to me,non_debt,-
hbase,10899,comment_4,As per Anoop's suggestion changed the comparison logic to that in HBASE-10854.  Thanks for the review.,non_debt,-
hbase,10899,comment_5,"+1 on v2, if tests pass.",non_debt,-
hbase,10899,comment_7,Don't think the test failure is related to this patch. +1,non_debt,-
hbase,10899,comment_8,Committed to trunk and 0.99 Thanks everyone for the reviews.,non_debt,-
hbase,10899,comment_12,Closing this issue after 0.99.0 release.,non_debt,-
hbase,10920,summary,HBase throws,non_debt,-
hbase,10920,description,Configuration conf = hconnection = //(Parser.java:40) Results In Error: Method) Caused by: at Method) ... 11 more Caused by: org/htrace/Trace ... 16 more Caused by: org.htrace.Trace Method) ... 22 more,non_debt,-
hbase,10920,comment_0,Looks like you're missing the htrace jar on your client's classpath,non_debt,-
hbase,10920,comment_1,TableMapReduceUtil methods should be setting the correct classpath including htrace. Resolving this.,non_debt,-
hbase,10925,summary,"Do not OOME, throw RowTooBigException instead",non_debt,-
hbase,10925,description,"If 10M columns in a row, throw a RowTooBigException rather than OOME when Get'ing or Scanning w/o in-row scan flag set.",non_debt,-
hbase,10925,comment_0,"Do you have any thoughts on how the ""row too big"" condition should be calculated  ?",non_debt,-
hbase,10925,comment_1,"my 2 cents..may be go not by num of columns, but by total size of row (in bytes), and either compare that with configurable threshold (not more than 2 gb), or with fraction of the JVM/RS node memory available (like not more than 20% thereof)?",non_debt,-
hbase,10925,comment_2,I could pick up this one I guess..,non_debt,-
hbase,10925,comment_3,",  if the thoughts above make sense I could take it I guess, as it's unassigned now.",non_debt,-
hbase,10925,comment_4,Size accumulated sounds right boss.,non_debt,-
hbase,10925,comment_5,"thanks, will start working on patch then",non_debt,-
hbase,10925,comment_6,"First draft version of patch, to review general logic. I actually found 2 scenarios of OOME, running tests with -Xmx128. - one, really artificial, but still: when row contains the few huge cells (like 10Mb each), and they are put and without compaction, than upon read the all store file scanners get sought after the key being looked up, and HRS fails with OOME even before actual scanner.next() is called. Like that: Java heap space As a fix for that, I put the check in the code which does the initial seek in the scanner, just to illustrate. Not that it's valid usecase really? - second usecase, if when record just have a lot of columns, store files are compacted, but they scanners just attempt to load too much data. The fix I made is to calculate the total size of Cell elements which are getting appended to the result list. Error for second case (without fix) looks like this: Java heap space",non_debt,-
hbase,10925,comment_7,"Looks good. This should be a configuration: + long maxRowSize = 10 * 1024 * 1024; On the below: + totalReadSize += kv.getRowLength() + + + + + if (totalReadSize + throw new size of row is :"" + maxRowSize + + "", but the row is bigger than that.""); + } ... it is a bit of a pain having to do the above. I suppose we should add a getSize to CellUtils. Could do that in another patch. Was also thinking what if the Cell is offheap but then above is probably fine still because while it is offheap in the server, in the client it may not be and a 10G rowsize is likely larger than it can swallow w/o OOME.",design_debt,non-optimal_design
hbase,10925,comment_8,"Sure, I just wanted to show the draft to validate approach. That shall be in hbase-site.xml I think (what would be the appropriate default limit I wonder? 1Gg? I think size expressed in mb or gb is better than % of heapsize.) Yes - or may be in the Cell interface itself? I can add this in another patch, yep. I thought off-heap-related work is in design/prototyping stage? Will check out. But anyway, it'd be off-heap (JVM-wise), but still in-process for HRS, right? So still counts towards total memory usage on the server process, and should be limited to reasonable size, or idea is that off-heap-allocated cells would be taken care off.._manually_, and no Scanner-level checks are needed? Also curious about how valid the first usecase actually is?",non_debt,-
hbase,10925,comment_9,"1Gig seems fine for first cut. Suggest adding to CellUtil for now. Having Cell implement HeapSize got some pushback IIRC elsewhere. Yeah, though offheap in server, it could be in heap when gets to client and OOME it.... so I think this scheme works for both cases.... Go for it.",non_debt,-
hbase,10925,comment_10,"Heh, actually there is already in CellUtil. Going to use that unless there's reason not to. Will rework patch today.",non_debt,-
hbase,10925,comment_11,Revised patch. - added max row size to hbase conf - use to get cell size,non_debt,-
hbase,10925,comment_13,"fixed regression tests failures, added missed license header. Findbugs don't look related to this patch.",non_debt,-
hbase,10925,comment_15,Very nice. +1. Leaving it open a while in case others want to review. Will commit in a day or so unless objection.,non_debt,-
hbase,10925,comment_16,Thanks for review!,non_debt,-
hbase,10925,comment_17,Committed to trunk. Thanks for the patch,non_debt,-
hbase,10925,comment_18,Thanks!,non_debt,-
hbase,10925,comment_20,Closing this issue after 0.99.0 release.,non_debt,-
hbase,10925,comment_21,What do you guys think about adding another configurable option to the client where you can potentially skip large rows if the configuration is set. A nice example is if you have jobs running and don't want them to fail because of a large row. You can simply catch the exception and then resume the scan with the (row_key + pad a empty byte) thus your jobs / scans wont fail. I think it would be best to have this as a parameter to the Scan but having an API change would not be ideal.,non_debt,-
hbase,10968,summary,Null check in is redundant,design_debt,non-optimal_design
hbase,10968,description,"Here is related code: context was dereferenced first, leaving the null check ineffective.",design_debt,non-optimal_design
hbase,10968,comment_0,"I've the fix in HBASE-7987 patch, but +1 to have the one line fix in now.",non_debt,-
hbase,10968,comment_1,Thanks Matteo. Patch also removes an unused local variable.,code_debt,dead_code
hbase,10968,comment_2,1,non_debt,-
hbase,10968,comment_3,"Thanks for the review, Matteo.",non_debt,-
hbase,10968,comment_5,Closing this issue after 0.99.0 release.,non_debt,-
hbase,10981,summary,taskmonitor use subList may cause recursion and get a,non_debt,-
hbase,10981,description,"2014-04-14 11:52:45,905 ERROR Caught throwable while processing event M_RS_CLOSE_REGION Caused by:",non_debt,-
hbase,10981,comment_0,Dup of HBASE-10312?,non_debt,-
hbase,10981,comment_2,I'd rather we use my patch from HBASE-10312.,non_debt,-
hbase,10981,comment_3,"Should this be closed? The patch applies against code that was changed in HBASE-10312, so I think it is a non-issue now that JDC's has been committed.",non_debt,-
hbase,10981,comment_4,"Yup, resolving as dup of HBASE-10312.",non_debt,-
hbase,10997,summary,Add a modulo argument to PE to constrain the key range,non_debt,-
hbase,10997,description,Helps w/ keeping PE inside block cache but same number of clients.,non_debt,-
hbase,10997,comment_0,Small patch,non_debt,-
hbase,10997,comment_1,Committed to trunk. Helps w/ my testing.,non_debt,-
hbase,10997,comment_3,"Hey boss, does this supersede HBASE-9953?",non_debt,-
hbase,10997,comment_4,"Don't know about supercede. I missed hbase-9953. The modulo is a bit more basic. All random read clients were picking at random from the total namespace. The module narrows the namespace that clients operate in. If module is 10, then all clients will pick rows in range 0-10 even though rows might be 1M and clients 1000. Does that supercede? The patches look to do similar things. Your size confines where we operate in a similar way? (I can back this out np. You can imagine why I need this).",non_debt,-
hbase,10997,comment_5,"HBASE-9953 lets me think in terms of total GiB data size instead of ((rows * clients) / million rows-per-GiB). Given a total target size and a known rowsize, it will fill in the blank for --rows. This one limits the range of values a client will use. Sorry, caffeine hasn't set in yet; I'm not sure how this is useful. I guess it's for hotspotting whichever regions are hosting those first $(modulo) rows, is that it? In short, I think they're compatible.",non_debt,-
hbase,10997,comment_6,"I use it like this. I want to run a PE that does random reads and that runs for a good while, long enough for the latency pattern to emerge. I want to run ten clients. Block cache is small. I want the random reads to stay in block cache. As is, the random reads range over a namespace that is clients X rows. To stay inside block cache, I can set rows small but my test is over before a pattern is well established. If I set rows to 1B, then clients will get 1B rows but if I add the modulo, the keys will be confined to the lower part of the range (dependent on what modulo is). If I want to bust block cache but stay inside the os cache, I up the modulo till just before i/o comes on. I like your 'size' better than my modulo. I'll remove this if your size works. Thanks. Sorry for committing this w/o waiting on review. Didn't think anyone cared (missed your size issue).",non_debt,-
hbase,10997,comment_7,Going to revert this. The --size argument does what this adds better.,non_debt,-
hbase,10997,comment_8,My revert.,non_debt,-
hbase,10997,comment_9,Reverted this patch. Removed fix version. Shouldn't show up in any release.,non_debt,-
hbase,10997,comment_10,Closing to shut it down.,non_debt,-
hbase,11011,summary,Avoid extra getFileStatus() calls on Region startup,design_debt,non-optimal_design
hbase,11011,description,"On load we already have a StoreFileInfo and we create it from the path, this will result in an extra fs.getFileStatus() call. In we do a fs.exists() and later a fs.getFileStatus() to create the StoreFileInfo, we can avoid the exists.",design_debt,non-optimal_design
hbase,11011,comment_0,lgtm,non_debt,-
hbase,11011,comment_2,"What should one do when seeing this message? The fact that a file is missing seems pretty bad, yet it's at DEBUG.",non_debt,-
hbase,11011,comment_3,"In this case is not bad, is an ""expected"" situation, where the RS died before moving the compacted file. Anyway, I think that all the loop can be removed, since is trying to lookup files in and if they are there are already loaded for sure.",code_debt,low_quality_code
hbase,11011,comment_4,"This might be, but the log message doesn't convey any of that. Taken out of context (so most users reading our logs), it just says a file is missing. I'm trusting you on this one :)",code_debt,low_quality_code
hbase,11011,comment_5,Does the changed code in require a unit test? Or is there already one? Also fix those lines:,non_debt,-
hbase,11011,comment_6,"none of the above, the removed code was doing nothing (see the comment inside the function). The output files in the entry are simply not useful",code_debt,low_quality_code
hbase,11011,comment_7,"Alright, +1",non_debt,-
hbase,11011,comment_14,Closing this issue after 0.99.0 release.,non_debt,-
hbase,11050,summary,Replace empty catch block in with @Test(expected=),non_debt,-
hbase,11050,description,"This change refactor The test basically create path, conf);}} and after that parent {{path}} is renamed followed to another call path, conf);}} in this moment is expected IOException, because the parent {{path}} doesn't exist more. The second call monitored by a block try-catch with an empty {{catch}}: The patch proposed removes the {{try-catch}} and use to capture exception produced by the test.",non_debt,-
hbase,11050,comment_0,"Patch looks good to me. It would be great, if you can also update the description section for this issue.",non_debt,-
hbase,11050,comment_2,"Hi, . Has been filled up a description. Thanks for review.",non_debt,-
hbase,11050,comment_3,Nice description :),non_debt,-
hbase,11050,comment_4,"Integrated to trunk. Thanks for the patch, Gustavo.",non_debt,-
hbase,11050,comment_6,"Since this has been committed, can we close this JIRA?",non_debt,-
hbase,11050,comment_7,I think this would be closed when hbase 1.0 is released.,non_debt,-
hbase,11050,comment_8,Closing this issue after 0.99.0 release.,non_debt,-
hbase,11053,summary,Change DeleteTracker APIs to work with Cell,non_debt,-
hbase,11053,description,DeleteTracker interface (marked as Private) should work with Cells.,non_debt,-
hbase,11053,comment_0,Patch uses Cell and the refactoring of the test case is also done.,non_debt,-
hbase,11053,comment_2,"+1, lgtm",non_debt,-
hbase,11053,comment_3,Will commit this soon. Will be needed for the parent JIRA (visibility expression deletes) .,non_debt,-
hbase,11053,comment_4,Resubmitting for QA to run again.,non_debt,-
hbase,11053,comment_6,"Committed to trunk. If I commit this to 0.98 one thing is as there is no Cell related changes in 0.98, so passing kv to the DeleteTracker would make it do , kv.getTimeStamp(), kv.getTypeByte() once again which is already extracted out in the SQM itself.  But if we do a visibility deletion strategy based on DeleteTracker, then this change is very much and later would be easier in 0.99. Also would help in extracting the tag details easily from the cell itself. Would wait before committing to 0.98.",non_debt,-
hbase,11053,comment_9,"Yes, well we need this in 0.98 so please consider making the change and attach what you commit here. +1 if what you describe above is the only change necessary.",non_debt,-
hbase,11053,comment_10,Patch that I committed to 0.98 is attached here. Thanks for the review,non_debt,-
hbase,11053,comment_13,Javadoc fix. I missed this. Thanks Ted for the heads up.,documentation_debt,low_quality_documentation
hbase,11053,comment_14,Committed the addendum to 0.98 and trunk.,non_debt,-
hbase,11053,comment_18,Closing this issue after 0.99.0 release.,non_debt,-
hbase,11083,summary,ExportSnapshot should provide capability to limit bandwidth consumption,non_debt,-
hbase,11083,description,"This capability was first brought up in this thread: The rewritten distcp already provides this capability. See MAPREDUCE-2765 distcp implementation utilizes which provides bandwidth throttling on a specified InputStream. As a first step, we can * add an option to ExportSnapshot which expresses bandwidth per map in MB * utilize in",non_debt,-
hbase,11083,comment_0,assuming that the distcp/hadoop2 dependency is ok for the project the patch looks good to me,non_debt,-
hbase,11083,comment_2,Patch v2 adds optimization where is only created when bandwidth parameter is non-default value.,non_debt,-
hbase,11083,comment_3,"+1, is this trunk only since it depends on hadoop2?",non_debt,-
hbase,11083,comment_4,Yes.,non_debt,-
hbase,11083,comment_6,"Thanks for the review, Matteo.",non_debt,-
hbase,11083,comment_7,We use 0.94.7 and Hadoop 2.0.0-cdh4.2.0. Can this patch work with this combination?,non_debt,-
hbase,11083,comment_8,"You can check whether MAPREDUCE-2765 is in the distro you're using - I assume it is. If so, it should work after minor modification.",non_debt,-
hbase,11083,comment_12,Closing this issue after 0.99.0 release.,non_debt,-
hbase,11129,summary,Expose Scan conversion methods in TableMapReduceUtil as public methods,non_debt,-
hbase,11129,description,Scan#readFields() from 0.92 has been removed. TableMapReduceUtil has the following package private methods: We should consider exposing them as public methods so that user can interpret Scan objects easily in mapreduce jobs.,non_debt,-
hbase,11129,comment_0,Changed access level to public.,non_debt,-
hbase,11129,comment_1,exposes scan parameters via configuration. Can you read the required scan attributes from conf instead? It looks like should be modified to break up the scan into pieces instead of serializing the opaque blob -- then we can do away with the base64 encoding entirely and there's no internal vs external API. Will either of these solutions work for you?,non_debt,-
hbase,11129,comment_2,"That would not be compatible with existing Scan serialization, right ?",non_debt,-
hbase,11129,comment_3,"Passing the scanner on the jobconf is a private API, now that the serialization details are private methods. This implementation detail should be isolated within a single job -- either it picks up the 0.X.Y hbase-server jar or it has 0.X.Z version, there's no mixing. We'd need to test it out, but I think making this change could be acceptable for a patch release. Looking at either is respected, or params are used. What I propose does away with the former. This way, these configs become part of the public API.",test_debt,lack_of_tests
hbase,11129,comment_4,"Looking for some context here, I cannot find the commit that makes this interface private. Do you know off the top of your head , )?",non_debt,-
hbase,11129,comment_5,We could enjoy this issue to refactor and plan some tests. This changes require voting?,non_debt,-
hbase,11129,comment_6,These methods are currently public.,non_debt,-
hbase,11134,summary,Add a -list-snapshots option to SnapshotInfo,non_debt,-
hbase,11134,description,Add a -list-snapshots option to SnapshotInfo to show all the snapshots available. Also add a -remote-dir option to simplify the usage of SnapshotInfo in case the snapshot dir is not the one of the current hbase cluster,non_debt,-
hbase,11134,comment_1,"Binding these vars to FSUtils so they can be picked up later seems strange -- can't this be made more explicit (e.g. when remote is specified we set some vars, if not we load defaults, and then make the static that lists use those vars)",non_debt,-
hbase,11134,comment_2,"This isn't specific of list snapshot. this is a quick way to specify a different location. currently you have to do I don't want pass around locations all around the code because you may want a different root-dir than the one that you have in the conf file. It is just an helper. getSnapshotList() is doing just what is supposed to do, from the configuration that you have specified list the snapshots.",non_debt,-
hbase,11134,comment_3,+1 on v1,non_debt,-
hbase,11174,summary,show backup/restore progress,non_debt,-
hbase,11174,description,"h2. Feature Description the jira is part of and depend on full backup [HBASE-10900| and incremental backup [HBASE-11085| for the detail layout and frame work, please reference to [HBASE-10900| A backup/restore operation may take a while to complete, sometimes hours. It will be helpful to show the estimated progress as percentage to user. The jira will provide such functionally",non_debt,-
hbase,11174,comment_0,"Moving out of 1.0. No progress. Parent is not against 1.0. If it comes in before 1.0 release, great.",non_debt,-
hbase,11212,summary,Fix increment index in KeyValueSortReducer,non_debt,-
hbase,11212,description,"The index is never incremented inside the loop, therefore context.setStatus also is never set.",non_debt,-
hbase,11212,comment_0,"Hi, I found the same problem, reported for you. Could you please review again :) ? Thanks.",non_debt,-
hbase,11212,comment_2,Test failure is unrelated. +1 for commit to trunk and 0.98. Commit on hold until the SVN->GIT migration is finished.,non_debt,-
hbase,11212,comment_3,Thanks.,non_debt,-
hbase,11212,comment_4,+1 on 0.94 as well.  0.96? Or are we letting that one die?,non_debt,-
hbase,11212,comment_5,Too trivial...,non_debt,-
hbase,11212,comment_6,"Does that mean it ""too trivial"" to bother? Or ""too trivial"" in terms of risk and do you do want it? :)",non_debt,-
hbase,11212,comment_7,Too trivial to bother.,non_debt,-
hbase,11212,comment_8,"Committed to 0.94, 0.98, master for now.",non_debt,-
hbase,11229,summary,Change block cache percentage metrics to be doubles rather than ints,design_debt,non-optimal_design
hbase,11229,description,"See parent issue. Small changes in the hit percentage can have large implications, even when movement is inside a single percent: i.e. going from 99.11 to 99.87 percent. As is, percents are ints. Make them doubles.",design_debt,non-optimal_design
hbase,11229,comment_0,Try this. Also renames the metric in jmx from - String = + String = ... all other blockcache metrics have the 'blockCache' prefix but this one.,non_debt,-
hbase,11229,comment_1,1,non_debt,-
hbase,11229,comment_2,Committed to master. Thanks for the review .,non_debt,-
hbase,11229,comment_4,failing since this patch went in. Small addendum to fix the unit test.,non_debt,-
hbase,11229,comment_6,Closing this issue after 0.99.0 release.,non_debt,-
hbase,11293,summary,Master and Region servers fail to start when and Kerberos is enabled,non_debt,-
hbase,11293,description,Setting causes the _HOST substitution in to result in which in turn causes kerberos authentication to fail.,non_debt,-
hbase,11293,comment_0,Do you need to be listening on multiple interfaces?,non_debt,-
hbase,11293,comment_1,"Yes, we have multiple nics on all hosts.",non_debt,-
hbase,11293,comment_2,"The problem is that the hostname passed to the login call ends up being 0.0.0.0 instead of the actual hostname (the substitution of _HOST string is not the problem). In HMaster.java, the server's hostname is also obtained via Hadoop's DNS library code In Hadoop's DNS.java, by default this hostname would be given by Maybe, we should pass the same in the login call ?",non_debt,-
hbase,11293,comment_3,"That could work, as long as we also respect the configuration settings (the name of the interface we should retrieve the local IP address to look up the canonical hostname with) and (the nameserver we should use for queries, including reverse IP lookups)",non_debt,-
hbase,11293,comment_4,This patch has been tested internally.,non_debt,-
hbase,11293,comment_5,", could you please take a look at the patch. Thanks!",non_debt,-
hbase,11293,comment_6,lgtm,non_debt,-
hbase,11293,comment_7,1,non_debt,-
hbase,11293,comment_9,Will upload a trunk patch shortly. The patch uploaded last was for 0.98.x.,non_debt,-
hbase,11293,comment_10,"+1, would be good to have a test also",test_debt,lack_of_tests
hbase,11293,comment_11,", this is not done yet. Trunk needs to have a patch.",non_debt,-
hbase,11293,comment_12,should we go forward with this patch?,non_debt,-
hbase,11293,comment_13,"Yes, will get to it this week.",non_debt,-
hbase,11293,comment_14,"From what I can tell, this issue doesn't affect the branch-1+ branches.",non_debt,-
hbase,11352,summary,"When HMaster starts up it deletes the tmp snapshot directory, if you are exporting a snapshot at that time the job will fail",non_debt,-
hbase,11352,description,"We are exporting a very large table. The export snapshot job takes 7+ days to complete. During that time we had to bounce HMaster. When HMaster initializes, it initializes the SnapshotManager which subsequently deletes the .tmp directory. If this happens while the ExportSnapshot job is running the reference files get removed and the job fails. Maybe we could put some sort of token such that when this job is running HMaster wont reset the tmp directory.",non_debt,-
hbase,11352,comment_0,"Created a configuration option for expiration of in progress snapshots, now master checks this as well as the modified timestamp for the snapshotInProgress and only deletes when the snapshot is expired. Thought 30 days would be a conservative starting value. This really hurt us when we did an ExportSnapshot that took 1+ week to complete.",non_debt,-
hbase,11352,comment_1,Expiration would be measured in days. Maybe use different unit for the above config ? There're long lines in the patch - line length should be 100 or shorter.,code_debt,low_quality_code
hbase,11352,comment_2,Fixed the long log lines and updated the expiration time to hours.,code_debt,low_quality_code
hbase,11352,comment_3,Can you attach patch for trunk ? Thanks,non_debt,-
hbase,11352,comment_4,In newer versions of HBase you can just select the skipTmp option when Exporting Snapshots this will resolve this issue.,non_debt,-
hbase,11421,summary,HTableInterface javadoc correction,documentation_debt,low_quality_documentation
hbase,11421,description,"HTable#batch() APIs javadoc says * @param actions list of Get, Put, Delete, Increment, Append, RowMutations objects But we can not pass RowMutations in batch. Small patch to correct the doc.",documentation_debt,low_quality_documentation
hbase,11421,comment_0,one related question RowMutations and mutateRow() is this really needed now? What if the same set of Mutations (Puts and Deletes) where added in one batch and called batch() - In the same row mutations will be executed in a single mini batch go. This is because the row lock is reentrant now. Previously it was not and so there was significance for this API,non_debt,-
hbase,11421,comment_1,+1 on javadoc fix. Maybe ask question on dev list? I see batch mostly deprecated and seems like a push toward use of mutateRow. Would be nice if we could strip a marker interface.,non_debt,-
hbase,11421,comment_2,Pushed to master. Thanks Stack.,non_debt,-
hbase,11421,comment_4,Closing this issue after 0.99.0 release.,non_debt,-
hbase,11443,summary,times out,non_debt,-
hbase,11443,description,"After HBASE-10070 is merged to trunk, times out in Jenkins builds. This is due to not counting down in The following new method in HStore.java: calls false) which is not overridden by",non_debt,-
hbase,11443,comment_0,passes locally with the patch.,non_debt,-
hbase,11443,comment_1,+1. You beat me to it.,non_debt,-
hbase,11443,comment_2,"TestIOFencing passes reliably. Integrated to trunk. Thanks for the review, Enis.",non_debt,-
hbase,11443,comment_3,Nice test fix jira.,non_debt,-
hbase,11443,comment_6,Closing this issue after 0.99.0 release.,non_debt,-
hbase,11461,summary,Compilation errors are not posted back to JIRA during QA run,non_debt,-
hbase,11461,description,Compile errors are not posted to JIRA because misses call to submitJiraComment Here is an example:,non_debt,-
hbase,11461,comment_1,"lgtm, +1",non_debt,-
hbase,11461,comment_3,"Ted, can you please set the fixVersion when you commit and resolve an issue.",non_debt,-
hbase,11461,comment_4,Will set fixVersion in the future.,non_debt,-
hbase,11511,summary,Write flush events to WAL,non_debt,-
hbase,11511,description,"We used to write COMPLETE_FLUSH event to WAL until it got removed in 0.96 in issue HBASE-7329. For secondary region replicas, it is important to share the data files with the primary region. So we should reintroduce the flush wal markers so that the secondary region replicas can pick up the newly flushed files from the WAL and start serving data from those. A design doc which explains the context a bit better can be found in HBASE-11183.",design_debt,non-optimal_design
hbase,11511,comment_0,"Here is a patch that writes START_FLUSH, COMMIT_FLUSH and ABORT_FLUSH events to WAL. I tried to take special care to the exception code paths to make sure that we do not cause hangs, etc on flush fail and region close. COMMIT_FLUSH is written after the store files are actually committed, and it does not happen atomically ( of course). This means that we can miss COMMIT_FLUSH events after START_FLUSH if the region server fails, etc. Corresponding HBASE-11512 might help with WAL tailers to pick up new files if we persist all the hfiles for the region at region open time to WAL. I am running the unit tests.",non_debt,-
hbase,11511,comment_1,Attaching rebased patch.,non_debt,-
hbase,11511,comment_2,RB here:,non_debt,-
hbase,11511,comment_3,"Before we would sync all outstanding edits. Now when we flush we flush just the start flush tx? - if (wal != null && !shouldSyncLog()) wal.sync(); + if (wal != null) { + try { + wal.sync(trxId); // ensure that flush marker is sync'ed + } catch (IOException ioe) { + exception while log.sync(), ignoring. Exception: "" + + + } + } That is ok? What if an edit after the start flush edit was added? Is this last line in the right location? Should it be inside the bracket? + } ... or are we still dealing with failure at this point? Patch looks good otherwise.",non_debt,-
hbase,11511,comment_4,"Thanks Stack for taking a look. Good point. We writelock the updatesLock, then start the mvcc transaction, so it should be the case that sync() and sync(trx) should be the same thing I guess. But let me change it to be plain old sync() to be failure-proof. Indeed. The block above is a catch block for catching an ignoring any exception from writing the abort marker to WAL. The call to should still happen in the outer block, resulting in",design_debt,non-optimal_design
hbase,11511,comment_5,+1 then,non_debt,-
hbase,11511,comment_6,v3 addresses Stack's and Ted's feedback. Thanks for reviews. I'll commit if hadoopqa passes.,non_debt,-
hbase,11511,comment_9,Attaching the final version that has been committed. Minor rebase.,non_debt,-
hbase,11511,comment_10,Committed this to master and branch-1. Thanks Stack for review.,non_debt,-
hbase,11511,comment_13,"Newly added test sometimes fails flakily. Although I could not reproduce it locally, I think it is because of a fs list ordering issue. Will commit the addendum shortly. Let's see whether this solves the issue. Example trace:",non_debt,-
hbase,11511,comment_16,Closing this issue after 0.99.0 release.,non_debt,-
hbase,11575,summary,Pseudo distributed mode does not work as documented,non_debt,-
hbase,11575,description,"After master-RS colocation, now the pseudo dist-mode does not work as documented since you cannot start a region server in the same port 16020. I think we can either select a random port (and info port) for the master's region server, or document how to do a pseudo-distributed setup in the book.  wdyt?",non_debt,-
hbase,11575,comment_0,Probably it's better to use a random port. Let me take a look.,non_debt,-
hbase,11575,comment_1,"Looked into it and attached a patch that fixed some documentation mismatches, changed the pseudo distributed mode setting a little so that we just start one instance. Additional regionservers can be started with the local regionserver sh script.",documentation_debt,low_quality_documentation
hbase,11575,comment_2,"So we are not doing the random port approach? It should be ok. With set to 1, master should be able to finish initialization. Can user regions be opened in this mode from the RS inside msater?",non_debt,-
hbase,11575,comment_3,"We can do random port for web UI, but not for IPC port. Yes, user regions can be opened in the RS inside the master. If additional regionservers are started, balancer can move user regions out of the master.",non_debt,-
hbase,11575,comment_4,We probably should avoid random port. See HBASE-10289.,non_debt,-
hbase,11575,comment_5,+1 for branch-1+ if you have tested the script changes. Thanks Jimmy for nailing this.,non_debt,-
hbase,11575,comment_6,"Yes, the patch worked for me. Pseudo distributed mode is fixed. Thanks for reporting the issue and reviewing the patch. Integrated in branch 1 and master.",non_debt,-
hbase,11575,comment_9,Does it have to be this way Jimmy? This is in branch-1 too? That'd be a pain for users. Would be ok for master I'd say.,non_debt,-
hbase,11575,comment_10,"Unfortunately, it is in branch-1 too. The problem is that master uses the same RPC port as a region server now. Not sure how to handle this better.",non_debt,-
hbase,11575,comment_11,Release note is removed since we are going to change the code so that additional config is not needed. See HBASE-11604.,non_debt,-
hbase,11575,comment_12,Closing this issue after 0.99.0 release.,non_debt,-
hbase,11612,summary,Remove,non_debt,-
hbase,11612,description,Can be removed now from 0.98 onwards? The javadoc says it should be removed on a major release after 0.96. It does a full Meta scan which takes quite some time especially if there are around 1M regions.,design_debt,non-optimal_design
hbase,11612,comment_0,That was already raised before and there's some discussion in other jira. Suggesting to close this one as a dup.,non_debt,-
hbase,11621,summary,Make MiniDFSCluster run faster,code_debt,slow_algorithm
hbase,11621,description,Daryn proposed the following change in HDFS-6773: With this change in runtime for TestAdmin went from 8:35 min to 7 min,code_debt,slow_algorithm
hbase,11621,comment_0,Tentative patch. Running test suite to see which test(s) break.,non_debt,-
hbase,11621,comment_1,Test suite for 0.98 passed with patch:,non_debt,-
hbase,11621,comment_3,+1 for 0.98. Ping  for branch-1,non_debt,-
hbase,11621,comment_4,"Here was 0.98 test suite with patch - on the same host as the previous run: With patch, hbase-server module went from 66 min to 51 min.",code_debt,slow_algorithm
hbase,11621,comment_5,"Looks like some reflection is needed if this change goes to 0.98 For hadoop-1, I got:",non_debt,-
hbase,11621,comment_6,"Patch for 0.98 If is not accessible, log the fact and continue.",non_debt,-
hbase,11621,comment_8,Integrated to 3 branches. Thanks Andrew for the review.,non_debt,-
hbase,11621,comment_13,Thanks,non_debt,-
hbase,11621,comment_14,Closing this issue after 0.99.0 release.,non_debt,-
hbase,11650,summary,Write hbase.id to a temporary location and move into place,non_debt,-
hbase,11650,description,We should write hbase.id to a temporary location and move it into place. Will avoid potential race issues between create and write.,non_debt,-
hbase,11650,comment_0,Attaching patch. All tests pass using 0.98.,non_debt,-
hbase,11650,comment_1,1,non_debt,-
hbase,11650,comment_2,"would {{new Path(new Path(x,y),z)}} be cleaner that {{new Path(x, y+FILE_SEP+z)}}?",non_debt,-
hbase,11650,comment_3,Probably but this the same as previous change. I have a +1 so will commit this. Shout if you want different.,non_debt,-
hbase,11650,comment_4,Good to go :),non_debt,-
hbase,11650,comment_5,Committed to 0.98+,non_debt,-
hbase,11650,comment_13,Closing this issue after 0.99.0 release.,non_debt,-
hbase,11693,summary,Backport HBASE-11026 (Provide option to filter out all rows in tool) to 0.94,non_debt,-
hbase,11693,description,Backport HBASE-11026 (Provide option to filter out all rows in tool) to 0.94 branch.,non_debt,-
hbase,11693,comment_0,"Patch ready. Not setting patch available because Jenkins won't know what to do with a 0.94 patch. Ping , tested manually",non_debt,-
hbase,11693,comment_1,+1 Same gentle reminder to try it with both local and mapred modes.,non_debt,-
hbase,11693,comment_2,"Right, I did only test this in threaded mode. I made several passes over the changes and believe all MR details are accounted for. I don't have access to a cluster but can test on a single node YARN setup. Will do that and report back.",test_debt,low_coverage
hbase,11693,comment_3,1,non_debt,-
hbase,11693,comment_4,Set up a single node HDFS+HBase+YARN cluster. Application logs show '--filterAll' taking effect.,non_debt,-
hbase,11693,comment_5,Pushed to 0.94,non_debt,-
hbase,11730,summary,Document release managers for non-deprecated branches,non_debt,-
hbase,11730,description,"New development goes against trunk and is backported as desired to existing release branches. From what I have seen on the jira, it looks like each branch's release manager makes the call on backporting a particular issue. We should document both this norm and who the relevant release manager is for each branch. In the current docs, I'd suggest adding the RM list to the ""Codelines"" section (18.11.1) and add a brief explanation of pinging the RM as a new section after ""submitting a patch again"" (18.12.6). Post HBASE-4593, the note about pinging a prior branch RM should just go as a bullet in the ""patch workflow.""",documentation_debt,low_quality_documentation
hbase,11730,comment_0,"From watching Jira and talking to , the current RM list is: * < 0.94:  * 0.94:  * 0.96:  (eol or soon to be) * 0.98:  * 1.0:",non_debt,-
hbase,11730,comment_1,"Yes. My understanding of Apache foundation level process is the individuals in the RM role assemble release candidate artifacts for voting on by the PMC. What the RM puts together can be anything that can be traced back to a revision in SCM. This will only work if the RM and PMC are in general agreement, so the RM has in effect a filtering function delegated by the PMC. In HBase we don't have anything formalized beyond the basics, which I think is a good thing. Whether the RM wishes to assert that filtering function prior to commit to a branch or after is up to the individual. Our general practice is to ping a RM and wait for acknowledgement before backport of discretionary changes. Important bug fixes don't need this. My personal preference is I'd like an opportunity to review significant changes before commit but in all other cases please feel free to commit to 0.98 branch, I'll handle changes on a C-T-R basis.",non_debt,-
hbase,11730,comment_2,+1 to everything Andy said. Not sure we need to document this. An RM is just somebody who volunteers to do that. In some projects there's an RM for each individual release. In HBase we happen to come to a model where the same person does all the releases for a particular major version. We're not very formal about this.,non_debt,-
hbase,11730,comment_3,"At a minimum, I think having a ""ask Foo about inclusion in release X"" would be useful, in the same way we have an [explanation for and pointers to the component It gives new people a leg up on figuring out the existing social norm instead of requiring them to reverse engineer it from ""ping Foo for branch X"" notes in jira. Actually maybe this should be under the community roles section and not in codelines as I suggested earlier.",non_debt,-
hbase,11730,comment_4,That sounds reasonable to me,non_debt,-
hbase,11730,comment_6,"BTW. a ""permanent"" release manager for a branch is something we informally made up. Other Apache project choose a RM for each release they do. I really think we should not make this role more than it is. An RM is not a ""decider"" about what goes somewhere and what not doesn't (not more than a committer would anyway), just somebody who keeps the a release (or branch) coherent.",non_debt,-
hbase,11730,comment_7,"+1 on the  sentiment above though reading the patch, I do not see any violation. Maybe  on commit you could add a sentence derived from his formulation of what a RM does? Otherwise the patch lgtm. Fix 'maintaned' on commit. I was going to suggest that you add note that 0.96 is EOL'd but then you'd have to add a note for all before EOL'd too. Might not be bad to do?",non_debt,-
hbase,11730,comment_8,"Could we just leave out EOL'd versions? Seems like if an issue is severe enough to warrant resurrecting one of them, it's going to require a dev@hbase email anyways. That would remove the entries for 0.96 and < 0.94.",non_debt,-
hbase,11730,comment_9,That'd work.,non_debt,-
hbase,11730,comment_10,Committed to master and branch-1.,non_debt,-
hbase,11730,comment_13,Closing this issue after 0.99.1 release.,non_debt,-
hbase,11846,summary,should log if a full HFile verification will be performed during a bulkload,non_debt,-
hbase,11846,description,"If is set to true in the Region Server, we should log that we are about to perform a full scan of the HFiles that are going to be bulk loaded, it might be helpful to correlate other performance issues if the operator has enabled that feature.",non_debt,-
hbase,11846,comment_0,Good idea. Are you going to provide a patch for that?,non_debt,-
hbase,11846,comment_1,patch attached .,non_debt,-
hbase,11846,comment_2,1,non_debt,-
hbase,11846,comment_3,+1 (non-binding),non_debt,-
hbase,11846,comment_4,Thanks for the patch. Pushed to 0.98+. The 0.98 version needed a trivial fixup.,non_debt,-
hbase,11846,comment_5,Thanks for the review  and the quick commit !,non_debt,-
hbase,11846,comment_11,Closing this issue after 0.99.0 release.,non_debt,-
hbase,11863,summary,WAL files are not archived and stays in the WAL directory after splitting,non_debt,-
hbase,11863,description,"In patch HBASE-11094, it seems that we changed the constructor we are using for SplitLogManager, which does not archive the log files after splitting is done to the archive folder. The log files stays in the splitting directory forever and re-split every time the master restarts. It is surprising that our unit tests are passing (since 0.94.4) without any issues. Part of the reason is that the split is actually carried, but the WAL is not moved and thus the -splitting directory never gets deleted. It seems critical to fix in 0.98.6, FYI.",non_debt,-
hbase,11863,comment_0,"It seems that this has already been fixed by HBASE-11654 for 0.98. I am now checking to see whether master and branch-1 has this issue. In any case, I have 2 more unit tests that we can commit as a part of this issue.",non_debt,-
hbase,11863,comment_1,"Ok, making this a blocker. I'll wait for the unit tests as additional confirmation.",non_debt,-
hbase,11863,comment_2,Attaching a patch which adds the unit tests.,non_debt,-
hbase,11863,comment_3,Thanks Enis,non_debt,-
hbase,11863,comment_4,Both patches look good to me!(+1). Thanks.,non_debt,-
hbase,11863,comment_5,+1 here too. Applied the 0.98 patch and the modified tests passed.,non_debt,-
hbase,11863,comment_6,The master patch here is stale after HBASE-11072,non_debt,-
hbase,11863,comment_7,"You mean 0.98.4 in description, right? (not 0.94.4)",non_debt,-
hbase,11863,comment_8,"yes, it should be 0.98.4.",non_debt,-
hbase,11863,comment_9,"I'd like to roll 0.98.6 RC1 on Monday. I'll push the 0.98 patch to that branch then if this issue isn't otherwise resolved, unless objection.",non_debt,-
hbase,11863,comment_10,"I had to rebase the patch due to HBASE-11072, but the master tests are failing for me, so I am not able to commit this one yet. I think the problem maybe due to an earlier commit (I think HBASE-11689).",non_debt,-
hbase,11863,comment_11,"Are we good with the 0.98 patch though Enis? Applies cleanly and passes, I'd say yes.",non_debt,-
hbase,11863,comment_13,Yep. Feel free to commit the 0.98 patch for the RC.,non_debt,-
hbase,11863,comment_14,I've committed v3 patches (trivial changes) to 0.98+. Thanks for reviews.,non_debt,-
hbase,11863,comment_15,Thanks!,non_debt,-
hbase,11863,comment_20,Closing this issue after 0.99.0 release.,non_debt,-
hbase,11928,summary,Modified 'class Get' to support TFilterV2,non_debt,-
hbase,11928,description,Added TFilterV2 on filter getter / setter for 'class Get',non_debt,-
hbase,11928,comment_0,0.89-fb is no longer being actively maintained. If issues persist open an issue against the current master or stable versions.,non_debt,-
hbase,11935,summary,ZooKeeper connection storm after queue failover with slave cluster down,non_debt,-
hbase,11935,description,"We just ran into a production incident with TCP SYN storms on port 2181 (zookeeper). In our case the slave cluster was not running. When we bounced the primary cluster we saw an ""unbounded"" number of failover threads all hammering the hosts on the slave ZK machines (which did not run ZK at the time)... Causing overall degradation of network performance between datacenters. Looking at the code we noticed that the thread pool handling of the Failover workers was probably unintended. Patch coming soon.",non_debt,-
hbase,11935,comment_0,"Attaching fix for 0.98. Worked up with ,  and rest of the folks at Salesforce. Not sure if there is a good way to test for this behavior that isn't completely pointless.",non_debt,-
hbase,11935,comment_1,We could get unbounded ReplicationSource allocation in,non_debt,-
hbase,11935,comment_2,Patch for trunk Replication tests pass locally:,non_debt,-
hbase,11935,comment_3,Let's add some logging too... The queue failover if very quiet.,code_debt,low_quality_code
hbase,11935,comment_5,"Patch for 0.98 w/ a log message every time we start a new ReplicationSource and the logs its taking over, at debug level",non_debt,-
hbase,11935,comment_6,Attaching patch for trunk with same log line added,non_debt,-
hbase,11935,comment_8,Fixed patch for trunk,non_debt,-
hbase,11935,comment_9,That's what i get for doing my coding in vim. Thanks,non_debt,-
hbase,11935,comment_11,"passed 10 out of 10 times locally for me, e.g.",non_debt,-
hbase,11935,comment_12,Turns out that this is not the full story. We're still leaking ZK Connection somewhere when the slave cluster's ZK ensemble is not up. Debugging...,code_debt,low_quality_code
hbase,11935,comment_13,This one has held up to initial testing. Will post a trunk patch soon and report back with more actual cluster testing tomorrow. The core of the change is that we do schedule every single queue on a thread (there might be 1000's or even 100's of 1000's). Instead we schedule a thread per failed RS and handle all that RSs queue inside the same thread (by calling ReplicationSource's run() inline). While we looked at the code we all felt it is time to rethink the architecture and rewrite the replication source side from scratch. It has reached the point where incremental changes are no longer appropriate... But that is for another jira.,design_debt,non-optimal_design
hbase,11935,comment_14,And a trunk version.,non_debt,-
hbase,11935,comment_16,Removed patches from parent. Will open two subtasks for discussion,non_debt,-
hbase,11935,comment_17,Can be resolved after subtasks,non_debt,-
hbase,12017,summary,Use instead of HTable constructors.,non_debt,-
hbase,12017,description,"Now that Connection and ConnectionFactory are in place, internal code should use them instead of HTable constructors as per HBASE-11825.",design_debt,non-optimal_design
hbase,12017,comment_0,"HBASE-11825 creates Connection and Connection factory.  sugggested that is the way to obtain a Table instance"" rather than HTable constructors.",non_debt,-
hbase,12017,comment_1,This seems to be a huge change. I'm going to have to add more issues to track this. I'm going to start with: 1) Replace internal uses of signatures with byte[] and String tableNames to use the TableName equivalents. 2) Replace uses of HTableInterface with Table 3) Change all uses of HTable constructors to use the constructors with Connections rather than Configuration.,non_debt,-
hbase,12017,comment_2,"Yeah, those patches will be quite big. Breaking up makes a lot of sense. Let me know how I can help.",non_debt,-
hbase,12017,comment_3,Closing this issue after 0.99.2 release.,non_debt,-
hbase,12030,summary,Wrong compaction report and assert when MOB compaction switches to minor,non_debt,-
hbase,12030,description,"when zookeeper is down during a major compaction or a sweep tool run in progress, we switch to a minor. but the ""request start"" is ""major"" and increments the major-compactions counter while the ""request end"" is ""minor"" and decrements the minor-compactions counter triggering the assert newValue >= 0, since we are decrementing the wrong counter",non_debt,-
hbase,12030,comment_0,"Thanks for the good catch . I had doubt on this major - minor degradation. Checked other angles bit missed the part of metric. Thinking in that angle I am coming up with a patch. Trying to not degrade major to minor. Just providing way in CompactionRequest to forcefully say that keep delete markers. If this is set, whatever be the type of compaction, we will keep the delete tomb stones after the compaction. patch coming soon Also Jingcheng noticed and informed me that there is an issue in HMobStore#compact() where we check whether we need major- minor degrade. We check just Actually we should check isAllFiles() because that decided whether to keep the delete markers or not. I will fix that issue also in this patch",non_debt,-
hbase,12030,comment_2,"Ping , ,",non_debt,-
hbase,12030,comment_3,"looks ok to me, it will be nice having a test to cover the path when zk throws an exception.. maybe the easiest way there is mocking zk?",test_debt,low_coverage
hbase,12030,comment_4,Thanks . The patch contains the unit test is uploaded.,non_debt,-
hbase,12030,comment_6,"Thanks a lot Jingcheng for the test. 2 comments on the test We flush compactionThreshold with puts. This itself can trigger an auto compaction (before the delete going in) So the assert + 1, countStoreFiles()) can get failed some times. Better we add compactionThreshold -1 times put and one time delete and then go with compaction. compaction: store files"", 1, countStoreFiles()); -> The msg to be After compaction",non_debt,-
hbase,12030,comment_7,Fixing the comments in tests,non_debt,-
hbase,12030,comment_9,+1 on patch. Looks good.,non_debt,-
hbase,12030,comment_10,lgtm +1.,non_debt,-
hbase,12030,comment_11,Pushed to hbase-11339 branch. Thanks a lot Jingcheng for helping with a nice test. Thanks all for the reviews,non_debt,-
hbase,12059,summary,Create hbase-annotations module,non_debt,-
hbase,12059,description,Different versions of hadoop have different annotations. We can smooth this out by providing our own.,design_debt,non-optimal_design
hbase,12059,comment_0,Have examples ? What pain we saving ourselves doing this sir? Patch seems fine to me. Should get it into 0.99 if we are going this route.,non_debt,-
hbase,12059,comment_1,Mostly this is the start of making it possible to run on hadoop 20. While that will never be a recommended I'm trying to make it possible. Additionally this will allow us to annotate the hbase-hadoop-compat module.,non_debt,-
hbase,12059,comment_2,"I'm good w/ this then if it helps you folks migrate. +1. You should probably get another one or so from fellas who like this annotation stuff ...  and , you fellas have done bunch of work in here.... You good w/ this?",non_debt,-
hbase,12059,comment_4,"Elliott, can you generate docs and make sure the doclet generates two sets of javadoc -- one with the public api only and one with the full dev api? +1 if that works. I'd imagine we'd start getting errors with new files where folks use the hadoop classification instead of the hbase one -- would be good to have a quick grep precheck in the qabot that warns if the hadoop one is included in a diff. (fine as a follow on)",non_debt,-
hbase,12059,comment_5,Sure checking that now. And I'll file a follow on issue.,non_debt,-
hbase,12059,comment_6,"branch-1 actually requires jdk7 support. Do we need this: Also we've dropped hadoop1 support. Not sure how hard it will be to bring it back. If you guys need it, it may be ok.",non_debt,-
hbase,12059,comment_7,Here's what ultimately worked. I hadn't gotten the jar onto the javadoc classpath before.,non_debt,-
hbase,12059,comment_8,Thoughts on this one in 0.98? Would be nice for us but I totally understand if you don't want the churn.,non_debt,-
hbase,12059,comment_9,"It's not a functional change, one more jar that has to be shipped with newer releases. I'm game if others are.",non_debt,-
hbase,12059,comment_13,can you please also address this from my earlier comment: bq. branch-1 actually requires jdk7 support. Do we need this:,non_debt,-
hbase,12059,comment_14,Sorry. Yeah we didn't need it so I just kept the jdk7 part. We might need to add a jdk8 part when we support that more fully.,non_debt,-
hbase,12059,comment_15,Cool. Thanks. Should we resolve this jira or are you working on 0.98 patch?,non_debt,-
hbase,12059,comment_16,I'm working on the 0.98 patch right now. It should be up in ~15 mins.,non_debt,-
hbase,12059,comment_17,Thanks for the review everyone. I know this was a large patch. I appreciate all the help.,non_debt,-
hbase,12059,comment_19,Wonder why we have not changed InterfaceAudience to hbase's? I can see only InterfaceStability being changed in all files.,non_debt,-
hbase,12059,comment_20,I must have missed adding those in the sed. I'll add a follow on jira for that.,non_debt,-
hbase,12059,comment_22,Thanks,non_debt,-
hbase,12059,comment_23,Closing this issue after 0.99.1 release.,non_debt,-
hbase,12106,summary,Move test annotations to test artifact,non_debt,-
hbase,12106,description,Test annotation interfaces used to be under then moved to We should move them to,code_debt,low_quality_code
hbase,12106,comment_0,Simple patch. Pls review pom changes.,non_debt,-
hbase,12106,comment_2,Attaching patch for 0.98 and 0.99 as well.,non_debt,-
hbase,12106,comment_3,updated patch.,non_debt,-
hbase,12106,comment_6,Can I get a review? Fairly trivial patch.,non_debt,-
hbase,12106,comment_7,"+1, will commit shortly.",non_debt,-
hbase,12106,comment_8,Pushed to 0.98+,non_debt,-
hbase,12106,comment_9,I'm going to need to commit an addendum momentarily to get 'mvn test' passing with this change,non_debt,-
hbase,12106,comment_10,Attached addendum I pushed to 0.98+. We need to add hbase-annotations as a test-jar dep to hbase-testing-util or 'mvn test' will fail at that module. This is from a 0.98 build:,non_debt,-
hbase,12106,comment_17,Closing this issue after 0.99.1 release.,non_debt,-
hbase,12115,summary,Fix NumberFormat Exception in,non_debt,-
hbase,12115,description,DNS.reverseDns doesn't work well with IPv6 addresses. This patch is to fix that.,non_debt,-
hbase,12115,comment_0,Removed unnecessary changes from the commit.,code_debt,dead_code
hbase,12115,comment_1,Here is the diff on phabricator :,non_debt,-
hbase,12115,comment_2,Falling back to InetAddress in case DNS class fails to produce a reverseDNS lookup.,non_debt,-
hbase,12115,comment_3,1,non_debt,-
hbase,12115,comment_5,Sorry missed that. You need to add the annotation to the test.,non_debt,-
hbase,12115,comment_6,+1 after adding size annotation to test (copy from adjacent test. This looks like a smalltest),non_debt,-
hbase,12115,comment_7,Fixing the by adding the category annotations to my test case.,non_debt,-
hbase,12115,comment_8,+1 for 0.99+.,non_debt,-
hbase,12115,comment_13,Closing this issue after 0.99.1 release.,non_debt,-
hbase,12211,summary,hbase-daemon.sh direct the ulimit output to log .out instead of log .log?,non_debt,-
hbase,12211,description,"In the hbase-daemon.sh file line 199, it has the content: The variable loglog is defined as: For my understanding, this information should be printed to the ""logout"" variable; we should not mix this ""ulimit"" information with the actual log printed by hbase java program.",design_debt,non-optimal_design
hbase,12211,comment_0,"What are you thinking ? We want it in the log so that if the log is passed to a hbase supporter, they can check what the ulimit was...",non_debt,-
hbase,12211,comment_1,"Hi , we should not put everything in to one single file to make it contains everything. The .out file is used to store stdout and stderr (in case user misconfigured log4j, all hbase logs will goto this file). The .log file is meant to store only the log4j outputs from the hbase codes. This .out and .log files together makes the ""hbase log"". Customer need to let the supporter to know the ulimit value, but not need it in the "".log"" file.",non_debt,-
hbase,12211,comment_2,"Around here .out is ignored searched only if suspect out of memory or a segfault or looking for JVM output. .log has a bunch of other 'environment' stuff printed at startup like vitals on the jvm, classpath, etc. Having all env stuff but the ulimit in .log would be splitting where to look for info, don't you think?",non_debt,-
hbase,12211,comment_3,"what do you mean by ""around here""? If we check the hadoop log, we can find that the ulimit information is in the .out file;",non_debt,-
hbase,12211,comment_4,"Apache HBase Project. In all builds since near project inception, we've done it this way. What do you mean by 'hadoop'? I changed the 'should' in the subject till you've made a case. Thanks.",non_debt,-
hbase,12211,comment_5,"At least tell us what pain this causes you, if any, or how it would make your life easier? Thanks.",non_debt,-
hbase,12211,comment_6,"In our scenario, we need to process the logs; while for hadoop-hdfs, this ulimit information is in .out file and for hbase, this ulimit information is in .log file.",non_debt,-
hbase,12211,comment_7,"Would suggest you ask out on the mailing list if anyone minds you moving the ulimit info into the .out. If no one objects, put up a patch? My suggestion that you ask on the mailing list is because not many pay attention to JIRA. This is a change in a long-established behavior... my guess is that no one will object but the change needs an airing wider than just JIRA (someone else may have log processing tools that expect it in .log). Thanks.",non_debt,-
hbase,12211,comment_8,"Thanks , this is a good suggestion. I will send mail out in user list.",non_debt,-
hbase,12211,comment_9,"There was no discussion on the mailing list about this topic, unlikely to get picked up anymore.",non_debt,-
hbase,12235,summary,Backport to 0.94: HBASE-9002 should test correct region,non_debt,-
hbase,12235,description,"I had fail on a internal test rig. It's very infrequent. The proximal failure looks like HBASE-9002, the assertion finds 0 edits instead of 1000. Looking through the logs, the #regions printed is 1 And then in the makeHLog method I never see any log entry for the actual writing I think what's happening is we're picking a region that is META or ROOT, so the checks after the ""#regions="" line removes the region and we end up inserting no data. It looks like the patch for HBASE-9002 should fix this.",non_debt,-
hbase,12235,comment_0,patch from HBASE-9002 rebased to work on 0.94. ran {{mvn test}} successfully.,non_debt,-
hbase,12235,comment_2,Looks good. +1,non_debt,-
hbase,12235,comment_3,Pushed to 0.94 Thanks mighty Sean.,non_debt,-
hbase,12238,summary,A few ugly exceptions on startup,design_debt,non-optimal_design
hbase,12238,description,"Let me fix a few innocuous exceptions that show on startup (saw testing 0.99.1), even when regular -- will throw people off. Here is one: More to follow...",design_debt,non-optimal_design
hbase,12238,comment_0,In standalone mode this is a little disorientating... it shows up frequently:,design_debt,non-optimal_design
hbase,12238,comment_1,"Remove logging of NSRE from MTL. Other minor fixup. Spent some time on Unable to read additional data from client sessionid 0x0, likely client has closed socket"" Turning off ipv6 seemed to clear this one up locally. But it is a common complaint if you search around: In this latter case it is not ipv6. There is also a stall when standalone for a couple of seconds. Will look at that in another issue. This will do for this issue for now.",non_debt,-
hbase,12238,comment_3,Retry. Minor.,non_debt,-
hbase,12238,comment_5,Committed small patch that is mostly doc and log emissions edits.,non_debt,-
hbase,12238,comment_8,Closing this issue after 0.99.2 release.,non_debt,-
hbase,12255,summary,NPE in OpenRegionHandler after restart hdfs without stop hbase,non_debt,-
hbase,12255,description,"I have a phoenix table 'EVENT', and the table have a index 'IDX_DATE_HOUR_X'. I restarted hdfs without stop hbase, after then, the hbase table cann't be scaned. I try to restart hbase, the all hbase table still cann't be scaned. the regionserver log have many exception like ., starting to roll back the global memstore size. After deleteing the 'recovered.edits' in the 'EVENT' table's region, the hbase table can be scanned. so i think the reason maybe that: after restart hbase, the regionserver begin opening the region and replaying WAL, when replaying the EVENT's WAL, because EVENT has a index table IDX_DATE_HOUR_X, so the replay process should operate IDX_DATE_HOUR_X, but at this moment, the IDX_DATE_HOUR_X table's region is in OPENING stats, it is't unavaiabled, so the EVENT replaying process Time and time again to retry and throw Exception aging and again. if I disable EVENT first, others hbase table recory successfully, but when i enable EVENT, the region log occur this make the region transition to OPEN_FAILED.",non_debt,-
hbase,12255,comment_0,"This looks like an HBase bug, so I moved it to the HBase project. By any chance is a complete stacktrace for those logged? Can you provide it here?",non_debt,-
hbase,12255,comment_1,The log only has a single line no more than stacktrace,non_debt,-
hbase,12255,comment_2,Not enough info here to go off of on an EOL version of HBase. Cannot Reproduce.,non_debt,-
hbase,12266,summary,Slow Scan can cause dead loop in ClientScanner,code_debt,low_quality_code
hbase,12266,description,see,non_debt,-
hbase,12266,comment_0,any particular purpose to set it to true there? thanks.,non_debt,-
hbase,12266,comment_2,"Thanks for filing. It looks like that was set to true due to a VERY similar issue so that it would only reset/retry for one rpc call. In my case though, the ""reset/retry only for one call"" doesn't work because it will have a few successful rpc calls after the reset (the earlier ones are getting a tiny amount of data, and then the super slow rpc happens on about the 4th rpc). Looking a bit further down, there is logic for the that will make sure we're not over the scannerTimeout. Should that logic maybe be used for all exceptions? Maybe even at the top of the while loop, but I think changing it only in the exception case would fix my issue...it would stop retrying after 60s (the default for scannerTimeout).",non_debt,-
hbase,12266,comment_3,"This is a very similar issue. The fix in HBASE-7070 doesn't work in the case where a very selective scan hits a large region, and an rpc call times out. The flag fix assumes that only one rpc call is causing trouble (because it is reset on the next successful rpc). In my case, after the scanner is reset, several rpc calls succeed (they get a small amount of data), and the 4th or 5th rpc call hits an rpcTimeout due to hitting large region section and it is a highly selective scan.",non_debt,-
hbase,12266,comment_4,This is not helping? The change in the patch makes that in one next() call at max one time a reset and re scan can happen (when,non_debt,-
hbase,12266,comment_5,Is this endless loop really?,code_debt,low_quality_code
hbase,12266,comment_6,"That isn't helping because as I've mentioned, that guard only assumes that only one rpc call (the current/next) is the misbehaving one. In my case, it is a very selective scan, and before the reset, will be set to false, but the next rpc call will succeed just fine (because it gets a tiny amount of data, but not enough to fulfill the scan batch size) and then is set back to true. It really is an endless loop :) I've stopped it after running for over a day. The logs show it is doing the same loop about every 11s. My vote here honestly would be to keep the line, but move the scanTimeout check to be done for all (vs just what it is currently checking scanTimeout only for when it is a",non_debt,-
hbase,12266,comment_7,Something like this ?,non_debt,-
hbase,12266,comment_8,"Yes, that is what I was thinking.",non_debt,-
hbase,12266,comment_10,"Thanks Guys, frankly it looks to me such retry(including HBASE-7070) just makes code more complicated to read and easy to create new bugs in complex system. and it is hard to be covered by test. As mentioned in that jira: since it is caused by client side timeout, why not just throw exception so that user(or app layer code) knows it and set a bigger value. the timeout value is case by case, that is why we make it configurable, right?",code_debt,complex_code
hbase,12266,comment_11,", you mean never retry after",non_debt,-
hbase,12266,comment_12,Sounds like this issue is related to the heartbeat/keepalive idea in HBASE-13090. The idea over there is to track how long a scan has been executing server side and to return periodic heartbeat/keepalive messages in the event that the scan is taking a long time. The frequency of these heartbeats messages would be dependent upon the configured scanner timeout (a more restrictive timeout would lead to more frequent heartbeat messages). This solution would address the issue of a slow scan and would also remove the possibility of this dead loop. Thoughts? Think we could close this one out?,non_debt,-
hbase,12266,comment_13,"Resolving as duplicate of HBASE-11295, HBASE-13090, and probably others. See HBASE-11295 for previous discussion we've had about this code. See HBASE-13090 as the way forward.",non_debt,-
hbase,12271,summary,Add counters for files skipped during snapshot export,non_debt,-
hbase,12271,description,"It's incredibly handy to see the number of files skipped to know/verify delta backups are doing what they should, when they should.",non_debt,-
hbase,12271,comment_0,1,non_debt,-
hbase,12271,comment_1,+1 Running in production with it.,non_debt,-
hbase,12271,comment_2,Patch didn't apply quite cleanly on master and branch-1 so I had to mess with it a little.,non_debt,-
hbase,12271,comment_3,Can you provide a paragraph of background? I.e. how do you use this for delta backup? (Just curious),non_debt,-
hbase,12271,comment_4,"export snapshot send only the difference between src and dst, so if two snapshots share some of the same files (e.g. no compaction between the two ""take snapshots"") you export just the delta. this is just a counter to verify if the tool does this for real :)",non_debt,-
hbase,12271,comment_6,"ExportSnapshot will skip files that are the same (either via checksum or via name+length comparison). We use this to cut down on transfer time. First we snapshot the table, then export that snapshot to the remote DFS. Next run, we instead start by copying the previously exported snapshot into the current snapshot's destination (all on the remote DFS). We then do the snapshot and export dance again, and it dutifully copies only the changed files. Our (fb) HDFS has hard links (last I heard upstream does not) so we can copy backups around pretty cheaply. In the future we may just throw everything into one directory and let the manifest be the decider of what files are involved, but in the mean time it's copy and differential.",design_debt,non-optimal_design
hbase,12271,comment_7,Didn't see that  beat me to it :p,non_debt,-
hbase,12271,comment_12,"Thanks for explaining , and .",non_debt,-
hbase,12271,comment_13,Closing this issue after 0.99.2 release.,non_debt,-
hbase,12293,summary,Tests are logging too much,design_debt,non-optimal_design
hbase,12293,description,"In trying to solve HBASE-12285, it was pointed out that tests are writing too much to output again. At best, this is a sloppy practice and, at worst, it leaves us open to builds breaking when our test tools can't handle the flood. If  would be willing give me a little bit of mentoring on how he dealt with this problem a few years back, I'd be happy to add it to my plate.",design_debt,non-optimal_design
hbase,12293,comment_0,The easiest thing would be to change the log4j properties on tests to warn: Then it's just a matter of looking for the few tests that change things to be Trace and removing those lines.,non_debt,-
hbase,12293,comment_1,"tests should be at info level at the minimum, as in production: if not we will discover in test that we log too much (or worse triggers NPE or stuff like this). For the same reason, I prefer to use the debug level in tests, to be sure that I won't have surprises (NPE) if I try to use them. What I did in the past is reusing the info from the apache build (run time and logs), and looked at the both the log size and the log rate per test to prioritize the tests I was looking at. Then I was just improving the logs around these area.",design_debt,non-optimal_design
hbase,12293,comment_2,Resolving as dup of parent issue. Thats where the action is.,non_debt,-
hbase,12300,summary,Hadoop QA should attempt to execute common Maven goals,non_debt,-
hbase,12300,description,HBASE-12299 and HBASE-12294 have exposed a hole in our pre-commit testing; we need to confirm that commits don't break common Maven goals (like those used to build documentation and assemble HBase).,non_debt,-
hbase,12300,comment_0,"Since I don't have an account on build.apache.org, I can't see what gets run when ""Patch Available"" is selected. If someone can let me know where this might belong, I can whip up some Bash to extend whatever currently gets called.",non_debt,-
hbase,12300,comment_2,see,non_debt,-
hbase,12300,comment_3,This is now handled by Yetus.,non_debt,-
hbase,12400,summary,Fix refguide so it does connection#getTable rather than new HTable everywhere: first cut!,non_debt,-
hbase,12400,description,"The refguide has bits of code in it. The code does 'new HTable' to get a table instance. Rather, it should be promoting the new style where we get a Connection and then do a getTable on it. Ditto for references to 'new HBaseAdmin'. See ConnectionFactory for new style. See also package-info.java in Client for updated example. Misty, if you are game for this one, I can help w/ how it should look.",documentation_debt,outdated_documentation
hbase,12400,comment_0,"Stack, could you give more information on how to do it? Thanks Stephen",non_debt,-
hbase,12400,comment_1,"Thank you for taking this on . In this patch find See how it changes our example code to do the 'new' way. I was thinking of changing places where we have bits of code so there is a 1.0 version, the more prominent, and then the old way of doing it as it is now. I can do this one, np... might take you a good while more time than I since I've had my head in this a while now. Otherwise, ask more questions if not clear. When HBASE-12404 goes in, there will be more examples to pull from. Thanks.",documentation_debt,low_quality_documentation
hbase,12400,comment_2,"Are you working on this? Otherwise, I can put up a patch. Or if you want, I can put up a start patch to illustrate and you can take it from there?",non_debt,-
hbase,12400,comment_3,"A start. Cleans up our client package-info even more. Looking at refguide, we are good in a few places since we do sketching of the api. Elsewhere, we have 'example' code but I'm thinking the code should point back to package-info... if possible so we do code once only. Might not be able to get away with that. Any input mighty ?",non_debt,-
hbase,12400,comment_4,Ok. Patch that does more fixup on the client package-info and then makes some change in the refguide -- but this is a big job and needs and expert. Let me see if might @misty is game in a new issue. Committing this for now.,non_debt,-
hbase,12400,comment_5,I committed the attached patch. Filed HBASE-12585 for more thorough fixup.,non_debt,-
hbase,12400,comment_7,Closing this issue after 0.99.2 release.,non_debt,-
hbase,12417,summary,Scan copy constructor does not retain small attribute,non_debt,-
hbase,12417,description,"See Scan(Scan), the small member is not copied over.",non_debt,-
hbase,12417,comment_0,Trivial fix. Going to commit to all branches unless I hear objections in the next hour or so.,non_debt,-
hbase,12417,comment_1,"Not an issue in 0.94 as it is using scan attributes for API compatibility there, so only 0.98+.",non_debt,-
hbase,12417,comment_2,1,non_debt,-
hbase,12417,comment_3,Thanks . Committed to 0.98+,non_debt,-
hbase,12417,comment_8,Closing this issue after 0.99.2 release.,non_debt,-
hbase,12421,summary,Clarify ACL concepts and best practices,non_debt,-
hbase,12421,description,None,non_debt,-
hbase,12421,comment_1,"No, permissions granted on a cell apply only to that exact cell coordinate. This allows for policy evolution along with data. To change an ACL on a specific cell, write an updated cell with new ACL to the precise coordinates of the original. If you have a multiversioned schema and want to update ACLs on all visible versions, you'll need to write new cells for all visible versions. The application has complete control over policy evolution. The exception to the above rule is append and increment processing. Appends and increments can carry an ACL on the operation. If one is included in the operation, then it will be applied to the result of the append or increment. Otherwise, we carry forward the ACL of the existing cell we are appending to or incrementing. Otherwise the semantics would be really surprising.",non_debt,-
hbase,12421,comment_2,"Thanks for the correction, . I apologize for the delay. Please review the latest patch and be sure I've got everything right.",non_debt,-
hbase,12421,comment_4,1,non_debt,-
hbase,12421,comment_5,Pushed to master branch. Thanks  and . Building site now.,non_debt,-
hbase,12428,summary,region_mover.rb script is broken if port is not specified,non_debt,-
hbase,12428,description,TypeError: can't convert nil into String getFilename at unloadRegions at (root) at,non_debt,-
hbase,12428,comment_0,Patch to get the default port correctly in the region_mover.rb script. Verified locally.,non_debt,-
hbase,12428,comment_1,New version after Jesse's comments.,non_debt,-
hbase,12428,comment_2,"+1 looks good to me. We need a trunk patch too, if you have a moment",non_debt,-
hbase,12428,comment_5,LGTM,non_debt,-
hbase,12428,comment_7,"I don't quite follow, what was wrong before? Is it the unless part that throws the exception?",non_debt,-
hbase,12428,comment_8,"the problem is that is doesn't handle the port correctly. If there isn't one specified, it will throw an exception when it tries to cleanup old files. However, we are also doing the ""... unless port"" check in two different places, which is not as clean as it could be. So this just sets up the port in a single place, if its not set via the command line.",design_debt,non-optimal_design
hbase,12428,comment_9,"says this should also apply to master (just checked, seems like it does). I'll be applying this afternoon/evening to master and 0.98 branches, unless there are objections. Are there any other branches I should hit too ?",non_debt,-
hbase,12428,comment_10,"master, branch-1 and 0.98  +1 on patch if you've test the script still works. Good stuff.",non_debt,-
hbase,12428,comment_11,"I believe you guys. :) Still weird, we're pulling this out of any method into the global context so it's executed as soon as the script is loaded by the interpreter, and set as a default everywhere? At least warrants a comment, I think.",documentation_debt,outdated_documentation
hbase,12428,comment_12,Isn't this (-maybe) a cleaner fix? Keep logic local to the method where it is needed. Sorry to be pedantic. If you prefer your version here's a +1 for that too :),code_debt,low_quality_code
hbase,12428,comment_14,"No, stop it Lars. Its a smaller fix but not better. This script is called once from the outside and has a single flow of execution. It already does the correct check for the hostname, but doesn't check the port being set. That means you end up having the two null checks local to the load/unload methods for a variable that should be script-global anyways. This way its checked one place - think of it as setting up the defaults if user didn't provide overrides - and never has to be worried about again. But thanks for the +1 on the original. I'll be committing today, if thats ok with you boss. :)",non_debt,-
hbase,12428,comment_15,All good :),non_debt,-
hbase,12428,comment_20,Closing this issue after 0.99.2 release.,non_debt,-
hbase,12464,summary,meta table region assignment stuck in the FAILED_OPEN state due to region server not fully ready to serve,non_debt,-
hbase,12464,description,"meta table region assignment could reach to the 'FAILED_OPEN' state, which makes the region not available unless the target region server shutdown or manual resolution. This is undesirable state for meta tavle region. Here is the sequence how this could happen (the code is in Step 1: Master detects a region server (RS1) that hosts one meta table region is down, it changes the meta region state from 'online' to 'offline' Step 2: In a loop (with configuable maximumAttempts count, default is 10, and minimal is 1), AssignmentManager tries to find a RS to host the meta table region. If there is no RS available, it would loop forver by resetting the loop count (BUG#1 from this logic - a small bug) Step 3: Once a new RS is found (RS2), inside the same loop as Step 2, AssignmentManager tries to assign the meta region to RS2 (OFFLINE, RS1 = Based on the document ( ), this is by design - ""17. For regions in FAILED_OPEN or FAILED_CLOSE states , the master tries to close them again when they are reassigned by an operator via HBase Shell."". However, this is bad design, espcially for meta table region (it is arguable that the design is good for regular table - for this ticket, I am more focus on fixing the meta region availablity issue). I propose 2 possible fixes: Fix#1 (band-aid change): in Step 3, just like Step 2, if the region is a meta table region, reset the loop count so that it would not leave the loop with meta table region in FAILED_OPEN state. Fix#2 (more involved): if a region is in FAILED_OPEN state, we should provide a way to automatically trigger after a short period of time (leaving any region in FAILED_OPEN state or other states like 'FAILED_CLOSE' is undesirable, should have some way to retrying and auto-heal the region). I think at least for 1.0.0, Fix#1 is good enough. We can open a task-type of JIRA for Fix#2 in future release.",design_debt,non-optimal_design
hbase,12464,comment_0,This V1 patch prevents meta table region go into the FAILED_OPEN state when maximumAttempts reaches (Note: the smallest maximumAttempts is 1). would keep retrying until the meta table region is successfully assigned.,non_debt,-
hbase,12464,comment_2,Mind taking a look here sir?,non_debt,-
hbase,12464,comment_3,"It's not good for meta to stuck in FAILED_OPEN. Agree we should handle it differently. The patch looks good. Just couples things: 1. Can we add a log (info/debug level may be fine) when we reset the retry count to 0? 2. We also need to prevent meta region goes to FAILED_OPEN at method How about FAILED_CLOSE? It should be fine since the meta region is still available? Is this essentially the same as setting maximumAttempts to a huge number? In many cases, a region may not be able to heal automatically without a pill. Personally, I think a better monitoring system could be better in this case.",design_debt,non-optimal_design
hbase,12464,comment_4,"This looks good, except for Jimmy's comments above. Nit. should be renamed to Can you rebase the patch on top of current master code. FAILED_CLOSE should be fine I think. Not much to do there. We default to 10 attempts to assign. Maybe we should bump it up to 30 or so. I did not check how long we are sleeping overall, but for client -> server operations we are retrying 35 times for a total of 10 min before giving up. We can do the similar for region assignment.",code_debt,low_quality_code
hbase,12464,comment_5,The HBASE-12464.v1-1.0 patch is based on the feedback from  and . The target is for 1.0 - Add log warning entries if retries reaching to maximumAttempts - Handle if maximumAttempts is reached.,non_debt,-
hbase,12464,comment_7,The HBASE-12464.v2-2.0 patch is based on the feedback from  and . The target is for 2.0: - Add log warning entries if retries reaching to maximumAttempts - Handle if maximumAttempts is reached.,non_debt,-
hbase,12464,comment_9,"The test failure looks unrelated to my change. I can repro without my change; I also run the test with my changes under breakpoints, none of the breakpoints was hit. The strange part is that I can consistently repro in my machine (the latest master) with or without my change; however, the last 3 official commits passed the test (though the last one b6dd9b4 shown a strange passing message, instead of 'PASS', it said 'Fixed). The javadoc warnings have nothing to do with this change and are pre-exist (checked the recent committed JIRAs and they are there):",non_debt,-
hbase,12464,comment_10,The patch for 2.0 looks good to me. Thanks.,non_debt,-
hbase,12464,comment_11,"Integrated to master branch. Thanks for the patch, Stephen. Thanks for the reviews.",non_debt,-
hbase,12464,comment_12,I assume this is also needed in branch-1. Ran the following tests on branch-1: What else should be run ?,non_debt,-
hbase,12464,comment_14,"I think we need this in branch-1 and also 0.98 as well. Ted, can you please push to those as well. cc",non_debt,-
hbase,12464,comment_15,Thanks,non_debt,-
hbase,12464,comment_20,Closing this issue after 0.99.2 release.,non_debt,-
hbase,12467,summary,Master joins cluster but never completes initialization,non_debt,-
hbase,12467,description,"While diagnosing a rare failure in I discovered this scenario. Master was restarted by CM. Upon rejoining the cluster it successfully assumes responsibility as active master, but apparently the method never completes. The last log line from that thread is I see region states populated from existing znodes. AM inventoried the online regions, acknowledged that this was master failover. There it sits, responding to RPC's with Master is initializing}}. For the sake of resiliency, we should detect this scenario and at least release control as active master.",non_debt,-
hbase,12467,comment_0,Here's a fix to at least detect this kind of thing with the option to let the operator follow the Erlang approach.,non_debt,-
hbase,12467,comment_1,"Tried raising SIGKILL before exiting but I guess it does not invoke the vm-level handler, at least on a Mac.",non_debt,-
hbase,12467,comment_3,Ok. I wonder why we hung? Maybe do a thread dump before you system.exit? See down in its guts. +1,non_debt,-
hbase,12467,comment_4,"Thanks Stack, that's exactly what I was looking for.",non_debt,-
hbase,12467,comment_7,This looks fine. Will commit in a bit. I'll make a pass at putting it in 0.98 also but will have to move to .10 if that gets hung up on anything.,non_debt,-
hbase,12467,comment_8,Pushed to 0.98+,non_debt,-
hbase,12467,comment_9,Thanks .,non_debt,-
hbase,12467,comment_14,Closing this issue after 1.0.0 release.,non_debt,-
hbase,12494,summary,Add metrics for blocked updates and delayed flushes,non_debt,-
hbase,12494,description,None,non_debt,-
hbase,12494,comment_0,89-fb is deprecated.,non_debt,-
hbase,12562,summary,Handling memory pressure for secondary region replicas,non_debt,-
hbase,12562,description,"This issue will track the implementation of how to handle the memory pressure for secondary region replicas. Since the replicas cannot flush by themselves, the region server might get blocked or cause extensive flushing for its primary regions. The design doc attached at HBASE-11183 contains two possible solutions that we can pursue. The first one is to not allow secondary region replicas to not flush by themselves, but instead of needed allow them to refresh their store files on demand (which possibly allows them to drop their memstore snapshots or memstores). The second approach is to allow the secondaries to flush to a temporary space. Both have pros and cons, but for simplicity and to not cause extra write amplification, we have implemented the first approach. More details can be found in the design doc, but we can also discuss other options here.",non_debt,-
hbase,12562,comment_0,"Here is a patch that implements a policy for freeing memstores by doing a ""refresh store files"" if necessary. Since secondary regions cannot flush by themselves, we use this extreme measure if needed. When there is global memory pressure, we have a chore to find out the best possible region to flush. This patch extends that to find the best possible primary region and best possible secondary region (biggest memstore), and then we compare their memstore sizes. We use a multiplier (default 4) for the comparison because we want to flush primary regions as much as possible, but not do refresh store files (because of semantics as detailed in the design doc).",non_debt,-
hbase,12562,comment_2,+1. Looks good to me with some some minor comments: 1) You can break the loop after set canDrop to false 2) Just to check acquiring lock on writestate and memstore are always in this order 3) There maybe no need for the following condition 4. Rename to may be better,code_debt,low_quality_code
hbase,12562,comment_3,"Thanks Jeff for the review. done. Good point. Moved the {{synchronized (this)}} block out. Yes, it was there to skip doing refresh if the secondary is already in between replaying a start flush and commit flush. But removed that as a safe guard (we can end up with all regions in that state). done",non_debt,-
hbase,12562,comment_4,Attaching v2 patch addressing review comments.,non_debt,-
hbase,12562,comment_6,with -p0 this time.,non_debt,-
hbase,12562,comment_8,Something wrong with the branch pickup script it seems.,non_debt,-
hbase,12562,comment_10,v3 should fix findbugs warnings.,code_debt,low_quality_code
hbase,12562,comment_12,I've pushed this. Thanks for review.,non_debt,-
hbase,12562,comment_15,Closing issues released in 1.1.0.,non_debt,-
hbase,12647,summary,Truncate table should work with C as well,non_debt,-
hbase,12647,description,"indicates that truncateTable works with A or C (similar to createTable, but it is actually only A. Seems logical to require A or C.",non_debt,-
hbase,12647,comment_0,Simple patch.,non_debt,-
hbase,12647,comment_1,+1 Mind putting this in 0.98 also?,non_debt,-
hbase,12647,comment_4,Pushed to 0.98+. Thanks for review.,non_debt,-
hbase,12647,comment_9,Closing this issue after 1.0.0 release.,non_debt,-
hbase,12655,summary,miscalculates append/sync statistics for multiple regions,non_debt,-
hbase,12655,description,"Right now the tool sets up each new region by asking for a WAL and then adding a listener. The listener updates running stats on appends / syncs using a shared object in the evaluation tool. For WALProviders that reuse WALs (such as the default one always using a single WAL), this results in adding a listener to WALs that already have them. Each of those listeners is then updated on each append/sync.",non_debt,-
hbase,12655,comment_1,the release audit is HBASE-12661.,non_debt,-
hbase,12655,comment_2,1,non_debt,-
hbase,12655,comment_3,pushed to branch-1+.,non_debt,-
hbase,12655,comment_6,Closing this issue after 1.0.0 release.,non_debt,-
hbase,12673,summary,Add a UT to read mob file when the mob hfile moving from the mob dir to the archive dir,non_debt,-
hbase,12673,description,"add a unit test to scan the cloned table when deleting the original table, and the steps as following: 1) create a table with mobs, 2) snapshot it, 3) clone it as a a different table 4) have a read workload on the snapshot 5) delete the original table",non_debt,-
hbase,12673,comment_0,upload the patch,non_debt,-
hbase,12673,comment_1,"Hi Jon , do you want to look at this patch? Thanks.",non_debt,-
hbase,12673,comment_3,"hi, can you review this patch?",non_debt,-
hbase,12673,comment_4,"Hi , I don't think this unit test covers the case I'm concerned about. This unit test covers a fairly coarse grained happening -- a read is happening and inbetween read operations a table delete happens. What I'm concerned about is finer grained -- Let's say I'm reading a from a snapshot mob file which is pointing to the mob in the original dir. While this happens (while still in the middle of the read operation) a table deletion on the original table happens which move the original mob file to the archive. The read operation may fail (can't find more data from the file). With the HFilelink, we'd intercept that exception and then point the read to the moved location if the file ends up in the correct place. With the current code, I believe we get an IO exception and fail to return the proper data, or return an error. I'm in the process of crafting a rig that will constantly exercise these concurrently and hopefully will be able to produce a stack trace when this fails in a day or two.",test_debt,low_coverage
hbase,12673,comment_5,"hi, as you know when hbase reads mob cell, it has two steps. # Read the ref cell from the HBase, and get the cell value which is the mob file name. # HBase has two possible locations to read the mob cell, one is the When the mob file is not in the mobWorkingDir, HBase will try the second location.But now we only retry after the Do you means a table deletion on the original table happens in the middle of the read operation will throw other IOExceptions? I don't know the HFileLink how to guarantee the case in the MOB? Can you please give a more detailed description? Thanks",non_debt,-
hbase,12673,comment_6,Thanks for the pointer in HMobStore @ 312. The retry there should address the situation I was concerned about. Looking at that code again one other concern comes up -- do you know if line 319 in there will throw another exception if we got the FNFE on line 313? (we'd have an open file instance that got moved  not sure what would happen on close on line 319). Can we change it so that we capture the other exceptions that could be caught there [1] and then try the next location? HFileLink essentially makes this file redirection mechanism transparent and would potentially make the code easier to follow. I'd prefer it if we could use that code so if we find other cases we can just fix it in one centralized place. [1],design_debt,non-optimal_design
hbase,12673,comment_7,"I have tested to close a deleted mobfile , that will not get other exception . It's hard to use the FileLink in the Mob when read cell, because the mob reference cell only contain the filename, and this is not a hfilelink pattern, so I think the HFileLink can't used here, do you have any idea? Can we also capture the other exceptions that could be caught in HFileLink? Now we are trying to reproduce this case more fine-grained to catch more exception.",non_debt,-
hbase,12673,comment_8,"Hi, can you look at above comment and give some advise? Thanks",non_debt,-
hbase,12673,comment_9,"Ok, my main ask is to use the wrapping that the FileLink/HFileLink provides when a reader to the file is opened. You don't need to use the funny encoded name sine we aren't in a situation where we have a placeholder file present. We are in a place where we open a file and it could move to the archive. I spent a little bit of time in there and between the StoreFile, StoreFileInfo, Reference and HFileLink it is a bit messy. I'm planning to spend a little bit of time to refactor/clean things up in there so we have only have one mechanism to use. I think the short term solution is to add the other exception checks -- e.g. handle more than just the FNFE and catch these two other exception cases.",design_debt,non-optimal_design
hbase,12673,comment_10,"Thanks, I have changed the patch in which catch the and AssertionError, can you look on that?",non_debt,-
hbase,12673,comment_11,"Hi, I've upload a new patch, in this UT will try to read a opened hfile after deleting the orignal table, and IOException(not will be found, so I will change the patch in",non_debt,-
hbase,12673,comment_13,", unless we have a case where we have the move happen concurrently, the case that this unit test exercises is actually covered in Specifically we cover the snapshot, clone, delete original case here. I've spent some time refactoring HFileLink to make it suitable to use here in the mob case (it is starting to be used in other places too and needs to be done). I'm in the process of cleaning unit tests failures. I'll post at HBASE-12332 later today after I clean it up a bit.",non_debt,-
hbase,12673,comment_14,"hi, sorry I can't understand the meaning of ""it is starting to be used in other places and needs to be done"". Do you means it used in the SnapshotInfo tool? The process of reading MOB cell is through getting the filename from the reference cell, then open the file through the possible locations. The reference cell in hbase is only a filename, so I think hfilelink can't be used in reading mob cell.",non_debt,-
hbase,12673,comment_15,I've posted code in HBASE-12332 and HBASE-12749. Take a look there?,non_debt,-
hbase,12673,comment_16,I've posted code in HBASE-12332 and HBASE-12749. Take a look there?,non_debt,-
hbase,12729,summary,Backport HBASE-5162 (Basic client pushback mechanism) to 0.98,non_debt,-
hbase,12729,description,None,non_debt,-
hbase,12729,comment_0,This is mildly complicated by the fact we can't change the Public annotated interface HConnection in 0.98. Hacking around with alternatives to find something reasonable.,design_debt,non-optimal_design
hbase,12729,comment_1,"Hey , in the trunk patch, in RsRpcServices, this hunk: This is applying the mutations twice. I think you meant to remove the second invocation of mutateRows() right?",code_debt,duplicated_code
hbase,12729,comment_2,"Parking a WIP patch. Not ready yet. Because we can't change HConnection, I introduce a new interface that extends HConnection. HConnectionManager now creates that implement Where appropriate we check if the connection is of type in the unlikely case someone passes in a custom implementation of HConnection. I'm sure the client changes didn't break AsyncProcess because the client tests all pass. The new end to end test in hbase-server TestClientPushback is not passing yet.",non_debt,-
hbase,12729,comment_3,Just realized the test is in main/ . Will fix that,non_debt,-
hbase,12729,comment_4,Updated patch. The new end to end test TestClientPushback works with this one. All client and server tests pass locally also. I needed to make changes in and for AsyncProcess differences in 0.98.,non_debt,-
hbase,12729,comment_5,Setting patch available but expect Jenkins won't be able to apply it to a precommit build.,non_debt,-
hbase,12729,comment_7,"Yeah, good catch. Sorry for the late response, this got lost in the noise.",non_debt,-
hbase,12729,comment_8,I'll submit a patch for trunk to fix it.,non_debt,-
hbase,12729,comment_9,"Did a 10 minute looksee, lgtm. Spelling Nit: Is the synchronized needed here: Could get the ServerStatitics, if null create one and then use putIfAbsent. In the race case we'd create the object in vain, but we'd save the synchronized. Is this even on the hot path?",documentation_debt,low_quality_documentation
hbase,12729,comment_10,"Thanks for the comment Lars. As this is a backport I'm not modifying logic like this that comes from the trunk diff. We can definitely make this change but it should be a follow up I think, we will need a trunk patch.",non_debt,-
hbase,12729,comment_11,"Nah, nevermind. We can commit HBASE-12839 then I'll incorporate it here.",non_debt,-
hbase,12729,comment_12,"I've incorporated HBASE-12839. Will do HBASE-12840, commit it, then roll the additional tests in here, and then commit this.",non_debt,-
hbase,12729,comment_13,"Updated patch for 0.98 incorporating HBASE-12838, HBASE-12839, and HBASE-12840. New end to end test TestClientPushback passes locally 10 of 10 times. Will commit if the 0.98 unit test suite completes successfully in total.",non_debt,-
hbase,12729,comment_15,"I'd like to make this issue the inaugural use of HBASE-12808 for 0.98 but am not yet able to make it work for me, see the comment at the tail of HBASE-12808. Should be resolved soon. /cc",non_debt,-
hbase,12729,comment_16,I didn't expect problems but ran the compliance checker from HBASE-12808 and here's the result: Pushed to 0.98,non_debt,-
hbase,12729,comment_19,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,12749,summary,Tighten HFileLink api to enable non-snapshot uses,non_debt,-
hbase,12749,description,"In HBASE-12332 we'd like to use the FileLink's IO redirecting powers but want to be able to specify arbitrary alternate link paths and not be tied to the SnapshotFileLink file pattern (aka, To do this we need change the constructors and some internals so that it is more generic. Along the way, we remove the FileStatus constructor arguments in favor of Path's and reduce the number of ways to create HFileLinks, and tighten up the scope privacy of many methods.",non_debt,-
hbase,12749,comment_0,"Hey , can you take a look?",non_debt,-
hbase,12749,comment_1,review board,non_debt,-
hbase,12749,comment_3,"HFileLink is supposed to be a link to an HFile not to a Snapshot the has no idea about snapshots, but it just a pointer to an HFile. so stuff like in do not make sense, because you are not creating a link from a snapshot but you are just referencing an hfile somewhere. so the hfilelink is already being used on non-snapshot files.",non_debt,-
hbase,12749,comment_4,"Ok, Is the concern the only name or is there more? Do you have an alternative name? How is ""HfileLinkPattern"" or Over in HBASE-12332 we create hfilelink's that have the redirection capabilities but essentially set arbitrary alternate paths in the constructor without having the file present (what I've called the ""snapshot link file""). I'm trying to capture that idea the file of the pattern table=region-hfile present is an artifact specific to clones of snapshotted tables. In the instance, you are right -- the code is creating a fake path in the table=region-hfile pattern -- which seems a little strange to me. Really the HFileLink constructor/creator method should just have this signature instead of only being able to that the funny path. TableName, String regionEncName, String familyName, String hfileName)",non_debt,-
hbase,12749,comment_5,Or we should change code in other places so that we use the convenience constructors of HFileLink but just have FileLink fields everywhere else instead.,non_debt,-
hbase,12749,comment_6,"the only other concern is the removed FileStatus from the StoreInfo and the addition of the FileSystem, but I still have to figure out why is that necessary. both sounds good to me. sounds good, it wasn't clear to me from the jira description what was the purpose of this patch. you throw me off with that snapshot thing. but the problem is basically that the only way to create an hfileLink is to create the fancy name first and then create an hfileLink from there (like the is doing). You can add/use the new ""constructor"" in this patch for as example.",non_debt,-
hbase,12749,comment_7,"check out HBASE-12332. We don't have a FileStatus to attach to in that particular case initially. In the snapshot case we have the pattern file and get a fileStatus, and in the replicas case we have a file status as well[1] The fs was needed for (and and Probably can just as easily as of a FS in Didn't tackle those pieces yet (they also seem more tightly coupled than ideal) [1]",architecture_debt,violation_of_modularity
hbase,12749,comment_8,Looks like I have some digging to do becuase I think the snapshot export tests are legitimately broken.,non_debt,-
hbase,12749,comment_9,updated version fixes,non_debt,-
hbase,12749,comment_10,Apparently I was using the wrong constructor in v1 of the patch inside ExportSnapshot. This should fix that problem. I've also renamed the new create* functions as build* functions since there is a different set of create* functions present in HFileLink.,non_debt,-
hbase,12749,comment_12,"v3 tackles the javadoc, fingbugs, checkstyle and line length issues.",code_debt,low_quality_code
hbase,12749,comment_14,I think checkstyle and javac warnings are fighting each other. In this case I prefer the no empty ';' version.,non_debt,-
hbase,12749,comment_16,1,non_debt,-
hbase,12749,comment_17,Thanks matteo. commited to master and branch-1,non_debt,-
hbase,12749,comment_20,Closing issues released in 1.1.0.,non_debt,-
hbase,12778,summary,HBase,non_debt,-
hbase,12778,description,None,non_debt,-
hbase,12778,comment_0,Nothing in this resolving as invalid,non_debt,-
hbase,12778,comment_1,"Hi , if you have a question, please post to the mailing lists",non_debt,-
hbase,12833,summary,[shell] table.rb leaks connections,code_debt,low_quality_code
hbase,12833,description,TestShell is erring out (timeout) consistently for me. Culprit is OOM cannot create native thread. It looks to me like test_table.rb and hbase/table.rb are made for leaking connections. table calls for every table but provides no close() method to clean it up. test_table creates a new table with every test.,code_debt,low_quality_code
hbase,12833,comment_0,if table.rb also create a connection per ruby {{table}} instance than it looks more critical to fix.,non_debt,-
hbase,12833,comment_1,Looks like we missed this in HBASE-12595.,non_debt,-
hbase,12833,comment_2,from branch-1.0,non_debt,-
hbase,12833,comment_3,fyi,non_debt,-
hbase,12833,comment_4,hm... Interesting ripple affect. I'll see if I can figure this one out.,non_debt,-
hbase,12833,comment_5,Closing connections in shell tests.,non_debt,-
hbase,12833,comment_7,"Thanks for looking at this one . I started down a similar patch myself, but I'm concerned about breaking API compatibility for folks through the upgrade. By moving over to the new API we're forcing folks to modify their existing jruby scripts. We'll also need to audit all the utility scripts we ship. FYI .",non_debt,-
hbase,12833,comment_8,"I can revert the initialize() signatures, but still keep the new close() methods. Do you think that would be better?",non_debt,-
hbase,12833,comment_9,"Nice work , +1 for this patch for 2.0/master. I'm now wondering if we'll need to skip this patch and revert the work committed on HBASE-12495 for 1.x, so that shell scripts continue to use the old managed/shared connection code. This would be an API,semantics compatibility question.",non_debt,-
hbase,12833,comment_10,"I personally have not encountered a user that has used jruby classes directly (rather than shell usage). Even if there are some, I would argue that it should be pretty rare. I am in favor of breaking this for branch-1.",non_debt,-
hbase,12833,comment_11,"A couple of comments on this: 1) the interface changes and the managed/unmanaged connection issues are separate concerns. 2) On the mailing list you said: ""Yes, the concept of connection caching, aka, managed connections are going away."" If managed connections are going away, then let's make them go away across the board. If they're staying, then let's keep them and rework the interfaces to support managed connections as a first class citizen. I only see disadvantages for inconsistency in this aspect. That said, this can totally be something that I'm missing. Do you see any advantages to keeping a mix of managed and unmanaged connections around?",design_debt,non-optimal_design
hbase,12833,comment_12,"Agreed. I also think that the implementation details of the shell *should not* be considered a part of our public API contract. Point remains that this is code we've shipped that folks may be relying on. I asked the question on dev@ yesterday. If no one chimes in, I think we can go forward with the close and initialize changes for branch-1.0+ (and so long as RM is in agreement). Not long term, no. However, we will continue to support both for 1.x series as a backward compatibility consideration.",non_debt,-
hbase,12833,comment_13,"On closer inspection, it looks like SecurityAdmin and need updated with this new style as well. ReplicationAdmin appears to manage it's own connection by design, though the rational eludes me.",code_debt,low_quality_code
hbase,12833,comment_14,Reattach for hadoopqa.,non_debt,-
hbase,12833,comment_15,That is a different jira right? I'm +1 for this if hadoopqa passes.,non_debt,-
hbase,12833,comment_16,"I guess it could be, but won't connections still be leaked when these classes are used? All this shows is our test usage of this part of the client is not as through as others. Whatever you say .",test_debt,low_coverage
hbase,12833,comment_17,I've committed this. Thanks Solomon for the patch. We'll do a follow up jira for the remaining leaks as Nick points out.,non_debt,-
hbase,12833,comment_21,Closing this issue after 1.0.0 release.,non_debt,-
hbase,12846,summary,is unstable in 0.98,design_debt,non-optimal_design
hbase,12846,description,"Several cases in frequently fail with timeouts. On Jenkins: Locally, when failing: Running 17, 0, Errors: 13, Skipped: 0, 842.524 sec <<< FAILURE! - in Locally, when passing: Running 17, 0, Errors: 0, Skipped: 0, 30.072 sec - in",non_debt,-
hbase,12846,comment_0,Haven't seen this in a while,non_debt,-
hbase,12888,summary,Display number of regions on the table jsp page.,non_debt,-
hbase,12888,description,This is a simple patch to display the number of regions in the table jsp page in the info web page. Currently we have only the total number of regions in the regionserver. This adds the per table count of the regions.,non_debt,-
hbase,12888,comment_1,"Looks good to me, +1",non_debt,-
hbase,12888,comment_2,"For consistency sake, it should also print this number of regions for meta. Right now that's fixed at 1.",code_debt,low_quality_code
hbase,12888,comment_3,Updated to include count of meta regions as well.,non_debt,-
hbase,12888,comment_4,Thanks. This was worked on by one of the new engineers doing bootcamp in facebook. Is there a formal procedure to create Jiras for new people who are trying hbase tasks just to try out how hbase feels like. We can follow the same procedure when we assign tasks to bootcampers internally.,non_debt,-
hbase,12888,comment_6,+1 for I'd like to attribute this upon commit to Nick and this referenced person: Would it be ok if we know his/her identity for this purpose?,non_debt,-
hbase,12888,comment_7,"Appropriate procedure would be the same as any other contribution. They create an account on JIRA. They can open issues as they like, attach patches for review, etc. With their own account we can attribute work to them accordingly. Have your mysterious contributor come back and comment here. Next thing I'll ask of them is to make sure the patch applies cleanly on all 4 applicable, active branches (as marked in the 'fix for' files): 0.98, branch-1.0, branch-1, and master.",non_debt,-
hbase,12888,comment_8,Cancelling patch. One month without response. I'm thinking of resolving this as Later. Concerns?,non_debt,-
hbase,12888,comment_9,Useful but patch died. Resolving as later as per  suggestion.,non_debt,-
hbase,12905,summary,Clean up presence of hadoop annotations in 0.98,code_debt,low_quality_code
hbase,12905,description,"while working on HBASE-12898, hadoop-annotations showed up as an undeclared dependency. Looks like a hand full of hadoop InterfaceAudience annotations made it in through backports.",code_debt,low_quality_code
hbase,12905,comment_1,patch for 0.98. manually checked compilation and building the site.,non_debt,-
hbase,12905,comment_3,1,non_debt,-
hbase,12905,comment_5,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,12933,summary,[0.98] Fold into HConnection,non_debt,-
hbase,12933,description,See parent issue.,non_debt,-
hbase,12933,comment_0,Patch for 0.98,non_debt,-
hbase,12933,comment_2,"Pushed to 0.98, see parent",non_debt,-
hbase,12933,comment_5,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,12981,summary,FSHLog: UNEXPECTED -4,non_debt,-
hbase,12981,description,"A user reported the below. It happens after the RS has been running a while. 015-01-20 22:33:23,031 ERROR UNEXPECTED -4 2015-01-20 22:33:23,035 INFO exiting ################## Similarly on Node 23 - on 12-20-2014 05:13: 2014-12-20 05:13:40,715 ERROR UNEXPECTED -3 ################### Looking in code, I can't see how this could come about other than our write seqid ran over the top of a long (unlikely). I think this a 0.98 issue since 1.0+ is different here. It does: int index = % I'm going to add logging of the circumstance that produces a negative index and then defense against our using negative indices; there could be more going on in here, more than I can see.",non_debt,-
hbase,12981,comment_0,Patch for 0.98. Logs (every minute) if we come across a negative index. Outputs the inputs used in the modulo so we can get better understanding.,non_debt,-
hbase,12981,comment_1,"Whoa, that can make things much worse, no?",non_debt,-
hbase,12981,comment_2,Could this be HBASE-11200 ? We have seen this in action using earlier 0.98 versions.,non_debt,-
hbase,12981,comment_3,"At this point idx should be in the range so taking the absolute value should be okay. Usually I'd do a < 0 check and add the devisor, but I don't think it makes a difference unless this is a very hot code path.",non_debt,-
hbase,12981,comment_4,Thanks  I think you are right. I was looking at wrong 0.98 version. This is a patched 0.98.1 (CDH5.1.3). It doesn't have HBASE-11200. Let me close this as a duplicate (Thanks for reviews  and  -- agree w/ Sean).,non_debt,-
hbase,12981,comment_5,Dup of HBASE-11200,non_debt,-
hbase,13155,summary,Fix TestPrefixTree,non_debt,-
hbase,13155,description,"It should be my fault when addressing HBASE-12817, I commented a line in master and not in branch-1. This breaks the branch-1 build after HBASE-11544",non_debt,-
hbase,13155,comment_2,"Thanks for the patch, Duo.",non_debt,-
hbase,13155,comment_5,Closing issues released in 1.1.0.,non_debt,-
hbase,13184,summary,Document turning off memstore for region replicas,non_debt,-
hbase,13184,description,None,non_debt,-
hbase,13184,comment_0,please review. I did the bare minimum because I think the whole region replica area could use a rewrite and reorganization.,non_debt,-
hbase,13184,comment_1,"HBASE-10513 added user documentation for Phase 1, but the book still does not contain changes reflecting Phase 2. I have written up something already, but did not get around to get them into asciidoc yet. I'll create a subtask in HBASE-10070 for that. Do you have any suggestions for how to organize it better ?",documentation_debt,outdated_documentation
hbase,13184,comment_3,Ping. Want to get some changes into the book for 1.1.0?,non_debt,-
hbase,13184,comment_4,I'd like to commit this for 1.1 -- it's better than nothing. Will do so in the next couple of hours if there's no objections. Thanks.,non_debt,-
hbase,13184,comment_5,"^^^ ,",non_debt,-
hbase,13184,comment_6,Obviously it's fine with me as long as it is technically accurate.,non_debt,-
hbase,13184,comment_7,"Committed this to master, but wasn't sure what to do on branch-1/1.1 since the file doesn't exist there.  ?",non_debt,-
hbase,13184,comment_8,We don't push docs to other branches so you're good. :),non_debt,-
hbase,13184,comment_10,Resolving committed ticket. Thanks folks.,non_debt,-
hbase,13341,summary,Add option to disable filtering on interface annotations for the API compatibility report,non_debt,-
hbase,13341,description,"The API compatibility checker script added in HBASE-12808 passes a file containing annotations to the JavaACC tool. When JavaACC is invoked with that option it will filter out all interfaces that do not have that annotation. We should add a command line option to the compatibility checker which turns off this filtering in case we want to look at the impact of changes to all interfaces, even private ones.",non_debt,-
hbase,13341,comment_0,Good call.,non_debt,-
hbase,13341,comment_1,Patch might apply with fuzz because I made these changes after HBASE-13340 on the same working branch. Adds a new short option -a and long option --all that will disable filtering. Updated help: Tested to produce (if you're curious) I also confirmed that without the new command line option filtering by interface annotation still happens correctly.,non_debt,-
hbase,13341,comment_3,"Any reason to explicitly declare the array? Either way, +1.",non_debt,-
hbase,13341,comment_4,"Oops, disregard the above. (Wrong JIRA). Only two things are nits: 1. Put a period at the end of the usage message addition to be consistent with the other lines. 2. Change the dereference from {{""$ALL"" != ""true""}} to {{""$\{ALL}"" != ""true}}, again, just to be consistent with the rest of the script. +1!",code_debt,low_quality_code
hbase,13341,comment_5,"Thanks for the review , I will make those changes on commit.",non_debt,-
hbase,13341,comment_6,Pushed to master,non_debt,-
hbase,13395,summary,Remove HTableInterface,non_debt,-
hbase,13395,description,"This class is marked as deprecated, probably can remove it, and if any methods from this specific class are in active use, need to decide what to do on callers' side. Should be able to replace with just Table interface usage.",code_debt,dead_code
hbase,13395,comment_0,"I think our language in the is such that we need to have something deprecated for an entire release. Meaning, if it was first marked deprecate in 1.0.0, it must be present for all 1.x releases and all 2.x releases.  and  may have some interpretation on this as well.",non_debt,-
hbase,13395,comment_1,"has a thread going on dev@ where they're starting to prune these off. The way I read the ref guide, deprecated in 1.0.0 would mean that it's deprecated for 1.y as ""one major release"", which would mean removal in 2.0 is fine. This is what I told Lars F on the thread, so if we want to be more conservative someone should chime in.",non_debt,-
hbase,13395,comment_2,"It seems I've read it the same way as  - So I assumed in current master we can do changes in general, and removal of old APIs is fine. Let me look at the mail thread again, maybe I missed something.",non_debt,-
hbase,13395,comment_3,"I'd like to bring it up again. Do we think it's fine to remove this class in current master/2.0.0, or should we target it for removal in some 3.0.0 or so?",non_debt,-
hbase,13395,comment_4,As far as I understand the documentation a major version allows to remove deprecated APIs. The section mentioned by Nick also contains a short example which matches our case: A user using a newly deprecated API does not need to modify application code with HBase API calls until the next major version.,non_debt,-
hbase,13395,comment_5,It is deprecated in 1.0 I think. So we can remove it.,code_debt,dead_code
hbase,13395,comment_7,1,non_debt,-
hbase,13395,comment_9,1,non_debt,-
hbase,13395,comment_10,"There are some comment which reference the ""HTableInterface"" in the Please fix it. Thanks.",non_debt,-
hbase,13395,comment_11,Thanks for the comments. I updated the comments in and also a small part in the docs which was referencing *HTableInterface*.,non_debt,-
hbase,13395,comment_12,Please fix both of architecture.adoc and cp.adoc also. Thanks.,non_debt,-
hbase,13395,comment_15,"Am +1  if you are fine, could you commit it?",non_debt,-
hbase,13395,comment_16,pass locally. Will commit it tomorrow if there is no objections.,non_debt,-
hbase,13395,comment_17,Thanks for the patch.,non_debt,-
hbase,13395,comment_19,please add a release note that lets downstream folks know what has happened and what they should do to account for it. This will be particularly important for folks coming from 0.98.,documentation_debt,outdated_documentation
hbase,13395,comment_20,ping,non_debt,-
hbase,13455,summary,Procedure V2 - master truncate table,non_debt,-
hbase,13455,description,"master side, part of HBASE-12439 and replaces the truncate table handlers with the procedure version.",non_debt,-
hbase,13455,comment_5,Closing issues released in 1.1.0.,non_debt,-
hbase,13528,summary,A bug on selecting compaction pool,non_debt,-
hbase,13528,description,"When the selectNow == true, in the compaction pool section is incorrect. as discussed in:",non_debt,-
hbase,13528,comment_0,"I think this line is also redundant? The if selectNow is false, then we will not execute throttleCompaction then 'size' is useless Thanks.",code_debt,low_quality_code
hbase,13528,comment_1,"Yes, it's redundant, just like this is OK?",code_debt,low_quality_code
hbase,13528,comment_2,This will cause NPE...compaction will be null if selectNow == false. Try this?,non_debt,-
hbase,13528,comment_3,"OK, will atach patch soon.",non_debt,-
hbase,13528,comment_5,refine the patch as comments from zhangduo,non_debt,-
hbase,13528,comment_6,+1. And can you use git format-patch to generate the patch file? Using git am with sign off can retain the author of the patch.  What do you think? You replied on mailing list. Thanks.,non_debt,-
hbase,13528,comment_8,lgtm,non_debt,-
hbase,13528,comment_9,Should go into all branches? Thanks.,non_debt,-
hbase,13528,comment_10,+1 for 1.2 and 1.1,non_debt,-
hbase,13528,comment_11,+1 for 0.98,non_debt,-
hbase,13528,comment_12,"Thanks for the patch, Shuaifeng. Thanks for the review, Duo, Nick and Andrew.",non_debt,-
hbase,13528,comment_16,haven't replied yet...Is it safe to commit to branch-1.0? Seems an RC is on-going...,non_debt,-
hbase,13528,comment_22,Closing issues released in 1.1.0.,non_debt,-
hbase,13582,summary,Update docs for HTrace,non_debt,-
hbase,13582,description,the ref guide currently points to HTrace at its old location. update it to point at the ASF project. Should also verify that the usage example is still correct.,documentation_debt,outdated_documentation
hbase,13582,comment_0,I think I remember seeing some references to the old package (org.htrace) instead of the one for the ASF project (org.apache.htrace) in the book which could be updated as well.,documentation_debt,outdated_documentation
hbase,13582,comment_1,This patch fixes all the issues I could find with the documentation. It also includes a few Asciidoc things and typos etc. so I changed the JIRA title slightly,documentation_debt,low_quality_documentation
hbase,13582,comment_3,Not sure if the line length is enforced in the documentation as well. If it is I'll provide a new patch. The failing tests must be unrelated.,non_debt,-
hbase,13582,comment_4,Pushed to master. Thanks for the patch  This is an improvement. We will want to do another pass later.,non_debt,-
hbase,13592,summary,RegionServer sometimes gets stuck during shutdown in case of cache flush failures,non_debt,-
hbase,13592,description,Observed that RegionServer sometimes gets stuck during shutdown in case of cache flush failures. On adding few debug logs and looking through the stack trace RegionServer process looks stuck in closeWAL - From the RegionServer logs we see there are multiple attempts to flush cache for a particular region which increments the beginOp count in DrainBarrier but all the flush attempts fails somewhere in wal sync and the DrainBarrier endOp count decrement never happens. Later on when shutdown is initiated RegionServer process is permanently stuck here In this case hbase stop also does not work and RegionServer process has to be explicitly killed using kill -9,non_debt,-
hbase,13592,comment_0,Adding details related to the stack and log trace,non_debt,-
hbase,13592,comment_1,"tried a patch with changes suggested by  wherein we move wal.sync also with the try catch block of cache flush, and in this case all the affected RegionServers successfully shutdown without any RegionServer going into hung state. The RegionServers that don't shutdown are fully operational and working fine. Current implementation in HRegion.java The patch submitted contains the following changes",non_debt,-
hbase,13592,comment_2,please review the changes,non_debt,-
hbase,13592,comment_4,lgtm,non_debt,-
hbase,13592,comment_5,"I know I suggested the fix. Now looking at the code I see that we only sync the WAL when the CF is setup with SKIP_WAL or ASYNC_WALL, so we shouldn't have done the sync'ing (see Maybe it's rather the wait for the previous MVCC transactions that times out? In either case. Moving the try up this way will cover any failures and release the flush barrier, so we should do this. Also  verified experimentally that this solves the issues we've been seeing with stuck region servers. Will commit in a bit. Thanks for doing all the detective work, Vikas!",non_debt,-
hbase,13592,comment_6,"Looks like this is not an issue in 1.0 and later. The code has changed a lot in 1.0 and then again for trunk, but it looks like this is covered there. Only committing to 0.98.",non_debt,-
hbase,13592,comment_7,Nice work,non_debt,-
hbase,13592,comment_10,"looks suspicious, but only failed in the one build and passes consistently locally.",non_debt,-
hbase,13592,comment_11,Let's keep an eye on it as more build results come in.,non_debt,-
hbase,13592,comment_12,This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).,non_debt,-
hbase,13629,summary,Add to 1.0: HBASE-12957 may be extremely slow on region with lots of expired data,code_debt,slow_algorithm
hbase,13629,description,Parent introduced a typo. should be,documentation_debt,low_quality_documentation
hbase,13629,comment_0,Also looks like parent was omitted from 1.0. Lemme add it there as part of this issue.,non_debt,-
hbase,13629,comment_1,1.1 and later are fine.,non_debt,-
hbase,13629,comment_2,Already fixed in HBASE-13475. So lemme use this one to add 1.0.,non_debt,-
hbase,13629,comment_3,"Currently in the description of this issue the before and after examples are the same text. :-) More typo-ing, whee. On the parent I remarked we should figure out how to test the bin scripts. Or convert them all to Java utilities and test those. Let's not make that a requirement to get this fix in though.",test_debt,low_coverage
hbase,13695,summary,Cannot timeout or interrupt Hbase bulk/batch operations,non_debt,-
hbase,13695,description,"Using the Hbase 1.0.0 client. In HTable there is a batch() operation which calls AyncRequest ars ... ars.waitUntilDone() This invokes waitUntilDone with Long.Max. Does this mean batch operations cannot be interrupted or invoked with a timeout? We are seeing some batch operations taking so long that our client hangs forever in ""Waiting for.. actions to finish"".",non_debt,-
hbase,13695,comment_0,This one is probably better up on the list than an issue  until we figure what is going on. Suggest you add a bit more info too.. Below waitUntilDone we are running timers and if we timeout we'll return errors which should bubble up as exceptions...,non_debt,-
hbase,13710,summary,Remove use of Hadoop's ReflectionUtil in favor of our own.,non_debt,-
hbase,13710,description,HttpServer makes use of Hadoop's ReflectionUtil instead of our own. AFAICT it's using 1 extra method. Just copy that one over to our own ReflectionUtil.,design_debt,non-optimal_design
hbase,13710,comment_1,test failure is related.,non_debt,-
hbase,13710,comment_2,-02 * fixes use of reflection for FilterInitializer to use HBase's method instead of Hadoop's.,non_debt,-
hbase,13710,comment_3,Patch LGTM. +1 if hadopqa likes it.,non_debt,-
hbase,13710,comment_5,no tests because existing ones cover the changed bits. The listed zombie tests all pass when I run them locally. good to go ?,non_debt,-
hbase,13710,comment_6,1,non_debt,-
hbase,13776,summary,Setting illegal versions for HColumnDescriptor does not throw,non_debt,-
hbase,13776,description,HColumnDescriptor hcd = new HColumnDescriptor( new .setInMemory(true) final int minVersions = 123; final int maxVersions = 234; //no exception throw,non_debt,-
hbase,13776,comment_0,When you try to use this HCD in create/alter table exception is getting thrown?,non_debt,-
hbase,13776,comment_1,"when I trying to create a test table with this HColumnDescriptor, there is no exception thrown. And, describe that table it shows ""VERSIONS = If you setMinVersions first, you will got an exception, because setMaxVersions checked the value before it set.",non_debt,-
hbase,13776,comment_2,"when I trying to create a test table with this HColumnDescriptor, there is no exception thrown. And, describe that table it shows ""VERSIONS = If you setMinVersions first, you will got an exception, because setMaxVersions checked the value before it set.",non_debt,-
hbase,13776,comment_3,"when I trying to create a test table with this HColumnDescriptor, there is no exception thrown. And, describe that table it shows ""VERSIONS = If you setMinVersions first, you will got an exception, because setMaxVersions checked the value before it set.",non_debt,-
hbase,13776,comment_4,You are planning to give a patch for this?,non_debt,-
hbase,13776,comment_5,"In HMaster, sanity check test min versions <0.. It should check also.",non_debt,-
hbase,13776,comment_6,"I have no experience about contribute to the community. But yes, I'm willing to give a try, and I will do my best.",non_debt,-
hbase,13776,comment_8,lgtm : Is it possible to add a unit test so that there is no regression in the future ?,test_debt,lack_of_tests
hbase,13776,comment_9,"Can we add the wrong values of min and max versions configured, in the message? Yes pls add a test to cover this scenario. Thanks.",test_debt,lack_of_tests
hbase,13776,comment_10,Add test case in,non_debt,-
hbase,13776,comment_11,"Hi, I added a test case in as you suggested. Thanks for the review.",non_debt,-
hbase,13776,comment_13,Fix test case failure,non_debt,-
hbase,13776,comment_15,"Thanks for the patch, Yuhao Thanks for the review, Anoop",non_debt,-
hbase,13776,comment_18,this commit breaks the build in 1.0 does not exists in 1.0,non_debt,-
hbase,13776,comment_19,"Thanks for the reminder, Matteo. I reverted from branch-1.0",non_debt,-
hbase,13776,comment_20,Patch for branch-1.0,non_debt,-
hbase,13776,comment_21,I ran in branch-1.0 with latest patch and the test passed. Jenkins is down at the moment. Committing without waiting for QA run.,non_debt,-
hbase,13776,comment_25,Closing this issue after 1.0.2 release.,non_debt,-
hbase,13799,summary,javadoc how Scan gets polluted when used; if you set attributes or ask for scan metrics,non_debt,-
hbase,13799,description,Matteo is noticing that Get and Scan get altered when used. Doc it for Scans (Matteo is 'fixing' it for Get).,non_debt,-
hbase,13799,comment_0,Fixup of javadoc on Scan class.,documentation_debt,low_quality_documentation
hbase,13799,comment_1,why the part was removed? other than that looks ok to me.,non_debt,-
hbase,13799,comment_2,"This stuff should not be front and center as it used to be. It will only confuse now that scanner takes care of this under the covers for you (Scanner is now 'size-based' where before it was 'row-based'...) I considered 'explaining' the new regime in the class comment but thought it would only confuse more than it helped (We should do HBASE-13441 sooner rather than later -- especially given review of the rest of Scan javadoc, it all needs revamp).",non_debt,-
hbase,13799,comment_3,sounds good +1,non_debt,-
hbase,13799,comment_4,"Pushed to branch-1 and master. Attached is the smaller patch I pushed to 0.98, branch-1.0, and branch-1.1.",non_debt,-
hbase,13799,comment_11,Closing this issue after 1.0.2 release.,non_debt,-
hbase,13871,summary,"Change RegionScannerImpl to deal with Cell instead of byte[], int, int",non_debt,-
hbase,13871,description,This is also a sub item for splitting HBASE-13387 into smaller chunks.,non_debt,-
hbase,13871,comment_1,"LGTM. nit, createFirstOnRow can have a small doc.",documentation_debt,outdated_documentation
hbase,13871,comment_3,"Do we have to have a class named FirstOnRowFakeCell at top level of our class hierarchy? Is it only used in CellUtil? Could it be an inner class of it? Any reason for this change? 5252 this.isScan = scan.isGetScan() ? -1 : 0; 5252 this.isScan = scan.isGetScan() ? 1 : 0; Otherwise, seems good.",architecture_debt,violation_of_modularity
hbase,13871,comment_4,Yes this class need not be top level.. Let me see how we can make it inner. Yes this change is intended. Pls see the compare in isStopROw is changed. So we need this change.,architecture_debt,violation_of_modularity
hbase,13871,comment_5,I see. Thanks.,non_debt,-
hbase,13871,comment_6,"I will change it in next patch. BTW , we might add some more similar classes in future. This is byte[] backed fake Cell impl and we will have a BB backed one also. Also LastOnRowFakeCell will come in.. JFYI. I will keep this class(es) private audience only even if CellUtil is public exposed. Thanks Stack.",architecture_debt,violation_of_modularity
hbase,13871,comment_7,We do have javadoc for this method. I think it is clear saying what cell it makes. Any thing more to add? You have any suggestion?,non_debt,-
hbase,13871,comment_9,Test failure seems not related to patch,non_debt,-
hbase,13871,comment_10,Thanks for the reviews Ram and Stack.,non_debt,-
hbase,13905,summary,failing consistently on branch-1.1,non_debt,-
hbase,13905,description,The reruns failed because the test is not idempotent. Perhaps we should have the test startup clean up it's workspace before starting.,design_debt,non-optimal_design
hbase,13905,comment_0,Patch to have test clear region directory if it already exists.,non_debt,-
hbase,13905,comment_1,Look reasonable to you ?,non_debt,-
hbase,13905,comment_2,That patch allows this test to get further. It's now getting cut off by the 30sec limit.,non_debt,-
hbase,13905,comment_3,Test is passing reliably on my rig with this patch.,non_debt,-
hbase,13905,comment_4,Pushed to branch-1.0+. Does not apply to 0.98.,non_debt,-
hbase,13905,comment_9,"Test is still in the flakey category, occasionally timing out with this stack",test_debt,flaky_test
hbase,13905,comment_11,Closing this issue after 1.0.2 release.,non_debt,-
hbase,13924,summary,Description for is wrong,documentation_debt,low_quality_documentation
hbase,13924,description,"The description in the following is wrong: The is *not* used for coprocessors, but only for filters, comparators, and exceptions. Fix.",documentation_debt,low_quality_documentation
hbase,13924,comment_0,Should we fix the doc to remove coprocessors? Or should we fix the code to use this config value to load jars for coprocessors?,documentation_debt,low_quality_documentation
hbase,13924,comment_1,"doc fix okay for 1.2. if we change how jars for coprocessors load, please only in 2.0.",non_debt,-
hbase,13924,comment_2,+1 for doc fix only if < master,non_debt,-
hbase,13924,comment_3,HBASE-13867 also targets to improve coprocessor documentation.,documentation_debt,low_quality_documentation
hbase,13924,comment_4,Ready for review.,non_debt,-
hbase,13924,comment_5,Contains a bunch of unrelated changes. Please either rescope the JIRA description or the patxh.,non_debt,-
hbase,13924,comment_6,Weird. I have no idea why that happened. Must be something with my editor. Let me try to regenerate the patch.,non_debt,-
hbase,13924,comment_8,1,non_debt,-
hbase,13924,comment_10,Committed to master.,non_debt,-
hbase,13928,summary,Correct doc bug introduced in HBASE-11735,non_debt,-
hbase,13928,comment_0,I don't think this is actually a bug. The hbase-default.xml has the following: Maybe can confirm.,non_debt,-
hbase,13928,comment_1,"is right, the added key is wrong, and never used. It is missing the {{.bucket.}} as mentioned and should be reading Only then an operator can set the sizes. The wrong property is misleading at most, but needs fixing anyways.",code_debt,low_quality_code
hbase,13928,comment_2,Oh and what he is saying is - just to clarify - that the is wrong.,non_debt,-
hbase,13928,comment_3,Was fixed in HBASE-15528.,non_debt,-
hbase,13942,summary,HBase client stalls during region split when client threads >,non_debt,-
hbase,13942,description,Performing any operataion using a single hconnection with client threads daemon prio=10 nid=0x62ff waiting on condition WAITING (parking) at Method) - parking to wait for <0x00000007d768bdf0,non_debt,-
hbase,13942,comment_0,It is becoming increasingly difficult with having 256 threads/cluster. Is it not possible to reduce this? We are afraid the same issue might prop up in case of drastically reducing the threads.,design_debt,non-optimal_design
hbase,13942,comment_1,This issue still exists and getting easily reproducible by reducing the max threads. Any possible patch for this?,non_debt,-
hbase,13942,comment_2,The issue seems to be fixed in 1.1.0.,non_debt,-
hbase,13942,comment_3,Re-resolving for release tracking.,non_debt,-
hbase,13944,summary,Prefix_Tree seeks to null if the seeked Cell is greater than the last key in the file,non_debt,-
hbase,13944,description,"When using any DBE other than Prefix_Tree when the internal scanners seek to a key greater than the last key in the file, when we do we get the last key in the file, whereas PrefixTree seeks to null. This is a behaviour change. May be in the actual scan case we may not end up in this scenario, need to check with a testcase. But the test in has a clear illustration of this problem. Will take this up after the current activities.",non_debt,-
hbase,13944,comment_0,The contract in interface says abt the return value for seek(Cell) and the result of call to next(). But not getKeyValue() after the call to seek(). The caller expected to call next() after doing the seek (?),non_debt,-
hbase,13944,comment_1,hbase-prefix-tree was removed in hbase2. Resolving as won't fix.,non_debt,-
hbase,13949,summary,Expand hadoop2 versions built on the pre-commit,non_debt,-
hbase,13949,description,For the HBase 1.1 line I've been validating builds against the following hadoop versions: 2.2.0 2.3.0 2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0. Let's do the same in pre-commit.,non_debt,-
hbase,13949,comment_0,"Sorry, slow-JIRA double-submit with HBASE-13948.",non_debt,-
hbase,13973,summary,Update documentation for 10070 Phase 2 changes,non_debt,-
hbase,13973,description,HBASE-10513 added documentation for 10070. Let's update it with the Phase 2 changes.,non_debt,-
hbase,13973,comment_0,Here is the patch. It has been sourced from updated  mind doing a pass?,non_debt,-
hbase,13973,comment_1,Attaching regenerated book.html.,non_debt,-
hbase,13973,comment_2,Some nits: 1. You should talk about the fact the storefile refresher is not needed when Async WAL replication is ON. Basically throw some color on this to make it clear as to when to use what. 2. There is a copy-paste issue in the section on configuration - the description of talks about meta replication which it shouldn't..,documentation_debt,low_quality_documentation
hbase,13973,comment_3,Thanks. Here is an updated version.,non_debt,-
hbase,13973,comment_4,1,non_debt,-
hbase,13973,comment_5,Committed this. Thanks Devaraj for the review.,non_debt,-
hbase,14161,summary,Add hbase-spark integration tests to IT jenkins job,non_debt,-
hbase,14161,description,expand the set of ITs we run to include the new hbase-spark tests.,non_debt,-
hbase,14161,comment_0,"The test suites under are the ones which are being referred here? They collectively run in <100 sec, should we consider adding them as large tests? Or is it because they use minicluster that we want them as ITs?",non_debt,-
hbase,14161,comment_1,/cc,non_debt,-
hbase,14161,comment_2,I don't see the spark ITs in [the trunk IT  could you add a note about how this is a duplicate?,non_debt,-
hbase,14161,comment_3,Let me take a look. Maybe I misunderstood the JIRA.,non_debt,-
hbase,14161,comment_4,"Ok. The tests in hbase-spark are run as regular unit tests now after HBASE-17574. We may need to write new integration tests for the spark module. But until that is available, there is real IT yet.",test_debt,lack_of_tests
hbase,14161,comment_5,sounds good to me.,non_debt,-
hbase,14161,comment_6,"We don't have a branch-2 specific IT matrix job yet, but the master/3.0.0-SNAP IT job is now running the new test from HBASE-18175",non_debt,-
hbase,14162,summary,Fixing maven target for regenerating thrift classes fails against 0.9.2,non_debt,-
hbase,14162,description,"HBASE-14045 updated the thrift version, but our enforcer rule is still checking 0.9.0.",non_debt,-
hbase,14162,comment_0,The command I used for generating thrift bindings for 0.9.2 is: {{mvn compile -Pcompile-thrift,non_debt,-
hbase,14162,comment_1,"On second thoughts, should we open a new jira for updating generated thrift bindings and push only pom related changes as part of this issue?",non_debt,-
hbase,14162,comment_3,Attaching the patch for fixing only maven target. Will create a separate jira for updating thrift bindings.,non_debt,-
hbase,14162,comment_5,Using the variable thrift.version instead of hard coding version 0.9.2 in v3.,code_debt,low_quality_code
hbase,14162,comment_7,-1 on v3. the point of the check is to make sure the thrift.version variable doesn't get updated without a review of compatibility. also it's a regular expression so using the variable will be unnecessarily permissive.,code_debt,low_quality_code
hbase,14162,comment_8,+1 on v2,non_debt,-
hbase,14162,comment_9,"Sure, let me push v2 changes.",non_debt,-
hbase,14253,summary,update docs + build for maven 3.0.4+,non_debt,-
hbase,14253,description,our new hbase-spark module raises our minimum maven version from 3.0.0 (though I've only tried 3.0.3) to 3.0.4: Update the docs to call out 3.0.4 and add an enforcer rule so that this failure can happen at the start of a build rather than 15 minutes in.,non_debt,-
hbase,14253,comment_0,Will do this tomorrow,non_debt,-
hbase,14253,comment_1,Happy to take a stab at this today if you don't mind,non_debt,-
hbase,14253,comment_2,Go for it !,non_debt,-
hbase,14253,comment_3,I only found two places where the Maven version number is referenced. Can you think of anything I might have missed?,non_debt,-
hbase,14253,comment_4,those were the only ones I can remember. thanks for the patch. will push after QA bot finishes.,non_debt,-
hbase,14253,comment_5,Thanks Sean,non_debt,-
hbase,14253,comment_6,"I can't imagine the build bot backlog is this extensive. Barring objection, I'm going to push this later today.",non_debt,-
hbase,14253,comment_7,1,non_debt,-
hbase,14253,comment_8,just a quick reminder that this is still open. QA Bot never ran though... Thanks for taking a look,non_debt,-
hbase,14253,comment_10,This seems to be a genuine failure: {{mvn: command not found}}. Any idea how Jenkins is configured and why it might fail to find Maven?,non_debt,-
hbase,14253,comment_11,that generally means the build toolchain is bad on the host. we had similar problems with H0. I've removed H10 from the rotation now.,non_debt,-
hbase,14325,summary,Add snapshotinfo command to hbase script,non_debt,-
hbase,14325,description,"Since we already have commands like hbck, hfile, wal etc. that are used for getting various types of information about HBase components it make sense to me to add SnapshotInfo tool to collection. If nobody objects i would add patch for this.",non_debt,-
hbase,14325,comment_0,Here is patch,non_debt,-
hbase,14325,comment_1,1,non_debt,-
hbase,14325,comment_12,Updating fix version based on release audit vs what's committed.,non_debt,-
hbase,14325,comment_13,Bulk closing 1.1.3 issues.,non_debt,-
hbase,14385,summary,Close the sockets that is missing in connection closure.,non_debt,-
hbase,14385,description,As per heading. Due credit to one of our awesome customers for digging into this and helping me craft the unit test.,non_debt,-
hbase,14385,comment_0,+1 Want to assert that this exception actually happens rather than presume? 105 } catch e) { 106 } Fix on commit. Nice test.,non_debt,-
hbase,14385,comment_1,Addressing the feedback and awaiting QA bot blessings.,non_debt,-
hbase,14385,comment_3,Test failures not related. Will push this tonight unless objection.,non_debt,-
hbase,14385,comment_11,Bulk closing 1.1.3 issues.,non_debt,-
hbase,14494,summary,Wrong usage messages on shell commands,code_debt,low_quality_code
hbase,14494,description,"noticed that the grant shell command was missing commas in-between the method arguments in the usage/help messages. After taking a look at some of the others, it's not alone. I'll make a pass over the shell commands and try to find some more help messages which tell the user to run a bad command.",code_debt,low_quality_code
hbase,14494,comment_0,Simple patch which adds some commas to the help message for the follow shell commands: * * grant.rb * * revoke.rb,code_debt,low_quality_code
hbase,14494,comment_2,1,non_debt,-
hbase,14494,comment_3,Committed,non_debt,-
hbase,14494,comment_4,Thanks !,non_debt,-
hbase,14494,comment_14,Bulk closing 1.1.3 issues.,non_debt,-
hbase,14517,summary,Show regionserver's version in master status page,non_debt,-
hbase,14517,description,"In production env, regionservers may be removed from the cluster for hardware problems and rejoined the cluster after the repair. There is a potential risk that the version of rejoined regionserver may diff from others because the cluster has been upgraded through many versions. To solve this, we can show the all regionservers' version in the server list of master's status page, and highlight the regionserver when its version is different from the master's version, similar to HDFS-3245 Suggestions are welcome~",design_debt,non-optimal_design
hbase,14517,comment_0,Patch for master,non_debt,-
hbase,14517,comment_1,This looks really nice  Operators will like it. Why you move the VersionInfo from RPC to HBase protos? Will that break anyone (I don't think so.. since you do not change the pb data structure),non_debt,-
hbase,14517,comment_2,Because RPC.proto has depended on HBase.proto and there will be a cycle dependency if the VersionInfo is put in RPC.proto.,build_debt,under-declared_dependencies
hbase,14517,comment_3,Same patch to trigger the ci build.,non_debt,-
hbase,14517,comment_5,Fix checkstyle errors,code_debt,low_quality_code
hbase,14517,comment_7,Pushed to branch-1.2+ Thanks for nice patch,non_debt,-
hbase,14517,comment_13,"I just ran into issue where I had a cluster of mixed versions. It would have taken me a while to notice but at the base of my list of regionservers, there was a helpful ' 9 nodes with inconsistent version'.. Nice. Thanks.",non_debt,-
hbase,14579,summary,Users authenticated with KERBEROS are recorded as being authenticated with SIMPLE,non_debt,-
hbase,14579,description,"That's the HBase version of HADOOP-10683. We see: ??hbase.Server - Auth successful for (auth:SIMPLE)?? while we would like to see: ??hbase.Server - Auth successful for (auth:KERBEROS)?? The fix is simple, but it means we need hadoop 2.5+. There is also a lot of cases where HBase calls ""createUser"" w/o specifying the authentication method... I don""'t have the solution for these ones.",non_debt,-
hbase,14579,comment_1,Yes. Is that an issue for the 2.0 branch?,non_debt,-
hbase,14579,comment_2,I'd say it would be fine to drop 2.4 hadoop for 2.0 hbase.,non_debt,-
hbase,14579,comment_3,"+1, I think we're already saying people should use 2.6.1 or later with 1.x.",non_debt,-
hbase,14579,comment_4,I'm not sure I understand correctly the test patch script. Can I just change the property to Or is there is a risk to hide a problem a patch could cause to the 0.98 release (and even the 0.94)? We will need to update the matrix in the hbase book as well...,non_debt,-
hbase,14579,comment_5,"I think test-patch is used for all builds. So, we should pass in this string as an argument instead and then have jenkins set it per build? i.e. for master, we'd drop the 2.4? Yeah, need to update the book matrix too.",non_debt,-
hbase,14579,comment_6,I can do this for you if you want ? Let me know.,non_debt,-
hbase,14579,comment_7,"Thanks Stack, yes, it would be great. I could change the script, but I can't easily test it right now.",non_debt,-
hbase,14579,comment_8,"Does this also happen for users authenticated with authentication tokens (""auth:SIMPLE"" instead of ""auth:TOKEN"" or ""auth:DIGEST"")? Over in HBASE-14700, I'm looking at a related change to the audit log for authenticated users.",non_debt,-
hbase,14579,comment_9,The latest patch (v3) for HBASE-14700 contains a fix for the UGI auth method logged. Please take a look there if you have a chance.,non_debt,-
hbase,14579,comment_10,"For digest, I tink it's ok, the code is RpcServer is Looking...",non_debt,-
hbase,14604,summary,Improve MoveCostFunction in,non_debt,-
hbase,14604,description,"The code in MoveCoseFunction: It uses cluster.numRegions + META_MOVE_COST_MULT as the max value when scale moveCost to [0,1]. But this should use maxMoves as the max value when cluster have a lot of regions. Assume a cluster have 10000 regions, maxMoves is 2500, it only scale moveCost to [0, 0.25]. Improve moveCost by use maxMoves) as the max cost, so it can scale moveCost to [0,1].",design_debt,non-optimal_design
hbase,14604,comment_0,patch for 98,non_debt,-
hbase,14604,comment_1,Any chance of a unit test ?,test_debt,lack_of_tests
hbase,14604,comment_2,"Add ut for MoveCostFunction. T E S T S Running 1, 0, Errors: 0, Skipped: 0, 12.08 sec - in Running 1, 0, Errors: 0, Skipped: 0, 0.812 sec - in Running 14, 0, Errors: 0, Skipped: 0, 28.241 sec - in Running 5, 0, Errors: 0, Skipped: 0, 0.601 sec - in Results : 21, 0, Errors: 0, Skipped: 0",non_debt,-
hbase,14604,comment_4,The classes are in the same package. Please make the new methods package private. Patch for 0.98 branch should contain 0.98 in the filename. Extension should be either .txt or .patch. Please attach patch for master branch.,code_debt,low_quality_code
hbase,14604,comment_5,@Ted Thank you very much. Attach new patch for master and 0.98.,non_debt,-
hbase,14604,comment_8,"Thanks for the patch, Guanghao",non_debt,-
hbase,14617,summary,Procedure V2: Update to interact with assignment procedures,non_debt,-
hbase,14617,description,"this JIRA tracks the update of to interact with assignment procedures. This is very critical (and most tricky part of) work that deals with dead region server when assignment is happening. - remove region server queue when the region server is dead - notify assignment procedures that are doing assignment operation in the dead server - assign regions in dead server to other RS (we have to deal with RIT, deal with in-progress table DDL)",non_debt,-
hbase,14617,comment_0,Part of HBASE-14616,non_debt,-
hbase,14622,summary,Purge TestZkLess* tests from branch-1,non_debt,-
hbase,14622,description,"I saw this in branch-1.2 tests just now: We are failing a test for an unsupported feature that is doing assignment for a new facility, replicas. Let me purge all TestZKLess* tests in branch-1.",non_debt,-
hbase,14622,comment_0,Pushed to branch-1.1+,non_debt,-
hbase,14622,comment_1,Removed four tests: diff --git diff --git diff --git diff --git,non_debt,-
hbase,14622,comment_5,"This rationale applies to 0.98 also, so let me do that there too.",non_debt,-
hbase,14622,comment_6,", you purged this on branch-1.x except for branch-1.0. Probably should do that there too. I'm just going to go ahead and do it.",non_debt,-
hbase,14622,comment_7,"Pushed removal of these tests to branch-1.0 and 0.98, reusing Stack's commit message",non_debt,-
hbase,14622,comment_8,Why are we calling this feature un-supported. It has a lot of nice properties. We have a blog post up about it. From what I remember the only reason we disabled it was so that things would be rolling upgradable.,non_debt,-
hbase,14622,comment_13,"zkless is the future.. It will be default in 2.0, or at least a version of it -- one w/ state preserved in procedure store rather than up in meta. zkless in 1.0 is not well tested and I thought no one was using it (Only case I know of someone using it, is on a branch made of 0.98) I removed the tests here because it was an 'unsupported' feature test running on top of read replicas, a feature that is not on by default throwing an NPE. Rather than try and figure what was up in the test, and not expecting that someone would be jumping at the opportunity to fix issues in here, I just removed these zkless tests from branch-1 so they can't fail again. I can put them back if you'd like , just say.",test_debt,lack_of_tests
hbase,14622,comment_15,Bulk closing 1.1.3 issues.,non_debt,-
hbase,14677,summary,BucketAllocator freeBlock is the bottleneck in bulk block cache evictions,non_debt,-
hbase,14677,description,"BucketCache implementation of freeBlock has a bad performance / concurrency and affects bulk block evictions from cache on a file close (after split, compactions or region moves).",code_debt,slow_algorithm
hbase,14677,comment_0,Duplicate of HBASE-14624,non_debt,-
hbase,14677,comment_1,There is still an issue if evictOnClose is true right? we need to look into why ore?,non_debt,-
hbase,14753,summary,TestShell is not invoked anymore,non_debt,-
hbase,14753,description,Not sure whether it is due to HBASE-14679 or not. does not run any test at all.,non_debt,-
hbase,14753,comment_0,It was disabled over in HBASE-14678 along with as part of the effort at stabilizing builds. Builds seem to be settling. Will start reenabling the stuff removed over in HBASE-14678 after we get a good run of blue builds.,non_debt,-
hbase,14753,comment_1,"Ok, sorry for the spam then. I was trying to write a shell test, and was confused. Do we need to keep this open for tracking re-enabling or HBASE-14678 covers that?",test_debt,lack_of_tests
hbase,14753,comment_2,I was thinking HBASE-14678 would do. I'd file issues for stuff I don't want to bring back in because flakey still... tests on shell are pretty important.,test_debt,flaky_test
hbase,14753,comment_3,Resolving as dup of HBASE-15023 which enabled TestShell,non_debt,-
hbase,14783,summary,Proc-V2: Master aborts when downgrading from 1.3 to 1.1,non_debt,-
hbase,14783,description,"I was running ITBLL with 1.3 deployed on a 6 node cluster. Then I stopped the cluster, deployed 1.1 release and tried to start cluster. However, master failed to start due to: The cause was that written in some WAL file under MasterProcWALs from first run, was absent in 1.1 release. After a brief discussion with Stephen, I am logging this JIRA to solicit discussion on how customer experience can be improved if downgrade of hbase is performed.",non_debt,-
hbase,14783,comment_0,"Thanks,  for reporting this. We are keeping adding new procedure types in new releases. If a customer does not like the new release after upgrade; if they downgrade, they could hit this issue (newly introduced procedure type could not be replayed in older release.) One solution is that during procedure load, ignore the and log a warning or error in exception and continue.",non_debt,-
hbase,14783,comment_1,"there are 3 problems here - on a not clean shutdown your cluster is in a non consistent state, starting the master and ignoring the exception is a bad idea. you have run hbck or you can restart the cluster and do a clean shutdown before the downgrade. - on a clean shutdown and restart with lower version you will not have procedure running. so nothing to load and you'll not hit this problem the 3rd problem is related to how we load, completed procedures (but not yet deleted) are loaded using convert() which creates a Procedure instance, but we don't really need that. in fact we remove that instance later when we realize that we just need the result. so even in case of a clean shutdown we may get the exception because we are trying to call convert().",non_debt,-
hbase,14783,comment_2,We do not support downgrade. Never have. Its a bunch of work -- code-wise and testing-wise. Suggest we close this issue as invalid.,non_debt,-
hbase,14783,comment_3,"we can change the title to change the load code, my step 3. which was something that I was going to do anyway just to save some ops on load",non_debt,-
hbase,14783,comment_4,"If folks want to support downgrade, need to raise on the dev list. I went looking for where we explicitly state [that we do not support downgrade] in the refguide but all I found was the bit in our section on semver that says even between patch versions, downgrade may not be possible. See toward end of",non_debt,-
hbase,14941,summary,locate_region shell command,non_debt,-
hbase,14941,description,"Sometimes it is helpful to get the region location given a specified key, without having to scan meta and look at the keys. so, having in the shell something like:",non_debt,-
hbase,14941,comment_0,"btw, as far as I know is public and it has no ACLs restriction. is this something that we have to restrict? ping",non_debt,-
hbase,14941,comment_1,"My guess is we haven't to date because all this information is available to all clients through a scan of META, which by design must be available for reads by all.",non_debt,-
hbase,14941,comment_2,"yeah, I was thinking if we have to filter meta result as we do for table list (where we return only accessible table). but maybe we end up slowing down meta too much?",non_debt,-
hbase,14941,comment_4,"We need close the locator at the end, right ?",non_debt,-
hbase,14941,comment_5,Can we use begin ensure block to ensure we do not leak it ?,design_debt,non-optimal_design
hbase,14941,comment_7,+1 for v2,non_debt,-
hbase,14941,comment_8,LGTM.,non_debt,-
hbase,14956,summary,[HBase ZKcli] JLine support is disabled. Better to enable this in HBase.,non_debt,-
hbase,14956,description,"To perform the zkcli operations using hbase, jline is disabled, To enable this jline-<versioneg: jline-2.11.jar should be exist in hbase/lib directory.",non_debt,-
hbase,14956,comment_0,We cannot bump JLine to a newer version since the version of JRuby we ship is an old one (see HBASE-13338). Also this shouldn't cause any issue to use zkcli since it only disables the auto complete functionality. Another option is to use a version of ZK that doesn't have ZOOKEEPER-1718.,architecture_debt,using_obsolete_technology
hbase,14956,comment_1,"Its better to have auto complete and proper zkcli prompt. However it impact hbase shell if jline jar is placed at hbase/lib, so we can place the jar at ""hbase/lib/jline"" and set it in the classpath on zkcli command.",non_debt,-
hbase,14956,comment_2,"That approach will cause issues when we bump JRuby to a newer version and altering the classpath in that way it will make difficult to keep things consistent. As I mentioned, this was caused by ZOOKEEPER-1718 and looking at master and branch-1 we still require ZooKeeper 3.4.6 so it shouldn't be a problem unless you are using a different version of ZK in your deployment.",non_debt,-
hbase,14956,comment_3,Based on the discussion I'm going to resolve as Not A Problem. Reopen if you disagree,non_debt,-
hbase,14984,summary,Allow memcached block cache to set optimze to false,non_debt,-
hbase,14984,description,In order to keep latency consistent it might not be good to allow the spy memcached client to optimize.,non_debt,-
hbase,14984,comment_1,+1. I can't figure out what the javadoc warning is supposed to be based on looking at the precommit artifacts and the patch.,documentation_debt,low_quality_documentation
hbase,14984,comment_2,Yeah that's been happening lately. Thanks,non_debt,-
hbase,14984,comment_3,is this worth a release note?,non_debt,-
hbase,14984,comment_4,"Yep, let me get that.",non_debt,-
hbase,15066,summary,Small improvements to Canary tool,non_debt,-
hbase,15066,description,From an internal request from a user: This patch: - Adds debug statements for pre-operation steps. - Adds bin/hbase canary support - Fixes table names with patterns where the pattern was not matched previously.,non_debt,-
hbase,15066,comment_0,v1 patch.,non_debt,-
hbase,15066,comment_2,LGTM,non_debt,-
hbase,15066,comment_3,Pushed to branch-1+. Thanks Stack for looking.,non_debt,-
hbase,15066,comment_7,This was marked as in 0.98.17 but I couldn't find it in 0.98 branch history. I suspect I had it queued up to go on a working branch but never pushed it up. Have done so now. Fix version is now 0.98.18.,non_debt,-
hbase,15192,summary,is flaky,test_debt,flaky_test
hbase,15192,description,"fails intermittently due to failed assertion on cleaned merge region count: Before calling the test does: newcount1 is not cleared at the beginning of the loop. This means that if the check for newcount1 <= 1 doesn't pass the first iteration, it wouldn't pass in subsequent iterations. After timeout is exhausted, is called. However, there is a chance that has been called by the Chore already (during the wait period), leaving the cleaned count 0 and failing the test.",test_debt,flaky_test
hbase,15192,comment_0,First attempt for a fix. The log level change in CatalogJanitor is for collecting more information in case the test fails. It will be taken out in the final patch.,non_debt,-
hbase,15192,comment_2,I can change the sleep to something else so that we can assert the store file count after the chore is run.,non_debt,-
hbase,15192,comment_3,"Since the test fails if merge references are not cleaned, we can call more than once if needed. runCatalogScan() is the only method exposed by CatalogJanitor, otherwise we can poll CatalogJanitor for the value of mergeCleaned and pass the test when mergeCleaned crosses 1. Patch v2 passes 30 iterations of test runs. Previously the test failed within the first 5 iterations.",non_debt,-
hbase,15192,comment_4,"Try it. Leave issue open and if it quits failing after a week, close it.",non_debt,-
hbase,15192,comment_6,Looks like there are other flaky sub-tests.,test_debt,flaky_test
hbase,15192,comment_9,"If there is no objection, I plan to resolve this JIRA. There is still flaky subtest, e.g. : which we can track in separate issue(s)",test_debt,flaky_test
hbase,15207,summary,Stuck balancer,non_debt,-
hbase,15207,comment_0,Seems like a blocker if balancer got stuck.,non_debt,-
hbase,15207,comment_1,"is this ticket for the balancer getting stuck, or for the logging rate when it is stuck?",non_debt,-
hbase,15207,comment_2,Fixed summary and description. It is about the stuck balancer. Should also fix the crazy logging. Can do that in subtask. Thanks,code_debt,low_quality_code
hbase,15207,comment_3,I'd say blocker status will depend on if we can determine if the root cause is new in 1.2,non_debt,-
hbase,15207,comment_4,"Happened again in new loading. Its hard to make sense of because the logging overwhelms. Let me address that in a patch first. Can then look at hang. In this current case we filled 10 log files of 256MB each... but it looks like we 'recovered'. LB reports: 2016-02-02 17:15:05,123 DEBUG Finished computing new load balance plan. Computation took 3761ms to try 217600 different iterations. Found a solution that moves 31 regions; Going from a computed cost of 341.5035724035313 to a new cost of 47.8518130393211 And going back over logs, yeah, I get sessions of log spewing.... maybe the load balancer is ok... its just this crazy logging phenomenon",code_debt,low_quality_code
hbase,15207,comment_5,"Resolving as cannot reproduce. I think balancer is fine, not stuck. It just looked that way because of the log spew caused by the subtask.",non_debt,-
hbase,15287,summary,returns incorrect result with binary row key inputs,non_debt,-
hbase,15287,description,"takes optional start/end key as inputs (-range option). It would work only when the string representation of value is identical to the string. When row key is binary, the string representation of the value would look like this: ""\x00\x01"", which would be incorrect interpreted as 8 char string in the current implementation: To fix that, we need change how the value is converted from command line inputs: Change to Do the same conversion to end key as well. The issue was discovered when the utility was used to calcualte row distribution on regions from table with binary row keys. The hbase:meta contains the start key of each region in format of above example.",non_debt,-
hbase,15287,comment_0,You have a patch?,non_debt,-
hbase,15287,comment_1,"Hi , Randy's description of the issue (and fix) are correct and would be a quick patch. I had been investigating the larger question of whether other HBase MapReduce functions should get the same change. I grabbed the ticket figuring on a 3/30 ETA for patch submission but am ok giving it up to someone else.",non_debt,-
hbase,15287,comment_2,Great. No worries. We'll wait for the patch.,non_debt,-
hbase,15287,comment_3,"Apologies, I've been busy with other things. Hopefully should have a patch ready this weekend.",non_debt,-
hbase,15287,comment_4,Attached patch switches row value for HBase's mapred/mapreduce jobs from Bytes.toByte() to Family and qualifier were not switched to binary bytes as I assume they are unlikely to contain binary values.,non_debt,-
hbase,15287,comment_6,Can you modify the following tests to cover your fix ?,test_debt,lack_of_tests
hbase,15287,comment_7,"Ok, attached adds binary row test cases for TestImportExport, TestCopyTable, and TestCellCounter.",non_debt,-
hbase,15287,comment_9,Ran the following locally:,non_debt,-
hbase,15287,comment_10,Reattaching for QA run.,non_debt,-
hbase,15287,comment_11,@Matt: Mind attaching patch for branch-1 ? Thanks,non_debt,-
hbase,15287,comment_14,Attached ports this patch to branch-1. Note that TestRowCounter was not ported because the --range argument does not work on this branch (see for details).,non_debt,-
hbase,15287,comment_17,"Thanks for the patch, Matt.",non_debt,-
hbase,15293,summary,Handle TableNotFound and IllegalArgument exceptions in table.jsp,non_debt,-
hbase,15293,description,"table.jsp accepts ""name"" parameter for table name and in case when table does not exist or empty parameter is passed it will throw 500 error.",non_debt,-
hbase,15293,comment_0,Here is patch for this issue.,non_debt,-
hbase,15293,comment_1,Patch is tested on master branch. If needed i can provide patch for other branches.,non_debt,-
hbase,15293,comment_3,"Here is v1 with minor change avoiding write of unvalidated ""fqtn"" parameter.",non_debt,-
hbase,15293,comment_5,Have you checked whether failure was related to the patch ?,non_debt,-
hbase,15293,comment_6,Yes. I have run few times on my laptop. All pass. Let me reattach patch.,non_debt,-
hbase,15293,comment_7,Retry.,non_debt,-
hbase,15293,comment_9,"Please insert space around 'catch' There're 7 tabs in the patch. Please remove them. Once these are addressed, it should be good.",code_debt,low_quality_code
hbase,15293,comment_10,here is new patch addressing formating issues. How did you check for tabs? 'git diff --check' passed for v2.,non_debt,-
hbase,15293,comment_11,"Using vi, you can search for ""\/t""",non_debt,-
hbase,15293,comment_13,Here is new patch tabs cleaned. Thanks for tip  and sorry for inconvenience with tabs :),non_debt,-
hbase,15397,summary,Create bulk load replication znode(hfile-refs) in ZK replication queue by default,non_debt,-
hbase,15397,description,"Create bulk load replication znode(hfile-refs) in ZK replication queue by default same as hbase replication znode. Otherwise the problem what happens is currently replication admin directly operates on ZK without routing through HM/RS. So suppose if a user enables the replication for bulk loaded data in server but fails to do the same in the client configurations then add peer will not add hfile-refs znode, resulting in replication failure for bulk loaded data. So after fixing this the behavior will be same as mutation replication.",non_debt,-
hbase,15397,comment_0,Please review.,non_debt,-
hbase,15397,comment_2,Why is the Boolean removed ? Thanks,non_debt,-
hbase,15397,comment_3,It is not used any more.,code_debt,dead_code
hbase,15397,comment_4,"Since this config is not in 1.2.0 release, it is Okay to take it out.",non_debt,-
hbase,15397,comment_5,Yes. This feature is yet to release publicly.,non_debt,-
hbase,15397,comment_6,Planning to integrate the patch if there is no other review comment(s).,non_debt,-
hbase,15490,summary,Remove duplicated in branch-1,code_debt,duplicated_code
hbase,15490,description,"Currently there're two in our branch-1 code base (one in package, the other in and both are in use. This is a regression of HBASE-14969 and only exists in branch-1. We should remove the one in and change the default compaction throughput controller back to to keep compatible with previous branch-1 version Thanks  for pointing out the issue.",code_debt,duplicated_code
hbase,15490,comment_0,"Checking current branch-1 codes, the to-be-removed in is only called by our UT case, and only the new one in will take effect in the real world. Since 1.1.3 is released where is set as default with 20MB/s as throttle threshold, I'm wondering whether we should change the default back to or add a release note in HBASE-14969 identifying the change (probably need a discussion on the default lower/upper bound also). Please let me know your thoughts guys. Thanks.",non_debt,-
hbase,15490,comment_1,"Please ignore this question, just checked and confirmed the change only included in 1.3.0 rather than 1.1.3 or any released version. So I'll just change the default back to",non_debt,-
hbase,15490,comment_2,A straight forward patch,non_debt,-
hbase,15490,comment_3,Submit patch to see what HadoopQA says.,non_debt,-
hbase,15490,comment_5,Re-submit patch with correct suffix...,non_debt,-
hbase,15490,comment_7,From the [test we could see the only failed UT case is which is tracked by HBASE-15307 and irrelative to the change here (also confirmed it could pass in local). Will commit the patch after review.,non_debt,-
hbase,15490,comment_8,lgtm,non_debt,-
hbase,15490,comment_9,Pushed into branch-1. Thanks  for review.,non_debt,-
hbase,15490,comment_12,"Belated +1 from me as well. Thanks for the fix. Was this pushed to branch-1.3 as well? I see the message from Hudson that it was integrated in HBase-1.3-IT, but I don't see the corresponding commit in branch-1.3. If not, please push there as well. cc:",non_debt,-
hbase,15490,comment_13,"No, only pushed to branch-1 yesterday. Have just pushed to branch-1.3, thanks for the reminder",non_debt,-
hbase,15490,comment_14,"Thanks for the fix ! Yeah, I'm planning to start stabilizing branch 1.3 in a week or two, and fixes and cleanups are always welcome :)",non_debt,-
hbase,15550,summary,Backport HBASE-12220 hedgedRead metrics to 1.2+,non_debt,-
hbase,15550,description,hedgedRead metrics are in master branch only. They were put in branch-1 but then reverted because old versions of hadoop didn't supported hedgedReads. 1.2+ has minimal hadoop version. Retry backport. FYI,non_debt,-
hbase,15550,comment_0,Lets see how this patch does.,non_debt,-
hbase,15550,comment_1,Should we move Hadoop 2.2 and 2.3 from NT to X in the [basic prerequisites list for since we know it'll fail with this patch?,non_debt,-
hbase,15550,comment_3,Thanks . Think it'd be ok for 1.2? These metrics are kinda important. We should be encouraging hedged reads. I forgot we'd not backported this stuff until a prod from Mr. Carroll.,non_debt,-
hbase,15550,comment_4,"I'm fine with flagging Hadoop 2.2 and 2.3 as X in HBase 1.2+, since I personally dislike the distinction between X and NT in our compat guide. Should probably get some lazy consensus over on dev@ though.",non_debt,-
hbase,15550,comment_5,Lets do this for 1.3+ at least.,non_debt,-
hbase,15550,comment_6,Doing it.,non_debt,-
hbase,15550,comment_7,1,non_debt,-
hbase,15617,summary,Canary in regionserver mode might not enumerate all regionservers,non_debt,-
hbase,15617,description,"When running in regionserver mode the Canary is expected to probe for service health one time per regionserver during a probe interval. Each time the canary chore fires, we create a which uses (via to enumerate over all table descriptors, find the locations for each table, then assemble the list of regionservers to probe from this result. The list may not contain all live regionservers, if there is a regionserver up but for some reason not serving any regions. To ensure we have the complete list of live regionservers I think it would be better to use and enumerate the live server list returned in the result.",design_debt,non-optimal_design
hbase,15617,comment_1,"The new depends on some internal details likely to change. We should treat ZK state as a black box and avoid relying on it. This is a layering violation, really. Why not use the supported Admin API's to get the live server list to enumerate? That unit test failure appears unrelated.",design_debt,non-optimal_design
hbase,15617,comment_2,v1 patch lgtm,non_debt,-
hbase,15617,comment_4,Looks all good. Committing today. I'll port it back to all active branches.,non_debt,-
hbase,15626,summary,won't return the full message,non_debt,-
hbase,15626,description,"The will include server addresses as: However, the returned value is {{s}}, only includes the exceptions. To include the server addresses, the returned value should be",non_debt,-
hbase,15626,comment_0,Same problem as,non_debt,-
hbase,15626,comment_1,.,non_debt,-
hbase,15640,summary,L1 cache doesn't give fair warning that it is showing partial stats only when it hits limit,design_debt,non-optimal_design
hbase,15640,description,"I was baffled looking at L1 Cache because it was stuck at 100k blocks and only 'loading' 3G of data according to the UI. When I looked in log, all the numbers looked write in the Cache summary reported in log. Turns out our buckecache UI template will cut off calculating stats at a limit --which is probably what we want -- but the fact that it has done this should be more plain.",design_debt,non-optimal_design
hbase,15640,comment_0,"Here is small patch which ups our limit to 1M blocks from 100k so less likely we'll hit limit and then if we do, puts in place a message in red with how to address the limit (I couldn't figure what the previous message was trying to say... didn't seem to make sense). This could be better.. but will do for now.",design_debt,non-optimal_design
hbase,15640,comment_2,Retry,non_debt,-
hbase,15640,comment_4,Better to use css classes for this. I believe ( though it's off the top of my head ) that will do what you want.,non_debt,-
hbase,15640,comment_5,Implement  suggestion,non_debt,-
hbase,15640,comment_7,"What I pushed to branch-1.2+ Without it, the UI gives wonky picture of whats in cache.... For operators.",non_debt,-
hbase,15640,comment_8,Thanks,non_debt,-
hbase,15640,comment_9,What was applied to master branch. Includes minor fixup around prefetch logging fixes that came of testing this patch.,non_debt,-
hbase,15679,summary,Assertion on wrong variable in,non_debt,-
hbase,15679,description,"Sometimes failed with: Looking closer, we can see that the wrong variable is used for the assertion:",non_debt,-
hbase,15679,comment_0,"+1 Looks good to me. Ted, thanks for catching this.",non_debt,-
hbase,15679,comment_2,Test passed: The hanging / failing tests were not touched by the patch.,non_debt,-
hbase,15679,comment_3,"Thanks for the review, Stephen.",non_debt,-
hbase,15704,summary,"Refactoring: Move HFileArchiver from backup to tool package, remove backup.examples",non_debt,-
hbase,15704,description,This class is in backup package (as well as backup/examples classes) but is not backup - related. Remove examples classes from a codebase,code_debt,low_quality_code
hbase,15704,comment_0,patch v1.,non_debt,-
hbase,15704,comment_1,Patch v2.,non_debt,-
hbase,15704,comment_2,"Was checking the code, but it is not clear whether these classes are actually getting used or not. It seems that you can actually configure in your configuration and in-theory use for archiving table's hfiles. However, I don't think this is getting used much if at all. If not, I would rather get rid of these, instead of putting it in the example package where they will just bit-rot. HBASE-5547 added these, so maybe  or  might give some insight.",design_debt,non-optimal_design
hbase,15704,comment_3,"I do not think we should delete examples, there is a good place for them.",non_debt,-
hbase,15704,comment_4,"They were from some of the work Salesforce was doing at the time around supporting our backups. However, as everything is moving towards the ""new"" backup impl (based on wals - that still happening?), I don't know if we need to keep it around.",non_debt,-
hbase,15704,comment_5,"happening, happening :). HBASE-7912, HBASE-14030, HBASE-14123, HBASE-14414.",non_debt,-
hbase,15704,comment_6,"Thanks . Let's remove the code in master then. It is not good to keep un-used code around. Vladimir, can we remove the backup.example classes in master, and do the rename only in HFileArchiver. In branch-1, we can keep the example classes and do the HFileArchiver rename only.",code_debt,dead_code
hbase,15704,comment_7,v3. Removes example package and renames (moves) HFileArchiver. cc:,non_debt,-
hbase,15704,comment_9,when you will have time please take a look at this patch.,non_debt,-
hbase,15707,summary,ImportTSV bulk output does not support tags with,non_debt,-
hbase,15707,description,"Running the following command: The content of input is like: When running hfile tool with the output hfile, there is no ttl tag.",non_debt,-
hbase,15707,comment_0,How often does it go zombie ?,non_debt,-
hbase,15707,comment_1,", Sorry, it should not go zombie at all, a copy paste error. Uploading a new patch which removes this line, thanks for review.",non_debt,-
hbase,15707,comment_3,Please fix checkstyle warning.,code_debt,low_quality_code
hbase,15707,comment_5,"Understood the checkstyle warning, will provide a new patch, thanks !",code_debt,low_quality_code
hbase,15707,comment_6,"Looking closer at the checkstyle warning: Since the method was exceeding recommended length of 150 before the patch, I think we can address the above in another issue. +1 from me.",non_debt,-
hbase,15707,comment_7,"Yeah, the function has an anonymous class inline so making it exceeding the 150 line limit. I was considering to separate the anonymous class..., thanks .",design_debt,non-optimal_design
hbase,15707,comment_8,Patch doesn't compile on branch-1 Mind attaching patch for branch-1 ? Thanks,non_debt,-
hbase,15707,comment_9,"Thanks , I will work on a branch-1 patch.",non_debt,-
hbase,15707,comment_11,patch for branch-1,non_debt,-
hbase,15707,comment_13,"Thanks for the patch, huaxiang.",non_debt,-
hbase,15732,summary,hbase-rsgroups should be in the assembly,non_debt,-
hbase,15732,description,"{{hbase-rsgroup}} is a new module that does not appear in the assembly. The binary tarball still contains the jars through dependencies, but we need the test-jar as well for running the  can you take a quick look.",build_debt,build_others
hbase,15732,comment_0,Simple patch.,non_debt,-
hbase,15732,comment_1,+1 Thanks for catching this. Will need to include this in backport.,non_debt,-
hbase,15732,comment_3,Committed. Thanks Francis for taking a look. Agreed on the branch-1 backport.,non_debt,-
hbase,15738,summary,Ensure artifacts in project dist area include required md5 file,non_debt,-
hbase,15738,description,From the 0.98.19RC0 thread:,non_debt,-
hbase,15738,comment_0,Patch for make_rc.sh and the releasing section of the book. You fellas think it's appropriate to retroactively add these files for released versions? I'm thinking not; we'll just pick this up going forward.,non_debt,-
hbase,15738,comment_2,"+1 on the patch ASF policy says we're responsible for all the artifacts in our distribution area, so I'd say we ought to add them. preferably by extracting the relevant line from the {{mds}} files we voted on originally.",non_debt,-
hbase,15738,comment_3,"ACK. Sounds simple enough, I'll look into it this evening.",non_debt,-
hbase,15738,comment_4,I went through all the mds files extracted the MD5 and SHA512 lines into their own files. This is the change I've staged. Care to spot-check? I also noticed that the md5 files in 1.2.1 are of a different format than these lines. Probably generated with a different tool (openssl vs gpg perhaps?),non_debt,-
hbase,15738,comment_6,"Sure. staged where? Yes, on 1.2 I use the either openssl {{md5 -r}} or coreutils {{md5sum}} for generating the file. I don't much care which we settle on, though I suspect the coreutils version will be more common.",non_debt,-
hbase,15738,comment_7,"Staged as in staged in the dist sandbox on my laptop. I've attached the file md5_and_sha.patch here, which is the output of `svn diff`. More common than gpg, which people are using to validate signatures anyway? I use gpg throughout because it's a single dependency we already have. I'm excited about adding another tool as a dependency, but it's not a big deal to me either way. I can redo the md5's with coreutils or whatever.",non_debt,-
hbase,15738,comment_8,"I would suggest coming up with something using gpg, since we have to date been both generating combined sums files with it and signing artifacts.Needing to use another tool would only be a minor annoyance through, in which case perhaps md5sum since it will be more common.",non_debt,-
hbase,15738,comment_9,Settling on the gpg-based solution for now. Staged changes pushed to dist svn and attached patch pushed to all 6 (!!!) active branches.,non_debt,-
hbase,15835,summary,"throws ""HMasterAddress already in use"" RuntimeException when a local instance of HBase is running",non_debt,-
hbase,15835,description,"When a MiniCluster is being started with the method (most typically in the context of JUnit testing), if a local HBase instance is already running (or for that matter, another thread with another MiniCluster is already running), the startup will fail with a RuntimeException saying ""HMasterAddress already in use"", referring explicitly to contention for the same default master info port (16010). This problem most recently came up in conjunction with HBASE-14876 and its sub-JIRAs (development of new HBase-oriented Maven archetypes), but this is apparently a known issue to veteran developers, who tend to set up the @BeforeClass sections of their test modules with code similar to the following: A comprehensive solution modeled on this should be put directly into HBaseTestUtility's main constructor, using one of the following options: OPTION 1 (always force random port assignment): OPTION 2 (always force random port assignment if user has not explicitly defined alternate port):",design_debt,non-optimal_design
hbase,15835,comment_0,"Do the new, stricter spam-prevention settings prevent me (a basic contributor) from assigning this JIRA entry to myself? (I don't seem to be able to do that.)",non_debt,-
hbase,15835,comment_1,you should be able to assign to yourself now.,non_debt,-
hbase,15835,comment_2,"Yep, thanks!!",non_debt,-
hbase,15835,comment_3,"Note that I went with ""OPTION 2"" as outlined in the description to this JIRA item (i.e. force random port assignment if user has not explicitly defined alternate port).",non_debt,-
hbase,15835,comment_5,"Curious: It appears that TestFSHLogProvider is (inadvertently, I imagine) instantiating an HBaseTestingUtility with a *null* Configuration (this within the context of kicking off a thread). This might be worthy of further investigation (since it's probably not intended to be this way), but for now I will simply precede my newly inserted logic with a check for a null Configuration, and thus avoid the that my original patch encountered.",design_debt,non-optimal_design
hbase,15835,comment_7,"Hey , can you remove instances of setting the port to -1 in existing tests? I did a quick grep and there looks to be only a handful. Can you add some class-level javadoc about the minicluster ports going random instead of default. Do you think it might be worth a debug-level logging message too? I could image someone setting a value and in one special case having it fail.",code_debt,low_quality_code
hbase,15835,comment_8,"Thanks for the feedback! Yes, I'll look into all those issues soon. Also, I'll look into the addition of an appropriate new @Test method in",non_debt,-
hbase,15835,comment_9,"Submitting a revised patch which includes all of the following... Subtask 1: Remove instances of setting the ports to -1 in existing tests. The following modules were modified to remove their (now apparently extraneous) setting of master-info-port and region-server-port: Subtask 2: Add some class-level javadoc. The following was added to the HBaseTestingUtility class-level javadoc comment: Subtask 3: Add a debug-level logging message for when port values are overridden to ""-1"". The following code now appears at the end of the main constructor for Note the ""debug"" logging that has been added: Subtask 4: Add new method to for testing port overrides. The following new method assures that port override is taking place when it should, and is NOT taking place when it should NOT:",documentation_debt,low_quality_documentation
hbase,15835,comment_11,"Okay, the above-listed QA-failure prompted me to read in more detail those emails regarding the problem of ""flaky|flakey"" tests. I see that is indeed on the list, and also that testing of HBASE-15876 ran into exactly the same (apparently ""flaky|flakey"") failure. In this case, an awkward aspect is that the changes I made could conceivably have caused this kind of a failure!! So, just for good measure, I did a fresh clone/install of the Master branch and ran the same test: yep, it failed in exactly the same way.",test_debt,flaky_test
hbase,15835,comment_12,Please let me know if any further actions are required from me on this issue. Thanks!,non_debt,-
hbase,15835,comment_13,"Pushed to master branch. Thanks @daniel vimont for the nice patch. I like in particular how this is something we've just put up with for ever but because you are new, you recognize that this is just broke... and then fixed it. Will help other new users.",non_debt,-
hbase,15835,comment_15,"-- Sorry if I'm tossing this into the 1.3 ""in-box"" too late! I hope it's not too late to squeak it in, because it might make life easier for the early adopters of the two archetypes that are going into this release (HBASE-14877 and HBASE-14878).",non_debt,-
hbase,15835,comment_16,"Please let me know if the patch does not go through the cherry-picking process in a friendly way. (It may not, due to all the changes made to existing Test* modules to remove now-extraneous port setting statements. Hopefully, though, all those Test modules haven't been modified in ages, and there'll be no prob with the cherry-pick!)",non_debt,-
hbase,15835,comment_17,"looking at the patch, bunch of Mob related test aren't needed in branch-1 as there're no mobs in there.. Sorry for being late here, but at this point I don't really want to cherry pick this in 1.3 as I'm in the process of stabilizing the branch and the changes around test harness would be something I'd rather gladly take in 1.3.1. With straighforward corrections this patch should apply to branch-1 I think. I can do it a bit later.",non_debt,-
hbase,15835,comment_19,"No problem -- this can certainly wait until 1.3.1! But I'm very appreciative that you're getting HBASE-14877 and HBASE-14878 into 1.3!! And when it gets time for 1.3.1, the most straightforward thing to do might be to only backport the modifications in and and simply rip out all the other Test* module changes that are there (in the file). While it was a good idea to get that extraneous port-setting code out of the ongoing master branch (since existing code tends to be used as a model for new code added in the future), it's likely a lot less important to remove it from the 1.x branches.",non_debt,-
hbase,15835,comment_20,"Yes, the relation between the archetypes and this issue is NOT obvious -- only became apparent to me after a number of weeks of working with archetypes. Ultimately, I took the complete beginner's point of view, imagining them (a) installing and starting up a local standalone version of HBase 1.3, then (b) running {{mvn to automatically generate a new Maven project for them (which is populated with fully-functioning sample code to run against HBase AND sample test code which runs via an instance of and then (c) opening the project in their IDE and running ""clean install"". At this point, the test phase of the Maven ""clean install"" process goes kerblooey, giving our HBase beginner the mysterious-to-them RuntimeException saying ""HMasterAddress already in use"", referring explicitly to the default master info port (16010). ""Oh no, what did I do wrong?"", our poor beginner asks. Hopefully, they will go to the new section of the Reference Guide on using Maven Archetypes, in which I will have put a note to tell them that they have to modify the code which instantiates their HBaseTestingUtility instance, so as to set the two ports in question to ""-1"".",non_debt,-
hbase,15835,comment_21,Closing this because it went into 2.0.0. Given the amount of follow up in HBASE-20224 and HBASE-20544 I think a branch-1 backport should be based on the aggregate.,non_debt,-
hbase,15859,summary,Fix failing tests where assertions fail due to not accounting for hbase:backup table,non_debt,-
hbase,15859,description,Currently some tests are failing in HBASE-7912 branch due to not accounting for the presence of hbase:backup table. Below is an example:,non_debt,-
hbase,15859,comment_0,"Draft patch. Once I get confirmation on the approach, I will upload new patch covering other similar cases.",non_debt,-
hbase,15859,comment_1,"Ted, why do not add hbase:backup to systemTables set in these tests?",non_debt,-
hbase,15859,comment_2,doesn't have systemTables.,non_debt,-
hbase,15859,comment_3,TestNamespaces has.,non_debt,-
hbase,15859,comment_4,The systemTables in TestNamespaces has been modified in later patches.,non_debt,-
hbase,15859,comment_5,Looks good to me.,non_debt,-
hbase,15859,comment_6,"Thanks for the review, Vlad.",non_debt,-
hbase,15892,summary,"submit-patch.py: Single command line to make patch, upload it to jira, and update review board",non_debt,-
hbase,15892,description,"Adds The script builds a new patch (using specified branch/tracking branch as base branch), uploads it to jira, and updates diff of the review on ReviewBoard. Remote links in the jira are used to figure out if a review request already exists. If no review request is present, then creates a new one and populates all required fields using jira summary, patch description, etc. *Authentication* Since attaching patches & changes links on JIRA and creating/changing review request on ReviewBoard requires a logged in user, the script will prompt you for username and password. To avoid the hassle every time, I'd suggest setting up ~/.apache-creds with the login details and encrypt it as explained in scripts help message footer. *Python dependencies* To install required python dependencies, execute {{pip install -r",non_debt,-
hbase,15892,comment_0,test comment,non_debt,-
hbase,15892,comment_3,"Created 2 RBs just to test that stuff doesn't start colliding when we backport patches. Things look good on my end, good to go for widespread testing.",non_debt,-
hbase,15892,comment_6,Pushed to master branch. Lets try it.  helped me w/ test. I needed to do this: Run this first : pip install gitpython && pip install rbtools Excellent work  Bit of doc please in refguide when you get a chance so we can easily point folks this route.,documentation_debt,outdated_documentation
hbase,15892,comment_7,"This sounds pretty good. I'll try it. My prev attempts at using the RB cli client was not that successful. BTW, there is a jira cli client as well in case we need more functionality.",non_debt,-
hbase,15892,comment_9,I think we should backport this to other branches too. It works with all branches.,non_debt,-
hbase,15892,comment_10,What you want me to apply ? The patch w/ branch-1 in it (hard to distingush what to do going by names alone -- I am too lazy to looksee),non_debt,-
hbase,15892,comment_11,"Deleted confusingly named patch. So for backporting now, apply these two patches: HBASE-15909",non_debt,-
hbase,15892,comment_12,This patch won't go into branch-1. There is no Want to make a new branch-1 patch?,non_debt,-
hbase,16034,summary,Fix,non_debt,-
hbase,16034,description,"implementation is wrong, and it ends up not setting the max value",non_debt,-
hbase,16034,comment_0,"Is your working dir clean? The fix of maxProcId looks good, but the patch you posted looks like it's modifying code that's not pushed yet",non_debt,-
hbase,16034,comment_1,"yeah, I have other patches before this one. added v1 which is the one applies to master and 1.x. thanks!",non_debt,-
hbase,16034,comment_2,1,non_debt,-
hbase,16149,summary,Log the underlying RPC exception in,non_debt,-
hbase,16149,description,"In We log the msg. But may not provide the underlying cause.. For example, in Let's add the underlying exception cause to the message as well.",non_debt,-
hbase,16149,comment_1,"Pushed to master branch. Doesn't apply cleanly to branch-1. If you put up a patch for branch-1, I'll commit it. Thanks  Good one.",non_debt,-
hbase,16149,comment_3,"Thanks,  Attached patch for branch-1.",non_debt,-
hbase,16149,comment_4,Looks like you applied this to branch-1 ? Correct me if I have it wrong. Thanks boss.,non_debt,-
hbase,16149,comment_5,"Yes,  It is good you asked. Because I didn't see the Hudson messages for branch-1 when you asked, I was curious. I missed the JIRA number in the commit message. Just corrected it.",non_debt,-
hbase,16157,summary,The incorrect block cache count and size are caused by removing duplicate block key in the LruBlockCache,non_debt,-
hbase,16157,comment_0,nit: name the variable found. You can use the following annotation for the two helper methods:,non_debt,-
hbase,16157,comment_2,copy that.,non_debt,-
hbase,16157,comment_3,The following test is passed in my local. 1) 2) test,non_debt,-
hbase,16157,comment_5,Failed tests don't seem to be related to the patch. +1 on v2.,non_debt,-
hbase,16157,comment_6,lgtm. Is the test stable enough? Looks like it might end up being a flaky test.,test_debt,flaky_test
hbase,16157,comment_7,"The may fail because the evict thread is too fast to get the ""in progress"" state. I'll update the patch asap.",non_debt,-
hbase,16157,comment_10,It seem to me the v4 patch is better. The reasons are shown below 1) The v4 patch makes return the correct number of evicted block. 2) The v4 patch use the threads to test the thread-safe of cache eviction.,non_debt,-
hbase,16157,comment_13,Ran the failed test locally which passed.,non_debt,-
hbase,16157,comment_14,resubmit,non_debt,-
hbase,16157,comment_15,"ChiaPing: If you re-attach the patch, QA would run the tests.",non_debt,-
hbase,16157,comment_16,Do I need to delete the older patch before re-attaching ? thanks,non_debt,-
hbase,16157,comment_17,re-attach,non_debt,-
hbase,16157,comment_19,Failed tests: TestHCM is tracked by HBASE-16171,non_debt,-
hbase,16157,comment_20,+1 on v4.,non_debt,-
hbase,16157,comment_22,is this committed? Why it is still open with no fixVersions set?,non_debt,-
hbase,16157,comment_23,"Thanks for the patch, ChiaPing. Thanks for the reviews.",non_debt,-
hbase,16182,summary,Increase timeout,non_debt,-
hbase,16182,description,"We have seen fail recently with a timeout. Further inspection, the root cause seems to be a very underpowered node running the test caused the timeout since there is no BLOCKED thread, both for handlers, readers, listener, or the client side threads.",non_debt,-
hbase,16182,comment_0,Simple patch.,non_debt,-
hbase,16182,comment_1,"mind a quick review, since you worked on this recently.",non_debt,-
hbase,16182,comment_3,I've pushed this. It is a trivial change.,non_debt,-
hbase,16204,summary,Makefile for running tests using make instead of BUCK,non_debt,-
hbase,16204,description,"This patch consists of addition of Makefile.tests.mk. Currently, make check runs tests using BUCK. This makefile will compile all the sources written in *-test.cc and and run them.",non_debt,-
hbase,16204,comment_0,"This patch consists of addition of Makefile.tests.mk. Currently, make check runs tests using BUCK. This makefile will compile all the sources written in *-test.cc and and run them.",non_debt,-
hbase,16204,comment_1,do we need this anymore?,non_debt,-
hbase,16204,comment_2,Not required. Please let me know how to delete it and I will do the needful. I couldn't find a way to remove it.,non_debt,-
hbase,16237,summary,Blocks for hbase:meta table are not cached in L1 cache,non_debt,-
hbase,16237,description,As discussed in this thread: blocks for hbase:meta table are not cached in L1 cache. The missing call for the method is the cause.,non_debt,-
hbase,16237,comment_0,Thanks to  who spotted this defect first.,non_debt,-
hbase,16237,comment_2,not present in master?,non_debt,-
hbase,16237,comment_3,The defect is not in master branch. See above discussion thread.,non_debt,-
hbase,16237,comment_4,"+1 (for future note, if you generate patches as documented in the ref guide, I would have preferred to just push this change while doing reviews this morning.)",non_debt,-
hbase,16237,comment_5,"Thanks for the review, Sean.",non_debt,-
hbase,16249,summary,Remove direct use of Hadoop Path/FileSysem 1/5,non_debt,-
hbase,16249,description,Remove or document exceptions for direct use of Hadoop FileSystem APIs in,non_debt,-
hbase,16249,comment_0,obviated by subtask of HBASE-14439,non_debt,-
hbase,16253,summary,Remove direct use of Hadoop Path/FileSysem 5/5,non_debt,-
hbase,16253,description,Remove or document exceptions for direct use of Hadoop FileSystem APIs in,non_debt,-
hbase,16253,comment_0,obviated by subtask of HBASE-14439,non_debt,-
hbase,16254,summary,hbase:backup table should have proper description in system table UI,non_debt,-
hbase,16254,description,In there is handling for system tables except for hbase:backup. We should add description for hbase:backup table as well.,non_debt,-
hbase,16254,comment_0,"patch v1. , can you take a look?",non_debt,-
hbase,16254,comment_2,"Thanks for the patch, Vlad.",non_debt,-
hbase,16271,summary,Fix logging and re-run the test in,non_debt,-
hbase,16271,description,"failed on us one time, but the reducers are throwing instead of the log statements. We should fix to get some more information. Also re-running the verify helps a lot in and ITBLL. We should do that here as well. It helps us with differentiating ephemeral issues (like scan visibility, etc) than real data loss issues.",non_debt,-
hbase,16271,comment_0,Simple patch. Will know more if the test fails again. But we can commit this for now.,non_debt,-
hbase,16271,comment_2,Go for it.,non_debt,-
hbase,16273,summary,Display ServerLoad sizes in appropriate units,non_debt,-
hbase,16273,description,Currently store file size is reported in MB and index & bloom filter in kb. There are two problems here. 1) They are calculated from bytes by dividing by 1024 / 1024 and then converted to an int. So for small regions it always reports 0. 2) And this conversion is done per store so even if there are multiple stores and all of them combined would be larger than 1 MB it'd be reported as 0 I'd suggest to count them in bytes and then when displaying them on the RegionServer UI page decide on an appropriate unit to use.,design_debt,non-optimal_design
hbase,16273,comment_0,Is HBASE-13581 related ?,non_debt,-
hbase,16273,comment_1,Thanks for catching this. Definitely looks to be the exact same one.  and I looked but have missed that issue. I'll close this one as a duplicate.,non_debt,-
hbase,16472,summary,when running CREATE from Phoenix,non_debt,-
hbase,16472,description,"When I created a table in HBase and then tried running create statement in phoenix. Phoenix returned me the following error. Error trying to modify table=t21sample t21sample The same error occurred in the case of altering an existing table. I some how managed to create a table in Phoenix by the same sequence if I disable the table after creating it. The phoenix returned me table disabled error. But after enabling the table in hbase, the table was created in phoenix, same worked for alter statement as well.",non_debt,-
hbase,16472,comment_0,"unless you set the configuration property to true, you have to follow the same sequence of operations to modify a table (disable, modify, enable).",non_debt,-
hbase,16784,summary,Make use of os) for the default,non_debt,-
hbase,16784,description,"Initially this I was thinking we need to add an interface to represent the fact that the key is contiguous. But since Extendedcell is added encapsulating all the internal interfaces and adds a write(OutputStream , boolean) and tries to exploit the fact that the Cell is in KV serialized format. Hence we can make use of it in code in case of No encoding case.",non_debt,-
hbase,16784,comment_0,Patch for QA.,non_debt,-
hbase,16784,comment_1,"All these can be replaced with calling Cell cell, final OutputStream out, final boolean withTags) and passing boolean. Now this KVUtil method is not returning the #bytes written. We can change that. The ExtendedCell API do return that. So this will avoid many length parsing.",non_debt,-
hbase,16784,comment_2,There are so many doing the same thing!!! This can we do? Because in hfilewriter case we write the tag len even if it is 0 if isIncludeTags is true.,non_debt,-
hbase,16784,comment_3,I need to see whether the tag len always has to be written even if it is 0. But that is another topic. May be we should.,non_debt,-
hbase,16784,comment_5,Updated patch.,non_debt,-
hbase,16784,comment_7,"The write(OS, boolean) method already return the #bytes writter.",non_debt,-
hbase,16784,comment_8,I see. Updated patch. Will commit unless objections.,non_debt,-
hbase,16784,comment_9,Sorry this is the right one,non_debt,-
hbase,16784,comment_11,V3 patch LGTM,non_debt,-
hbase,16784,comment_12,Pushed to master. Thanks for the review.,non_debt,-
hbase,16789,summary,Remove directory layout/ filesystem references from CompactionTool,non_debt,-
hbase,16789,description,Remove directory layout/ filesystem references from CompactionTool and use APIs provided by MasterStorage/ RegionStorage instead.,non_debt,-
hbase,16789,comment_0,"Removed directory layout reference from CompactionTool and moved it to The new tool named CompactionTool is added with changed interface. The new CT takes table, regions, column families as an input command line arguments. Both the legacy and new CT use APIs provided by MasterStorage/ RegionStorage classes. Map Reduce functionality of old CT is not yet implemented in the new CT as there are on-going discussions about it. Manually tested old CT and the new CT.",architecture_debt,violation_of_modularity
hbase,16789,comment_1,Could you post a summary of the current tradeoffs on having the MR implemented or not? Curious to see if anyone else has feedback.,non_debt,-
hbase,16789,comment_2,", here are a few points that are discussed: * This is an offline Compaction Tool (CT). Without MR option, CT will compact files for input table/ region/ column family on local node where CT is run. * Current CT, decides on node to run MR jobs based on location of first block of a first file in an input directory. * This can be improved to consider nodes based on last know region assignments with fallback on location of first block of first file in a table/ region/ column family. This will provide better locality. * Even with the improved logic, locality cannot be guaranteed. * So, whether to run with MR and MR job node selection can be determined by code outside of CT or a User. CT will be just responsible for compaction of files for input table/ region/ cf without deciding on MR or node selection for MR. * CT may query/ consider local regions and only compact files belonging to local regions. Workaround with -force option can be provided for the default behavior.",design_debt,non-optimal_design
hbase,16808,summary,"Included the generated, shaded java files from ""HBASE-15638 Shade protobuf"" in our src assembly",non_debt,-
hbase,16808,description,"found that I forgot to include the generated, shaded files in our src tgz. Let me fix. This is a follow-on to HBASE-16793",non_debt,-
hbase,16808,comment_0,I tested by building source tgz ensuring I could then build src tgz from what was in first tgz and then starting an hbase instance.... seems to work.,non_debt,-
hbase,16808,comment_2,Pushed to master. Small patch that is easy to prove (see above note that assembly now works and that I can build from the unpacked source tgz).,non_debt,-
hbase,16817,summary,Write length header inside,non_debt,-
hbase,16817,description,"Current we write length header before it is more efficient to write length header inside it, so we only to calculate the length only once.",design_debt,non-optimal_design
hbase,16817,comment_0,", what do you think?",non_debt,-
hbase,16817,comment_1,It might be odd to pass this boolean.. Even withTags we plan to remove. Intentionally it was removed that the oswrite within Cell impl write the length of the cell. Whether the length to be written or no is up to the caller. In case of HFIle write we dont need to write the length. In case of KVCodec we need it. Some other codec may come tomorrow which want to write the length not as an int but as a varint. So IMO it is better to leave it and not club the length write part within write method in Cell. Got ur point of one extra calc but it is ok.,design_debt,non-optimal_design
hbase,16817,comment_2,"Ok, close this.",non_debt,-
hbase,16856,summary,Exception message in SyncRunner.run() should print currentSequence,design_debt,non-optimal_design
hbase,16856,description,"A very small bug, a typo in exception message: It should print currentSequence and syncFutureSequence, but print two syncFutureSequence",documentation_debt,low_quality_documentation
hbase,16856,comment_1,"Thanks for the patch, Allan.",non_debt,-
hbase,16872,summary,Implement mutateRow and checkAndMutate,non_debt,-
hbase,16872,description,None,non_debt,-
hbase,16872,comment_0,"Ah there is a typo in the comment. ""We need to the MultiRequest"" = Will fix on commit.",documentation_debt,low_quality_documentation
hbase,16872,comment_2,Please fix below javadoc warnings as well as the comment mentioned above: Other parts lgtm. +1,documentation_debt,low_quality_documentation
hbase,16872,comment_3,Fix javadoc issues.,documentation_debt,low_quality_documentation
hbase,16872,comment_5,Pushed to master. Thanks  for reviewing.,non_debt,-
hbase,16934,summary,Revert ITBLL misconfiguration check introduced in HBASE-16562,non_debt,-
hbase,16934,description,"The misconfiguration check in HBASE-16562 does insufficient input validation / handling, resulting in a NPE during normal operations.",non_debt,-
hbase,16934,comment_4,Bulk closing issues after 1.1.8 release.,non_debt,-
hbase,16998,summary,[Master] Analyze table use reports and update quota violations,non_debt,-
hbase,16998,description,"Given the collected table usage reports from RegionServers, the Master needs to inspect all filesystem-use quotas and determine which need to move into violation and which need to move out of violation.",non_debt,-
hbase,16998,comment_0,".001 The ""hard"" stuff. This patch adds another Chore to the master which numerates the region space reports the Master received from RegionServers and decides if a Table needs to have some violation policy enacted or disabled. The actual enacting/disabling of that policy is not included.",non_debt,-
hbase,16998,comment_2,.002 is .001 rebased on top of HBASE-16961.,non_debt,-
hbase,16998,comment_4,"Next review for the space quota work. , I would be all the happier if you could find some more time to take a look :)",non_debt,-
hbase,16998,comment_5,.003 Fixes a bug in QuotaObserverChore which would prevent it from triggering quota updates to RS's.,non_debt,-
hbase,16998,comment_6,".004 latest from rb. Missing InterfaceAudience annotations, constructor cleanup in QuotaObserverChore.",code_debt,low_quality_code
hbase,16998,comment_8,The failure of appears to be unrelated.,non_debt,-
hbase,16998,comment_9,.005 latest from rb,non_debt,-
hbase,16998,comment_11,1,non_debt,-
hbase,16998,comment_12,"Thanks for the review, Ted!",non_debt,-
hbase,17025,summary,[Shell] Support space quota get/set via the shell,non_debt,-
hbase,17025,description,Need to make sure that admins can use the shell to get/set the new space quotas.,non_debt,-
hbase,17025,comment_0,".001 Parking a patch for the shell additions. {{set_quota}} has some new additions, but {{list_quotas}} should be reusable as-is.",non_debt,-
hbase,17025,comment_2,.002 Fixes some bugs,non_debt,-
hbase,17025,comment_4,.003 set_quota and list_quota shell commands expanded.,non_debt,-
hbase,17025,comment_6,"Thoughts on this one, ?",non_debt,-
hbase,17025,comment_7,Is it possible to add some .rb test ? Thanks,test_debt,lack_of_tests
hbase,17025,comment_8,"Oh yeah, for sure. I never noticed that we actually had some examples :)",non_debt,-
hbase,17025,comment_9,.004 Adds some ruby tests.,non_debt,-
hbase,17025,comment_11,1,non_debt,-
hbase,17025,comment_12,"Thanks for the review, Ted.",non_debt,-
hbase,17101,summary,FavoredNodes should not apply to system tables,non_debt,-
hbase,17101,description,"As described in the doc (see HBASE-15531), we would like to start with user tables for favored nodes. This task ensures FN does not apply to system tables. System tables are in memory and won't benefit from favored nodes. Since we also maintain FN information for user regions in meta, it helps to keep implementation simpler by ignoring system tables for the first iterations.",design_debt,non-optimal_design
hbase,17101,comment_0,"including a rough patch, will cleanup and upload.",code_debt,low_quality_code
hbase,17101,comment_3,1,non_debt,-
hbase,17101,comment_4,Updated patch with review comments from reviewboard addressed.,non_debt,-
hbase,17101,comment_7,The unit test failure is unrelated to the patch.,non_debt,-
hbase,17101,comment_8,1,non_debt,-
hbase,17101,comment_9,Committed to master. Thanks Thiruvel.,non_debt,-
hbase,17180,summary,Let HBase thrift2 support,non_debt,-
hbase,17180,description,Add for HBase Thrift2,non_debt,-
hbase,17180,comment_0,Same with HBASE-17181.,non_debt,-
hbase,17180,comment_1,"sorry, first create issue",non_debt,-
hbase,17184,summary,Code cleanup of LruBlockCache,code_debt,low_quality_code
hbase,17184,description,"Initially I only wanted to fix a badly worded Log message but found a bunch of code convention violations etc. so I decided to clean up the class. It is only whitespace changes with the exception of the one log message (""too many"").",code_debt,low_quality_code
hbase,17184,comment_1,No tests because no actual code was changed. Only a single log message.,non_debt,-
hbase,17184,comment_2,Pushed to master branch. Thanks  for the cleanup,non_debt,-
hbase,17184,comment_3,Thank you Looks like the patch number is missing from the commit. I remember that I made a mistake here in the past. Was this my fault again?,non_debt,-
hbase,17184,comment_4,np  I fixed it. I did git am because it looked like the patch was complete. I should have noticed the missing JIRA number. Fixed now. Thanks for contrib boss.,non_debt,-
hbase,17192,summary,remove use of scala-tools.org from pom,non_debt,-
hbase,17192,description,our pom makes use of scala-tools.org for a repository. That domain currently issues redirects for all URLs; for maven coordinates those redirects lead to 'not found' and the 'permantenly moved' HTML gets saved. this corrupts the local maven repository in a way that cause the mvn:site goal to give an opaque error: Rerunning in debug mode with {{mvn -X}} gives no additional useful information. All artifacts from scala-tools.org are now found in maven central.,non_debt,-
hbase,17192,comment_0,"For the curious on how I found this error: # Searching for the odd string led to [a stack overflow question that suggested clearing maven repository would it didn't in this case. It also led to another [stackoverlow question that suggested a corrupt pom in the maven repository would give the same # After clearing my local repository so that only things downloaded in the {{mvn install && mvn site}} would be present, I looked at first lines that weren't xml declarations and one line stood out. # Grepping for that moved string showed the impacted pom # That's a core maven plugin, so it's an odd failure. Maven tracks what repository things came from: # That's not the right place to get the assembly plugin. Checking the domain confirms: # From what I can tell through old blog posts, scala-tools.org transitioned everything to sonatype in 2012. The domain was expected to stop working in that year, but I guess it must have just happened now. testing fix locally now.",non_debt,-
hbase,17192,comment_1,#NAME?,non_debt,-
hbase,17192,comment_2,-01.test.patch - includes a trivial change to a scala file (as done in HBASE-15644) to get the scaladoc bit to run.,non_debt,-
hbase,17192,comment_3,+1. Nice work.,non_debt,-
hbase,17192,comment_5,thanks for the quick review! using this fix requires deleting any impacted poms. In the simplest case that means just wiping out ~/.m2/repository. I've set the flag on the website job to do just that. we can unset it after a successful run.,non_debt,-
hbase,17192,comment_6,+1. Thanks for debug steps (ugly).,code_debt,low_quality_code
hbase,17192,comment_8,+1 Yuck,non_debt,-
hbase,17259,summary,Missing functionality to remove space quota,requirement_debt,requirement_partially_implemented
hbase,17259,description,"I'm noticing that while I have create and update APIs for quotas, I missed the remove functionality. Need to add public API for that and some tests.",requirement_debt,requirement_partially_implemented
hbase,17259,comment_0,.001 parking a patch here for the additions. Need to get some more changes onto the feature branch before triggering QA.,non_debt,-
hbase,17259,comment_3,.003 rebased patch on the feature branch. FYI .,non_debt,-
hbase,17259,comment_4,"lgtm, pending tests.",non_debt,-
hbase,17259,comment_6,"Thanks for the review, Ted. Pushed to the branch HBASE-16961",non_debt,-
hbase,17325,summary,Add batch delete capability to ImportTsv,non_debt,-
hbase,17325,description,"Considering to batch delete data in table which we load from external files,this feature add a switch key to enable batch delete. First,using the file we load to table and the bulkoutput function of ImportTsv to generate hfiles in hdfs which contain the keyvalue of 'DeleteFamily' marker .Then the tool of is used to load hfiles into table intending to cover the data of whole family we need to delete.",non_debt,-
hbase,17325,comment_0,Do you use this in your cluster ? Can you share your experience ? Thanks,non_debt,-
hbase,17325,comment_1,"yeah,it is used to rollback the data which we batch load from bulkload tool if we found the data is not we want.Now we only consider to delete the whole family,but it also can batch delete by the column and version",non_debt,-
hbase,17325,comment_2,"DeleteFamily can be injected through shell command, e.g. Can you make the deletion facility more general ?",non_debt,-
hbase,17325,comment_3,"Do you mean delete or deleteall? If this way ,it may be slow and complicate to inject deletefamily when using delete one by one. We provide an option in importTsv to choose inject deletefamily or not . It likes we bulkload the data again,so it's fast and easy to do .",design_debt,non-optimal_design
hbase,17338,summary,Treat Cell data size under global memstore heap size only when that Cell can not be copied to MSLAB,non_debt,-
hbase,17338,description,"We have only data size and heap overhead being tracked globally. Off heap memstore works with off heap backed MSLAB pool. But a cell, when added to memstore, not always getting copied to MSLAB. Append/Increment ops doing an upsert, dont use MSLAB. Also based on the Cell size, we sometimes avoid MSLAB copy. But now we track these cell data size also under the global memstore data size which indicated off heap size in case of off heap memstore. For global checks for flushes (against lower/upper watermark levels), we check this size against max off heap memstore size. We do check heap overhead against global heap memstore size (Defaults to 40% of xmx) But for such cells the data size also should be accounted under the heap overhead.",design_debt,non-optimal_design
hbase,17338,comment_1,"This is a new test added recently. Flakey? The test log says cluster not able to get started properly. I believe not related to this patch any way. Ping for reviews ,",non_debt,-
hbase,17338,comment_2,Do we have to check if on or offheap MSLAB? 86 /** 87 * @return Whether off heap based MSLAB in place. 88 */ 89 boolean isOffheap(); Can we not have MSLAB work same whether on or offheap? This sort of check... 101 // issues or even OOME. 102 if (this.memStoreLAB != null && { 103 heapOverheadDelta += cellLen; 104 } ... presumes that MSLAB is done in either of two ways. This check is done apart from the implementation. Is there copy/paste of code (going by your dup'ing the comment?). I need to read on why Append/Increment can't be out in offheap. This is good stuff though,non_debt,-
hbase,17338,comment_3,"In case of on heap MSLAB, all the size calc doing global memstore data size + heap overhead based. That is why was not adding the cell data size in case of on heap. But I think I can do it bit diff way.. So instead of tracking the data size and heap overhead, we will track cell data size and heap size. Pls see it is not heap overhead alone.. This is the total heap space occupied by memstore(s). I can change the calc accordingly. For on heap MSLAB , this heap space count will be the same as what we have before off heap write path work (ie. It will include all cell data size and overhead part). The new accounting ie. cellDataSize will include only the cell data bytes size part. This will be a subset of the former one then. Will do all necessary changes.. This will be a bigger patch then as I will rename all the related area from heapOverhead to heapSize or so. I believe that way will look cleaner. Thoughts? Append/Increment is not adding cells into MSLAB area.. This is to avoid MSLAB wastage. Say same cell is getting incremented 1000 times and cell key+ value size is 100 bytes. If every increment (add to memstore) cell was added to MSLAB, we will overall take ~100KB MSLAB space whereas only 100 bytes is valid at any point.. All the former cells are getting deleted by the addition of a new cell.. The Cell POJO as such is removed from CSLM. But we can not free up that space in MSLAB.. MSLAB is not designed to do this way. That the chunk allocation and offset allocation within a chunk is serially incremented way happening. We can not mark some in btw space as free and reuse.. That will make things very complex for us.. So to avoid these, the simplest way was to NOT use MSLAB for upsert.",design_debt,non-optimal_design
hbase,17338,comment_4,"New patch with changed approach. Now in MemstoreSize itself, we track cell data size and heapSize. Not like heap overhead. We were doing this special overhead tracking and on heap MSLAB cases had to add both of these for checks etc. Now we have heapSize accounting itself (here for on heap MSLAB cells and cells not in MSLAB area cell data size also included in heapSize accounting)",non_debt,-
hbase,17338,comment_8,+1 Add more comment on commit to MemstoreSize about diff between heap and data size.... (can be a version of comment that is later in class...) Glad of the simplification. Good stuff @anoop sam john,code_debt,complex_code
hbase,17338,comment_9,Rest is all good. But as discussed internally that isOffheap() addition to MSLAB is better so that we can really avoid adding the dataSize to the MemstoreSize when the memstore is offheap but still the entire Cell is onheap. Now currently we just account for the dataSize also. We can do in another JIRA. +1 otherwise.,design_debt,non-optimal_design
hbase,17338,comment_10,"Thanks for the reviews Stack & Ram.. I have to fix the failed test as those were having assert on heapOverhead size after cell addition to Memstore. Now we track heapSize not overhead. Will add more comments as Stack asked Regarding Ram's comment. I have different opinion. I dont think we need change this way. We track dataSize (irrespective of cell data in on heap or off heap area).. This dataSize been used at Segment level for in memory flush decision, Region level for on disk flushes and globally to force flush some regions. At the 1st 2 levels, it is not doubt that we have to track all the cell data size together. Now the point Ram says is when we have off heap configured and max off heap global size is say 12 GB, once the data size globally reaches this level, we will force flush some regions. So his point is for this tracking, we have to consider only off heap Cells and on heap Cell's data size should not get accounted in the data size but only in the heapSize. (At global level. But at region and segment level it has to get applied). 2 reasons why I am not in favor of this 1. This makes the impl so complex. We need to add isOffheap check down the layers. Also at 2 layers we have to consider these on heap cell data size and one level not. 2. When off heap is enabled, (We have the MSLAB pool off heap in place), we will end up in having on heap Cells when the pool is out of BBs. We will create on demand LABs on heap. If we dont consider those cell's data size at global level, we may reach forced flush level a bit late. That is the only gain. But here the on demand LAB creation is a sign that the write load is so high and delaying the flush will add more pressure to the MSLAB and more and more on demand BBs (2 MB sized) need to get created. One aim of the off heap work is to reduce the max heap space need for the servers. So lets consider the cell data size globally also (how we do now) and make global flushes. Now even if MSLAB is used, the append/increment use cases wont use MSLAB. The cells will be on heap then. But for such a use case, enabling MSLAB (and pool) is totally waste. That is a mis configuration. More and more on demand BB creation, when MSLAB pool is a bad sign. We have a WARN log for just one time as of now.. May be we should repeat this log at certain intervals. (Like for every 100th pool miss or so.) We should be able to turn MSLAB usage ON/OFF per table also. Now this is possible? Am not sure. These 2 things need to be checked and done in another jiras IMO.",design_debt,non-optimal_design
hbase,17338,comment_11,Fixing comments and test case failures,non_debt,-
hbase,17338,comment_13,Trying to follow-along.... Dumb question. dataSize is KV infrastructure + key content + value + trailing tags and sequenceid if any? i.e. the whole KV? And CellSize is infrastructure only or rather key+infrastructure? I'm listening (smile). You can probably guess what I think on the above. .... There is one global threshold whether data is onheap or offheap (I probably got this wrong?) We could probably but the direct memory would remain allocated until we restart. Thanks. Lets figure it and update your,non_debt,-
hbase,17338,comment_14,"Not sure whether used dataSize/cellSize here and there. Its data size only which is the way as u said above. There is no cellSize as such. There is only cell heap size which is the heap size (total) occupied by the cell impl object Ya there is one global threshold in both cases. Moreover in case of off heap, there is a heap global threshold also. ie. By def 40% of xmx above which we will force flush. In case of offheap, we have to check this extra thing also or else there is possibility of global memstore size getting oversized and GC impacts/OOME. Sorry for not clear here. I dont mean ON/OFF MSLAB over the RS run time. Now we have a way to turn it ON at cluster level means at RS level. All the regions in this RS will use this MSLAB then. (Ya we know some cells wont get copied to MSLAB - increment/append) My Q was whether there is a way for turning off MSLAB usage for a specific table. Say in RS MSLAB is ON. But the regions from this particular table wont use MSLAB at all.",non_debt,-
hbase,17338,comment_15,"Thanks. So, datasize vs heapsize, have we written these up? datasize is serialized size... whether serialized on rpc or in blockcache and heapsize is how large a 'live' Cell in java heap is? If the Cell data is offheap, it is the the onheap Cell proxy object? How does the global threshold work in the case where we are doing offheap accounting too. The global check will look at the onheap limit and the offheap limit and if we hit the offheap limit before we hit the onheap limit, we'll flush? Should be easy enough to do? Thanks",non_debt,-
hbase,17338,comment_16,"Yes In case of off heap MSLAB in place, the global memstore size is specified using a new config as u know. So we will have this check against the aggregated dataSize . Also we will be having a max possible on heap memstore global size (Defualt 40% of xmx) This check also done. Any one of the condition met, we will do some forced flush. (What we do now also) with blocking writes. Should be IMO",non_debt,-
hbase,17338,comment_17,Thanks boss. You think the doc is up-to-date w/ this state of things? Thanks.,non_debt,-
hbase,17338,comment_18,Yes Stack. I have done edit to the doc so as to change the solution detailing part as per the discussion in this jira. Thanks for the remind. Are you ok with this patch as such. I may have to rebase it. Let me check. Tks,documentation_debt,low_quality_documentation
hbase,17338,comment_19,+1 on commit. Lets get up on this new basis. Nice cleanup  (Thanks too for doc edit).,code_debt,low_quality_code
hbase,17338,comment_20,Rebased patch. Will commit one QA is fine. Thanks for the reviews Stack & Ram.,non_debt,-
hbase,17338,comment_22,Pushed to master. Thanks for the reviews,non_debt,-
hbase,17383,summary,Improve log msg when offheap memstore breaches higher water mark,code_debt,low_quality_code
hbase,17383,description,Currently we get this log Here the global offheap memstore size is greater than the blocking size. The memstore heap overhead need not be included in this log unless the higher water mark breach is only due to the heap overhead.,code_debt,low_quality_code
hbase,17383,comment_0,Noticed this while doing patch for HBASE-17338.. Latest patch there having this fix already. Pls have a look.. I did not check the test failures.. Will do that soon.,non_debt,-
hbase,17383,comment_1,We need this still  and  ?,non_debt,-
hbase,17383,comment_2,Unscheduling. Looks like we don't need this anymore? Resolve  /  ?,non_debt,-
hbase,17383,comment_3,No longer this case as some other cleanup corrected the log. Just closing as can not reproduce,code_debt,low_quality_code
hbase,17394,summary,Correct logic,non_debt,-
hbase,17394,description,"It is supposed to be [minimumTimestamp, maximumTimestamp), the following logic suggests [minimumTimestamp, maximumTimestamp] needs to be modified to set(l, l + 1)",non_debt,-
hbase,17394,comment_0,"Forgot to mention that the logic there needs to be corrected accordingly, given that it is [minimumTimestamp, maximumTimestamp)",non_debt,-
hbase,17394,comment_1,"Update, the current logic is correct, for TimeRangeTracker, it is [minimumTimestamp, maximumTimestamp], when it converts to TimeRange, the TimeRange is also [minStamp, maxStamp]. I am going to update comments in the code.",documentation_debt,outdated_documentation
hbase,17394,comment_2,"The logic is correct, the definition is [min, max], I added comments through HBASE-12148 to make it clear and hopefully, it will help code reading a bit easier.",code_debt,low_quality_code
hbase,17461,summary,"HBase shell ""major_compact"" command should properly convert parameter to java byte array properly before simply calling method",non_debt,-
hbase,17461,description,"On HBase shell, *major_compact* command simply passes the received parameter straight to java method. On some corner cases, HBase tables row keys may have special characters. Then, if a region is split in such a way that row keys with special characters are now part of the region name, calling *major_compact* on this regions will fail, if the special character ASCII code is higher than 127. This happens because Java byte type is signed, while ruby byte type isn't, causing the region name to be converted to a wrong string at Java side. For example, considering a region named as below: Calling major_compat on it fails as follows: An easy solution is to convert parameter properly, prior to calling in the same way as it's already done on some other shell commands, such as *get*:",non_debt,-
hbase,17461,comment_0,Have you tried by calling the same command with simple quotes instead of double quotes? Does it make a difference?,non_debt,-
hbase,17461,comment_1,"It doesn't, still shows same error message because of the same behaviour. Since it finds no region with the changed name, it then tries to run major_compact on a table. The error printed in the shell relates to the table naming validation checks, but it isn't the real issue here. Putting additional logging message on a test version of HBaseAdmin, I could see the java equivalent string gets converted to a different one: Noticed that the real region name: Was being logged with the value below, at the java side: Then, realized the only characters being changed are the ones which ASC code higher than 127, which in java should be negative numbers. Tried add the mentioned change on *major_compact.rb* file, and it has fixed it.",non_debt,-
hbase,17461,comment_2,There is a similar JIRA HBASE-8865.,non_debt,-
hbase,17461,comment_3,"HBASE-8865 issue seems the same problem, however related to split command. Maybe applying the same conversion prior to HBaseAdmin.split would also fix it.",non_debt,-
hbase,17461,comment_4,"Just quickly tested that adding same conversions as follows, also fix the split command: Here some tests: 1) Before applying the change on *split.rb*, had put a row with same value described on HBASE-8865, then call split on that row key. The region name is created with different value: After adding the change to *split.rb*:",non_debt,-
hbase,17461,comment_5,These issues are actually fixed already by changes applied on HBASE-14767. It turned out I was not testing latest Master branch. Apologies for the noise.,non_debt,-
hbase,17480,summary,Remove split region code from Region Server,non_debt,-
hbase,17480,description,"HBASE-14551 moves the split region logic to the master-side. With the code in HBASE-14551, the split transaction code from region server side became un-used. There is no need to keep region_server-side split region code. We should remove them to avoid code duplication.",code_debt,dead_code
hbase,17480,comment_1,V2 patch fixed the UT failure.,non_debt,-
hbase,17480,comment_2,+1 That is a nice lump of code removed. You remove Do we have an explicit test of the splitting path any more post this removal?,code_debt,dead_code
hbase,17480,comment_4,", we still have UTs in and TestAdmin1 to test Split functionality overall. The tests split in component level.",non_debt,-
hbase,17500,summary,Implement methods,non_debt,-
hbase,17500,description,None,non_debt,-
hbase,17500,comment_0,Add a initial version. table will use Procedure v2. And the ut was copied from TestAdmin1.,non_debt,-
hbase,17500,comment_2,Fix the javadoc warning.,documentation_debt,low_quality_documentation
hbase,17500,comment_5,Pushed to master and thanks  for review.,non_debt,-
hbase,17640,summary,Unittest error in TestMobCompactor with different timezone,non_debt,-
hbase,17640,description,Jingcheng reported an unittest error which he debugged and found that it is a timezone issue. I am attaching the patch here.,non_debt,-
hbase,17640,comment_0,"Hi , can you help to review the patch? Thanks.",non_debt,-
hbase,17640,comment_2,"+1, thank you for the patch .",non_debt,-
hbase,17640,comment_3,Thanks . The test failures are unrelated as the change is in TestMobComparator.,non_debt,-
hbase,17640,comment_4,Pushed to master branch. Thanks for the patch !,non_debt,-
hbase,17693,summary,UT fails with Timeout error,non_debt,-
hbase,17693,description,testFailover is failing with Timeout Error.,non_debt,-
hbase,17693,comment_0,"Another related issue. I was able to work around this tweaking the timeout, but this isn't a scalable solution for one-offs..",non_debt,-
hbase,17709,summary,New MoveCostFunction that respects MaxMovePercent,non_debt,-
hbase,17709,description,"The balancer does not fully respect the maxMovePercent configuration. Specifically, if the number of regions being moved is less than 600, the balancer currently allows that number of region moves regardless of what value is set for maxMovePercent. This patch fixes that behavior and simplifies the moveCost function as well. In addition, this patch adds short-circuiting logic to the balancer to terminate early once the maximum number of moves are reached (and assuming the new plan has enough of a cost improvement).",design_debt,non-optimal_design
hbase,17709,comment_1,"Not sure why this build failed, but I resubmitted a patch and made sure it was properly based off master. I can build locally and all tests pass locally",non_debt,-
hbase,17726,summary,[C++] Move implementation from header to cc for request retry,architecture_debt,violation_of_modularity
hbase,17726,description,This is a follow up work related to HBASE-17465.,non_debt,-
hbase,17726,comment_0,Straightforward patch.  let me know if you have concerns with the patch.,non_debt,-
hbase,17726,comment_1,Pushed this to the branch. Follow up patches depends on this.,non_debt,-
hbase,17785,summary,fails to assign new table regions when cloning snapshot,non_debt,-
hbase,17785,description,"A novice starting out with will want to enable it and, before assigning tables to groups, may want to create some test tables. Currently that does not work when creating a table by cloning a snapshot, in a surprising way. All regions of the table fail to open yet it is moved into ENABLED state. The client hangs indefinitely.",non_debt,-
hbase,17785,comment_0,"Yet if you restart the master, somehow all regions of the table are assigned out.",non_debt,-
hbase,17785,comment_1,You get the bit that there is a dynamic 'default' group that all belongs to by default (at least that is how it is supposed to work). I'm interested,non_debt,-
hbase,17785,comment_2,"Thanks, yeah, after more poking around looks like a bug specific to cloning snapshots, so I changed the issue description. Creating a new table works but cloning a snapshot does not.",non_debt,-
hbase,17785,comment_3,"Patch with a new test. The equivalent for a branch-1 backport would be different. I'm carrying that on an internal dev branch. Not sure where to put it. FWIW, for branch-1 implementing coprocessor interfaces is done a bit differently, because we are not using default methods there. New unit test case is the same.",non_debt,-
hbase,17785,comment_5,WDYT?,non_debt,-
hbase,17785,comment_7,So all we do is also assign the table for a snapshot. +1,non_debt,-
hbase,17808,summary,FastPath for RWQueueRpcExecutor,non_debt,-
hbase,17808,description,"FastPath for the FIFO rpcscheduler was introduced in HBASE-16023. But it is not implemented for RW queues. In this issue, I use in RW queues. So anyone who want to isolate their read/write requests can also benefit from the fastpath. I haven't test the performance yet. But since I haven't change any of the core implemention of it should have the same performance in HBASE-16023.",requirement_debt,requirement_partially_implemented
hbase,17808,comment_1,"I have done a little performance test. I used YSCB's workloadc with 1 client 100 threads against one regionserver. But surprisedly I noticed performance regression with 'fastpath', no matter with I implemented or the original Is it something wrong with fastpath or am I doing something wrong? Ping , since Stack is the original author of fastpath. YSCB workloadc with 100 threads |32982| issue)|32287| fastpath)|34563|",code_debt,slow_algorithm
hbase,17808,comment_3,Why not extends RWQueueRpcExecutor?,non_debt,-
hbase,17808,comment_4,"Good question, is backed by it is also not good as RWQueueRpcExecutor in my tests. Still trying to figure out what's wrong.",non_debt,-
hbase,17808,comment_6,", still pursuing this or can we close this ticket?",non_debt,-
hbase,17808,comment_7,"No, currently I'm not. Maybe we should just close this one.",non_debt,-
hbase,17883,summary,release 1.4.0,non_debt,-
hbase,17883,description,Let's start working through doing the needful; it's been almost 3 months sine 1.3.0.,non_debt,-
hbase,17883,comment_0,"Here's the [current list of non-resolved/closed issues for No blockers, so I'll start booting things out after I make a 1.4.1 version.",non_debt,-
hbase,17883,comment_2,"kicked a bunch of stuff out of 1.4.0. there are still 11 unresolved jiras. 9 of them are ""patch available"" jiras that weren't obviously unready.",non_debt,-
hbase,17883,comment_3,Making 1.4.0 RC0 today. Let me take this.,non_debt,-
hbase,17883,comment_4,"HBASE-19194 blocks, otherwise good to go",non_debt,-
hbase,17883,comment_5,Closer. Almost to the point where a build with release audits passes. Not quite. On HBASE-19232 right now,non_debt,-
hbase,17883,comment_6,Now stuck on HBASE-19239. Many of the problems are in unit tests so need fixing in order for tests to be correct and actually cover functionality as intended.,test_debt,low_coverage
hbase,17883,comment_7,HBASE-18233 is the big rock now. Will apply if tests look good. Hopefully today.,non_debt,-
hbase,17883,comment_8,"A couple of straightforward issues remaining, found by tests: HBASE-19395, HBASE-19406",non_debt,-
hbase,17883,comment_9,I propose to eject hbase-native-client to GitHub on HBASE-19419,non_debt,-
hbase,17883,comment_10,"Does anyone view compilation problems against Hadoop 3.0.0 in branch-1 as a release blocker? See HBASE-19421 . In my view, we don't support Hadoop 3 with branch-1 in any formal capacity, so working builds against it will be nice to have but would not be a release blocker.",non_debt,-
hbase,17883,comment_11,Blocked on HBASE-19429,non_debt,-
hbase,17883,comment_12,"I resolved HBASE-19429 by disabling checkstyle reporting in the site build, so, a workaround, not a fix. There's something off about Maven's behavior when executing the site target in general, but we do get a good binary build as a result.",design_debt,non-optimal_design
hbase,17883,comment_13,I have tagged 1.4.0RC0 as 7123ddfadc. Feel free to continue to commit to branch-1 and branch-1.4 while I finish up with the RC.,non_debt,-
hbase,17891,summary,How to modified the result in the postScannerNext()?,non_debt,-
hbase,17891,description,"As it stayed in the API document, the the result in the postScannerNext() to return to the client, can be modified. How can I do this in the java code? Thanks a lot. Best regards, land Li",non_debt,-
hbase,17891,comment_0,Please send an email to to ask the question. Please remember to register to the mailing list first. Thanks.,non_debt,-
hbase,17891,comment_1,this link resolved this issue.,non_debt,-
hbase,17918,summary,document serial replication,non_debt,-
hbase,17918,description,"It looks like HBASE-9465 addresses one of the major flaws in our existing replication (namely that order of delivery is not assured). All I see in the reference guide is a note on Instead we should cover this in the replication section, especially given that we call out the order of delivery limitation.",documentation_debt,low_quality_documentation
hbase,17918,comment_0,"Let me do this, thanks.",non_debt,-
hbase,17918,comment_1,still working on this ?,non_debt,-
hbase,17918,comment_2,will start working on this. We have already used this feature in production. Thanks.,non_debt,-
hbase,17918,comment_5,Pushed to master and branch-2(maybe not necessary). Thanks  for contributing.,non_debt,-
hbase,17918,comment_9,Will reapply the patch after HBASE-20046 is done.,non_debt,-
hbase,17918,comment_10,"This patch is in branch-2, branch-2.0, and master. Should we close it? (I'll do an edit of the refguide once I copy it from master to branch-2.0 and will remove this part of the doc from branch-2.0 version). Thanks.",non_debt,-
hbase,17918,comment_11,I've already reverted the patch since we have reverted serial replication feature from branch-2 and branch-2.0. Let me change the fix version.,non_debt,-
hbase,17918,comment_12,Serial replication reverted from branch-2/2.0.0 in HBASE-20048; to be redone later in HBASE-20046.,non_debt,-
hbase,17918,comment_14,Pushed to master and branch-2.,non_debt,-
hbase,17939,summary,Backport HBASE-17914 to branch-1,non_debt,-
hbase,17939,description,None,non_debt,-
hbase,17939,comment_0,HBASE-17914 is an incompatible change as we change public methods of StoreFile class which is declared as IA.LimitedPrivate.,non_debt,-
hbase,18085,summary,Prevent parallel purge in ObjectPool,non_debt,-
hbase,18085,description,"Parallel purge in ObjectPool is meaningless and will cause contention issue since has synchronization (source code shown below) We observed threads blocking on the purge method while using offheap bucket cache, and we could easily reproduce this by testing the 100% cache hit case in bucket cache with enough reading threads. We propose to add a purgeLock and use tryLock to avoid parallel purge.",code_debt,multi-thread_correctness
hbase,18085,comment_0,Attaching the contention we reproduced in our testing. It's late now on my side so will upload the patch later,non_debt,-
hbase,18085,comment_1,"Good find and ur point make sense. We should just come out of purge call when already a purge in progress. As per impl, we should need a boolean state indicating the status of run and which needs to be volatile (?) Then again we have a perf cost here. May be we should take a new approach all together? Than calling purge on every call to getLock()?",non_debt,-
hbase,18085,comment_2,"I thought about using a) volatile boolean, b) AtomicBoolean and c) and finally chose option c. Quoting from about volatile boolean v.s. AtomicBoolean: And here each thread might both read and update the field, so AtomicBoolean wins. And comparing AtomicBoolean and [this suggests the latter one. Let me know your point if different opinion sir, thanks.  Yeah, I also thought about control purge frequency, but a little bit lost in the counting standard. Based on counter or timing? And what should the threshold be? Also not sure whether any regression after such changes, so I chose to limit the change to control the risk.",non_debt,-
hbase,18085,comment_3,From : Should the timed version of tryLock be used ?,non_debt,-
hbase,18085,comment_4,"I guess this is discussing the case that there're both timed and untimed tryLock, and some threads waiting with timed tryLock, but the lock might be grabbed by the untimed tryLock when lock released? Here the purpose is to prevent parallel purge and no fairness required (it doesn't matter which threads performs the purge), and there's no timed tryLock ongoing, agree?",non_debt,-
hbase,18085,comment_5,Submit current patch for HadoopQA to check,non_debt,-
hbase,18085,comment_6,Any chance for a micro benchmark tests Yu? Appreciate ur effort to consider all possible options :-),test_debt,lack_of_tests
hbase,18085,comment_7,Using untimed tryLock is fine.,non_debt,-
hbase,18085,comment_9,"Reading the code in I can see it uses a state variable up in the layer in which is declared volatile. So we ourselves using a boolean volatile will be better any way right? Pls check once whether I misread some code path,",code_debt,low_quality_code
hbase,18085,comment_10,"Here is the JMH result with command ""java -jar benchmarks.jar -t 200 -i 10 -wi 10 And the simple test code:",non_debt,-
hbase,18085,comment_11,"From the micro benchmark result, tryLock is much better. Let me know if any question on the benchmark code/result, thanks.",non_debt,-
hbase,18085,comment_12,That looks to be so big of a diff and I fear some compiler level optimization would have affected. Can we use the return value of purgeLock.tryLock() passed to BlackHole or so?,non_debt,-
hbase,18085,comment_13,"Sorry, don't quite catch you sir, mind clarify? While writing the JMH case, I felt that using AtomicBoolean or volatile boolean we're actually re-implementing the tryLock logic. And I won't be surprised if JIT has any optimization for its native implementation (smile).",code_debt,duplicated_code
hbase,18085,comment_14,"In ur test method for tryLock, there is no logic other than just try lock and release. If that is been removed as dead code by compiler? One way to avoid that is using the return value. See BlackHole in JMH and its usage.. The diff in numbers that reported by JMH benchmark is so huge! The impl of tryLock is having a volatile read and all. So this huge diff in numbers looks strange no? That was my doubt.",code_debt,dead_code
hbase,18085,comment_15,Ok got your point now. Have changed the testLock method to use Blackhole like: and the result turned out: Checking the compiled class: Let me know your thoughts sir,non_debt,-
hbase,18085,comment_16,"Referring to this we need a non-local state variable to avoid JIT optimization, so I changed the testLock method as below: And get a more reasonable result (the previous result was not that reasonable considering the underlying implementation of",non_debt,-
hbase,18085,comment_17,Atomic boolean and tryLock are so close now.. May be AtomicBoolean is good enough? Thanks for the efforts in doing the JMH multiple rounds. Appreciate u Yu Li. :-),non_debt,-
hbase,18085,comment_18,"To summarize, volatile boolean is not suitable for this usage since this is not a ""single thread updating, multiple thread reading"" case, while AtomicBoolean and have similar performance according to JMH. In theory AtomicBoolean is more light-weight, but JMH shows a similar perf, and in our project tryLock is more frequently used for similar scenario, such as the and etc. So I still suggest to follow the old way to use tryLock here, but please feel free to raise different voice. Thanks.",non_debt,-
hbase,18085,comment_19,1,non_debt,-
hbase,18085,comment_20,+1 for the patch once QA result is fine. Thanks for the nice finding and patch Yu.,non_debt,-
hbase,18085,comment_21,Minor change to trigger UT on hbase-server,non_debt,-
hbase,18085,comment_23,"{{TestLockManager}} failure is irrelative with change here, and confirmed it could pass locally. Will commit soon if no objections.",non_debt,-
hbase,18085,comment_24,Pushed into master branch. Thanks  and  for review.,non_debt,-
hbase,18092,summary,Removing a peer does not properly clean up the state and metrics,code_debt,low_quality_code
hbase,18092,description,Removing a peer does not clean up the associated metrics and state from walsById map in the,code_debt,low_quality_code
hbase,18092,comment_0,Picking this up.,non_debt,-
hbase,18092,comment_2,lgtm,non_debt,-
hbase,18092,comment_3,Thank  for the review.,non_debt,-
hbase,18092,comment_4,"Attaching another patch for master. I found a redundant piece of code related to the patch. The only difference between the previous and this patch is {Code} @@ -528,9 +542,7 @@ public class implements ReplicationListener { */ public void src) { LOG.info(""Done with the recovered queue "" + - if (src instanceof ReplicationSource) { - - } + {Code} Also attaching patches for branch-1 and branch-1.3 as they both have diverged a little bit.",code_debt,low_quality_code
hbase,18092,comment_6,Patch for branch-1.3 doesn't apply to branch-1. Can you attach patch for branch-1 ?,non_debt,-
hbase,18092,comment_8,"I actually attached three patches, one each for master, branch-1 and branch-1.3. is the branch-1 patch.",non_debt,-
hbase,18092,comment_9,"Thanks for the patch, Ashu.",non_debt,-
hbase,18092,comment_13,"Just a nitpick, I think you merged the incorrect patch for master. I was looking at my latest master checkout and see that patch for master that went in is dated May 25, not yesterday. It's not a big deal in this case because the two patches have no functional difference, only that the newer patch got rid of a redundant piece of code.",code_debt,dead_code
hbase,18180,summary,Possible connection leak while closing BufferedMutator in TableOutputFormat,code_debt,low_quality_code
hbase,18180,description,"In TableOutputFormat, connection will not be released in case when ""mutator.close()"" throws exception.",non_debt,-
hbase,18180,comment_0,"Simple patch, please review.",non_debt,-
hbase,18180,comment_1,Have a look at TableOutputFormat inside Connection leak problem has already been addressed in this package. Code sample for reference.,code_debt,low_quality_code
hbase,18180,comment_2,"Thanks  for looking into this issue, this JIRA is to address below problem",non_debt,-
hbase,18180,comment_4,lgtm Better resubmit for QA run.,non_debt,-
hbase,18180,comment_5,Reattaching same patch files for QA run.,non_debt,-
hbase,18180,comment_7,TestLockProcedure is a small test. Many tests were skipped.,non_debt,-
hbase,18180,comment_8,You can attach branch-1 patch alone where TestLockProcedure is not flaky.,test_debt,flaky_test
hbase,18180,comment_9,"Ok, attached branch-1 patch.",non_debt,-
hbase,18180,comment_11,Please re-attach patch for master branch.,non_debt,-
hbase,18180,comment_12,Done.,non_debt,-
hbase,18180,comment_14,TestLockProcedure is successful on local execution.,non_debt,-
hbase,18180,comment_17,"Thanks for the patch, Pankaj",non_debt,-
hbase,18180,comment_19,"Thanks  for committing this patch. How about fixing this issue in branch 1.3, attaching branch-1.3 patch.",non_debt,-
hbase,18180,comment_20,Ping .,non_debt,-
hbase,18246,summary,Proc-V2 AM: Maintain Data locality in,non_debt,-
hbase,18246,description,"Before HBASE-18036, SSH would use round-robin to re-distribute regions during processing. Round-robin assignment would loss data locality. HBASE-18036 retains data locality if the dead region server has already restarted when the dead RS is processing. With Proc-V2 based AM, the change of HBASE-18036 in Apache HBASE 1.x releases is no longer possible. We need to implement the same logic under Proc-V2 based AM.",non_debt,-
hbase,18246,comment_0,Is this a problem still ?,non_debt,-
hbase,18246,comment_1,Making standalone issue.,non_debt,-
hbase,18246,comment_2,Resolving as not a problem. Open new issue if evidence.,non_debt,-
hbase,18304,summary,Start enforcing upperbounds on dependencies,non_debt,-
hbase,18304,description,would be nice to get this going before our next major version.,non_debt,-
hbase,18304,comment_0,"Hi, I've added the enforcer plugin to the project which immediately complained about the use of old version of From the main pom.xml I've already seen that it won't be simple: Okay, but let's try to update it to 3.2.0 agains the comment, let's see what happens (it can show the cause of the comment): Only five errors, because the build died after the first failed subproject. These classes (LiteralByteString and BoundedByteString) became private inner classes of ByteString recently. (I guess with version 3.0) But there might be other changes in the API too. Should I create a separate ticket for updating ProtoBuf which we use all over the code? I could set it as a dependency for this ticket. Thank, Tamaas",non_debt,-
hbase,18304,comment_1,"We intentionally use two versions of protobuf - there's some light reading at if you're inclined to understand the history of why that is. To help you move forward, is it possible to exclude the protobuf versions while enforcing for everything else?",non_debt,-
hbase,18304,comment_2,"Hi , when I said to exclude the protobuf dep I meant to exclude it from the configuration, not the actual dependency tree. I think we can use the mechanism in MENFORCER-273 to do this.",non_debt,-
hbase,18304,comment_4,"Hi , We do have other dependencies which have conflicts. * * * * * * * * io.netty:netty Should I exclude all of them from the check (just like protobuf) or fix the issues by updating minor/build versions? Or should I exclude them now and update the versions/re-include them in a separate ticket? None of them need a major version update, so they should not cause problems, but I cannot guarantee. scala version from 2.10.4 to 2.10.5 (doesn't look risky) from 3.6.2.Final to 3.8.0.Final (might be risky) Some dependencies are only transitive at the moment, but the conflicts can be solved by adding them as provided dependencies with the following versions numbers: paranamer.version -guava.version -jets3t.version - It is also possible to exclude these dependencies with the older version from the tree and only keep the newest ones, but it would be harder to maintain. The enforcer plugin must also be updated to a newer version to handle excludes: - Regards, Tamaas",build_debt,build_others
hbase,18304,comment_5,"Scala versioning was discussed as part of the spark support, I think even bugfix versions there are risky was the consensus. Stack has been talking about shading Netty over on HBASE-18271 because we pull an older version from Hadoop I think, so we're stuck with the conflict there. Guava... we use a shaded 22, but hadoop depends on an older version as well, so we're stuck with that. I would not add it as declared. I'm not sure about the others, I assume it's a similar situation. Starting to think this issue causes more problems than it solves.",non_debt,-
hbase,18304,comment_6,"Just to be agile, I have uploaded my second patch. It only updates the maven enforcer plugin and adds exception list of all conflicting dependencies to it. This way the conflicts can be solved as separate tasks later after proper analysis and careful consideration.",non_debt,-
hbase,18304,comment_8,uploaded again to re-run tests,non_debt,-
hbase,18304,comment_10,"Thanks for digging into this . I like the incremental approach of making sure we don't get new mismatched dependencies and then working through the list of current excludes. Rough back of the envelope, I expect the current list to go like this: This is going to be a nightmare due to our purposeful handling of multiple versions. But maybe I'm misunderstanding it, since shouldn't our internal use of protobuf 3 be masked since we relocate it in third-party-deps? This one should be easy to just set to latest. Maybe solved for us by our move to third-party-deps? Shouldn't only Hadoop's show up? or is the conflict in spark or some such? (questions for the eventual follow-on JIRA) These should go okay. These are probably just an error in our spark module. Best not to try to address it until we close out HBASE-16179 I think also solved by our move to third-party-deps on HBASE-18271",non_debt,-
hbase,18304,comment_11,"Hi , Please see my comments inline. We do only reference protobuf 3.3.0 in now, but it is a dependency of hbase-client, hbase-procedure and hbase-server. Through the transitive dependencies it causes conflict in this three module. If I exclude protobuf from the dependency in these three modules, it looks okay. Is it? If I add to hbase-client as dependency it solves the problem. Almost solved. uses guava 14.0.1, and is referenced directly and transitively from Otherwise we only use guava version 11.0.2. If I can exclude it from spark-core_2.10 transitive dependencies in hbase-spark and hbase-spark-it it works. Go okay as being excluded from the check or if I add them to hbase-spark and hbase-spark-it as direct dependency? Okay. They stay excluded from the check. Just as with guava causes the problem. It uses netty version 3.8.0.Final as transitive dependency while we use 3.6.2.Final everywhere else. Should I exclude it from spark-core's dependencies manually? Thanks, Tamaas",build_debt,build_others
hbase,18304,comment_13,"I tried to pick this up again and so far I've been running into several dependencies where we don't depend on it at all, but enforcer plugin complains because it turns out hadoop is internally inconsistent (usually between hadoop-client and hadoop-minicluster having transitive dependancies on things). This might be a bug in the enforcer plugin, or we should push hadoop to be cleaner before we start trying to enforce things on our own side.",build_debt,over-declared_dependencies
hbase,18337,summary,hbase-shaded-server brings in signed jars,non_debt,-
hbase,18337,description,hbase-shaded-server brings in some digital signatures which causes problems to our friends that need to use them at least in crunch and a few other places as well. they get errors like:,non_debt,-
hbase,18337,comment_0,Patch to exclude signature files,non_debt,-
hbase,18337,comment_1,+1 presuming no surprises from QA.,non_debt,-
hbase,18337,comment_2,"i haven't had a chance to test these with crunch yet, only verified that the signatures are removed locally. let's not push until i can get some cross-project testing done.",non_debt,-
hbase,18337,comment_4,"My contacts in the crunch world didn't get a chance to run the full suite yet, but told me that it looks fine in isolation. I withdraw my previous request to wait before pushing.",non_debt,-
hbase,18337,comment_5,"- I got a chance to verify that this fix is good for crunch. we should push this on all branch-2 and master, I verified that we don't need it on branch-1.",non_debt,-
hbase,18337,comment_8,pushed to branch-2+. thanks!,non_debt,-
hbase,18346,summary,failing on branch-1 (branch-1.4),non_debt,-
hbase,18346,description,"Running 1, 1, Errors: 0, Skipped: 0, 12.343 sec <<< FAILURE! - in zing 12. 329 sec <<< FAILURE! null 2, 0, Errors: 2, Skipped: 0, 192.427 sec <<< FAILURE! - in izing 179 .859 sec <<< ERROR! test timed out after 180 seconds at Method)",non_debt,-
hbase,18346,comment_0,"Of course, now I'm having a hard time reproducing this",non_debt,-
hbase,18346,comment_2,Tail of the log is full of this,non_debt,-
hbase,18346,comment_3,The doesn't always go down.,non_debt,-
hbase,18346,comment_4,Latest tests hang here: I'm thinking about temporarily disabling this test.,non_debt,-
hbase,18346,comment_5,Disable test on branch-1 and branch-1.4,non_debt,-
hbase,18449,summary,Fix,non_debt,-
hbase,18449,comment_0,The failure is due to race condition. Will attach the patch later.,non_debt,-
hbase,18449,comment_1,I run the test 800 times. The failure doesn't happen again.,non_debt,-
hbase,18449,comment_2,1,non_debt,-
hbase,18449,comment_4,Will commit it soon,non_debt,-
hbase,18449,comment_5,Push this to branch-2 and master.  Thanks for the reviews.,non_debt,-
hbase,18501,summary,Use TableDescriptor and as far as possible,non_debt,-
hbase,18501,description,"We need to do some cleanup for the *public* class as much as possible. Otherwise, the HTD and HCD may linger in the code base for a long time.",code_debt,low_quality_code
hbase,18501,comment_0,All subtasks are done. This parent issue can be closed out?,non_debt,-
hbase,18501,comment_1,Let me double-check the code base...Thanks for the reminder.,non_debt,-
hbase,18501,comment_2,Close it as all subtasks are done.,non_debt,-
hbase,18501,comment_3,Nice,non_debt,-
hbase,18522,summary,Add RowMutations support to Batch,non_debt,-
hbase,18522,description,RowMutations is multiple Puts and/or Deletes atomically on a single row. Current Batch call does not support RowMutations as part of the batch. We should add this missing part. We should be able to batch RowMutations.,non_debt,-
hbase,18522,comment_0,Attached a branch-1 patch. Will work on a patch for master.,non_debt,-
hbase,18522,comment_1,Can we not use Integer ? All legitimate indices should be Please close t at the end of the test.,non_debt,-
hbase,18522,comment_3,"Hi, Ted Thanks for the review. Could you elaborate? The HashMap needs <Integer, Integer>",non_debt,-
hbase,18522,comment_4,"I was talking about 'Integer rowMutationsIndex' which can be declared as int. However, since we don't know whether the map contains i or not, you can keep the current formation. Please check failed tests (they should fail without patch).",non_debt,-
hbase,18522,comment_5,I ran the failed tests with the patch multiple times locally. The same tests fail and pass in different times. And I ran a few times without the patch as well. The same tests pass and fail from time to time. And,non_debt,-
hbase,18522,comment_6,"I looked at the master. The patch should be similar. But additionally AsyncTable is master branch (and branch-2). I could add the same support for AsyncTable as well. Or I could have a separate JIRA, if it is agreed to add it.",non_debt,-
hbase,18522,comment_9,lgtm,non_debt,-
hbase,18522,comment_10,"Hello, more review please. If no more, I will commit soon. Thanks.",non_debt,-
hbase,18522,comment_11,"Committed to branch-1.4, branch-1, branch-2 and master.",non_debt,-
hbase,18549,summary,Unclaimed replication queues can go undetected,non_debt,-
hbase,18549,description,We have come across this situation multiple times where a zookeeper issues can cause NodeFailoverWorker to fail picking up replication queue for a dead region server silently. One example is when the znode size for a particular queue exceed jute.maxBuffer value. There can be other situations that may lead to this and just go undetected. We need to have a metric for number of unclaimed replication queues. This will help in mitigating the problem through alerting on the metric and identifying underlying issues.,non_debt,-
hbase,18549,comment_0,partially related to:,non_debt,-
hbase,18549,comment_1,"my takeaway from this JIRA's description is to providing metrics regarding failed to recover replication queue. The reason of this failure could be ZK node oversize or other unknown issue. For ZK issue, the only solution is to increase its size via ""jute.maxBuffer"" from both client and server side. So, I am not changing any HBase code to try to handle it. Simply increase the count for this new metric and print out error to log to alert operators. Metrics look like this after this patch: ""name"" : ""modelerType"" : ""tag.Context"" : ""regionserver"", ""tag.Hostname"" : : 0, : 0, ... : 0, ... : 0, (this metric is in global source, so it's resilient to any particular source going down)*",non_debt,-
hbase,18549,comment_3,"Yes i believe that as well. The patch mostly lgtm. The word region is misspelled in some exception text. That exception could provide more information for debugging. You have: Add the region name, the znode path, and the stacktrace to the log line.",documentation_debt,low_quality_documentation
hbase,18549,comment_7,+1 modulo minor spelling nit: What do you think about a patch for branch-1 too?,documentation_debt,low_quality_documentation
hbase,18549,comment_8,added branch-1 patch. added master branch patch to address typo issue Andrew mentioned.,documentation_debt,low_quality_documentation
hbase,18549,comment_10,"Dropped this. Bringing it back. Let me see if more needs to be done before commit. Otherwise, will commit",non_debt,-
hbase,18549,comment_11,Committed. Thanks for taking this one on,non_debt,-
hbase,18588,summary,Verify we're using netty .so epolling on linux post HBASE-18271,non_debt,-
hbase,18588,description,This is a task to verify that indeed we are using .so native epoll on linux. This task is probably unnecessary since we'd fail on the linux build box if this was not in place but verify that our relocation is indeed finding the native code. Assigned myself.,non_debt,-
hbase,18588,comment_0,It fails for me in eclipse...,non_debt,-
hbase,18588,comment_1,Thanks  Is that linux?,non_debt,-
hbase,18588,comment_2,-Pushed to master and branch-2.- <= Mistake. Should have been written into subtask. Ignore.,non_debt,-
hbase,18588,comment_3,"It works for me on mac and up in eclipse. Builds on linux pass and I can run the unit tests there too. Can you specify the system property: (The trailing '.' is required) ... when running the unit test in your eclipse and see if that fixes it. If it does, I'll figure some way of getting it into eclipse context. Thanks boss.",non_debt,-
hbase,18588,comment_6,"Thanks sir, will try it soon and report back here.",non_debt,-
hbase,18588,comment_7,"OS: Ubuntu. java version ""1.8.0_131"". Apache Maven 3.3.9. I run ""mvn clean test -Dtest=TestXX"", then get a error. Is it related?",non_debt,-
hbase,18588,comment_8,It works for me sir. And I found that the UTs in hbase-spark are still failing when executing from command line. Need some maven magic?,non_debt,-
hbase,18588,comment_9,"Did you add the system property and then it started working? If so, I need to try and get that into eclipse runtime somehow. On the failing UTs, , for sure your failure is related, but it seems to work for me... the pom.xml should be providing the command-line arg. I'm on ubuntu..... more /etc/os-release NAME=""Ubuntu"" VERSION=""14.04.4 LTS, Trusty Tahr"" ID=ubuntu ID_LIKE=debian PRETTY_NAME=""Ubuntu 14.04.4 LTS"" VERSION_ID=""14.04"" And it seems to work... T E S T S Running 3, 0, Errors: 0, Skipped: 0, 20.198 sec - in Results : 3, 0, Errors: 0, Skipped: 0 Let me try more combinations...",non_debt,-
hbase,18588,comment_10,I tried w/ same mvn and java 8 and seems fine for me  Lets figure out the difference sir. You on master or branch-2? You updated? Thanks lads. Keep speaking out. I'd like to fix all these issues that come of the relocations.,non_debt,-
hbase,18588,comment_11,"sir, it is not test failed. It may be related to hbase-spark module?",non_debt,-
hbase,18588,comment_12,"master branch. Yes, test with latest code.",non_debt,-
hbase,18588,comment_13,Thanks @guanghao Zhang. Will look in morning,non_debt,-
hbase,18588,comment_14,"I added the setting of the system property for scalatest. Seems to work now. Try it  Thanks for reporting. I pushed addendum like this: ""HBASE-18271 Shade netty Purge mention of netty-all; ADDENDUM for sparktest"" on master and branch-2.",non_debt,-
hbase,18588,comment_15,It works fine. Thanks. :-),non_debt,-
hbase,18596,summary,[TEST] A hbase1 cluster should be able to replicate to a hbase2 cluster; verify,non_debt,-
hbase,18596,description,"From the mailing list thread ""[DISCUSS] hbase-2.0.0 compatibility expectations"",  asks: The latter should be a blocker. Verify it works.",non_debt,-
hbase,18596,comment_0,Will work on this.,non_debt,-
hbase,18596,comment_1,Initial run from PE to replicate from HBase 1.2 to HBase 2.0 worked so far and replication from HBase 2.0 to HBase 1.2 also worked without any major issue. I will try to code this into a proper it test in,non_debt,-
hbase,18596,comment_2,You messing w/ this one ? You going to run the test again? Thanks boss.,non_debt,-
hbase,18596,comment_3,"Trying this, yeah, a 1.2 seems to replicate fine to a 2.x cluster. The 1.2 cluster wanted to create the remote table else I saw the below complaint: I need to see more what it was objecting too. I did not seem to be able to replicate from hbase2 to hbase1... would hang at least on Let me see if can make it work if table already created... maybe not given table state is not in zk.",non_debt,-
hbase,18596,comment_4,"I put up 0.98.25-SNAPSHOT again on one cluster and tip of branch-2 on the other. I added respective peers on each end. The command will never succeed since it hashes the two TableDescriptors, the one on peer and the local source, and they'll never compare, so I enabled table replication by altering the table and setting REPLICATION_SCOPE to global scope ('1'). I then shoved edits through and they showed up on both ends promptly. As per , replication basically works. Lets file issues for more explicit problems replicating across versions. Resolving for now.",non_debt,-
hbase,18646,summary,"[Backup] make procedure timeout, thread pool size configurable",non_debt,-
hbase,18646,description,The default procedure timeout of 60 sec and pool size (1) may be not optimal for large deployements,design_debt,non-optimal_design
hbase,18646,comment_0,Patch v1.,non_debt,-
hbase,18646,comment_1,"If the config is specific to log roll, please reflect this in the name of config.",code_debt,low_quality_code
hbase,18646,comment_3,v2. cc:,non_debt,-
hbase,18646,comment_4,Isn't the above default too short for a large cluster ? Please remove commented out code from patch.,code_debt,dead_code
hbase,18646,comment_6,v3.,non_debt,-
hbase,18646,comment_7,v4 increased default timeout,non_debt,-
hbase,18646,comment_8,Needs the fix from HBASE-18689 to be integrated first before we can get QA run for the patch here.,non_debt,-
hbase,18646,comment_11,Resubmitting patch,non_debt,-
hbase,18646,comment_13,lgtm,non_debt,-
hbase,18646,comment_16,HBASE-19407 removed backup/restore from branch-2/2.0.0,non_debt,-
hbase,18692,summary,[compat 1-2] goes from void to int,non_debt,-
hbase,18692,description,You need this change? BBUtils was public back in 1.0 and returned a void. HBASE-12213 HFileBlock backed by Array of ByteBuffers (Ram) changed it to return an int. You think we need this? I can try undoing it. Thanks.,non_debt,-
hbase,18692,comment_0,I have not checked the code - if the return value is unused then we can revert it. Thanks.,non_debt,-
hbase,18692,comment_1,Return value seems not used at all by calling places. We can easily revert this change.,non_debt,-
hbase,18692,comment_2,Thanks boys. Small patch.,non_debt,-
hbase,18692,comment_3,1,non_debt,-
hbase,18692,comment_5,Pushed to master and branch-2. Thanks for reviews.,non_debt,-
hbase,18759,summary,Fix failure,non_debt,-
hbase,18759,description,Here's the error.,non_debt,-
hbase,18759,comment_0,hbase-shaded-server will be replaced with in meantime (HBASE-18697). Let's also check the dependency tree for that module as a final check later.,non_debt,-
hbase,18759,comment_1,-0 - relocate commons-lang 2.y from hadoop - exclude more jetty / glassfish / etc also includes a patch for HBASE-18760 to test it.,non_debt,-
hbase,18759,comment_4,1,non_debt,-
hbase,18788,summary,NPE when running,non_debt,-
hbase,18788,description,Maybe it can not cause the tests to fail but I still think we need to fix it.,non_debt,-
hbase,18788,comment_0,Please find attached the to fix the issue,non_debt,-
hbase,18788,comment_2,Pushed to branch-2 and master. It does not apply to branch-1. Thank you for the patch,non_debt,-
hbase,18788,comment_3,Is it expected to get a null reference here?,non_debt,-
hbase,18788,comment_6,Reopening to take a look. I was being lazy.,non_debt,-
hbase,18788,comment_7,Moving to subtask of HBASE-20046 since serial replication was dropped from 2.0 - not sure if this is still relevant in master,non_debt,-
hbase,18788,comment_8,The old has already been removed.,non_debt,-
hbase,18909,summary,Deprecate Admin's methods which used String regex,non_debt,-
hbase,18909,description,None,non_debt,-
hbase,18909,comment_0,"The API you are suggesting to use is marked deprecated below in the patch, So we should suggest to use in the first place also. also There is reference to this method in java doc of other public APIs to use this one, I suggest to remove their also and point it to actual API we would like user to use. General practice what I follow is when ever I mark a API as deprecated I replace all of its reference from the code with the one we are suggesting to use in the java doc. If you also want to do this please also take care of the references in ruby scripts...",documentation_debt,outdated_documentation
hbase,18909,comment_2,Attach a 002 patch which addressed  comments.,non_debt,-
hbase,18909,comment_3,+1. I searched through jsp and jamon file also and didn't find any of deprecated APIs in the patch references in them.,non_debt,-
hbase,18909,comment_5,Thanks for your double check. :-),non_debt,-
hbase,18909,comment_6,"Attach a patch to fix the too long line report. But why ruby-lint report ""undefined method java_import""?  Any ideas?",code_debt,low_quality_code
hbase,18909,comment_8,leave some comment on rb.,non_debt,-
hbase,18909,comment_9,Attach a 004 patch addressed comments.,non_debt,-
hbase,18909,comment_11,Attach a 005 patch to fix the whitespace and javadoc warnings.,documentation_debt,low_quality_documentation
hbase,18909,comment_13,Any more comments?,non_debt,-
hbase,18909,comment_14,Does this still reference to deprecated API?,code_debt,low_quality_code
hbase,18909,comment_15,Sorry to miss it. Attach a 006 patch addressed it.,non_debt,-
hbase,18909,comment_17,I've been looking into this and don't have a good answer. Ignore it for now and please file a follow on issue (and maybe assign it to me) to look into that.,non_debt,-
hbase,18909,comment_18,Open a new issue HBASE-18956 for the ruby report.,non_debt,-
hbase,18909,comment_19,Any more comments?,non_debt,-
hbase,18909,comment_20,1,non_debt,-
hbase,18929,summary,Hbase backup command doesnt show debug option to enable backup in debug mode,non_debt,-
hbase,18929,description,"... Usage: hbase backup create <type type ""full"" to create a full backup image ""incremental"" to create an incremental backup image backup_path Full path to store the backup image Options: -b <arg -q <arg -s <arg -t <arg -w <arg The above help doesn't show that we can actually run backups in debug mode easily by just providing -d option which is very helpful in debugging. The following code works well. and shows all debug logs.",non_debt,-
hbase,18929,comment_0,"Please review the PR : cc :  ,",non_debt,-
hbase,18929,comment_1,Attaching patch generated from PR. lgtm Waiting for QA.,non_debt,-
hbase,18929,comment_2,Do I need to do anything for QA ?,non_debt,-
hbase,18929,comment_3,"In the future, attach patch and click 'Submit Patch' button.",non_debt,-
hbase,18929,comment_4,"Sure, thanks",non_debt,-
hbase,18929,comment_7,"Thanks for the patch, Amit.",non_debt,-
hbase,19015,summary,fails against hadoop3-beta1,non_debt,-
hbase,19015,description,When using the following command trying to build tar ball against hadoop3 beta1: I got the following: See full output in attachment.,non_debt,-
hbase,19015,comment_0,Dup of HBASE-18838,non_debt,-
hbase,19031,summary,Align exist method in Table and AsyncTable interfaces,non_debt,-
hbase,19031,description,"Table and AsyncTable have the ability to pass a list to {{exist}} method similarly to other operations like get, delete. However, {{boolean[] existsAll(List<Get> gets)}} does not use the same pattern in Table. It should look like {{boolean[] exists(List<Get> gets)}} to match.",non_debt,-
hbase,19031,comment_0,There is a problem doing this change. {{RemoteHTable}} and {{HTableWrapper}} classes have a deprecated {{public Boolean[] exists(List<Get Shall we tolerate the difference between {{exists}} and {{existsAll}} method? Ping,non_debt,-
hbase,19031,comment_1,'s change on HBASE-19043 helps with {{HTableWrapper}}. In {{RemoteHTable}} the {{public Boolean[] exists(List<Get> gets)}} method is marked as deprecated but it does not have any documentation when it will be removed. Is it possible to remove that method before alpha4?,documentation_debt,outdated_documentation
hbase,19031,comment_2,Previously discussed with  that removing the already deprecated method ok since it does not have any note when we plan to remove it completely. Let me know if anyone disagree.,non_debt,-
hbase,19031,comment_3,Fixed a leftover TODO in version 2.,requirement_debt,requirement_partially_implemented
hbase,19031,comment_4,Is the patch ready to review or submit? Making the issue {{Patch available}} is a good notification to the reviewers and QA. (smile),non_debt,-
hbase,19031,comment_5,Submitted it. I think that is what Peter intended (He is traveling at moment).,non_debt,-
hbase,19031,comment_6,The deprecation in RemoteHTable was added by this which shipped in version 0.99 tree parent author Enis Soztutar Enis Soztutar <enis@apache.org HBASE-11797 Create Table interface to replace HTableInterface (Carter) ... so the removal is fine. +1 from me on patch.,code_debt,dead_code
hbase,19031,comment_7,"Yes, the patch is ready to review. I used submit-patch and apparently it did not change the status of the Jira. I thought it does that automatically.",non_debt,-
hbase,19031,comment_8,The is abort. retry,non_debt,-
hbase,19031,comment_11,+1. Will commit it later.,non_debt,-
hbase,19031,comment_12,Push to branch-2 and master. Thanks for the patch.,non_debt,-
hbase,19073,summary,Cleanup,code_debt,low_quality_code
hbase,19073,description,"- Remove the configuration - Keep following interface since they nicely separate ZK based implementation: ProcedureMemberRpcs - Replace CSM (interface) + BCSM (unnecessary middle hierarchy) with single CSM interface. - Don't pass whole CSM object around (with server in it which gives acess to pretty much everything), only pass the relevant dependencies. Discussion thread on dev@ mailing list.",design_debt,non-optimal_design
hbase,19073,comment_2,"So the only test passing in QA is But it's weird, this is second time am seeing it failing in a precommit, and the last patch was completely unrelated. It's not even in the flaky list. Let me dig.",test_debt,flaky_test
hbase,19073,comment_3,"Maybe it's just that timeout of 30 sec is less for the test? Running all master tests locally. {{mvn test -pl hbase-server}} Edit: Sorry, timeout is 300 sec not 30 sec. That should be enough.",non_debt,-
hbase,19073,comment_4,"So running all master tests passed. I had fork count=5 to make sure that it's not failing because of some weird concurrency issue in testing. I have no idea why its failing because the test being changed in this patch, runs after the test that's failing, testThreeRSAbort. So that can't be it. Doing another QA run.",non_debt,-
hbase,19073,comment_6,Pushed to master and branch-2. Thanks for the review .,non_debt,-
hbase,19183,summary,Removed redundant groupId from Maven modules,code_debt,low_quality_code
hbase,19183,description,Currently the modules hbase-checkstyle and hbase-error-prone define the groupId redundantly. Remove the groupId from these POMs.,code_debt,low_quality_code
hbase,19183,comment_1,1,non_debt,-
hbase,19183,comment_2,Committed to master.,non_debt,-
hbase,19187,summary,Remove option to create on heap bucket cache,non_debt,-
hbase,19187,description,"Since we are moving read and write paths to use direct memory, option to configure on heap bucket cache is irrelevant. We should remove that option.",non_debt,-
hbase,19187,comment_0,U will work on this issue or want me to provide a patch? Tks,non_debt,-
hbase,19187,comment_1,Bumping out to 2.1 as we're trying to limit 2.0.0 to bugs only at this point :),non_debt,-
hbase,19187,comment_2,"Better we can remove in 2.0 itself. We are trying to remove some other stuff like DLR, Prefix tree. (Reasons are diff there though).. Will give patch soon.",non_debt,-
hbase,19187,comment_3,"Ok, no worries if you're on top of it, Anoop!",non_debt,-
hbase,19187,comment_4,1,non_debt,-
hbase,19187,comment_6,+1 Needs release note.,non_debt,-
hbase,19187,comment_7,The following failure can be reproduced: Looks like can be dropped. Please check other failed tests.,non_debt,-
hbase,19187,comment_8,Correction of test failures. Removed the on heap BC tests. Fixed one javadoc warn.,code_debt,low_quality_code
hbase,19187,comment_9,QA run didn't continue: Triggering another run.,non_debt,-
hbase,19187,comment_11,lgtm,non_debt,-
hbase,19187,comment_12,Thanks all for the reviews.,non_debt,-
hbase,19241,summary,Improve javadoc for AsyncAdmin and cleanup warnings for the implementation classes,code_debt,low_quality_code
hbase,19241,description,None,non_debt,-
hbase,19241,comment_0,Add more javadoc for AsyncAdmin. Also cleanup the warnings of RawAsyncHBaseAdmin.,documentation_debt,low_quality_documentation
hbase,19241,comment_1,FYI.,non_debt,-
hbase,19241,comment_3,+1 Fix javadoc complaint on commit.,documentation_debt,low_quality_documentation
hbase,19241,comment_4,This good by you sir?,non_debt,-
hbase,19241,comment_5,"+1 after fix the checkstyle warnings. Meanwhile, please add one line change change to trigger hbase-server test. Thanks.",code_debt,low_quality_code
hbase,19241,comment_6,Fix checkstyle and javadoc warnings. Add one line change in hbase-server module to trigger the UTs.,code_debt,low_quality_code
hbase,19241,comment_8,The failed UT is introduced by HBASE-18601. Let me commit after fixing the checkstyle issue. Thanks.,non_debt,-
hbase,19241,comment_9,Pushed to master and branch-2. Thanks all for reviewing.,non_debt,-
hbase,19300,summary,fails in branch-1.4,non_debt,-
hbase,19300,description,From : I ran the test locally which failed. Noticed the following in test output:,non_debt,-
hbase,19300,comment_0,"After several test runs, it seems commit caused the test to fail. With commit the test passes.  FYI",non_debt,-
hbase,19300,comment_1,"After a brief bisect (cutting the patch in half and shrinking the number of files touched), I narrowed down to the changes in With the changes, the test times out.",non_debt,-
hbase,19300,comment_2,"As far as I can tell, synchronizing on outer (the context) is correct: I don't know why error-prone flagged {{synchronized (outer)}}",code_debt,low_quality_code
hbase,19300,comment_3,This patch allows the test to pass.,non_debt,-
hbase,19300,comment_4,: Not sure whether patch filename was not well formed or something else.,non_debt,-
hbase,19300,comment_5,Looped 10 times with patch which all passed.,non_debt,-
hbase,19300,comment_6,1,non_debt,-
hbase,19300,comment_8,Ran TestScanner locally with patch which passed.,non_debt,-
hbase,19300,comment_9,"Thanks for the review, Chia-ping.",non_debt,-
hbase,19300,comment_10,+1 Please commit. Relevant for both branch-1.4 and branch-1.,non_debt,-
hbase,19300,comment_11,Committed to branch-1 as well.,non_debt,-
hbase,19373,summary,Fix Checkstyle error in hbase-annotations,code_debt,low_quality_code
hbase,19373,description,Fix the remaining Checkstyle error regarding line length in the *hbase-annotations* module.,code_debt,low_quality_code
hbase,19373,comment_0,1,non_debt,-
hbase,19373,comment_2,"Pushed to master, branch-2, branch-1, branch-1.4, branch-1.3, branch-1.2 and branch-1.1.",non_debt,-
hbase,19373,comment_5,Belated binding +1,non_debt,-
hbase,19384,summary,Results returned by preAppend hook in a coprocessor are replaced with null from other coprocessor even on bypass,non_debt,-
hbase,19384,description,Phoenix adding multiple coprocessors for a table and one of them has preAppend and preIncrement implementation and bypass the operations by returning the results. But the other coprocessors which doesn't have any implementation returning null and the results returned by previous coprocessor are override by null and always going with default implementation of append and increment operations. But it's not the case with old versions and works fine on bypass.,non_debt,-
hbase,19384,comment_0,Ping,non_debt,-
hbase,19384,comment_1,"any chance you could share the specifics on how the coprocessors are set up in which you see this? Or, if you really want to go the extra mile, distill it down to a generic test case? Assuming there is a bug anyways, having a clear example of what is going wrong would be good both for identifying the problem as well as preventing a regression later :)",test_debt,lack_of_tests
hbase,19384,comment_2,Thanks for filing the issue. So you have multiple coprocessors stacked on a region. One intercepts preAppend (or preIncrement) to return its own result instead. Are you saying that this result is overwritten by the null that subsequent coprocessors return? Is the problem our removal of 'complete'? i.e. HBASE-19123 Purge 'complete' support from Coprocesor Observers ? Thanks.,non_debt,-
hbase,19384,comment_3,Sure will write test case for this.  Correct. Yes it's because of removal of complete so we are not able to skip running subsequent coprocessors which do not have any implementation for preAppend or preIncrement hooks.,test_debt,lack_of_tests
hbase,19384,comment_4,"If bypass() is called on a ObserverContext within a bypassable cp hook, we should just stop calling the remaining CP hooks? So bypass meaning will become old complete + old bypass(bypass core logic)",non_debt,-
hbase,19384,comment_5,I like this idea  Let me backfill HBASE-19123 enabling 'complete' when 'bypass' is set (let me know if this won't work for you,non_debt,-
hbase,19384,comment_6,So previously Phoenix CP was using this complete capability? Or internally HBase CP bypass logic was taking care of it?,non_debt,-
hbase,19384,comment_7,"Phoenix must have asked for it. In hbase1, you had to ask for it. bypass did not do it for you.",non_debt,-
hbase,19384,comment_8,Hopefully this will work for you,non_debt,-
hbase,19384,comment_9,+1 on patch,non_debt,-
hbase,19384,comment_11,It's working fine . +1,non_debt,-
hbase,19384,comment_12,Pushed to master and branch-2. Thanks for review  and for trying it,non_debt,-
hbase,19478,summary,Utilize multi-get to speed up WAL file checking in BackupLogCleaner,non_debt,-
hbase,19478,description,Currently issues one Get per WAL file: This is rather inefficient considering the number of WAL files in production can get quite large. We should use multi-get to reduce the number of calls to backup table (which normally resides on another server).,code_debt,slow_algorithm
hbase,19478,comment_0,I attached a patch that is added new method using multi-get.,non_debt,-
hbase,19478,comment_1,"It seems 'Future' is not needed in above sentence. For WALFiles, 'are' should be used - w.r.t. meaning of values in its returned Map, since you have this in the caller: please modify the javadoc to match the actual meaning.",documentation_debt,low_quality_documentation
hbase,19478,comment_2,Thanks . I just attached a new patch for the review.,non_debt,-
hbase,19478,comment_3,lgtm Pending QA result.,non_debt,-
hbase,19478,comment_7,"Thanks for the patch, Toshihiro",non_debt,-
hbase,19504,summary,Add TimeRange support into checkAndMutate,non_debt,-
hbase,19504,description,None,non_debt,-
hbase,19504,comment_0,Any more to be done here  sir?,non_debt,-
hbase,19504,comment_1,Not yet. Let me take over it if no one have interest in it.,non_debt,-
hbase,19504,comment_3,The patch includes some refactor to protobuf util. Try to avoid null TimeRange in code base.,non_debt,-
hbase,19504,comment_5,Thank  for the reviews.,non_debt,-
hbase,19504,comment_10,Reopen to push addendum.,non_debt,-
hbase,19504,comment_11,"Arghh...Reverted the above ADDENDUM and instead pushed to branch-2 and branch-2.0 the patch on HBASE-20272 -- which was in master only -- instead (For some reason, an issue was opened to push a super minor addendum to a test...)",non_debt,-
hbase,19531,summary,Remove needless volatile declaration,code_debt,low_quality_code
hbase,19531,comment_0,"Also, these fields can be {{final}}",non_debt,-
hbase,19531,comment_1,I have in interest in this issue.May u assign to me?,non_debt,-
hbase,19531,comment_2,done,non_debt,-
hbase,19531,comment_3,LGTM,non_debt,-
hbase,19531,comment_5,"I forgot change my username and email, so update a new one.",non_debt,-
hbase,19531,comment_6,Will commit it later,non_debt,-
hbase,19531,comment_7,Thanks for the patch.,non_debt,-
hbase,19570,summary,Add hadoop3 tests to Nightly master/branch-2 runs,non_debt,-
hbase,19570,description,None,non_debt,-
hbase,19570,comment_4,"Test run: (ignore jira id of job, that build is for this patch) Looks like already surfacing issues:",non_debt,-
hbase,19570,comment_5,Ready for commit. Ptal .,non_debt,-
hbase,19570,comment_6,Looks great  Fix this misspelling on commit. +1 PROJET_PERSONALITY,documentation_debt,low_quality_documentation
hbase,19570,comment_7,Fixed spelling. Thanks for the review stack. Pushed all the way back till branch-1.1.,documentation_debt,low_quality_documentation
hbase,19574,summary,TestBlockReorder fails running against hadoop 3,non_debt,-
hbase,19574,description,Here is related code: It seems the {{LOG}} field of DFSClient in hadoop 3 is not Log4JLogger any more.,non_debt,-
hbase,19574,comment_0,Dup of HBASE-19954,non_debt,-
hbase,19633,summary,Clean up the replication queues in the stage when removing a peer,non_debt,-
hbase,19633,description,"In the previous implementation, we can not always cleanly remove all the replication queues when removing a peer since the removing work is done by RS and if an RS is crashed then some queues may left there forever. That's why we need to check if there are already some queues for a newly created peer since we may reuse the peer id and causes problem. With the new procedure based replication peer modification, I think we can do it cleanly. After the are done on all RSes, we can make sure that no RS will create queue for this peer again, then we can iterate over all the queues for all Rses and do another round of clean up.",design_debt,non-optimal_design
hbase,19633,comment_0,Can you change logging to use slf4j parameter substitution?,non_debt,-
hbase,19633,comment_1,Sure. Will do.,non_debt,-
hbase,19633,comment_3,Change to use the slf4j style logging.,non_debt,-
hbase,19633,comment_4,FYI. I think one more thing is that we need to revisit the order at RS side to confirm that after refresh succeed there will be no moving nor creating for the given peer any more.,non_debt,-
hbase,19633,comment_6,This should move to the outer loop?,non_debt,-
hbase,19633,comment_7,"Moved the out. Scan two passes when removing all the queues for a peer, the reason is described in the comments. Also done some refactoring on the API of ReplicationPeers.",non_debt,-
hbase,19633,comment_8,Wrong patch.,non_debt,-
hbase,19633,comment_10,Fix failed UT.,non_debt,-
hbase,19633,comment_12,Let me do a rebase and then commit. There are lots of big changes on master already...,non_debt,-
hbase,19633,comment_13,Rebased and pushed to branch HBASE-19397. Thanks  for reviewing.,non_debt,-
hbase,19739,summary,Include thrift IDL files in HBase binary distribution,non_debt,-
hbase,19739,description,"Include thrift IDL files in HBase binary distribution, possibly at:",non_debt,-
hbase,19739,comment_0,Thrift IDL files can be included in separate client bundle when its available.,non_debt,-
hbase,19739,comment_1,"After discussing with , we used to and still include .proto files in hbase-protocol.jar file. Similarly thrift IDL files can be included in hbase-thrift.jar and .proto files can be included in files respectively.",non_debt,-
hbase,19739,comment_3,Should this be src/main/proto ? <directory I can fix it on commit. Patch LGTM,non_debt,-
hbase,19739,comment_4,Thanks for the review ! Path /src/main/ is intended as it nicely groups all *.proto files in protobuf/ directory. /src/main/protobuf/ causes all *.proto files at root folder of jar.,non_debt,-
hbase,19739,comment_5,Pushed to master and branch-2. Thanks,non_debt,-
hbase,19775,summary,hbase shell doesn't handle the exceptions that are wrapped in,non_debt,-
hbase,19775,description,"HBase shell doesn't have a notion of so it may not handle it correctly. For an example, if we scan not existing table the error look weird:",non_debt,-
hbase,19775,comment_0,"A simple fix. I'm not a ruby guy, so, possible there is a more fancy way to handle it exists.",non_debt,-
hbase,19775,comment_1,"FYI ,",non_debt,-
hbase,19775,comment_2,"If it were me, I'd turn it into a oneline thing, but it's semantically equivalent and maybe just my take :) We do have some Ruby tests in (invoked via some JUnit tests e.g. What was the context in which you saw this -- something we could reproduce easily?  has been in here recently too. Maybe he can drop a review :)",non_debt,-
hbase,19775,comment_4,"yep, @josh is right about the oneliner style - it's much more ruby-esque. {{cause = cause.getCause if cause.is_a? can we add a test in",test_debt,lack_of_tests
hbase,19775,comment_5,"I was able to manually verify the fix, so the logic is good.",non_debt,-
hbase,19775,comment_6,The updated version of the patch. Moved getCause before individual command handler. As for the UT - honestly speaking I'm not sure how to check that upper-level shell commands handled exception correctly.,non_debt,-
hbase,19775,comment_8,"rubocop/rubylint stuff looks benign enough for us. I think that's fine. It looks like this happens when loading the next value from ResultScanner errors out. I can't think of a good way to trigger this either. Looks like this came in via HBASE-17141 which is 2.0 only. +1 from me. , may I land this on branch-2?",non_debt,-
hbase,19775,comment_9,+1 for branch-2. Thanks for asking. Thanks for patch,non_debt,-
hbase,19775,comment_10,"Thanks for the patch, Sergey!",non_debt,-
hbase,19815,summary,Flakey,test_debt,flaky_test
hbase,19815,description,Saw the below in flakies failures Seems to be highest failing incidence in branch-2.,non_debt,-
hbase,19815,comment_0,.001 Fix cast failure.,non_debt,-
hbase,19815,comment_2,I pushed this to master and branch-2. Lets see if it fixes it.,non_debt,-
hbase,19815,comment_3,I fixed classcastexception but test still fails. Here the test fails on the commit that includes the patch attached here!,non_debt,-
hbase,19815,comment_5,"The random assigns lock up when hbase:meta fails, we schedule a crash, and then try to assign meta again but its 'owned' already by initial assign. The original assign will not run because it fostered the crash so is waiting for it to complete before it goes on. Seems like an issue in AMv2. Happens one in four times (this is the test that fails the most).",non_debt,-
hbase,19815,comment_6,seems to be our not cancelling the failed assign. We do this if it user-space region  see handleRIT in  but we do not seem to do similar if a failed hbase:meta assign because we go a different route via Patch coming...,non_debt,-
hbase,19815,comment_7,To be clear 2.001 has been committed. It cleans up the Cast error reported above. We then reopened this issue to address the fact that this test is still failing. .001 adds handleRIT from SCP to This is first cut. Will see if I can have SCP and RMP share a method but contexts are different...,non_debt,-
hbase,19815,comment_9,.002 Retry. We picked up wrong patch?,non_debt,-
hbase,19815,comment_12,Deleted 2.001 so hadoopqa won't keep picking it up.,non_debt,-
hbase,19815,comment_14,Resolving. Pushed .003 to branch-2 and master. Hopefully this does it. The test found a real issue (hurray for randomized tests). Will keep an eye on it.,non_debt,-
hbase,19823,summary,Make,non_debt,-
hbase,19823,description,Discussion happened in HBASE-19773 Follow from here:,non_debt,-
hbase,19823,comment_0,"Ping , .",non_debt,-
hbase,19823,comment_2,v2 LGTM,non_debt,-
hbase,19823,comment_3,Thanks for the review. Pushed to branch-2 and master.,non_debt,-
hbase,19862,summary,Fix - fake is not of type,non_debt,-
hbase,19862,description,"We have temporary (added in HBASE-19007) and concept of CoreCoprocessors which require that whichever they get, it should also implement This test builds mock RegionCpEnv for TokenProvider but it falls short of what's expected and results in following exceptions in test logs Patch adds the missing interface to the mock. Also, uses Mockito to mock the interfaces rather the crude way.",design_debt,non-optimal_design
hbase,19862,comment_0,Ping .,non_debt,-
hbase,19862,comment_2,Will cleanup checkstyles on commit. Ping  since you reviewed the related change too.,code_debt,low_quality_code
hbase,19862,comment_3,+1. This will be pushed to both master and branch-2 right?,non_debt,-
hbase,19862,comment_4,"Pushed to master and branch-2. Forgot to make checkstyle changes in initial commit, so made an addendum and pushed it. Also added Timeout Rule so that the test obeys our category based timeout rules. Thanks for the review .",non_debt,-
hbase,19866,summary,doesn't timeout,non_debt,-
hbase,19866,description,"So reading around junit docs looks like the reason is result of these two rules: - @Test(timeout=X) applies only on the test function, and not on whole test fixture (@After, @Before, etc) - Timeout rule applies on whole test fixture just has and no Timeout rule unlike we have in so many other tests. The test method, in the logs I have, runs in less then 60 sec. So it meets the timeout specified in @Test annotation. However, we get stuck in tearDown, and since there is no Timeout rule, it keeps on running until surefire kills the JVM after (set to 900 sec). Let use the ""Timeout"" rule instead of *However, note that this won't solve the root cause of hangup.* It'll just make the test fail neatly rather than getting stuck and requiring surefire plugin to kill the forked JVMs (see HBASE-19803).",non_debt,-
hbase,19866,comment_0,"As for the root cause, i see that backup master is not stopping. is backup HMaster thread And i think it's stuck on NettyRpcServer? Ping",non_debt,-
hbase,19866,comment_1,Is there a full log? I can take a look.,non_debt,-
hbase,19866,comment_2,Pushed just the timeout change to branch-2 and master. Feel free to keep using this jira for further debugging.,non_debt,-
hbase,19866,comment_3,Deleted my local logs. But i downloaded them from Nightly master. Build Artifacts Look in any run where it fails,non_debt,-
hbase,19866,comment_4,I need one hour to download the test_logs.zip... The network of my company sucks... Let me try it at home...,non_debt,-
hbase,19866,comment_6,"There is a dead lock. NettyRpcServer.stop is synchronized, and in the method, we will wait until all the connections to be closed. And one of the event loop thread calls RpcServer.authorize which is also synchronized, so it is blocked there and never returns back since we have already locked when calling The master thread. The event loop thread.",non_debt,-
hbase,19866,comment_7,Ping . I think we can use a different lock for authManager?,non_debt,-
hbase,19866,comment_8,A simple patch to eliminate the dead lock. Actually we do not need to lock here but the is implemented in hadoop so we can not modify it... It uses two volatile field to hold the entries which makes the upper layer must protect it with a lock... What a pity. Maybe we can use our own implementation in the future. Thanks.,non_debt,-
hbase,19866,comment_10,Any concerns? Thanks.,non_debt,-
hbase,19866,comment_11,Lgtm,non_debt,-
hbase,19866,comment_12,Lgtm,non_debt,-
hbase,19866,comment_13,The lock for authManager was introduced by,non_debt,-
hbase,19866,comment_14,"Pushed to master and branch-2. Thanks  for digging, and thanks all for reviewing.",non_debt,-
hbase,19866,comment_16,This is a nice fix. I think the authManager lock preceded netty rpc?,non_debt,-
hbase,19939,summary,and are failing with NPE,non_debt,-
hbase,19939,description,Error is: Exception from the output file: Value of 'htd' is null as it is initialized in the constructor but when the object is deserialized its null.,non_debt,-
hbase,19939,comment_1,"The flakey-finder fingered the above commit as breaking the split test. Here is when the split test went bad... Unstable Build #1372 (Feb 5, 2018 3:53:40 PM) add description Build Artifacts Changes HBASE-19703 Functionality added as part of HBASE-12583 is not working (detail) Started by timer This run spent: 3 min 39 sec waiting in the queue; 27 min building on an executor; 31 min total from scheduled to completion. Revision: Test Result (6 failures / +3) See how the commit is ""HBASE-19703 Functionality added as part of HBASE-12583 is not working (detail)""",test_debt,flaky_test
hbase,19939,comment_2,Thanks... . Nice finding. patch LGTM,non_debt,-
hbase,19939,comment_3,"is already in flaky list so the QA didn't run it for HBASE-19703.- -Ok, It is rather than I misunderstood the test name due to the topic...Let me correct the test name for the topic. The correct test name is rather than",test_debt,flaky_test
hbase,19939,comment_4,The latest QA in HBASE-19703 is shown below. The is already in flaky.  Do you intent to fix the test totally? I'm +1 to your patch even if the is still flaky.,test_debt,flaky_test
hbase,19939,comment_5,Thanks for correcting the name ! Let me fix the commit description as well and resubmit the patch. After this fix if the test is still flaky then I will take another look at it.,test_debt,flaky_test
hbase,19939,comment_8,Pushed to branch-2 and master. Thanks,non_debt,-
hbase,19944,summary,Fix timeout,non_debt,-
hbase,19944,description,h3. Error Message test timed out after 60000 milliseconds h3. Stacktrace test timed out after 60000 milliseconds,non_debt,-
hbase,19944,comment_0,1,non_debt,-
hbase,19944,comment_2,+1. The compile error is in hbase-thrift which is not related.,non_debt,-
hbase,19944,comment_3,"Test locally, there is no compile error. Let me help committing.",non_debt,-
hbase,19944,comment_4,Pushed to master and branch-2.,non_debt,-
hbase,19969,summary,Improve fault tolerance in backup merge operation,design_debt,non-optimal_design
hbase,19969,description,"Some file system operations are not fault tolerant during merge. We delete backup data in a backup file system, then copy new data over to backup destination. Deletes can be partial, copy can fail as well",design_debt,non-optimal_design
hbase,19969,comment_0,Patch v1. cc:,non_debt,-
hbase,19969,comment_2,"Add backup after 'Get' I don't think the above passes checkstyle Do you want to implement in this JIRA ? I don't think the above is right - we use org.slf4j Is the change to public for testing ? Drop commented out code. If we get into the if block, the rename() call below would fail, right ? Drop commented out code. Please address checkstyle, findbugs warnings. Will continue reviewing.",code_debt,low_quality_code
hbase,19969,comment_3,"I looked at the calls to where return value is wrapped by Path. It would be cleaner if returns Path. Can you add javadoc to the above method ? It is easier to understand what the two Paths are for. The method throws IOException. If newPath doesn't exist, throwing IOException is better. Unused variable.",documentation_debt,low_quality_documentation
hbase,19969,comment_4,Would be nice to see some javadoc on the methods added to Feels like this should be its own utility method. I think Ted covered all of the other stuff I noticed.,documentation_debt,low_quality_documentation
hbase,19969,comment_5,One more comment about the new test : You can store the return value from in a variable. The count would not change in between the log and the assertion. I looped the following tests locally which passed:,design_debt,non-optimal_design
hbase,19969,comment_6,"Patch v2 addresses your comments,",non_debt,-
hbase,19969,comment_7,"As I said previously, if the delete() call fails, the rename() would fail as well. I think IOE can be thrown for both failures. is no longer in the patch. Can you describe related changes ?",non_debt,-
hbase,19969,comment_8,Done. It is now,non_debt,-
hbase,19969,comment_9,Patch v3.,non_debt,-
hbase,19969,comment_10,: Do you want to take another look at v3 ?,non_debt,-
hbase,19969,comment_11,"Looks like my comments were addressed. +1 from me if you're happy with it, Ted.",non_debt,-
hbase,19969,comment_14,Vlad: Can you address checkstyle warnings ?,code_debt,low_quality_code
hbase,19969,comment_15,Patch v4 moves the static import to the beginning of TestBackupMerge.,non_debt,-
hbase,19969,comment_17,Looking at : The relative order of junit import is exactly the same as that in TestBackupMerge.,non_debt,-
hbase,19969,comment_18,"Thanks for the patch, Vlad. Thanks for the review, Josh.",non_debt,-
hbase,19969,comment_20,", looks like you might have some flaky tests on Hadoop3. Would be good to take a quick look to rule out test issues (the cnxn refused sounds like it might be just be the node itself). Logs are at",test_debt,flaky_test
hbase,19969,comment_21,Josh: See HBASE-20123. Looks like we would need hadoop 3.1.0+ (or 3.0.2+) to make backup tests fully working.,architecture_debt,using_obsolete_technology
hbase,19969,comment_22,"Thanks for the pointer, Ted, but HADOOP-15289 was also backported to 3.0.2. We don't need 3.1.0..",non_debt,-
hbase,19969,comment_23,"Seems like Hadoop3 issue,  - not backup. How am I supposed to fix this?",non_debt,-
hbase,19969,comment_24,"Well, in this case, it looks like you don't have to do anything. Ted has already done the necessary.",non_debt,-
hbase,19977,summary,FileMmapEngine allocation of byte buffers should be synchronized,non_debt,-
hbase,19977,description,"Recently we have been testing bucket cache with mmap mode. We found that after the multi threading way of allocating the byte buffers for offheap bucket cache, the creation of the mmapped byte buffers needs to be synchronized for allocating the right sized mmapped buffers.",non_debt,-
hbase,19977,comment_0,An AtomicInteger is enough?,non_debt,-
hbase,19977,comment_1,Yes . Using AtomicInteger will solve the problem. Will test this in a cluster.,test_debt,lack_of_tests
hbase,19977,comment_2,Patch for branch-2 and master. The integer 'pos' is now AtomicInteger.,non_debt,-
hbase,19977,comment_3,1,non_debt,-
hbase,19977,comment_4,1,non_debt,-
hbase,19977,comment_5,1,non_debt,-
hbase,19977,comment_6,Pushed to master and branch-2. Thanks for all the reviews.,non_debt,-
hbase,19991,summary,lots of hbase-rest test failures against hadoop 3,non_debt,-
hbase,19991,description,"mvn clean test -pl hbase-rest [ERROR] 106, 95, Errors: 8, Skipped: 1",non_debt,-
hbase,19991,comment_0,"This is failing due to loading jersey-1 classes via hadoop in the hadoop-3 configuration. This patch is my WIP, but I don't see anything jersey-1 left in dependency:tree report.",non_debt,-
hbase,19991,comment_1,"careful debugging revealed a few more places where the jersey deps were leaking in. I also need to add a comment somewhere that upgrading to jersey 2.26 (from our current 2.25.1) will likely need an upgrade to jetty 9.4, so should be done with great care, as that was one of the things i tried here and it didn't work as well as I thought it would",code_debt,low_quality_code
hbase,19991,comment_2,These changes made tests pass but {{mvn package && bin/hbase-daemon.sh start rest}} still failed,non_debt,-
hbase,19991,comment_3,"although starting the rest server outside of the test context fails for me before the patch as well, so I'd rather deal with that in a follow on",non_debt,-
hbase,19991,comment_4,"v2: accidentally included a dep change that caused license issues. looks like we might not need it anyway. ,  - you lads run into any of this? care to review?",non_debt,-
hbase,19991,comment_7,+1 That list of exclusions doesn't look like it was figuring.,non_debt,-
hbase,19991,comment_8,"pushed. thanks for review, stack",non_debt,-
hbase,19991,comment_10,"Sigh, apparently this broke against h3. Will reopen for the time, need to figure out what's going on here. I'm thinking we won't be able to have the same classpath for rest server as we do for MR applications, which is ok but a whole lot more work.",non_debt,-
hbase,19991,comment_11,easier to handle in follow on issues,non_debt,-
hbase,19998,summary,Flakey,test_debt,flaky_test
hbase,19998,description,"This is a good one. Its a timeout and though it has lots of test methods, the problem is one of them gets stuck. The test method kills a RegionServers then starts a new one. Usually all works out fine but the odd time there is this unexplained MOVE that gets interjected just as starts up. hbase:meta gets stuck (perhaps this is what is being referred to here: It is trying to run the MOVE by first unassigning from the server that has just crashed. It never succeeds. Need to fix this. Need to figure where these Move operations are coming from too. Let me add some debug. See here how we are well into and then two MOVEs cut-in... for hbase:meta and for namespace: Here is the failure of the unassign: ... and we keep trying: Test set: 2, 0, Errors: 2, Skipped: 0, 600.83 s <<< FAILURE! - in 583.188 s <<< ERROR! test timed out after 600 seconds at 583.227 s <<< ERROR! Appears to be stuck in thread",non_debt,-
hbase,19998,comment_0,.001 is some debug I pushed....to master and branch-2.,non_debt,-
hbase,19998,comment_1,Debug turned up the culprit 'mover': Its the facility added over in HBASE-18494 that is cutting in. In this case two RegionServers are killed and a new one started. This HBASE-18494 facility cuts in w/ its moves though is taking care of it. There is that and then the failure to give up if on unassign we timeout against the remote server.  FYI.,non_debt,-
hbase,19998,comment_2,".001 Only call if a node has been added. Do not call it if only one regionserver in cluster. Make it so is scheduled before it. Add logging if was responsible for MOVE (Previous was a mystery when it cut in). Undo debug added by previous patch. Previously, we'd call when there was a i.e. any flux in cluster membership. These nodeChildrenChanged notifications happen before nodeDeleted. If a server crashed, could run first, find the server that had not yet registered as crashed, find system tables on it and then try to move them. It would fail because server would not respond to RPC when we go to unassign. The region move would then suspend waiting on the to wake it up when done processing but this move had locked the region so SCP couldn't run.... As it was, even if the move succeeded, it could make for dataloss since it could skirt SCP WAL splitting.",non_debt,-
hbase,19998,comment_4,Could we add some commit message to remind readers that the commit is for debug? It can help readers to realize that the flaky hasn't been fixed.,test_debt,flaky_test
hbase,19998,comment_5,My mistake. I thought I had. Will do in future sir.,non_debt,-
hbase,19998,comment_6,I pushed the patch to master and branch-2. Leaving issue open to see if it fixes this interesting phenomenon.,non_debt,-
hbase,19998,comment_7,"The test failed in flakies a few times last night, lets see how it does.",test_debt,flaky_test
hbase,19998,comment_9,This is falling off the flakies list. Resovling as fixed.,non_debt,-
hbase,19998,comment_10,"OK, arrive here when implementing HBASE-20708. I wonder when will we call nodeCreated? I think nodeCreated will only be called when you call exists on an inexistent path on zk. But here, for we will only set watchers on the existent children. And obviously, we can not know what is the node name for a new RS, so it is impossible to get a server added notification through nodeCreated...",non_debt,-
hbase,20072,summary,remove 1.1 release line from the prerequisite tables,non_debt,-
hbase,20072,description,remove references to branch-1.1 releases from the prerequisite table for JDK and Hadoop versions.,non_debt,-
hbase,20072,comment_1,We don't have a test suite for the book other than building it and we don't have an automated check for building the book. :/,test_debt,lack_of_tests
hbase,20072,comment_2,filed HBASE-20077 for the lack of a test for building the book. that jira also has a command you can run locally if you want to quickly check the result of this change.,test_debt,lack_of_tests
hbase,20072,comment_3,bump. can I get a review here?,non_debt,-
hbase,20072,comment_4,lgtm,non_debt,-
hbase,20072,comment_5,"we either need a follow on issue for adding the 1.4.x line to the same table or can do it in this issue, don't have strong preference which",non_debt,-
hbase,20072,comment_6,there's already HBASE-20026 for the 1.4 release line.,non_debt,-
hbase,20072,comment_7,pushed to master. waiting on branch-2,non_debt,-
hbase,20072,comment_8,"This is good for branch-2 now. +1. If another RC, I'd want this in it.",non_debt,-
hbase,20072,comment_9,pushed to branch-2,non_debt,-
hbase,20100,summary,flakey,test_debt,flaky_test
hbase,20100,description,"Failed in the nightly in interesting way. A subprocedure of enable table is assigning regions. The test is doing a kill of the procedure store to ensure we can handle all state transtions even in face of failure. In this case, the kill of PE framework came in the finish of the assign procedure AFTER we'd updated hbase:meta and the internal AM state moving Region from OPENING to OPEN. Rerunning this step on restoration of the AMv2 WAL Store results in....",test_debt,low_coverage
hbase,20100,comment_0,"For posterity, here is the interesting part of the log... Below is a filter on region name only. We start from a store restart loading up the content of the meta table...",non_debt,-
hbase,20100,comment_1,".001 Allow that region may already be in the OPEN state. Allow OPEN as a possible state when we update region transition state. Usually state is OPENING but if crash before finish step is completed, on replay, master may have read that the state is OPEN from meta table and so will think it open.... Small fix.",non_debt,-
hbase,20100,comment_4,Linked to a follow-on issue. The AssignProcedure is too coarsely grained (feedback from  that makes sense). HBASE-20103 is about breaking it up to do finer steps. Will do in follow-on.,design_debt,non-optimal_design
hbase,20100,comment_5,This is not on the flakies list. A nightly did fail subsequent to this patch going in so we'll back here I think. Resolving this in the meantime.,non_debt,-
hbase,20108,summary,`hbase zkcli` falls into a non-interactive prompt after HBASE-15199,non_debt,-
hbase,20108,description,"HBASE-15199 pulls the jruby-complete jar out of the normal classpath for commands run in HBase. Jruby-complete bundles jline inside. ZK uses jline for its nice shell-like usage. The problem is that this uncovered a bug where we're not explicitly bundling a version of jline to make sure that {{hbase zkcli}} actually works. As long as we're expecting {{zkcli}} to be there, we should provide jline on the classpath to make sure the users get a real cli. Thanks to  for getting to the bottom of it quickly.",non_debt,-
hbase,20108,comment_0,We exclude jline from the zookeeper dependency. I think we just need to undo that..,non_debt,-
hbase,20108,comment_1,could we only include jline when zkcli is invoked? hate to add another runtime dependency that's only used in one place.,non_debt,-
hbase,20108,comment_2,Can try to do that!,non_debt,-
hbase,20108,comment_4,".002 (re) introduces jline in {{lib/zkcli}} and then adds all JAR files from {{lib/zkcli}} to the {{CLASSPATH}} when {{hbase zkcli}} is invoked. Something like what you were thinking, ?",non_debt,-
hbase,20108,comment_5,"looks like jline shows up in the modules now. the rest of the approach looks really good, though! I assume this works both in dev tree and out of a bin tarball?",design_debt,non-optimal_design
hbase,20108,comment_6,exactly the approach I was thinking,non_debt,-
hbase,20108,comment_7,Blargh. Thanks for catching! Tested a local bin-tarball. How do you run out of the dev tree? I always end up building a tarball if I want to launch something.,non_debt,-
hbase,20108,comment_8,After {{mvn package}} you can run {{bin/hbase}} directly. We generate some classpaths at and then dev-tree classpaths are built in,non_debt,-
hbase,20108,comment_9,"Ahh, thanks. Yes, this also works. Pinged Mike offline and he confirmed that he was looking at v1. v2 doesn't have this. +1 from you folks after QA?",non_debt,-
hbase,20108,comment_10,"Do we need to update hbase.cmd? Maybe a follow on issue for discussing the level of support that it needs? Testing against h2 and h3? I suspect there's no difference, and don't need it before commit, but a statement on what you've done would be good. Needs RN.",non_debt,-
hbase,20108,comment_11,"Yup, just tested H3. No issues (as expected). Looks like it, but I don't know how to test that one. Since Stack has asked for silence on branch-2, I'll put up a new patch with some copy-paste from elsewhere in hbase.cmd which I think will work.",non_debt,-
hbase,20108,comment_14,1,non_debt,-
hbase,20108,comment_15,"Thanks, Mike! FYI , this would be a good one if you re-roll another beta2 RC. Otherwise, I'll wait for your all-clear to commit to branch-2.",non_debt,-
hbase,20108,comment_16,Added RN. Users on previously release can workaround this by doing something like the following:,non_debt,-
hbase,20108,comment_18,", in case you're keeping a list, would like to land this one for 2.0.0 when permitted. Thanks.",non_debt,-
hbase,20108,comment_19,+1 on commit to branch-2 and branch-2.0. Thanks,non_debt,-
hbase,20108,comment_20,"Thanks, sir. Pushing to those two branches now. Adding my omitted Signed-Off-By on those branches (forgot it on Master, sorry Mike).",non_debt,-
hbase,20127,summary,Add UT for serial replication after failover,non_debt,-
hbase,20127,description,None,non_debt,-
hbase,20127,comment_1,"Forget to update variable seqId to be the latest seq id in ? The UT looks good to me, +1 if Hadoop QA says OK after fix the bug above.",non_debt,-
hbase,20127,comment_3,Let me commit.,non_debt,-
hbase,20127,comment_4,Pushed to master. Thanks  for reviewing.,non_debt,-
hbase,20145,summary,HMaster start fails with when HADOOP_HOME is set,non_debt,-
hbase,20145,description,It is observed that HMaster start is failed when HADOOP_HOME is set as env while starting HMaster. HADOOP_HOME is pointing to Hadoop trunk version. The same configs is working in HBase-1.2.6 build properly.,non_debt,-
hbase,20145,comment_0,Here is the tail log trace,non_debt,-
hbase,20145,comment_1,Do you have erasure coding enabled on that directory?,non_debt,-
hbase,20145,comment_2,"I haven't explicitly set any configurations for Hadoop. Hadoop cluster is running with default configurations. For HBase, I have set basics configuration such as _hbase.rootdir_ and",non_debt,-
hbase,20145,comment_3,"The meaning for this exception is that, the FSDataOutputStream which is used for logging does not support hflush/hsync, which is not acceptable since calling hflush/hsync on the stream does not mean the data is persistent. This works for 1.2.6 just because we do not have the check in the code base of 1.2.6, but this can lead to data loss. This usually happens when if the file system is not HDFS, for example, S3FileSystem. And IIRC there is a issue for deploying HBase with EC enabled. For a OutputStream which supports EC then hflush/hsync will not work. And we haven't tested with hadoop trunk version yet. No sure what is the real problem. Thanks.",non_debt,-
hbase,20145,comment_4,Hi I'd like to work on it and see if the Hadoop trunk has a regression. Thanks.,non_debt,-
hbase,20145,comment_5,"It doesn't seem to reproduce for me  tried hadoop trunk + hbase master, hadoop 3.0.0-beta1 + hbase 2.0.0-beta1, hadoop trunk + hbase 2.0.0-beta1. Neither reproduced.  could you share your hadoop and hbase configuration? Also, do hdfs ec -getPolicy -path / hdfs ec -getPolicy -path /hbase See if what the output looks like for both directories. I would actually think it makes sense to add additional check to see if the file system directory is erasure coded, and log extra message to avoid confusion.",non_debt,-
hbase,20145,comment_6,"Thanks  for trying out. I have found the root cause for the failure. Did you build HBase-2.0.0-beta1 from source or directly used hbase*.tar.gz which is available in mirrors? Btw, I have answered in stack over flow question, see . This should help",non_debt,-
hbase,20145,comment_7,"Thanks  now I understand why: Turns out that if you point env HADOOP_HOME to a Hadoop3 path with hbase2.0 convenience binary, the classpath will contain both Hadoop 2.7.4 and Hadoop 3 jar files. Effectively causing class conflicts. Since Hadoop 3 jars are available, HBase is able to query StreamCapabilities of the output stream. However, because Hadoop 2.7.4 jars is loaded before Hadoop 3 jars in classpath, it loaded the class implemented in Hadoop 2.7.4, which does not inherit from StreamCapabilities. This is not a problem in the code itself. But the code can be improved to log a warning message when output stream class does not inherit from StreamCapabilities.",design_debt,non-optimal_design
hbase,20145,comment_8,I just got the same issue and can confirm that it's mixing jars on the classpath the key issue for me was omission of as the stackoverflow article by  specifies. I only used,non_debt,-
hbase,20145,comment_9,"Posted a patch to log additional messages when 1. the stream class does not implement StreamCapabilities, 2. the result of StreamCapabilities test. Log the output stream class and the associated wrapped stream class. The 2nd item will be useful when troubleshooting hbase problem on a HDFS erasure coded cluster.",code_debt,low_quality_code
hbase,20145,comment_11,posted comments on reviewboard,non_debt,-
hbase,20145,comment_12,I *think* this is something that gets addressed via the bring-your-own hadoop clients that we make now.  WDYT?,non_debt,-
hbase,20145,comment_14,IIRC all of the server processes still get the classpath they always did. so probably this same error will happen if you point HADOOP_HOME somewhere and don't remove the hadoop jars that ship with your hbase installation.,non_debt,-
hbase,20145,comment_15,"What to do here? There are outstanding comments up in RB. Would be good to address. I like the idea of improving the logging hereabouts. Otherwise, we resolve this as not-a-problem because it is the old classic of mixed jars on CLASSPATH (but hopefully the improved logging will help user figure the problem easier?)",design_debt,non-optimal_design
hbase,20150,summary,Make branch-2.0 from branch-2 for 2.0.0 release,non_debt,-
hbase,20150,description,Create branch from which we will cut 2.0.0.,non_debt,-
hbase,20150,comment_0,Pushed branch-2.0 at,non_debt,-
hbase,20241,summary,does not work,non_debt,-
hbase,20241,description,In the shell and splitormerge_switch do not work.,non_debt,-
hbase,20241,comment_0,lgtm,non_debt,-
hbase,20241,comment_2,I checked rubocop and ruby-lint errors but I'm only changing the package. Is the patch good to go regardless the errors?,non_debt,-
hbase,20241,comment_3,Yes,non_debt,-
hbase,20241,comment_4,"Pushed to branch-2.0, branch-2 and master. Thanks for the review Ted!",non_debt,-
hbase,20302,summary,CatalogJanitor should log the reason why it is disabled,non_debt,-
hbase,20302,description,"when catalogJanitor is disabled it has various condition for which it can be true to disable CatalogJanitor and sends this message ""CatalogJanitor disabled! Not running scan."" Since this is an async thread, it is difficult to identify what is the exact reason for it disable. We could log all conditions alongwith disabled! Not running scan."") for better debugging.",design_debt,non-optimal_design
hbase,20302,comment_0,Fix was already there in branch-2 and above. porting it to branch-1 with minor changes.,non_debt,-
hbase,20302,comment_2,"+1, committing shortly. Thanks for the patch!",non_debt,-
hbase,20302,comment_3,Pushed to 1.2 and up. 1.2 and 1.3 needed a tiny change. Thanks for the patch,non_debt,-
hbase,20434,summary,Also remove remote wals when peer is in DA state,non_debt,-
hbase,20434,description,"Consider we have two clusters in A and S state, and then we transit A to DA. And later we want to transit DA to A, since the remote cluster is in S, we should be able to do it. But there are some remote wals on the HDFS for the cluster in S state, so we need to remove them first before transiting the cluster in DA state to A.",non_debt,-
hbase,20434,comment_0,"Talked with  offline, a better way is always remove the remote wal when the wal name is for a sync replication peer, no matter whether it is in A or DA. Add we need to add UTs to confirm it.",non_debt,-
hbase,20434,comment_1,Review board link:,non_debt,-
hbase,20434,comment_3,+1. just 1 comment on rb.,non_debt,-
hbase,20434,comment_4,Pushed to branch HBASE-19064 after adding the suffix check. Thanks  for reviewing.,non_debt,-
hbase,20453,summary,Shell fails to start with SyntaxError,non_debt,-
hbase,20453,description,"SyntaxError: syntax error, unexpected tDOT .map { |i| } ^ require at (root) at require at (root) at require at (root) at",non_debt,-
hbase,20453,comment_0,"Reopenend HBASE-20276 instead, duping this",non_debt,-
hbase,20487,summary,Sorting table regions by region name does not work on web UI,non_debt,-
hbase,20487,description,Table regions on {{table.jsp}} cannot be sorted by Name column.,non_debt,-
hbase,20487,comment_1,"Have you tried the patch and observed working sorting ? If so, +1",non_debt,-
hbase,20487,comment_2,"Yes, the sorting works now.",non_debt,-
hbase,20487,comment_4,"Thanks for the patch, Balazs",non_debt,-
hbase,20595,summary,Remove the concept of 'special tables' from rsgroups,non_debt,-
hbase,20595,description,"Regionserver groups needs to specially handle what it calls ""special tables"", tables upon which core or other modular functionality depends. They need to be excluded from normal rsgroup processing during bootstrap to avoid circular dependencies or errors due to insufficiently initialized state. I think we also want to ensure that such tables are always given a rsgroup assignment with nonzero servers. (IIRC another issue already raises that point, we can link it later.) Special tables include: * The system tables in the 'hbase:' namespace * The ACL table if the AccessController coprocessor is installed * The Labels table if the coprocessor is installed * The Quotas table if the FS quotas feature is active Either we need a facility where ""special tables"" can be registered, which should be in core. Or, we institute a blanket rule that core and all extensions that need a ""special table"" must put them into the 'hbase:' namespace, so the test will return TRUE for all, and then rsgroups simply needs to test for that.",design_debt,non-optimal_design
hbase,20595,comment_0,"The issue I was thinking of was HBASE-20500 which maintains one server in the 'default' group, but that is not the full scope of what we should have. We should guarantee the placement, specifically, of ""special tables"" into a rsgroup that must always have a nonzero number of servers, and not assume that will be the 'default' group. In fact I think we should have two default rsgroups, very similar to how we do namespacing: a ""default"" group into which goes all user level stuff not otherwise specified; and a system group into which goes system/special tables (in namespace terms, akin to the 'hbase' namespace). Special tables should not be allowed to move into rsgroups for user tables.",design_debt,non-optimal_design
hbase,20595,comment_1,"that makes sense. is the ""system group"" treated like a normal rsgroup in terms of exclusivity? If so, how do we handle it and the user group needing to be distinct? if we just delegate it to manual operator config that means we can't have the rsgroup on by default ever right? we'll probably need to document ""can't turn on rsgroup feature"" as a limitation of single-node deployments.",documentation_debt,outdated_documentation
hbase,20595,comment_2,"Ah, no, better that the constraint that system tables cannot be placed into the same rsgroup as user tables be what is optional, and allow single node deployments to put everything into ""default""",non_debt,-
hbase,20595,comment_3,"This is what I am thinking as far as how we achieve removal of the 'special tables' concept. Special == system. I haven't looked at all of the places where we create such tables though to determine if it is a compat problem, a move of the table to the system namespace. Pretty sure the security coprocessors are fine. Quotas is only in trunk so that would be ok too. Anyway, any objections to this?",non_debt,-
hbase,20595,comment_4,Never mind what I said above about another special rsgroup for system tables. I think we just need to set a flag if a rsgroup has a system table in it and guarantee that group always has at least one server. That works for a wider range of cases including the single node case.,non_debt,-
hbase,20595,comment_5,"Dropping a WIP patch for branch-1 that is even simpler than above discussion. We avoid a deadlock at initialization time by using the default group to deploy system tables. Later, when balancing, if the deploy to default at bootstrap has misplaced the table the assignment will be adjusted. Also, if a table has not yet been assigned a group during balancing we place it into the default group. This is equivalent to how ""special tables"" were formerly added to the default group in the in-memory representation only. HBASE-20500 maintains at least one server always in the default group. RSGroup tests pass for me locally with this change.",non_debt,-
hbase,20595,comment_7,Updated master patch removes the unused imports that checkstyle complained about.,code_debt,low_quality_code
hbase,20595,comment_9,This should also fix HBASE-20606,non_debt,-
hbase,20595,comment_10,"Test failure looks like the usual issue with running the RSgroup tests in parallel. Precommit ran 4 of them at once, one of them timed out. I don't think these should all be run in parallel like that on underpowered test resources.",non_debt,-
hbase,20595,comment_11,I am running 25 times locally just to make sure this isn't an intermittently visible problem. After 10 runs it looks good.,non_debt,-
hbase,20595,comment_12,1,non_debt,-
hbase,20595,comment_13,"Because this fixes a bug where tables that don't even exist can appear in rsgroup listings, I pushed this to all branches that include the rsgroups feature, including 2.0. /cc  It's an experimental feature so I think that is fine but let me know if you want a revert.",non_debt,-
hbase,20669,summary,[findbugs] autoboxing to parse primitive,non_debt,-
hbase,20669,description,HBASE-18864 introduced a change that's been flagged by findbugs. example on branch-1:,non_debt,-
hbase,20669,comment_0,Would this work? My intellij editor says the boxing/unboxing is not necessary though.,non_debt,-
hbase,20669,comment_1,should just use,non_debt,-
hbase,20669,comment_2,Silly me. Thanks Sean! Learned a new thing today. Quickly checked the hbase codebase (branch-1) and did not find any other statements of the same pattern,non_debt,-
hbase,20669,comment_4,+1 This is concerning. I'll run it locally. we should track that this is off in a jira so it can get fixed. This doesn't fit scope for a new or changed test since it just changes an implementation detail and not the outcome.,non_debt,-
hbase,20669,comment_5,pushed to branches-1. thanks !,non_debt,-
hbase,20669,comment_11,Thanks Sean!,non_debt,-
hbase,20693,summary,Refactor rest and thrift jsp's and extract header and footer,non_debt,-
hbase,20693,description,"Log Level page design was changed to include header and footers in HBASE-20577. Since, thrift and rest do not have header and footer jsp's, the log level page will be as it were before HBASE-20577 i.e without the navigation bar. This JIRA will refactor rest and thrift and extract 'header.jsp' and 'footer.jsp' from them.",non_debt,-
hbase,20693,comment_0,FYI.,non_debt,-
hbase,20693,comment_1,"This patch takes care of point 1. Point 2 is optional. I think we should do it as its always a good idea to break up jsp s into header/footers. Also as the rest/thrift UIs grow more complex, we will have to do it someday. Keeping it open for now, will do it if asked for.",design_debt,non-optimal_design
hbase,20693,comment_2,lgtm,non_debt,-
hbase,20693,comment_3,Hi I have removed the patch and moved it to HBASE-20577 as addendum. In this JIRA I will refactor the jsp's of rest and thrift.,non_debt,-
hbase,20693,comment_4,"Attached which refactors {{rest.jsp}} and {{thrift.jsp}} and extracts header and footer out of them. Also, fixed following typo in thrift.jsp. Please review. Ping , , .",documentation_debt,low_quality_documentation
hbase,20696,summary,Shell list_peers print useless string,code_debt,low_quality_code
hbase,20696,description,Interested contributors are welcome to fix this bug...,non_debt,-
hbase,20696,comment_0,The same problem for list_peer_configs,non_debt,-
hbase,20696,comment_1,"That's IRB, the Ruby interactive shell that underlies the hbase shell. Please see the example of configuring your local .irbrc file in our reference guide if you want to change it: Please use the mailing list to discuss configuration questions.",non_debt,-
hbase,20696,comment_2,"Got it , Thanks",non_debt,-
hbase,20787,summary,Rebase the HBASE-18477 onto the current master to continue dev,non_debt,-
hbase,20787,description,None,non_debt,-
hbase,20787,comment_0,I will also remove the various commits/reverts of the initial patch to simplify things.,code_debt,low_quality_code
hbase,20787,comment_1,Did a force push to clean the branch up.,non_debt,-
hbase,20787,comment_2,Rebasing again to pull in fixes for unit tests.,non_debt,-
hbase,20823,summary,Wrong param name in javadoc for,documentation_debt,low_quality_documentation
hbase,20823,description,"{{@param regionSizeStore}}, but actual name is regionSizes",non_debt,-
hbase,20823,comment_0,Hey ! I wanted to put in a patch for this one and a few other HBase tickets I found. I am a Hadoop contributor but not yet a contributor for HBase,non_debt,-
hbase,20823,comment_1,Please feel free to take it.,non_debt,-
hbase,20823,comment_2,"ping , can you help grant  contributor.",non_debt,-
hbase,20823,comment_3,"Ok, thanks . I sent a message [to I'll upload the patch as soon as I get elevated",non_debt,-
hbase,20823,comment_4,Looks like it was done. Shout if you need anything else sir.,non_debt,-
hbase,20823,comment_6,1,non_debt,-
hbase,20823,comment_7,Pushed to master branch. Thanks for your contribution,non_debt,-
hbase,20825,summary,Fix pre and post hooks of CloneSnapshot and RestoreSnapshot for Access checks,non_debt,-
hbase,20825,description,"found this issue during the testing. Steps to repro: 1) create a test namespace 2) grant 'CA' permission on that namespace to test user 3) access hbase shell as that test user, and create a table in that namespace 4) load some data into the test table 5) create a snapshot as test user for that test table 6) disable table 7) as test user, try restoring the snapshot onto test table restore_snapshot command for the test user requiring to have global admin access",non_debt,-
hbase,20825,comment_0,"Nice catch, Just tested out your fix locally and it works for me. +1 on the code change. Let me look to see if there's an easy test we can add to catch this in the future (I doubt it, high level).",non_debt,-
hbase,20825,comment_2,+1 from me as well. I checked hbase-1 code which is correct.,non_debt,-
hbase,20825,comment_3,", I think you want this for branch-2.0. , you want on branch-2.1?",non_debt,-
hbase,20825,comment_4,"Test failure related? Otherwise, +1 for branch-2.0. Thanks.",non_debt,-
hbase,20825,comment_5,"Nah, actually looked related to those RTRP issues we been chasing.",non_debt,-
hbase,20825,comment_9,"+1 for branch-2.1. And for the failed UT, let me open a issue for it. I've added a parameter for the constructor of MRP to skip the preflightChecks when it is used for reopening a region, but seems forgot to actually use it in the implementation...",non_debt,-
hbase,20975,summary,Lock may not be taken or released while rolling back procedure,non_debt,-
hbase,20975,description,"Find this one when investigating HBASE-20921, too. Here is some code from executeRollback in You can see my comments in the code above, reuseLock can cause the procedure executing(rollback) without a lock. Though I haven't found any bugs introduced by this issue, it is indeed a potential bug need to fix. I think we can just remove the reuseLock logic. Acquire and release lock every time. Find another case that during rolling back, the procedure's lock may not be released properly: see comment:",code_debt,multi-thread_correctness
hbase,20975,comment_0,"+1 on removing it for now. We have optimized too much before getting things correct... Let's keep the logic simple first. Also, there are some bad style issues. At least let's remove the space between 'stackTail' and '--', it looks like '-- And do not do assignment in the condition block of if. Let's change to",code_debt,low_quality_code
hbase,20975,comment_1,"Thanks,  I will change those style issues in the patch too.",non_debt,-
hbase,20975,comment_4,Please include a small change in hbase-server module so we can run more tests?,test_debt,low_coverage
hbase,20975,comment_5,Included a trivial change in hbase-server to trigger UT,non_debt,-
hbase,20975,comment_8,"The failed UTs are related, upload a patch to fix it. My fix reveals a bug which exists all the time... the acquire and release lock logic in procedure is a bit of mess here... For different procedures, we have holdLock() set to true of false, for procedures which holdLock() == true, we need to override hasLock() method to test whether we have the lock... when executing procedure, we acquire the lock and release it immediately. And when the procedure is success, we release the lock again in It works, since for procedure with holdLock = true, we call releaseLock() with force = false the first time, the lock won't release here, but in where we call releaseLock() with force = true, the lock will be released there. For procedure with holdLock = false, it also works since when the first time we call releaseLock() with force = false, the lock will release, and the second time we call relseaseLock in we only release it lock if procedure with holdLock=true. So releasing lock won't called twice But when rolling back, we acquire the lock but only release the lock for procedure with holdLock = true... *Rolling back for procedures with holdLock=false(e.g. most of the Table Procedures) will never release the lock...*",design_debt,non-optimal_design
hbase,20975,comment_10,Not fully understand but I trust your judgement. +1. Mind adding the above comments in code so that others will not try to add the wrong code back later for optimization?,non_debt,-
hbase,20975,comment_11,1,non_debt,-
hbase,20975,comment_12,"Rebasing, and trigger a run in branch-2.1. We need to fix it in branch-2.1 too. Otherwise, there will be some strange log:",non_debt,-
hbase,20975,comment_14,"Pushed to branch-2.0, branch-2.1, branch-2 and master, thanks all for reviewing!",non_debt,-
hbase,20990,summary,One operation in procedure batch throws an exception will cause all receive the same exception,non_debt,-
hbase,20990,description,"In AMv2, we batch open/close region operations and call RS with executeProcedures API. But, in this API, if one of the region's operations throws an exception, all the operations in the batch will receive the same exception. Actually, some of the operations in the batch is executing normally in the RS. I think we should try catch exceptions respectively, and call remoteCallFailed or remoteCallCompleted in respectively. Otherwise, there will be some very strange behave. Such as this one: The AssignProcedure failed with a what??? It is very strange, actually, the AssignProcedure successes on the RS, another CloseRegion operation failed in the operation batch was causing the exception. To correct this, we need to modify the response of executeProcedures API, which is the proto, to return infos(status, exceptions) per operation. This issue alone won't cause much trouble, so not so hurry to change the behave here, but indeed we need to consider this one when we want do some reconstruct to AMv2.",design_debt,non-optimal_design
hbase,20990,comment_0,"I prefer not returning anything when calling executeProcedure, instead, using and to send back the response... But the code is a bit complicated as we have done lots of work to be compatible with 1.x RS(and finally the solution is to upgrade RS first so the code is useless...)",non_debt,-
hbase,20990,comment_1,"Then you need to record the exceptions in the memory and send them back to master when reporting. The sync RPC call become a async one, what if the RS restarts before sending this info. The procedure in master even don't know whether the open/close procedure is executing, whether a RPC retry is needed.",design_debt,non-optimal_design
hbase,20990,comment_2,"If the RS restarts then it is the turn for SCP. And in fact, if the RS is dead, in the sync call you can do nothing either, you will get a connection refuse or some other exceptions, and it is not clear that whether this is a RS crash or a network error. SCP is the only way to change the target server. And if the rpc call is successfully returned then we can make sure that the procedure is executing at RS side, just in a background thread pool. It is the duty for the RS to make sure that you need to report back to master the result, unless you are dead.",non_debt,-
hbase,20990,comment_3,Not a problem any more I think. We use executeProcedures to schedule multiple assign/unassign procedures now.,non_debt,-
hbase,21103,summary,nightly test cache of yetus install needs to be more thorough in verification,non_debt,-
hbase,21103,description,"branch-1.2 nightly failed because it couldn't find yetus: walking through steps, we checked and thought we had an existing install: So we stashed and then tried to use it. Examining the workspace present before the stash shows the directory we look for was present, but the contents were garbage: we should probably check for an executable that will successfully give us a version or something like that.",non_debt,-
hbase,21103,comment_1,1,non_debt,-
hbase,21109,summary,hbase won't start,non_debt,-
hbase,21109,description,"HBASE won't start and gives a Java error when I try in Java 9 but seems to start in Java 8 The error is: export JAVA_HOME=/usr bin/start-hbase.sh script Error: A JNI error has occurred, please check your installation and try again Exception in thread ""main"" 64 Method) Method) Method) Error: A JNI error has occurred, please check your installation and try again Exception in thread ""main"" 64 Method) Method) Method) running master, logging to Error: A JNI error has occurred, please check your installation and try again Exception in thread ""main"" 64 : running regionserver, logging to : Error: A JNI error has occurred, please check your installation and try again : Exception in thread ""main"" 64 : at : at : at : at : at : at : at : at more Error: A JNI error has occurred, please check your installation and try again Exception in thread ""main"" 64 JarFile.java:1017) at a:399) 80) at dex.java:114) ve Method)h.java:386)h.java:376) at ve Method) at assPath.java:375) at assPath.java:352) at ClassPath.java:218) ve Method) :419) at erHelper.java:585) at",non_debt,-
hbase,21109,comment_0,Thanks for reporting this issue! In the future you should first address these kinds of problems to the user@hbase mailing list so we can confirm there's a problem with HBase. JIRA is used in our project to track specific tasks / fixes as they progress. Here's a web-viewable version of the list: Please note that [per our guidelines on JDK the HBase community recommends you not use Java 9. (edited after I reread my text and it seemed harsher than I intended.),non_debt,-
hbase,21160,summary,Assertion in is ignored,non_debt,-
hbase,21160,description,From (HBASE-21138 QA run): Here is related code:,non_debt,-
hbase,21160,comment_0,What about change Throwable to IOException ? what is you,non_debt,-
hbase,21160,comment_1,"Since the catch block currently re-throws IOException, that means the catch block is no longer needed. Please run with the change locally before attaching patch. Thanks",design_debt,non-optimal_design
hbase,21160,comment_2,Hi I found so many re-throws blocks in the file of . Should we resolve it all?,design_debt,non-optimal_design
hbase,21160,comment_3,"If there is no assertion in the try block where Throwable is caught, you don't need to change.",non_debt,-
hbase,21160,comment_4,Hi code review link is here,non_debt,-
hbase,21160,comment_5,"As I said above, when there is no assertion at the end of try block, you don't need to make change. Please also keep the try-with-resources structure which releases resource.",non_debt,-
hbase,21160,comment_6,Ran the test with patch locally which passed.,non_debt,-
hbase,21160,comment_7,Areadly has run with commd : And result is:,non_debt,-
hbase,21160,comment_9,Can you address these ?,non_debt,-
hbase,21160,comment_13,"Thanks for the patch, liubang.",non_debt,-
hbase,21167,summary,Master killed after IOE in on log roll,non_debt,-
hbase,21167,description,Logging this in case we see it again. I had a Master working furiously. It had assigned over 400k regions on startup. Then this happened which knocked the hard-working server out:,non_debt,-
hbase,21167,comment_0,In general I think we will retry many times when rolling a WAL?,non_debt,-
hbase,21167,comment_1,Thanks for taking a look. This is MasterProcWal. Maybe we don't. I'll take a look.,non_debt,-
hbase,21167,comment_2,Haven't seen this in a while.,non_debt,-
hbase,21173,summary,Remove the duplicate HRegion#close in TestHRegion,code_debt,duplicated_code
hbase,21173,description,"After HBASE-21138, some test methods still have the duplicate HRegion#close.So open this issue to remove the duplicate close",code_debt,duplicated_code
hbase,21173,comment_0,mind taking a look at it ?Thanks,non_debt,-
hbase,21173,comment_1,I think the intention of HBASE-21138 is to let do the cleanup. Can you remove the duplicate region.close() call in these subtests ? Thanks,code_debt,duplicated_code
hbase,21173,comment_2,"Thanks for working on this JIRA, . In previous discussion, I thought calling on closed/null region is no harm, while deleting the duplicate close may make some tests unhappy. So we were not very strict to make region close only once. Good discussion to revisit this. - The last one in was added after HBASE-21138 and can be removed here. - The in was followed by the assertion to verify that the .regioninfo file is still there. I saw the close-and-assertion happens in the same test method multiple times so I was not sure we could remove the close here. - In {{testSequenceId}} and the pattern in this patch, i.e. ""{{region.close() && region = null}}"", is not correct. The reason is that, it makes the in {{teardown()}} a no-op, leaving WAL not closed. One fix is to not set the null value and leave the test as-is; a better one I think is as suggested, we can replace the {{region.close()}} with and set {{this.region}} null value. - Other places to set {{this.region}} null value after is good to explicitly make the in {{tearDown}} a no-op.",design_debt,non-optimal_design
hbase,21173,comment_4,"Attach 002 patch as  suggestions.Thanks {{testSequenceId}} - In line 269, Replace region.close() with - In line 285, we need to verify that the value of is consistent before and after region.close(), so we keep region.close() and replace it with - In line 315, replace region.close() with - In line 317, remove duplicate and set this.region to null - In line 578, replace region.close() with and set this.region to null - In line 951, replace region.close() with - In line 1083, replace region.close() with - In line 1281, set this.region to null - In line 1281, set this.region to null - In line 4175, keep region.close() and set region to null as said by Mingliang Liu - In line 6234, remove region.close() Other places where set this.region null value after will be fine as said by .",code_debt,duplicated_code
hbase,21173,comment_6,"+1 (non-binding) Thanks for updating the patch, .",non_debt,-
hbase,21173,comment_7,1,non_debt,-
hbase,21173,comment_8,"Pushed to master and branch-2. 'branch-1' is a little different from master branch, let me generate another patch for it.",non_debt,-
hbase,21173,comment_10,"Pushed to branch-1, branch-1.4 and branch-1.3.Thank  for review.:)",non_debt,-
hbase,21238,summary,shouldn't call System.exit,non_debt,-
hbase,21238,description,Correct way of handling error condition is through return value of run method.,design_debt,non-optimal_design
hbase,21238,comment_0,"lgtm, pending QA",non_debt,-
hbase,21238,comment_2,"Thanks for the patch, Artem.",non_debt,-
hbase,21269,summary,"Forward-port to branch-2 "" HBASE-21213 [hbck2] bypass leaves behind state in RegionStates when assign/unassign""",non_debt,-
hbase,21269,description,A bunch of this patch does not apply to branch-2 and master now we don't have AP or UP anymore. Need to figure if we need override in branch-2 and master. Let me upload the forward-port done so far. Can finish this when move to branch-2.2 exercise. FYI,non_debt,-
hbase,21269,comment_0,Can I take this one? I am currently working on how to make the HBCK works for TSRP.,non_debt,-
hbase,21269,comment_1,@jingyoun tian that would be great.,non_debt,-
hbase,21269,comment_4,Question for you sir if you have a sec up on rb... Its about making Procedure async. Thanks.,non_debt,-
hbase,21269,comment_7,Failed UTs are not related to my patch. Could you help commit this patch?,non_debt,-
hbase,21269,comment_8,Will do. Let me rerun to see...,non_debt,-
hbase,21269,comment_10,Fixed check style problem..,non_debt,-
hbase,21269,comment_12,Pushed to branch-2 and master. Thank you for the nice forward port .,non_debt,-
hbase,21269,comment_13,Thanks for all your help.,non_debt,-
hbase,21370,summary,Revamp or remove OfflineMetaRepair,non_debt,-
hbase,21370,description,See It describes the OfflineMetaRepair tool. Does this work in hbase2? This issue is about removing or fixing it up so it works again.,non_debt,-
hbase,21370,comment_0,Resolving as duplicate. 5f9fe0c HBASE-22680 [HBCK2] OfflineMetaRepair for hbase2/hbck2 (#6) Above added offlinemetarepair to hbck2.,non_debt,-
hbase,21378,summary,[hbck2] add --skip version check to hbck2 tool (checkHBCKSupport blocks assigning hbase:meta or hbase:namespace when master is not initialized),non_debt,-
hbase,21378,description,"When I encounter the scenario that hbase:namespace is not online. Then I tried to assign it manually, but it throws Then I check the code and found it is because of checkHBCKSupport(), I assign hbase:namespace successfully by skipping this check. Thus I think the tool need an option to skip this check.",non_debt,-
hbase,21378,comment_0,"sir, please check this out.",non_debt,-
hbase,21378,comment_1,ping,non_debt,-
hbase,21378,comment_2,"The below was supposed to address this case. Did you have it in the hbase you were playing with? 4ad63d77be HBASE-21345 [hbck2] Allow version check to proceed even though master is 'initializing'. That said, this change looks like it could come in handy. I think the -s/--skip should be a general option rather than an option per command since every command calls check hbck/version before it runs? Thanks for working on this.",non_debt,-
hbase,21378,comment_3,"yeah, your issue should fix the problem. My code that time may not be the latest. So should I still work on this? -skip may be useful in the future.",non_debt,-
hbase,21378,comment_4,I think --skip may be useful. Maybe work on this as a low-priority item?,non_debt,-
hbase,21378,comment_5,Sure. It's not a big work. I'll submit a patch later.,non_debt,-
hbase,21378,comment_6,The skip option is added as a general option now.,non_debt,-
hbase,21378,comment_7,Pushed on the repo. Thanks for the nice patch,non_debt,-
hbase,21495,summary,Create 2.0.3 Release,non_debt,-
hbase,21495,description,Push a 2.0.3 release. Nightlies look pretty good:,non_debt,-
hbase,21495,comment_0,"* Tagged as 2.0.3RC0 (Pushed later): {{$ git tag -s 2.0.3RC0 -m ""Tag first 2.0.3 RC""}} * Ran the make_rc.sh script. Failed for me on mac osx (HBASE-21513). Did it on linux vm instead (need to make a dockerfile that takes in gpg keys... and settings.xml...) * Tried out the product; made sure I could build from src tarball and checked out bin tarball layout, ran it, loaded it, checked data loaded. * Signed it * Generated compat report: --annotation rel/2.0.2 2.0.3RC0 * Checked out release up on in staging.. then closed the repo. * When testing the release, it looked like it was slow doing pe sequentialRead so compared it to 2.0.1. They are about the same. * Pushed 2.0.3RC0 tag to remote * Sent out VOTE email.",non_debt,-
hbase,21495,comment_1,"* Vote passed * Moved the rc from dev to release: * Released up in * Release in JIRA * Update Whimsey: * Give the release a 'release' tag. Checked out 2.0.3RC0 and then tagged it as ""rel/2.0.3"" and pushed to origin: commit (HEAD, tag: rel/2.0.3, tag: 2.0.3RC0) * Move on the current release version from 2.0.3 to 2.0.4-SNAPSHOT (as subtask). * Add release to the website by editing to add in the new release. Will send announcement after website updates.",non_debt,-
hbase,21495,comment_2,"Resolving. The download page has mention of 2.0.3 so sent the announce from apache address to user@, dev@, and announce@.",non_debt,-
hbase,21502,summary,Update SyncTable section on RefGuide once HBASE-20586 is committed,non_debt,-
hbase,21502,description,SyncTable [refguide currently mentions limitation to run it on different kerberos realm. HBASE-20586 is ongoing to resolve this problem. This jira is to make sure RefGuide is updated accordingly once HBASE-20586 is resolved.,non_debt,-
hbase,21502,comment_0,Attached patch with updates to SyncTable section.,non_debt,-
hbase,21502,comment_2,thanks Wellington!,non_debt,-
hbase,21525,summary,clone_snapshot from hbase2.0.0 to hbase1.2.0,non_debt,-
hbase,21525,description,When I use to copy snapshot from hbase2.0.0 to hbase 1.2.0 and then use to restore the table; There are no errors on client and hbase master. but I can not scan any data in this new table. So when I search the log in regionserver which that region running on .I find the below error log,non_debt,-
hbase,21525,comment_0,"this issue seems only on cdh branch1.2.0, I will test on open source version",non_debt,-
hbase,21525,comment_1,"Ok, actually have solved this case.",non_debt,-
hbase,21525,comment_2,Closing this as a duplicate of HBASE-16189,non_debt,-
hbase,21535,summary,Zombie Master detector is not working,non_debt,-
hbase,21535,description,"We have thread in HMaster which detects Zombie Hmaster based on _and halts if set _true_. After HBASE-19694, HMaster initialization order was correted. Hmaster is set active after Initializing ZK system trackers as follows, But Zombie detector thread is started at the begining phase of During zombieDetector execution will be false, so it won't wait and cant detect zombie master.",non_debt,-
hbase,21535,comment_0,"We should revert back below code changes done in HBASE-19694, Or start zombieDetector after HM become active? Ping @Stack  .. Please provide your opinion.",non_debt,-
hbase,21535,comment_1,Move the creation of the zombie detector after we set the activeMaster flag to true?,non_debt,-
hbase,21535,comment_2,"Yeah..this would be fine. Thanks , will upload the patch.",non_debt,-
hbase,21535,comment_3,"Uploaded the patch, please review.",non_debt,-
hbase,21535,comment_6,Retry,non_debt,-
hbase,21535,comment_7,All failed test cases are passing locally.,non_debt,-
hbase,21535,comment_9,"Stack Sir, QA report is fine, we can go for it now. Thanks :)",non_debt,-
hbase,21535,comment_10,"All looks good. Pankaj, can you add an email to JIRA or else put up a version of the patch that includes your email.... You could use the tool or just git format-patch... Thank you sir.",non_debt,-
hbase,21535,comment_11,"My bad, sorry I missed somehow in the master branch patch. Attached V2, Kindly review. Thank you :)",non_debt,-
hbase,21535,comment_13,TestLockManager is passing locally.,non_debt,-
hbase,21535,comment_14,I pushed to master branch. I tried cherry-pick back but the zombieMaster init is in a different location in branch-2. You want to move its location in branch-2 or just leave stuff as is  ? Thanks!,non_debt,-
hbase,21535,comment_15,Thank you  Sir. This fix is applicable for branch-2/2.1/2.0 as well. Have already attached kindly apply,non_debt,-
hbase,21535,comment_17,Sir....Shall we push this to branch-2.x as well and close this Jira.,non_debt,-
hbase,21535,comment_18,ping,non_debt,-
hbase,21535,comment_19,1,non_debt,-
hbase,21535,comment_20,Pushed to branch-2+. Thanks  for contributing.,non_debt,-
hbase,21535,comment_25,Thanks  and  for reviewing and committing the patch. :),non_debt,-
hbase,21589,summary,TestCleanupMetaWAL fails,non_debt,-
hbase,21589,description,This test fails near all-the-time. Sunk two RCs. Fix. Made it a blocker.,non_debt,-
hbase,21589,comment_0,blocker on which versions boss?,non_debt,-
hbase,21589,comment_1,Sorry. Forgot versions. At least the two above.,non_debt,-
hbase,21589,comment_2,"It is very strange, it never failed in my environment. , can you upload an output or something, I can't find the failing test in jenkins or Flaky test board.",test_debt,flaky_test
hbase,21589,comment_3,can we get the output of {{mvn --version}} for folks that this fails for?,non_debt,-
hbase,21589,comment_4,I've got 26 consecutive passing runs so far.,non_debt,-
hbase,21589,comment_5,"fyi,",non_debt,-
hbase,21589,comment_6,"$ mvn --version Apache Maven 3.6.0 Maven home: Java version: 1.8.0_191, vendor: Oracle Corporation, runtime: Default locale: en_US, platform encoding: UTF-8 OS name: ""mac os x"", version: ""10.14"", arch: ""x86_64"", family: ""mac"" [INFO] T E S T S [INFO] [INFO] Running [ERROR] 1, 0, Errors: 1, Skipped: 0, 206.008 s <<< FAILURE! - in [ERROR] 206.005 s <<< ERROR! Shutting down Method) Caused by: Master not initialized after 200000ms ... 20 more [INFO] [INFO] Results: [INFO] [ERROR] Errors: [ERROR]  IO Shutting down [INFO] [ERROR] 1, 0, Errors: 1, Skipped: 0 Same for the below Sean. What you thinking sir? $ mvn --version Apache Maven 3.5.4 Maven home: Java version: 1.8.0_191, vendor: Oracle Corporation, runtime: Default locale: en_US, platform encoding: UTF-8 OS name: ""mac os x"", version: ""10.14"", arch: ""x86_64"", family: ""mac""",non_debt,-
hbase,21589,comment_7,It just feels like an edge case changed in he test environment. Can you grab the mini cluster logs from one of these failures? let me try grabbing 1.8u191. all 100 runs passed on 1.8u161,non_debt,-
hbase,21589,comment_8,Attached a log  (seems like different failure to one above but didn't look too close). Will try another JVM over here in morning.,non_debt,-
hbase,21589,comment_9,", the output you attached seems like a successful run?",non_debt,-
hbase,21589,comment_10,1.8u192-ea passed 100 runs. I'm going to try updating my maven version next.,non_debt,-
hbase,21589,comment_11,Maven 3.6.0 and Java 1.8u192-ea still passing. I guess it's only your logs to work from for me. :(,non_debt,-
hbase,21589,comment_12,".001 Changes the wait on the SCP finish from 10 seconds to 30 seconds. Odd is that in studying runs, it doesn't take 10 seconds to complete the SCP. I could see the test pass on occasion when it so happened that of the two RS in the test, after the move of the meta, all regions were on the RS that was NOT killed by the test but otherwise the waitFor on 10 seconds would fail.",non_debt,-
hbase,21589,comment_14,Pushed to branch-2.0+.  would be interested if you had any input sir. Let me roll new RCs now this is in.,non_debt,-
hbase,21599,summary,Fix findbugs and javadoc warnings from HBASE-21246,code_debt,low_quality_code
hbase,21599,description,and Pretty trivial stuff to clean up now.,code_debt,low_quality_code
hbase,21599,comment_0,FYI,non_debt,-
hbase,21599,comment_1,"LGTM , Thanks",non_debt,-
hbase,21599,comment_3,Thanks Ankit! Let me rebase and fix the checkstyle warnings.,code_debt,low_quality_code
hbase,21599,comment_5,"Pushed the v2 patch. Thanks for the review, Ankit!",non_debt,-
hbase,21678,summary,Port HBASE-20636 (Introduce two bloom filter type ROWPREFIX and to branch-1,non_debt,-
hbase,21678,description,None,non_debt,-
hbase,21678,comment_1,Test failures are unrelated. The checkstyle and whitespace issues reported are related. I didn't change formatting during the backport but can do so to make these tools happier. Let me do that,code_debt,low_quality_code
hbase,21678,comment_2,Adjusted some formatting in StoreFile. Let's see if that is better,code_debt,low_quality_code
hbase,21678,comment_4,"failure does look germane, checking",non_debt,-
hbase,21678,comment_5,Updated patch address checkstyle warnings and unit test failure,code_debt,low_quality_code
hbase,21678,comment_7,"Sigh, new checkstyle nits now that previous attempt is touching more files just to clean up checkstyle nits",code_debt,low_quality_code
hbase,21678,comment_8,Otherwise change looks good. I can fix the remaining checkstyle warns on commit. Would be good to get a +1 if you have any spare time to make a quick check   or original author,code_debt,low_quality_code
hbase,21678,comment_9,"This hasn't attracted any reviews and is not critical for inclusion in 1.5. I thought it would be a nice to have based on some Phoenix related discussion at my workplace, but even then it's not a must have. That's fine. Resolving this as Wont Fix.",non_debt,-
hbase,21731,summary,Do not need to use ClusterConnection in,non_debt,-
hbase,21731,description,We could just use the RegionLocator instead of,non_debt,-
hbase,21731,comment_2,1,non_debt,-
hbase,21731,comment_3,Pushed to branch-2.0+. Thanks  for reviewing.,non_debt,-
hbase,21754,summary,should be executed in priority executor,non_debt,-
hbase,21754,description,"Now, is executed in default handler, only region of system table is executed in priority handler. That is because we have only two kinds of handler default and priority in master(replication handler is for replication specifically), if the transition report for all region is executed in priority handler, there is a dead lock situation that other regions' transition report take all handler and need to update meta, but meta region is not able to report online since all handler is taken(addressed in the comments of But there is another dead lock case that user's DDL requests (or other sync op like moveregion) take over all default handlers, making region transition report is not possible, thus those sync ops can't complete either. A simple UT provided in the patch shows this case. To resolve this problem, I added a new to execute meta region transition report only, and all the other region's report are executed in priority handlers, separating them from user's requests.",non_debt,-
hbase,21754,comment_0,+1 on the approach.,non_debt,-
hbase,21754,comment_3,any comments on the patch? QA passed,non_debt,-
hbase,21754,comment_4,+1 on the general approach. Let me take a look at the patch.,non_debt,-
hbase,21754,comment_5,"Overall LGTM. Left a few comments on RB, if they are not problems then +1.",non_debt,-
hbase,21754,comment_6,", thanks for the review, I replied the comments, I think we'd better leave it for now, if no objection, I will commit this to branch-2.0+ later today.",non_debt,-
hbase,21754,comment_7,I havent seen the reply? Have you pushed the publish button?,non_debt,-
hbase,21754,comment_8,"Strange, I have published the comments, try again?",non_debt,-
hbase,21754,comment_9,"OK, +1.",non_debt,-
hbase,21755,summary,"RS aborts while performing replication with wal dir on hdfs, root dir on s3",non_debt,-
hbase,21755,description,"- _hbase.wal.dir_ : Configured to be on hdfs - _hbase.rootdir_ : Configured to be on s3 In replication scenario, while trying to get archived log dir (using method we get the following exception: Current code is: It considers root dir while we should use wal dir.",non_debt,-
hbase,21755,comment_0,Apparently the problem was fixed in master branch. Will take the changes in HBASE-21688 and provide a fix for branch-2. Or may be we can reopen HBASE-21688 and fix it there?  what do you suggest?,non_debt,-
hbase,21755,comment_1,"if we correct the jira information for HBASE-21688 I think that'd be fine. This should impact any release line that includes HBASE-17437, right?",non_debt,-
hbase,21755,comment_2,"Do I need to re-open HBASE-21688 ? Correct, it is going to impact all those releases.",non_debt,-
hbase,21755,comment_3,"Things like this tend to creep up because new features are added and contributors and reviewers either don't know to use the utility to get the WAL root directory or are missed in the review process. This feature, for example, looks to have landed around the same time as 17437 and was either missed in the review of 17437 or of this feature. Unfortunately there isn't a great way to ensure this won't happen again.... end rant :) Anyways, thanks for catching this! :)",non_debt,-
hbase,21755,comment_4,"All branches, including master still has the following (See HBASE-21688 somehow missed it.",non_debt,-
hbase,21755,comment_5,"Yes, this happened because I did search on only. Let me amend HBASE-21688.",non_debt,-
hbase,21755,comment_6,"HBASE-21688 amendment is ready for review. , , .",non_debt,-
hbase,21755,comment_7,", if this is being done via amendment of 21688, should this be closed as a duplicate?",non_debt,-
hbase,21755,comment_8,Right . Resolving this as duplicate.,non_debt,-
hbase,21755,comment_9,Thanks boss.,non_debt,-
hbase,21816,summary,Print source cluster replication config directory,non_debt,-
hbase,21816,description,"User may get confused, to understanding our HBase configurations which are loaded for replication. Sometimes, User may place source and destination cluster conf under ""/etc/hbase/conf"" directory. It will create uncertainty because our log points that all the configurations are co-located. Existing Logs, But it should be something like, This jira only to change the log-line, no issue with the functionality.",design_debt,non-optimal_design
hbase,21816,comment_2,Karthik asked me offline to assign this one to him. Added you to the contributors group. Happy developing :),non_debt,-
hbase,21816,comment_3,Why not just use the {{File confDir}} in the log message? (reverse the order of these calls),non_debt,-
hbase,21816,comment_4,"Thank you  for reviewing it. Yes, This should be a simple change to reverse it. I lined edge :)",non_debt,-
hbase,21816,comment_5,Please find the new patch.,non_debt,-
hbase,21816,comment_7,LGTM.  Can you please submit the patch by using This is a document for this script:,non_debt,-
hbase,21816,comment_9,Thanks . Let me commit.,non_debt,-
hbase,21816,comment_10,"Pushed to master, branch-2, branch-2.1 and branch-2.2. Thank you for contributing.",non_debt,-
hbase,21919,summary,tries to get table state before it is in meta,non_debt,-
hbase,21919,description,"Using the latest master branch. When creating a table, the following error could be seen in the master's log and it only happens when rsgroup is enabled.",non_debt,-
hbase,21919,comment_0,"The title of ""Do not try to get table status when it is not in meta yet"" is the possible cause I am not quit sure about. Will update it if the root cause is not as described.",non_debt,-
hbase,21919,comment_1,"calls and table state is checked before moving it. The is thrown by but caught So it does not abort the program. It might not a big deal since the exception being caught already, but the point is that the logic is not correct.",non_debt,-
hbase,21919,comment_2,"The root cause seems as follow: return MasterProcedureUtil nonceGroup, nonce) { @Override protected void run() throws IOException { newRegions); + "" create "" + desc); // TODO: We can handle/merge duplicate requests, and differentiate the case of // by saying if the schema is the same or not. // // We need to wait for the procedure to potentially fail due to ""prepare"" sanity // checks. This will block only the beginning of the procedure. See HBASE-19953. latch = submitProcedure( new desc, newRegions, latch)); latch.await(); newRegions); <-- postCreateTable is executed after latch is released } releaseSyncLatch() is called in Supposed to be called in A right way to go?",non_debt,-
hbase,21919,comment_3,"Ping    , please correct me if I get those wrong: calls latch.awaits after submitting a new but in the latch is released in the very first state of create table procedure. Does that means postCreateTable() could run when the subsequent states of create table procedure is running (from to",non_debt,-
hbase,21919,comment_4,"This is a known issue AFAIW, check this JIRA out HBASE-20690 This one is also related:HBASE-21693 #moveTables needs a revisit and probably some rework.",non_debt,-
hbase,21919,comment_5,Thanks Cang. I am studying HBASE-20690 and HBASE-21693.,non_debt,-
hbase,21919,comment_6,Close this JIRA as a dup of HBASE-20690. Continue the discussion on HBASE-20690.,non_debt,-
hbase,22017,summary,Master Fails to become active due to the data race bug in region server,non_debt,-
hbase,22017,description,"Test cluster: hadoop11(master), hadoop14(slave), haoop15(slave). before code execute at number), hadoop15 shutdown, then master startup fails",non_debt,-
hbase,22017,comment_0,"Looks like the Master crashed inside -- loading meta requires scanning the RS. And that's because the RS (hadoop15) was being shutdown. It looks like the RS scan has retry mechanisms, but if the RS was shutdown, it doesn't look like there's anything Master can do but to crash.",non_debt,-
hbase,22017,comment_1,"Hi: I was not very familiar with HBASE, so I have a question : We have two slaves, only one shutdown, but the master startup fails, is it a bug? Or it is a normal behavior! Thanks!",non_debt,-
hbase,22017,comment_2,"The one that was shutdown had hbase:meta table, which is the critical table and Master can't start without it.",non_debt,-
hbase,22017,comment_3,"So in this case, HBASE should move the table to the live node? Just like",non_debt,-
hbase,22017,comment_4,"After long time debug, I have found the reason of this bug. This is a total *data race bug*. While shutdown the RegionSever who hold the meta table, it will close the leases: I will give the patch soon.",non_debt,-
hbase,22017,comment_5,"HI:  Give the patch that fixes this bug and the logs that after fixing it, looks good.:D",non_debt,-
hbase,22017,comment_7,Removing the whitespace,code_debt,low_quality_code
hbase,22017,comment_9,"I think this would be better: And , should we handle LeaseException just like or and retry? If we do retry for LeaseException, then no need to fix this issue.",non_debt,-
hbase,22017,comment_10,"Hi: and  {code:java} Should we handle LeaseException just like or and retry? If we do retry for LeaseException, then no need to fix this issue.",non_debt,-
hbase,22017,comment_11,"By design we should try to reopen the scanner when we hit LeaseException, and I think this is what we have done for async client. Need to check the code for sync client. In general, I think this is a client side bug, we should not change the server side logic. Thanks.",non_debt,-
hbase,22017,comment_12,"Agree with , I will open another issue to fix the retry problem of LeaseException",non_debt,-
hbase,22017,comment_13,"hum, seems that no change of sever side can make its logic be simple. So should we fork a new issue to fix this problem? Or just fix it in this issue? Or there aleardy a issue track it? If we just fix it in this issue, due to I am not very be familiar with the client side logic, could any nice guy give the patch? I have make it be unassigned.",non_debt,-
hbase,22017,comment_14,"Hi  Great, could you plese tell me the new issue address? Thanks.",non_debt,-
hbase,22017,comment_15,Opened another issue HBASE-22047,non_debt,-
hbase,22028,summary,Deprecated method in thrift,non_debt,-
hbase,22028,description,As we have already deprecated the same methods in our Admin interface.,non_debt,-
hbase,22028,comment_0,comment says deprecated since 2.0.0.... is that possible given hbase 2.0.0 has shipped already but you are only adding deprecation now. Should it be deprecated since 2.1.4/2.2.0? Thanks. I submitted your patch so it gets a qa run.,non_debt,-
hbase,22034,summary,Backport HBASE-21401 and HBASE-22032 to branch-1,non_debt,-
hbase,22034,description,Branch-2 and master have good validation checks when constructing KeyValues. We should also have them on branch-1.,non_debt,-
hbase,22034,comment_0,Let me take this as I am committing HBASE-22032 already,non_debt,-
hbase,22034,comment_1,branch-1 patch posted to reviewboard as,non_debt,-
hbase,22034,comment_3,"Great, I munged some files so now I own their checkstyle and javadoc problems. :-/",code_debt,low_quality_code
hbase,22034,comment_4,Updated patch for checkstyle and javadoc warnings,code_debt,low_quality_code
hbase,22034,comment_6,Unit test failure is not related as far as I can tell. Does not reproduce locally. It looks like a flake in the precommit env. I can fix the one remaining checkstyle nit upon commit. (Indentation at,test_debt,flaky_test
hbase,22034,comment_7,"Mind taking a look if you have a sec This is something  worked on for later branches, now we are backporting it and the base commit to branch-1. Not essential but a nice to have sanity check.",non_debt,-
hbase,22034,comment_8,1,non_debt,-
hbase,22038,summary,fix building failures,non_debt,-
hbase,22038,description,"When building the hbase c++ client with Dockerfile, it fails, because of the url resources not found. But this patch just solve the problem in temporary, cos when some dependent libraries are removed someday, the failure will appear again. Maybe a base docker image which contains all these dependencies maintained by us is required in the long run.",build_debt,build_others
hbase,22038,comment_2,Dockerfile changes look good to me. What's up with removing all of the proto files? Is the expectation that we pull them directly from instead?,non_debt,-
hbase,22038,comment_3,"Yeah. As you see, the proto files will be copied from hbase-protocol instead of the right for now, but it cann't be compiled successfully even the urls are corrected. The error is that the protobuf files are inconsistent and duplicate for some messages.Discussed with , it seems that the is latest, so I changed the copy source dir from hbase-protocol to If so, it always keeps consistent between the client side and the server side, as the Java client. As you suggest, I agree that the native client is moved to another repo. In this situation, we should also keep them consistent in another way. That's another story.",non_debt,-
hbase,22038,comment_4,Yes. This are the protos we use internally and are what the java client uses to talk to the servers. This is what you need to use. They are also made with a much more recent version of pb (3.4 or something...) as opposed to 2.5 which is what the hbase-protocol files are generated with. Patch seems grand.,non_debt,-
hbase,22038,comment_5,"thanks to Stack.Your comment reminds me of of the protobuf version.In Dockerfile, the protobuf version is 2.7.0. I check the protobuf version in pom.xml of the module and it is 3.5.1. Let me upload another patch to change the protobuf version of the Dockerfile from 2.7.0 to 3.5.1 or other minor fix version.",non_debt,-
hbase,22038,comment_7,"+1 for now, but can you take on a follow-up Jira ? archive.a.o should not be used by build processes. www-us.apache.org is a load-balanced endpoint, whereas archive.a.o is a single service. We should just switch to newer versions of these libraries that still exist on www-us.apache.org. Would you be able to try that change out in a follow-on?",non_debt,-
hbase,22038,comment_8,"Thanks to Josh,sure, let me create a follow-up jira for that. update: the JIRA is HBASE-22069",non_debt,-
hbase,22038,comment_9,Pushed to HBASE-14850. Thanks  for contirbuting.,non_debt,-
hbase,22038,comment_10,"Thanks for the follow-on and for fixing this, ! Thanks for pushing, !",non_debt,-
hbase,22088,summary,Jenkins post-build step times out after HBASE-20522,non_debt,-
hbase,22088,description,The post Jenkins step timesout (takes 9 hours) after HBASE-22052 (the parent) was committed. Its hard to see why since don't have access to jenkins build box logs. I asked for access. Until it arrives going to mess around here. * What about HBASE-20522 made it so post step takes forever? * The post step is archiving and publishing an html report. At first I blamed the publishHMTL plugin but I think instead it the archiving step just before. HBASE-20522 made the build output way bigger? Let me get some signal....,non_debt,-
hbase,22088,comment_0,Hmm... Archive seems to work. I think I'm digging in wrong place... This is from the 'blue ocean' view. And in console view...,non_debt,-
hbase,22088,comment_1,I think HBASE-22059 may be what caused the shaded client to start failing its IT MR test. See parent for more on this exciting episode.,non_debt,-
hbase,22088,comment_2,Resolving as invalid. The actual issue was HBASE-22059 fixed by addendums in HBASE-22052.,non_debt,-
hbase,22174,summary,Remove error prone from our precommit javac check,design_debt,non-optimal_design
hbase,22174,description,As the result is not stable. Can add it back as a separated check later.,non_debt,-
hbase,22174,comment_0,"Let me push to branch HBASE-22174, and see whether it works.",non_debt,-
hbase,22174,comment_3,"Ah, a bit strange. Why we do not have the ""Re-executing"" message on github...",non_debt,-
hbase,22174,comment_5,No '-PerrorProne' which is good.,non_debt,-
hbase,22174,comment_11,"What can we do to get this enabled again in pre-commit? It's still enabled in nightly, where it's a little late. I would prefer that it's enabled everywhere or disabled everywhere.",non_debt,-
hbase,22174,comment_12,We can try newer version to see if it can give a stable result?,non_debt,-
hbase,22174,comment_13,"Oh, seems no newer version yet...",non_debt,-
hbase,22193,summary,Add backoff when region failed open too many times,non_debt,-
hbase,22193,description,"Now the default config is Integer.MAX_VALUE. The ITBLL failed to open the region as HBASE-22163 and retry 170813 to reopen. After I fixed the problem and restart master, I found it need take a long time to init the old procedure logs because there are too many old logs... Code in",code_debt,low_quality_code
hbase,22193,comment_0,"We set ASSIGN_MAX_ATTEMPTS to Int.Max because we can't let SCP fail, since failed, the region will remain unassigned forever(unless using HBCK2 tool to schedule a SCP again), as for the procedure log problem, I think we have considered that, before we changed ASSIGN_MAX_ATTEMPTS to In.Max, we asked  about whether purging the procedure log can work, it should not left so many un-deleted log here even lots of retries. Is it a bug here, ?",non_debt,-
hbase,22193,comment_1,"Talked with  offline, the problem here is not the retry number, but the retry interval. When a region is failed open, we will try to reassign it ASAP, the intention here is to make the region online soon. But sometimes, the region can not online on any RS because of config error or some other problems, then it is not a good idea to retry immediately as it will lead to so many proc wals... So the first thing is to detect this problem and increase the retry interval... And for a long term solution, I think we need to find out a way to better deal with config error. For now, the will hang there forever and the only way is to use HBCK2 to bypass the procedure and fix the table state, which is a bit difficult. For hbase version before 2.0, I think there is a straight forward way to fix this is to disable the table, fix the schema, and enable it again...",design_debt,non-optimal_design
hbase,22193,comment_2,"Is it a problem for Before HBase-2.2, retry backoff has been added to AssignProcedure.",non_debt,-
hbase,22193,comment_3,I think it is a good way to solve a online problem without HBCK2. Maybe we can let be able to break/bypass any procedure against this table or regions.,non_debt,-
hbase,22193,comment_4,"Yes, only for TRSP.",non_debt,-
hbase,22203,summary,Reformat DemoClient.java,non_debt,-
hbase,22203,description,The DemoClient.java currently uses a not consistent formatting and should be reformatted.,code_debt,low_quality_code
hbase,22206,summary,dist.apache.org must not be used for public downloads,non_debt,-
hbase,22206,description,"The dist.apache.org server is only intended for use by developers in staging releases. It must not be used on public download pages. Please use www.apache.org/dist (for KEYS, hashes and sigs) and the mirror system instead. The current download page has lots of references to dist.a.o; please replace thes.",documentation_debt,low_quality_documentation
hbase,22206,comment_0,please be specific on where you see a use of dist.a.o.,non_debt,-
hbase,22206,comment_1,or attach a patch.,non_debt,-
hbase,22206,comment_2,The 2.0.5 listing in {{downloads.xml}} links to d.a.o. The rest of the releases are good.,non_debt,-
hbase,22206,comment_3,Let's see if I still remember how to do this...,non_debt,-
hbase,22206,comment_5,+1 on the patch. The build failure can't be related. Do you remember how to commit  ?,non_debt,-
hbase,22206,comment_7,"the site failure is HBASE-22222. we can't build the site after this patch until that change goes in anyways, so we might as well wait for this fix until we can verify it.",non_debt,-
hbase,22206,comment_8,"Fine by me, I'll keep an eye on HBASE-22222 until then. Thanks for the heads up, .",non_debt,-
hbase,22206,comment_9,started a new precommit build,non_debt,-
hbase,22206,comment_11,+1 from me too. Let me know if you'd like me to push the commit Dima.,non_debt,-
hbase,22206,comment_12,pushed to master. Thanks !,non_debt,-
hbase,22213,summary,Create a Java based BulkLoadPartitioner,non_debt,-
hbase,22213,description,"We have a scala based partitionner, but not all projects are build in Scala. We should provide a Java based version of it.",non_debt,-
hbase,22213,comment_0,"3 things: 1) I'm unable to build hbase-connectors. 2) It might be doable to call the Scala BulkLoadPartioner directly from the Java code. Constructor is not useful but it should work 3) Below is a work Java version of it, with a useful easy to use constructor. Closing this Jira for now.",non_debt,-
hbase,22213,comment_1,Interested in why you can't do #1 might .,non_debt,-
hbase,22220,summary,Release,non_debt,-
hbase,22220,description,None,non_debt,-
hbase,22220,comment_0,pushed to dist.a.o: svn log entry: updated reporter.a.o with release date of today.,non_debt,-
hbase,22220,comment_1,"This is sweet: Any chance of adding the bin tgz there or at least adding a pointer? Lists the RCs as releases too... I'm not sure how this works. If you ain't looking at it , I can sir. Hurray for a release!",non_debt,-
hbase,22220,comment_2,"that's just a thing GitHub does for anything you tag in a repo. eg: * * IMHO, if we're going to add anything to these kinds of tags, it's pointers to the official artifacts. Anything else is likely to run afoul of ASF release policy.",non_debt,-
hbase,22220,comment_3,"Looking at protobuf, has nice release notes and bin artifacts listed (Yeah, does it for RCs too). Sure on pointing elsewhere for actual download:",non_debt,-
hbase,22220,comment_4,"sure, we could add our release notes pretty easily. the github ui for release annotations will render markdown. you can see how we do it over on YCSB for comparison: the issue for us as an ASF project is that policy prevents us from doing things that might encourage folks from outside of dev@hbase from downloading the source for a release from github.",non_debt,-
hbase,22220,comment_5,"Yeah, adding a release notes and binaries seems to be pretty easy. Should I do that?",non_debt,-
hbase,22220,comment_6,I think it'd be sweet. Make hbase-connector project look better and would encourage next RM to do similar. Thanks.,non_debt,-
hbase,22220,comment_7,Done.,non_debt,-
hbase,22220,comment_8,Thanks. Looks great.,non_debt,-
hbase,22225,summary,Profiler tab on Master/RS UI not working w/o comprehensive message,non_debt,-
hbase,22225,description,"As titled, when checking 1.5.0 RC3 binary package, clicking the ""Profiler"" tab on web UI, it complains page not found error like below:",non_debt,-
hbase,22225,comment_0,FYI.,non_debt,-
hbase,22225,comment_1,"After checking our there're some prerequisites to use this profiler servlet. However, not all users would check the book before clicking the button, so maybe a more comprehensive error message plus a link to the refguide is a better idea? Anyway I don't think this should be marked as a blocker any more.",documentation_debt,low_quality_documentation
hbase,22225,comment_2,Agreed on all counts. Error on the page that points to docs sounds wonderful and definitely something to improve for later rather than blocking the release.,non_debt,-
hbase,22225,comment_3,Thanks because i would close a blocker as wont fix. Come on. It works if you RTFM. Contributing to this project is more painful as time goes on. If you are unhappy about a cosmetic detail Improvement patches welcome!,non_debt,-
hbase,22225,comment_4,One option is to unconditionally register the servlet and have it put up an error message with a pointer to the refguide if its environmental prerequisites are not met. I can try a patch that does that. Back soon.,non_debt,-
hbase,22225,comment_5,Now we will present a simple error message that points to the relevant chapter of the book: !Screen Shot 2019-04-26 at 4.50.01 PM.jpg!,non_debt,-
hbase,22225,comment_6,"Since there haven't been any comments I am going to commit this trivial change to all branches the original commit landed, so I can make progress on 1.4.10 and 1.5.0 RCs.",non_debt,-
hbase,22225,comment_7,Pushed to all relevant places.,non_debt,-
hbase,22225,comment_18,+1 (belated). Just noticed the update and thanks for the efforts!,non_debt,-
hbase,22225,comment_19,another belated +1 from me. this is great!,non_debt,-
hbase,22228,summary,Remove deprecated ThrottlingException,non_debt,-
hbase,22228,description,ThrottlingException was deprecated in 2.0.0 and should be removed in 3.0.0.,code_debt,dead_code
hbase,22307,summary,Deprecated Preemptive Fail Fast,non_debt,-
hbase,22307,description,"Opened a discuss thread in dev & user mailing list but get no response, so I assume that there is no critical users for this feafure. And the problem we want to solve here is mainly the same with HBASE-16388, so I think user could make use of the config in HBASE-16388. Plan to deprecated the related classes and configs on branch-2, and the IA.Private classes will be removed in 3.0.0(on branch HBASE-21512 first), and the constants in HConstants class will be kept till 4.0.0, so we do not break the public API, although it is useless to config them on 3.0.0+.",non_debt,-
hbase,22307,comment_0,Pushed to master and branch-2. Thanks  for reviewing.,non_debt,-
hbase,22307,comment_1,Missed one IA.Public class.,non_debt,-
hbase,22307,comment_2,Pushed the addendum to branch-2+.,non_debt,-
hbase,22331,summary,Compile fails on hbase-spark-it module,non_debt,-
hbase,22331,description,On hbase-spark-it module the mvn install command fails with dependency issue if hbase-spark wasn't built previously. As a result precommit job fails if a file is changed only in hbase-spark-it.,non_debt,-
hbase,22331,comment_0,Fixed via HBASE-23059.,non_debt,-
hbase,22378,summary,HBase Canary fails with when table deleted during Canary run,non_debt,-
hbase,22378,description,"In 1.3.2 branch-1, we saw a drastic increase in thrown by HBase Canary. We traced the issue back to Canary trying to call isTableEnabled() on temporary tables that were deleted in the middle of the Canary run. In this version of HBase Canary, Canary throws (and then fails) if a table is deleted between admin.listTables() and function calls in RegionMonitor's sniff() method. Following the goal of which is to query all existing tables, in order to reduce noise we should skip over a table (i.e. don't check if it was enabled, or do anything else with it at all) if it was returned in listTables() but deleted before Canary can query it. Temporary tables which are not meant to be kept should not throw which fail the Canary. Patch in progress: Add a call to admin.tableExists() before tableEnabled() on line 1244 in",non_debt,-
hbase,22378,comment_1,"Thank you. The fix is good. +1 One more look into this brought me this question. Why do we delete tables while reading it? Turned out deletes tables when number of regions is smaller than (numOfServer * limit) Do you think we should serialize this test? I mean, only call it after sniff is done.",non_debt,-
hbase,22378,comment_3,"+1 Will commit to all applicable branches today.  Do you want to take a quick look at this patch too before I commit? (Since you were also involved with canary debugging a lot, thanks)",non_debt,-
hbase,22378,comment_4,"+1, the patch makes sense to me. Thanks  for the patch, and thanks !",non_debt,-
hbase,22378,comment_5,pushed to branch-1 branch-1.3 branch-1.4 branch-2 branch-2.1 branch-2.2 master,non_debt,-
hbase,22378,comment_14,This commit landed on all branches but Fix versions were set to None. I checked the branches and release dates and set it accordingly. The release notes for the affected releases are most probably incorrect.,documentation_debt,outdated_documentation
hbase,22400,summary,Remove the adapter code in async fs implementation for hadoop-2.7.x,non_debt,-
hbase,22400,description,None,non_debt,-
hbase,22400,comment_0,"Oh, have you started work? I've already done the work actually...",non_debt,-
hbase,22400,comment_1,Please ignore my RR. I was just fed up with the ugly adapter code and can't wait to remove them. :) Please continue.,design_debt,non-optimal_design
hbase,22400,comment_2,Strongly agree... That's why I open this issue even before I finish HBASE-22399...,non_debt,-
hbase,22400,comment_3,"The only thing is that, there are some deprecations field in DFSConfigKeys, but for 2.8.x, we haven't deployed the sources to maven... So I need to manually check the code to see what's happening here...",non_debt,-
hbase,22400,comment_4,Pushed to branch-2.2+. Thanks  and .,non_debt,-
hbase,22424,summary,Interactions in RSGroup test classes will cause and flaky,test_debt,flaky_test
hbase,22424,description,"When running rsgroup test class folder to run all the UTs together, and will flaky. Because TestRSGroupsAdmin1, TestRSGroupsAdmin2 and TestRSGroupsBalance are all extends TestRSGroupsBase, which has a static variable INIT, controlling the initialize of 'master 'group and the number of rs in 'default' rsgroup. Output errors of is shown in HBASE-22420, and will encounter NPE in because `master` group has not been added.",test_debt,flaky_test
hbase,22424,comment_1,"nice findings. Just curious, have you ran the flaky tests multiple times (such as 10) and all are passing?",test_debt,flaky_test
hbase,22424,comment_2,Hahaha...,non_debt,-
hbase,22424,comment_3,Please update the affect versions. 2.0 and 2.1 has the same problem?,non_debt,-
hbase,22424,comment_4,+1. Let me commit it later.,non_debt,-
hbase,22424,comment_5,2.0 2.1 didn't have this problem. Produced by HBASE-21265,non_debt,-
hbase,22424,comment_6,Pushed to branch-2.2+. Thanks  for contributing.,non_debt,-
hbase,22473,summary,Split TestSCP,non_debt,-
hbase,22473,description,It is time consuming.,non_debt,-
hbase,22473,comment_0,Pushed to branch-2.2+. Thanks  for reviewing.,non_debt,-
hbase,22529,summary,Sanity check for in-memory compaction policy,non_debt,-
hbase,22529,description,"Currently, if a column family is altered with an invalid in-memory compaction policy via {{Admin}} API, the regions for the table get stuck indefinitely. This patch adds a sanity check for the value so that such alter operation should not proceed.",non_debt,-
hbase,22529,comment_2,LGTM +1,non_debt,-
hbase,22529,comment_3,+1 for the v2 patch.,non_debt,-
hbase,22529,comment_4,It looks like the patch is needed to be rebased.,non_debt,-
hbase,22529,comment_6,Patch rebased to master.,non_debt,-
hbase,22529,comment_8,"The test failure in the laster QA is not related to this patch and when I ran the failed test locally, it was successful. +1, will commit this.",non_debt,-
hbase,22529,comment_9,"Pushed to master, branch-2 and branch-2.2. Thank you for the contribution!",non_debt,-
hbase,22570,summary,test-util.sh failed when executing,non_debt,-
hbase,22570,description,HBASE-16982 added a unit test with neither Catogory anatation nor ClassRule static member. Then testing with script test-util.sh will be stopped throwing out,non_debt,-
hbase,22570,comment_0,"The fix looks good. Please re-submit the patch by ""Submit Patch"", in that way, hadoop-QA will run tests for you automatically. thanks! Also please fix the formatting here:",non_debt,-
hbase,22570,comment_1,thanks. new patch uploaded.,non_debt,-
hbase,22656,summary,[Metrics] Tabe metrics 'BatchPut' and 'BatchDelete' are never updated,non_debt,-
hbase,22656,comment_0,"Checked, affect all versions.",non_debt,-
hbase,22656,comment_1,+1 (non-binding) Nice catch. The two method and are never used.,code_debt,dead_code
hbase,22656,comment_2,1,non_debt,-
hbase,22656,comment_3,"Thanks sir, and Mingliang! Pushed to branch-1, branch-1.4, branch-2, branch-2.1, branch-2.2, master.",non_debt,-
hbase,22707,summary,[HBCK2] MasterRpcServices assigns method should try to reload regions from meta if the passed regions isn't found under AssignmentManager RegionsStateStore,non_debt,-
hbase,22707,description,"Although HBCK2 related, this is a master side improvement. On situations where regions are missing in META, any online fix tool such as the one being implemented in HBASE-22567 would require a further master restart to get RegionsStateStore reloaded from META, so that master can be aware of the newly re-added regions. After regions are re-added to meta in CLOSED state, it should be possible to bring those by simply invoking hbck2 _assigns_ command. But before submits an _Assign_ procedure, it validates first if the given region is available on The current patch reloads meta on if the given region is not found on the first lookup, then try a new lookup again before giving-up on region assignment.",non_debt,-
hbase,22707,comment_0,"Interesting. We need something like this. I like that you hook it into assigns. I'm wary though of re-use of joinCluster. The messaging in logs will look strange. Will say stuff like 'Joining cluster...' and 'Waiting for RegionServers to join;....'. Then we re-add chores, do the unnecessary wait on RS. Should we just add a method that gets the new row in hbase:meta and then does what call does?",design_debt,non-optimal_design
hbase,22707,comment_1,"Yeah, using _joinCluster_ was a bit too much of an attempt of reusing existing code without changing it as much as possible, but I agree with all the side effects you had pointed out, . Had done some refactoring to re-use some of the adding a with the changes. Let me know what you think, if you feel this PR approach is fine, I will work on some UTs for it.",design_debt,non-optimal_design
hbase,22707,comment_2,"Closed the PR, prefer to merge commits locally and create a patch to master branch.",non_debt,-
hbase,22707,comment_4,"Latest build test failure is unrelated, got it passing locally.",non_debt,-
hbase,22707,comment_5,"Committed to master. Attaching a branch-2 patch for further validations, before applying it on branch-2.",non_debt,-
hbase,22707,comment_8,"Latest branch-2 test failure is also unrelated. Pushed branch-2 patch. Thanks for reviewing it, !",non_debt,-
hbase,22832,summary,Remove deprecated method in HFileOutputFormat2,code_debt,dead_code
hbase,22832,description,The method was deprecated in and should be removed for 3.0.0.,code_debt,dead_code
hbase,22837,summary,"Move ""Custom WAL Directory"" section from ""Bulk Loading"" to ""Write Ahead Log (WAL)"" chapter",non_debt,-
hbase,22837,description,"Currently, explanation about *Custom WAL Directory* configuration is a sub-topic of *Bulk Loading,* chapter, yet this subject has not much relation with bulk loading at all. It should rather be moved to a sub-section of the *Write Ahead Log (WAL)* chapter.",architecture_debt,violation_of_modularity
hbase,22837,comment_0,1,non_debt,-
hbase,22837,comment_1,1,non_debt,-
hbase,22837,comment_2,Thanks for reviewing it  . Pushed to master branch.,non_debt,-
hbase,22869,summary,JavaDoc for NullComparator is the same as BinaryComparator,non_debt,-
hbase,22869,description,"Looking at the list of default HBase one can not see what is null about {{NullComparator}} as its JavaDoc is identical to It would be nice to know how it is ""null"" for easier adoption.",documentation_debt,low_quality_documentation
hbase,22869,comment_1,Need to resubmit patch as I am unsure how to generate the JavaDoc report to see what I messed up.,non_debt,-
hbase,22911,summary,fewer concurrent github PR builds,non_debt,-
hbase,22911,description,we've been regularly getting 4-5 concurrent builds of PRs.,non_debt,-
hbase,22911,comment_0,v0 - limit builds to Hadoop since github PRs are involved. - only build one at a time for each PR.,non_debt,-
hbase,22911,comment_1,1,non_debt,-
hbase,22911,comment_3,Let me get this in.,non_debt,-
hbase,22911,comment_4,Pushed to all active branches. Thanks  for contributing.,non_debt,-
hbase,22911,comment_6,"is not going to have any impact on precommit build jobs: correct? If multiple JIRAs have patches uploaded around same time, they can have their builds run concurrently if I am not mistaken? Only concurrent executions of PRs will not happen.",non_debt,-
hbase,22911,comment_12,the disabling is only of concurrent builds of a given PR on github. If multiple PRs are open they will be tested concurrently. the job that handles patches attached to jiras on the HBASE project is also not impacted. That job will happily build patches for multiple issues at the same time. It will also continue to only be automatically triggered once for a given uploaded patch.,non_debt,-
hbase,22911,comment_13,Thank you,non_debt,-
hbase,22927,summary,Upgrade mockito version for Java 11 compatibility,non_debt,-
hbase,22927,description,"Pasting the discussion from HBASE-22534 here: ""Currently mockito-core version is at 2.1.0. According to looks like Java 11 compatibility was introduced in 2.19+. And 2.23.2 claims to have full java 11 support after byte-buddy fix etc.""",non_debt,-
hbase,22927,comment_0,"Hi , I updated the mockito-core version to 2.23.0 ran the tests for java 11.0.1 and tests failed for following classes: * * * * * * But none of the errors were due to updation of mockito version. Do you have any suggestions on how to proceed or should i run another test just to be sure??",non_debt,-
hbase,22927,comment_1,Do they fail w/o the change for you ? Do these tests use mockito? If you rerun them do they fail? Thanks. Try uploading your patch and submitting it?,non_debt,-
hbase,22927,comment_2,"I re-ran tests after updating mockito version 2.23.0 but errors were still there. However, none of the test classes used mockito. So will create a PR under the same ticket .",non_debt,-
hbase,22927,comment_3,"Pushed to branch-2.1+. Thanks  for contributing, and thanks all for reviewing.",non_debt,-
hbase,22927,comment_8,Enforcer fails on,non_debt,-
hbase,22927,comment_9,On which branch? Our pre commit didn find this problem?,non_debt,-
hbase,22927,comment_10,"All, check the Hudson comments above. General check runs the build with -Prelease flag and probably GitHub PR build does not. PR with a fix is linked here.",non_debt,-
hbase,22927,comment_11,Pushed addendum to branch-2.1+. Thanks  for reviewing.,non_debt,-
hbase,22933,summary,Do not need to kick reassign for rs group change any more,non_debt,-
hbase,22933,description,"The old implementation is a bit strange, the isStuck method is like this: It can only return true when there are ongoing procedures. But if we have a procedure, then the procedure will try to reassign region. Scheduling a new procedure does not make sense here, at least for branch-2.2+. I suggest we just remove the related code, since the default retry number for assigning a region is Integer.MAX_VALUE. And even if user set this to small value and finally the region is left in FAILED_OPEN state without a procedure, HBCK2 is used to deal with this, it is not necessary to deal it automatically.",design_debt,non-optimal_design
hbase,22933,comment_0,Pushed to branch-2.2+. Thanks  for reviewing.,non_debt,-
hbase,22936,summary,Close memStoreScanners in else memory leak,code_debt,low_quality_code
hbase,22936,description,"Via  from over on HBASE-22723 We need to close memStoreScanners in before this two return, someone else can take over the task.",non_debt,-
hbase,22936,comment_0,"If any one needs to give a patch I can review it. If not I can prepare one. , , .",non_debt,-
hbase,22936,comment_1,"Looking forward to your patch,",non_debt,-
hbase,22936,comment_2,"Looking forward to your patch also, .",non_debt,-
hbase,22936,comment_3,thanks  Looking forward to your patch !,non_debt,-
hbase,22936,comment_4,"Already implemented by other issues, for branch-2.1, it is HBASE-22929.",non_debt,-
hbase,22936,comment_5,"Looks like this one was resolved by HBASE-22929, so dropping fixVersions.",non_debt,-
hbase,22939,summary,SpaceQuotas- Bulkload from different hdfs failed when space quotas are turned on.,non_debt,-
hbase,22939,comment_0,Can you assign it to yourself ?,non_debt,-
hbase,22939,comment_1,I can't seem to change it. Because I'm not a contributor. Can you help me modify it?,non_debt,-
hbase,22939,comment_2,Have added you as a contributor. Could you please try assigning it to yourself now?,non_debt,-
hbase,22939,comment_3,"It has been modified, thank you. :)",non_debt,-
hbase,22939,comment_5,Test failure reports I have run all the tests and passed them. I think this failed have nothing to do with this commit.,non_debt,-
hbase,22939,comment_8,UT Failed have nothing to do with this commit.,non_debt,-
hbase,22939,comment_9,Merged to master. Tried to backport to branch-2 but failed. Reopen the issue and attach a backport if you'd like it backported . Thanks for the patch.,non_debt,-
hbase,22939,comment_11,"Hi,  thanks for review Backport to branch-2 fails. Ill fix it .",non_debt,-
hbase,22939,comment_12,Add new patch for branch-2 and reopen this issue. for branch-2 for branch-2.1 and branch-2.2,non_debt,-
hbase,22971,summary,Deprecated and make RSGroup feature always enabled,non_debt,-
hbase,22971,description,"As said in the design doc, rs group feature will always be enabled, but if you do not add any new rs groups, all the region servers and tables will be in the default group.",non_debt,-
hbase,22971,comment_0,Merged to branch HBASE-22514. Thanks  for reviewing.,non_debt,-
hbase,22981,summary,Remove unused flags for Yetus,build_debt,build_others
hbase,22981,description,"Nightly jobs are failing with yetus error. Starting from Yetus 0.11.0 flag needs to be passed to yetus, otherwise unrecognized flags cause build failures.",non_debt,-
hbase,22981,comment_0,It is better to remove the unused flags instead of just ignoring those.,code_debt,low_quality_code
hbase,22981,comment_1,It is a bit complicated since not all the plugins are active in all stages. JDK specific ones do not have @author checks so the option is one example that causes build failure there. Whitespace ignore list is a similar option. I'm thinking of removing the unused flags that are not used at all (--jenkins and and also add the flag to bypass the stage specific issues.,build_debt,build_others
hbase,22981,comment_2,Pushed to all branches. Thanks  for the review!,non_debt,-
hbase,22981,comment_4,1,non_debt,-
hbase,22995,summary,The is broken,non_debt,-
hbase,22995,comment_0,I already have a patch for this on HBASE-22979.,non_debt,-
hbase,22995,comment_1,"OK, Let me mark it as closed, and get the HBASE-22979 patch in.",non_debt,-
hbase,22995,comment_2,The is addressing this fix.,non_debt,-
hbase,23061,summary,Replace use of Jackson for JSON serde in hbase common and client modules,non_debt,-
hbase,23061,description,We are using Jackson to emit JSON in at least one place in common and client. We don't need all of Jackson and all the associated trouble just to do that. Use a suitably licensed JSON library with no known vulnerability. This will avoid problems downstream because we are trying to avoid having them pull in a vulnerable Jackson via us so Jackson is a 'provided' scope transitive dependency of client and its in-project dependencies (like common). Here's where I am referring to:,design_debt,non-optimal_design
hbase,23061,comment_0,This is a blocker because in some circumstances a downstreamer will get a CNFE. Jackson is neither needed nor desired in common and client. Fix by replacement. We need to sort this out before releasing. Forward port the changes from branch-1 once committed.,non_debt,-
hbase,23061,comment_1,Strongly related to HBASE-23052. This issue can cover the code changes to client and common where needed to use GSON from third-party,non_debt,-
hbase,23061,comment_2,What is needed beyond the implementation in HBASE-23015? At this point that change is just waiting for a suitable hbase-thirdparty version.,non_debt,-
hbase,23061,comment_3,"also part of the justification for going to hbase-thirdparty gson in HBASE-23015 is that branch-2 and master already do the same thing, so forward porting won't be needed.",non_debt,-
hbase,23061,comment_4,I think that issue has been recently modified because it wasn't clear that was proposed. At least to those who only have mobile device access to JIRA. :-) Feel free to close this as duplicate.,non_debt,-
hbase,23073,summary,Add an optional costFunction to balance regions according to a capacity rule,non_debt,-
hbase,23073,description,"Based on the work in users can now load custom costFunctions inside the main balancer used by HBase. As an example, we like like to add upstream an optional cost function called that will deal with our issue: how to balance regions according to the capacity of a RS instead of using the that is trying to avoid skew. A rule file is loaded from HDFS before balancing. It contains lines of rules. A rule is composed of a regexp for hostname, and a limit. For example, we could have: * rs[0-9] 200 * rs1[0-9] 50 RegionServers with hostname matching the first rules will have a limit of 200, and the others 50. If there's no match, a default is set. Thanks to the rule, we have two informations: the max number of regions for this cluster, and the rules for each servers. will try to balance regions according to their capacity. Let's take an example. Let's say that we have 20 RS: 10 RS, named through rs0 to rs9 loaded with 60 regions each, and each can handle 200 regions. 10 RS, named through rs10 to rs19 loaded with 60 regions each, and each can support 50 regions. Based on the following rules: rs[0-9] 200 rs1[0-9] 50 The second group is overloaded, whereas the first group has plenty of space. Moving a region from the first group to the second should provide a lower cost.",non_debt,-
hbase,23073,comment_0,I just opened the PR:,non_debt,-
hbase,23073,comment_1,"Thanks for the contribution,  . Merged on master, then cherry-picked into branch-2 and committed. For branch-1, after cherry-picking, am getting compilation errors on Mind work on a backport patch for that branch, ?",non_debt,-
hbase,23073,comment_2,"I will!, thanks again  for helping me contribute to HBase :)",non_debt,-
hbase,23073,comment_5,I added a patch for branch-1,non_debt,-
hbase,23073,comment_6,Thanks ! Just pressed the *submit-patch* button to have pre-commit jobs running on it.,non_debt,-
hbase,23073,comment_7,"Just realised pre-commit is not running for branch-1 patches submitted here. , would you mind open a PR for branch-1 instead?",non_debt,-
hbase,23073,comment_8,Directly on github? Doing :),non_debt,-
hbase,23073,comment_10,"Merged into branch-1. Thanks for the contribution, !",non_debt,-
hbase,23073,comment_11,Thank you  for your patience !,non_debt,-
hbase,23087,summary,Remove the deprecated bulkload method in,code_debt,dead_code
hbase,23087,description,The class is IA.Private and it has not been released yet so let's just remove this method to keep the class clean.,code_debt,dead_code
hbase,23087,comment_0,Merged to master. Thanks  for reviewing.,non_debt,-
hbase,23200,summary,incorrect description in,documentation_debt,low_quality_documentation
hbase,23200,description,// default = 24hrs long ret = but the default value is 7 days in,non_debt,-
hbase,23200,comment_0,Implemented by HBASE-23149,non_debt,-
hbase,23300,summary,Set version as 2.1.8 in branch-2.1 in prep for first RC of 2.1.8,non_debt,-
hbase,23300,description,None,non_debt,-
hbase,23300,comment_0,Pushed to branch-2.1. Thanks  for reviewing.,non_debt,-
hbase,23603,summary,Update Apache POM to version 21 for hbase-connectors,non_debt,-
hbase,23603,description,currently uses the Apache POM version 18. The latest version is 21.,non_debt,-
hbase,23603,comment_0,There's a newer version available (version 22). We should go for this one instead of version 21.,non_debt,-
hbase,23603,comment_1,Pushed to master.,non_debt,-
hbase,23624,summary,Add a tool to dump the procedure info in HFile,non_debt,-
hbase,23624,description,None,non_debt,-
hbase,23624,comment_0,Pushed to master and branch-2. Thanks  for reviewing.,non_debt,-
hbase,23646,summary,Fix remaining Checkstyle violations in tests of hbase-rest,code_debt,low_quality_code
hbase,23646,description,In {{hbase-rest}} Checkstyle reports a lot of violations. The remaining violations in the tests should be fixed.,code_debt,low_quality_code
hbase,23646,comment_0,"Pushed to master, branch-2, branch-2.2 and branch-2.1. Leaving it open for the backport to branch-1.",non_debt,-
hbase,23646,comment_5,Pushed to branch-1.,non_debt,-
hbase,23651,summary,Region balance throttling can be disabled,non_debt,-
hbase,23651,description,"HBASE-17178 Add region balance throttling, but it can not be disabled, sometimes we need no throttle and balance the cluster as fast as possible.",design_debt,non-optimal_design
hbase,23651,comment_0,Push to,non_debt,-
hbase,23651,comment_3,Please update the Release Note. Thanks.,documentation_debt,outdated_documentation
hbase,23651,comment_4,Done.,non_debt,-
hbase,23675,summary,Move to Apache parent POM version 22,non_debt,-
hbase,23675,description,Apache parent POM version 22 was released on 2020/01/09.,non_debt,-
hbase,23675,comment_2,"Pushed to branch-1, branch-2 and master.",non_debt,-
hbase,23675,comment_4,Reopening. Nightly on branch-1 failed.,non_debt,-
hbase,23675,comment_5,"I'm looking into the failure, I can't reproduce the failure locally. Retriggered a nightly run now.",non_debt,-
hbase,23675,comment_6,Maven tries to get the dependency from using http protocol ang gets 501 HTTPS Required. In the pom.xml and ASF parent pom I can't find any reference to http repository URL.,non_debt,-
hbase,23675,comment_7,*Create source tarball* step succeeds also in the nightly build. The difference I noticed is in the used maven version. For source tarball step 3.6.2 is used (same version on my local machine) and the other yetus checks use maven 3.0.5.,non_debt,-
hbase,23675,comment_8,I just noticed HBASE-23698 so probably not the parent pom switch caused this build problem.,non_debt,-
hbase,23675,comment_9,Committed HBASE-23698 to branch-1. Retriggered new a nightly run.,non_debt,-
hbase,23675,comment_10,Re-resolving.,non_debt,-
hbase,23696,summary,Stop WALProcedureStore after migration finishes,non_debt,-
hbase,23696,description,WALProcedureStore is left up with its sync thread running in background though we are done with after starting it inside the migration method. Add stop when done.,non_debt,-
hbase,23696,comment_0,I had filled HBASE-23694 on the same :).,non_debt,-
hbase,23696,comment_1,Resolving as dupe of HBASE-23694,non_debt,-
hbase,23752,summary,Fix a couple more test failures from nightly run,test_debt,low_coverage
hbase,23752,description,"After merging the patches HBASE-23731 and HBASE-23647, nightly run found the following issues. - This was a bug in the test itself, PR runs missed it somehow. - This was written pre master registry. Has to be updated. It was never run as a part of the test suite. Cluster restart did not reset the cached connection state  FYI because this is a blocker for 2.3.0 too, I tagged it accordingly. Will submit a PR shortly.",non_debt,-
hbase,23752,comment_2,"Mind taking a look at this? This is the last (hopefully) patch that fixes all the test failures from the full nightly run. I verified that the two failed tests are flakes, ran them locally.",test_debt,flaky_test
hbase,23752,comment_8,Adding 3.0.0 and 2.3.0 to fixVersion list for all issues set at HABSE-18095,non_debt,-
hbase,23773,summary,"Backport ""HBASE-23601 exception gets stuck and repeated indefinietly"" to branch-2.2",non_debt,-
hbase,23773,description,None,non_debt,-
hbase,23773,comment_0,Pushed to branch-2.2. Thanks for the investigation on this issue !,non_debt,-
hbase,23789,summary,[Flakey Tests] ERROR [Time-limited test] cannot read rules file located at ' ',test_debt,flaky_test
hbase,23789,description,"We can't find the balancer rules we just read in the test in high load conditions Test then goes on to fail with: Instead, have tests write rules to local test dir.",test_debt,low_coverage
hbase,23789,comment_0,Redid the persistence of the balancer plan so it used the test data dir rather than tmp to avoid clashes in test runs.,non_debt,-
hbase,23789,comment_1,Pushed fix on branch-2 and master. Lets see how it does.,non_debt,-
hbase,23792,summary,[Flakey Test],test_debt,flaky_test
hbase,23792,description,fails with,non_debt,-
hbase,23792,comment_0,"I can't repro this locally or find a source for the filesystem implementation getting flipped to a distributed fs. However, the only place in Hadoop code where I see this ""Wrong FS"" message thrown as an is in Looking closer at the xml report, I see that the test failed once with the above. Surefire tried to re-run it, but it failed the rerun with which implies to me that when surefire reruns a test method, it does not run the BeforeClass business. I also notice that the test method runs the same code twice, but both times it's using I think one of the invocations is supposed to be calling So. # Survive flakey rerunning by converting the static {{BeforeClass}} stuff into instance-level {{Before}}. # Break the test method into two, one for running over each of the snapshot manifest versions.",test_debt,flaky_test
hbase,23792,comment_1,I see that  spent some time tracking a similar issue in the same test on HBASE-22607. Linking.,non_debt,-
hbase,23792,comment_2,",  you folks mind taking a look at ?",non_debt,-
hbase,23792,comment_3,Backported to all the 2.x branches.,non_debt,-
hbase,23792,comment_8,"I saw this just now. The patch here is in place. [ERROR] 2, 0, Errors: 1, Skipped: 0, 115.741 s <<< FAILURE! - in 74.459 s <<< ERROR! Wrong FS: expected: had just run which does same thing and it was fine. All passes locally. Let me try in a subtask a Nick suggestion getting new HBaseTestingUtility per run.",non_debt,-
hbase,23792,comment_9,Argh.,non_debt,-
hbase,23825,summary,Increment proto conversion is broken,non_debt,-
hbase,23825,description,"While converting the request back to Increment using we incorrectly use the optimization to avoid copying the byte on a BoundedByteString. The optimization was only meant for LiteralByteString where it is safe to use the backing byte array, however it ends up being used to BoundedByteString which is a subclass of LiteralByteString. This essentially breaks increments since we end up creating wrong cells on the server side.",non_debt,-
hbase,23825,comment_0,This is not a problem in master and 2.x since we reverted HBASE-18026 from those branches. FYI,non_debt,-
hbase,23825,comment_1,Nasty bug. I'll start a round of 1.x releases after this goes in. /cc,non_debt,-
hbase,23825,comment_2,ugh ugh ugh. yeah I won't be by signing keys to push on 1.4.13 until next week. feel free to either do 1.4 last or take over HBASE-23743.,non_debt,-
hbase,23825,comment_3,Let me take care of HBASE-23743 .,non_debt,-
hbase,23825,comment_5,Ah... We considered BoundedByteString case there but did not do a check whether that is subclass of LiteralByteString BTW why no 1.5.1 fix version when it says affected at 1.5.0?,test_debt,low_coverage
hbase,23825,comment_6,Because the next release from branch-1 will be 1.6.0,non_debt,-
hbase,23825,comment_7,Linking HBASE-23743 (release 1.4.13) and HBASE-23220 (release 1.6.0).,non_debt,-
hbase,23825,comment_8,Do you need someone to take over HBASE-23220 ? Happy to do it after this lands. Sounds like Sakthi will do HBASE-23743.,non_debt,-
hbase,23825,comment_9,"Yup, let me assign that one to myself",non_debt,-
hbase,23825,comment_10,"I've pushed to branch-1, 1.4, 1.3. Thanks for the reviews",non_debt,-
hbase,23825,comment_12,"ah yes, good catch. please do.",non_debt,-
hbase,23863,summary,[Flakey Test] improvements,test_debt,flaky_test
hbase,23863,description,This one fails in an odd way about 15% of the time where it replicates 2499 edits of 2500.... And seemingly always these numbers. Odd.,non_debt,-
hbase,23863,comment_0,Pushed debug patch on branch-2. It puts up peer before replicates and uses trick from previous test to ensure it up. Lets see.,non_debt,-
hbase,23863,comment_2,Resolving. Pushed on branch-2 only. Not on master because it has changed. Doesn't seem to have definitively fixed this issue so changed title to say 'improvement'. More to come.,non_debt,-
hbase,23867,summary,[Flakey Test],test_debt,flaky_test
hbase,23867,description,Test has this on it: @Test // Test is flakey. TODO: Fix! Let me cut it down. It runs for a while. It fails nearly every run since HBASE-23865 Up flakey history from 5 to 10 which upped the flakey test so it ran with more fork count.,test_debt,flaky_test
hbase,23867,comment_0,What I pushed on branch-2 and master:,non_debt,-
hbase,23867,comment_1,!Screen Shot 2020-02-18 at 10.45.22 PM.png! What is currently showing for this test.,non_debt,-
hbase,23867,comment_2,!Screen Shot 2020-02-18 at 11.15.49 PM.png! Here are the flakies again after two builds that show this patch seems to have stopped this test failing.,non_debt,-
hbase,24011,summary,HMaster does not restart when rsgroup is enabled and /hbase/WALs is moved,non_debt,-
hbase,24011,description,"HMaster does not restart when rsgroup is enabled and /hbase/WALs is moved HMaster restarts properly if rsgroup is not enabled even if /hbase/WALs is moved. Steps to reproduce: # start the cluster # create a table do some put, delete # kill all the region servers and master # move WALs directory for backup (-mv /hbase/WALs /hbase/WALs2) # start the cluster # Master start fails, initialization keep failing",non_debt,-
hbase,24011,comment_0,"Why do you think this is a valid scenario, ? If you are removing all of the WALs, why didn't you just delete all data storage under hbase.rootdir? We don't provide you guarantees as to the ability of HBase to function when you remove pieces of persistent storage. Why did you remove the WALs directory?",non_debt,-
hbase,24011,comment_1,"*WALs directory is removed to speed up the cluster restart in some scenarios.* First WALs directory is removed then cluster restarted, it is up and running then meta is fixed and then custom tool is run to replay the WALs.",non_debt,-
hbase,24011,comment_2,This isn't a scenario which HBase intends to provide support for. I'll leave this open for a short while to see if anyone disagrees strongly before closing this as Won't Fix. Supporting you and some custom tool you've written to do manual WAL replay is orthogonal to why your master didn't initialize itself. Maybe there is an actual bug underneath your request -- but you've not provided enough information to determine this.,non_debt,-
hbase,24011,comment_3,What are the other logs coming in HM side? Is it because the NS table not coming up and because of which the HM start up timed out? Or the META table itself not coming online? I think mostly the latter?,non_debt,-
hbase,24011,comment_4,"Thanks  for the hint :-). Yes it is because META table is not coming online. It keeps waiting in method. HBASE-21191 for more detail. When META is not online and rsgroup is enabled, Master UI hangs as it needs to scan META region. HBASE-24212 for more detail. This is why it appeared as if this issue is related to rsgroup feature but it is not. It is equally important to ensure META region is online before proceeding with normal HM operations. I will revisit my solution. Thinking not to move META WAL file :-)",non_debt,-
hbase,24051,summary,"When the data directory is set to EC,the amount of storefiles reaches a relatively high level,heap Occupancy Percent is above heap occupancy alarm watermark,resulting in frequent gc and eventual failure",non_debt,-
hbase,24051,description,"When the data directory is set to EC, during the continuous load process, regionserver is found down ,to view the log: 0.91630864 is now below the heap occupancy alarm watermark (0.95) 0.969546 is above heap occupancy alarm watermark (0.95) [JvmPauseMonitor] Detected pause in JVM or host machine (eg GC): pause of approximately 1254ms GC pool 'G1 Young Generation' had collection(s): count=2 time=115ms GC pool 'G1 Old Generation' had collection(s): count=1 time=1487ms In particular, GC pauses occurred 283 times in half an hour. The memory analysis found that most of the memory footprint was generated by HeepByteBuffer,and produces the HeepByteBuffer See HDFS-14308but after the fix, hbase did not return to normal.",non_debt,-
hbase,24051,comment_0,"Within the HBase version that you use, is there unbuffer() calls. Am not sure which jira added this in Hbase to make we call unbuffer() after the HFile open.",non_debt,-
hbase,24051,comment_1,The wrap was added by: HBASE-9393 Hbase does not closing a closed socket resulting in many CLOSE_WAIT Signed-off-by: Andrew Purtell HBase on EC is a new domain. Would be interested in the use-case. Didn't think anyone would ever want to do this. Thanks.,non_debt,-
hbase,24051,comment_2,"I don't think you can pin your hopes on someone else thinking about perfection,So the hbase side as a client needs to be as thoughtful as possible.",non_debt,-
hbase,24051,comment_3,"I don't know why hbase keeps those inputstreams after load, just unbuffer.I'll keep watching.",non_debt,-
hbase,24051,comment_4,"Another easy way to avoid errors is to add ""implements CanUnbuffer"" to the",non_debt,-
hbase,24051,comment_5,"Sorry. What you mean by above? You mean client doing manual unbuffer calls will never be satisfactory? Agree. They were added to address an old plethora of CLOSE_WAIT issues, a common complaint. Do the CLOSE_WAITs return do you know if we apply this patch? Do you mean, apply the patch and you'll watch how it performs (or something else?) Would this be a HDFS change? Was interested in the use case that has hbase run on top of an EC FS. Thanks",non_debt,-
hbase,24051,comment_6,Why close the PR  ? Thanks.,non_debt,-
hbase,24051,comment_7,"I see you opened new one instead #1406. Makes sense. I merged it to master, branch-2 and branch-2.2. Where else do you want it? Reopen if this not enough places.",non_debt,-
hbase,24051,comment_8,Pushed to branch-2.2+ Thanks for the patch . I made you a contributor and assigned you this issue. Thanks.,non_debt,-
hbase,24051,comment_9,Reopening and reverting. I just saw the @openinx comment on one of the PRs asking for unit test to prove the old way doesn't work.,non_debt,-
hbase,24051,comment_10,"There was something wrong with that PR, so I closed it and opened a new one.SorryI'm a green hand at this",non_debt,-
hbase,24051,comment_11,No problem sir. No harm done. Just suggest that you comment on your actions so its easier for those of us trying to follow along. I pushed your patch because it looked good but on one of the old PRs there was a comment by a committer that we need a test that fails unless you make your change. That possible? Thank you.,non_debt,-
hbase,24051,comment_13,"in OK,I've tried again.",non_debt,-
hbase,24061,summary,Put up 2.1.10RC0,non_debt,-
hbase,24061,description,None,non_debt,-
hbase,24061,comment_0,RC0 is sunk due to the failure of TestQuotasShell. Will create RC1 after fixing the issue.,non_debt,-
hbase,24087,summary,Backport HBASE-8868 to branch-2.2,non_debt,-
hbase,24087,description,There's just a trivial conflict in import.,non_debt,-
hbase,24087,comment_0,The PR is merged. Resolve this jira.,non_debt,-
hbase,24181,summary,Add region info when log meessages in HRegion.,non_debt,-
hbase,24181,description,"Some log message do not have region info when log, need to add it.",non_debt,-
hbase,24181,comment_0,Pushed to branch-2.2+,non_debt,-
impala,3,summary,Opcodes.thrift missing from git repo,non_debt,-
impala,3,description,"It seems that Opcodes.thrift definition (required by Exprs.thrift in spec of TExprNode) is missing from git repository, causing project build failure.",non_debt,-
impala,3,comment_0,"Opcodes.thrift is a generated file. With the recent changes, you should be able to build the sources now.",non_debt,-
impala,9,summary,LPAD returns incorrect result,non_debt,-
impala,9,description,"I see different output when LPAD is used in hive and impala, here's my table definition, [localhost:21000] sessionid string cookie string country string browser string ...... Here's output from hive hiveOK 01 01 02 02 03 03 04 04 Here's output from Impala, [localhost:21000] 01 011 0112 01122 011223 0112233 01122334 011223344",non_debt,-
impala,9,comment_0,This will be fixed in the upcoming 0.4 release.,non_debt,-
impala,35,summary,Randomise order of channel construction in DataStreamSender,non_debt,-
impala,35,description,"If lots of backends have to open connections to lots of other backends (e.g. for broadcasting the small table in a join), the data stream sender should choose the order that channels are opened at random to avoid thundering herd problems.",non_debt,-
impala,35,comment_0,git:,non_debt,-
impala,41,summary,The TABLE keyword should be optional for INSERT commands,non_debt,-
impala,41,description,"The TABLE keyword should be optional for INSERT INTO/OVERWRITE commands. Impala currently supports inserts in the form: This is what Hive requires. However, in standard SQL the TABLE keyword is either optional or not allowed. Impala should support both forms of the command - such as:",non_debt,-
impala,41,comment_0,CR:,non_debt,-
impala,41,comment_1,Fixed in:,non_debt,-
impala,53,summary,Garbage getting logged at some point after call to DeregisterRecvr() when GLOG_v=2,non_debt,-
impala,53,description,Garbage getting logged at some point after call to DeregisterRecvr() when GLOG_v=2 and another case:,non_debt,-
impala,53,comment_0,I cannot repro this anymore. I talked to Lenni and he agrees that the issue has probably disappeared.,non_debt,-
impala,70,summary,Respect tbl properties to allow empty strings to be treated as NULL,non_debt,-
impala,70,description,"Impala and hive by default, will treat empty string fields in text files as empty strings and the '\N' string to mean NULL. There is a setting, TBLPROPERTIES ( ), which can be set to have empty strings also treated as NULL. Impala should support this.",non_debt,-
impala,72,summary,Partition keys with path separators erroneously result in multi-level partition directories,non_debt,-
impala,72,description,"Repro: in hive: ""create table string_part_test(id int) partitioned by(s1 string, s2 string);"" in impala: localhost:21000] Query: insert into table string_part_test partition(s1, s2) select from alltypessmall Inserted 100 rows in 22.46s [localhost:21000] Query: select * from string_part_test Query finished, fetching results ... Returned 0 row(s) in 0.76s < no rows It looks to me like the files are written into hdfs correctly but the hive metastore partition mappings are not. select * from hive also returns no results. inserting the table from hive works for both hive and impala.",non_debt,-
impala,72,comment_0,Hm - we have a test that tests almost exactly this case: We had better check that the test is running as expected.,test_debt,lack_of_tests
impala,72,comment_1,"The problem is that we're not correctly escaping slashes. date_col_string is DD/MM/YY, which we are creating as three subdirectories. Hive escapes slashes as %252F. This is curiously a double URL-encoding: %25 is '%' and %2F is '/'.",non_debt,-
impala,72,comment_2,This would be good to fix for Impala .7 release. It doesn't soound like a difficult fix.,non_debt,-
impala,72,comment_3,This would be good to fix for Impala .7 release. It doesn't sound like a difficult fix.,non_debt,-
impala,74,summary,Impala's hdfs-fs-cache values for namenode host and port should be read from core-site.xml file rather than input as command line args,non_debt,-
impala,74,description,"Impala hdfs-fs-cache values for namenode address and namenode port should be read from Impala's core-site.xml file rather than input as command line args. If these args are not specified when impalad starts, then it defaults to localhost:20500 which can be confusing. Definition: 20500, ""namenode port""); ""localhost"", ""namenode host"");",design_debt,non-optimal_design
impala,74,comment_0,"Just to let everyone know what happened here: -nn_port and -nn are still around, but if -nn is not specified (and it defaults to blank), Impala will try to read the value of {{fs.defaultFS}} (and if that doesn't exist, from the frontend which has loaded a Hadoop configuration. We will consider removing -nn and -nn_port if this proves to be robust enough. This change will be in Impala 0.6.",design_debt,non-optimal_design
impala,87,summary,Support INSERT without FROM,non_debt,-
impala,87,description,"There are two problems that cause INSERTs without FROM (e.g. {{INSERT INTO TABLE tbl SELECT 1}}) clauses to fail: * The planner generates a plan that does not have a sink * The be would not execute such a plan correctly, as it does not start an executor for SELECT with no FROM to drive the sink There is a workaround, which is to write: {{INSERT INTO TABLE tbl SELECT <constantwhere other_tbl has at least one row.",non_debt,-
impala,87,comment_0,Will this also involve support for bare {{SELECTs}} without a {{FROM}} ?,non_debt,-
impala,87,comment_1,"Probably I am misunderstanding but SELECTs without FROM are already supported (bare, in unions, in subqueries) except for the special case of INSERT.",non_debt,-
impala,92,summary,Significant performance difference between LIKE = 'x' AND = 'x',code_debt,slow_algorithm
impala,92,description,"I'm running the following two queries. The only difference between them is I'm using ""LIKE"" in one case and ""="" in another, though there is no ""%"" in the LIKE, so the effect is the same. I was surprised to see approximately a 10x difference in performance between them. I'm running I've attached the two query profiles. The basic difference is in the execution rate: Obviously I've fixed my query.",code_debt,slow_algorithm
impala,92,comment_0,We special case when the expr we're matching against does not require actually using regex's and can be instead done with substring (or something similar). I think we've missed this case.,non_debt,-
impala,92,comment_1,I post a patch to solve the problem.,non_debt,-
impala,92,comment_2,"Thank you for your contribution, Zuo! We actually already have a fix, but we appreciate your interest.",non_debt,-
impala,96,summary,BaseSequenceScanner should keep track of how many bytes were skipped due to corrupt data,non_debt,-
impala,96,description,In we skip badly formed blocks by advancing to the next sync marker. We should keep track of how many bytes were skipped.,non_debt,-
impala,96,comment_0,Added BytesSkipped counter to query profile,non_debt,-
impala,101,summary,Query Log needs to contain the default database,non_debt,-
impala,101,description,"The query log currently contains the query but not the current database. This makes it very hard to figure out what was run, particularly in our cluster setup where the db encodes the scale factor. We should just add this to the query log.",design_debt,non-optimal_design
impala,101,comment_0,"It wouldn't hurt to add all the session data - key, client address and current database.",non_debt,-
impala,101,comment_1,Let's add all of that to the RuntimeState.,non_debt,-
impala,126,summary,"DataErrorsTest failed due to failed DCHECK @ ""Check failed: counter->type() ==",non_debt,-
impala,126,description,"DataErrorsTest failed due to failed DCHECK: ""Check failed: counter-",non_debt,-
impala,126,comment_0,This is causing a master build break.,non_debt,-
impala,126,comment_1,Coredump and log files saved at:,non_debt,-
impala,126,comment_2,The clock on Impala lzo branch has not been updated. Nong has updated it.,non_debt,-
impala,137,summary,Analysis errors should be sent back to the client without the Java stacktraces,non_debt,-
impala,137,description,"As an example: (Build version: Impala v0.7 (06b3f21) built on Wed Mar 13 16:54:46 PDT 2013) [localhost:21000] Query: select * from alltypes limit 10 ERROR: Analysis exception (in select * from alltypes limit 10) Caused by: Unknown table: 'alltypes' ... 2 more The error here is ""Unknown table: 'alltypes'"" but we dump all this stuff that is not user friendly. This might be useful for development but we should have the default not show the stack traces.",design_debt,non-optimal_design
impala,139,summary,Impalad register/deregister with State-store frequiently during stress test,non_debt,-
impala,139,description,"During stress-test, impalad would register and deregister with state-store frequently because its connection to state-store is lost. Here's what we see from the log: ... W0313 20:45:11.408083 19186 Lost connection to the state-store, entering recovery mode I0313 20:45:11.409102 19186 Attempting to register service on address to subscriber at I0313 20:45:11.409564 19186 Attempting to register subscriber for services at I0313 20:45:11.409934 19186 Attempting to register subscriber for services at W0313 20:45:21.411942 19186 Lost connection to the state-store, entering recovery mode ... While there's connection failure from impalad to state-store, there's no sign of failure of connection from impalad to other impalad. With the new state-store coming, we won't investigate into the old one now. However, we should validate that we don't have a high churning rate with the new state-store. As a side note, the log also shows connection failure to other datanode (remote read): WARN0313 20:40:15.880000 Thread-7 Failed to connect to /10.20.90.18:50010 for block, add to deadNodes and continue. Connection timed out Connection timed out at Method)",non_debt,-
impala,139,comment_0,"Hi Shant - I don't actually see any patch attached; have I missed something? Regardless, I'm about 80% through a fairly large state-store rewrite at the moment, so I'll have to see how best to work any new patch in. Thanks! Henry",non_debt,-
impala,139,comment_1,"Hi Henry, Totally awkward I just accidentally clicked the patch attached button and couldn't edit the issue to change the status back... :)",non_debt,-
impala,146,summary,Code coverage for grouping by/joining on float/double cols,non_debt,-
impala,146,description,Currently grouping by or joining on float/double cols does not work. This JIRA is track the fix for this issue.,non_debt,-
impala,146,comment_0,Duplicate of IMPALA-136 (I think - let me know if 'code coverage' means something different here),non_debt,-
impala,148,summary,add short-circuit read validation to impalad startup,non_debt,-
impala,148,description,"There exists documentation that explains the steps to configure short-circuit reads however, several occurrences of performance issues directly related to this misconfiguration have come up. To help avoid situations where the system is running in sub-optimal mode, a validation check could be put in place that would make the startup of impalad fail if the configuration settings are not correct. This would help prevent people from running unknowingly in a degraded mode.",design_debt,non-optimal_design
impala,148,comment_0,"Greg, we should resolved it in 0.6. If not, please re-open it. Thanks.",non_debt,-
impala,153,summary,Logging/profiles need a way to match fragment id (UUID) to host address,non_debt,-
impala,153,description,"All of our logging and the content in the runtime profile uses fragment/query ids. It's currently not very easy to map those to host names. This makes diagnosing issues unnecessarily difficult. e.g. The log says fragment instance foo failed, we don't have an easy way to know which node to go to for the log/more details.",design_debt,non-optimal_design
impala,153,comment_0,The profiles now contain the uuid and the host name.,non_debt,-
impala,173,summary,Impalad segfaults executed targeted tests @,non_debt,-
impala,173,description,Impalad segfaults executing failure injection in,non_debt,-
impala,173,comment_0,failed when executing: core dumps/logs located:,non_debt,-
impala,173,comment_1,The mempool stack trace in java may well be a red herring. From what Lenni sent me (note the Assert in failed: Connection refused) F0331 08:07:15.072931 30899 Check failed: i != client_map_.end() Check failure stack trace: @ 0x17475ed @ 0x174b067 @ 0x174a5d6 @ 0x174b4dd @ 0xbf8204 @ 0xaa84bb @ 0xe0b254 @ 0xe09eff @ 0xe0a054 @ 0xc5d44d @ 0xc5c32b boost::scoped_ptr< @ 0xc5723d @ 0xaa106e @ 0xacc27e @ 0xaef624 @ 0xa6baa6 @ 0xa6bb1f @ 0xaa136c boost::shared_ptr< @ 0xaa1535 std::pair< @ 0xada63e @ 0xacc724 @ 0xabdbe4 @ 0xaae186 @ 0xa8cba6 @ 0xaf21c6 boost::_mfi::mf1< @ 0xaf17af boost::_bi::list2< @ 0xaf06df boost::_bi::bind_t< @ 0xaef476 @ 0xe21994 thread_proxy @ 0x3d552077f1 (unknown) @ 0x3d54ae5ccd (unknown),non_debt,-
impala,173,comment_2,Hopefully fixed in,non_debt,-
impala,187,summary,Impala should support a RIGHT SEMI JOIN,non_debt,-
impala,187,description,"I have a situation where my query is something like tableA LEFT SEMI JOIN tableB but tableB is much larger than tableA, so I'd rather have tableA be broadcast around.",non_debt,-
impala,187,comment_0,The way joins are performed right now in Impala (broadcast of right-hand side input) is a limitation that will be addressed by adding more join strategies (and having the planner chose between them). It's not a good idea to address that via additional syntax that otherwise doesn't really make sense.,design_debt,non-optimal_design
impala,204,summary,Parquet scanner hangs sometimes,non_debt,-
impala,204,description,One node hung after a bunch of queries ran. It looks like a hang in the scan node. core dump of the hang is archived at,non_debt,-
impala,204,comment_0,"This is a bug/race in the disk thread: #0 0x00000035f960b3dc in () from #1 0x00000000007b367b in (this=0x4f45aa8, m=...) at #2 0x00000000008d663b in (this=0x4f45500) at #3 0x0000000000941994 in thread_proxy () #4 0x00000035f96077f1 in start_thread () from #5 0x00000035f92e570d in clone () from /lib64/libc.so.6 (gdb) p this-$22 = 52 (gdb) p this-$23 = { table_ = allocators_ = current_ = false, funcs_ = {{data_ = {buf = """", align_ = 0 '\000'}}, {data_ = { buf = """", align_ = 0 '\000'}}}}, *size_ = 52*, mlf_ = 1, = 0x96a42780, max_load_ = 79}, <No data fields All the scanners are block but the disk thread did not wake up.",non_debt,-
impala,211,summary,"Running impala on 'localhost' causes log spew with GLOG_V=2+ @ - ""Only localhost addresses found for ___""",non_debt,-
impala,211,description,Running impala on 'localhost' causes log spew with GLOG_V=2 @,non_debt,-
impala,211,comment_0,"It's not a bug, it's the right behaviour. Do you want the log level bumped to GLOG_v=3?",non_debt,-
impala,211,comment_2,"I think it is useful to log it once, but logging it for each impalad every 1/2 second seems excessive.",design_debt,non-optimal_design
impala,211,comment_3,Fixed in 13ce6ef,non_debt,-
impala,231,summary,Impala HBase scan is very slow,code_debt,slow_algorithm
impala,231,description,"Impala's HBase scan is very slow. For scanning 40000 rows from an colocated HBase table, it took ~5.7sec. The query is: select count(*) from (select * from hbasetbl limit 40000); Majority of the time is spent inside I've done some experiment to figure out where we spent the time by adding a few more timers. Here's the runtime profile: HBASE_SCAN_NODE (id=0):(5s714ms 99.98%) - BytesRead: 4.26 MB - 483.594us - 5s710ms - MemoryUsed: 0.00 - MyOwnTimer1: 1s387ms <-- We should trim this time. - MyOwnTimer2: 2s798ms <-- - MyOwnTimer3: 688.179ms - MyOwnTimer4: 0ns - MyOwnTimer5: 0ns - NumDisksAccessed: 0 - 5.14 MB/sec - RowsReturned: 40.00K (40000) - RowsReturnedRate: 7.00 K/sec - ScanRangesComplete: 0 - 0 - 0ns - 0ns - 0ns - 0ns - 0 - 827.604ms <-- we spent only 800ms on fetching from HBase - 775.05 KB/sec I've attached the code with more timers in the attached file When I increase the limit to 80000, the perf is much worse. Here's the profile: HBASE_SCAN_NODE (id=0):(23s027ms 99.99%) - BytesRead: 8.51 MB - 249.40us - 23s018ms - MemoryUsed: 0.00 - MyOwnTimer1: 5s680ms - MyOwnTimer2: 11s401ms - MyOwnTimer3: 2s829ms - MyOwnTimer4: 0ns - MyOwnTimer5: 0ns - NumDisksAccessed: 0 - 2.76 MB/sec - RowsReturned: 80.00K (80000) - RowsReturnedRate: 3.47 K/sec - ScanRangesComplete: 0 - 0 - 0ns - 0ns - 0ns - 0ns - 0 - 3s085ms - 376.52 KB/sec I didn't see any disk activity. So, I dont' think HBase is going to disk. Also, the read time is only 3sec. The big chuck of time is spent within the block of MyOwnTimer1 and MyOwnTimer2. We spent 5x more time there even though we only scan 2x more rows.",code_debt,slow_algorithm
impala,231,comment_0,"We have to mark local var using ""DeleteLocalRef"" when the local var is done. This will help reduce mem pressure.",code_debt,low_quality_code
impala,231,comment_1,"With DeleteLocalRef, the perf are back to reasonble number now. Read throughput is back to 36.69 MB/sec per node.",non_debt,-
impala,231,comment_2,review:,non_debt,-
impala,242,summary,Impala Query UI Keeps Cancelled Query Above New Executing Query,non_debt,-
impala,242,description,"After cancelling a query in impala-shell with Control-C, the cancelled query remains in the executing queries section at the top of the debug UI web page and subsequently started queries end up sorted below it, which is contrary to the desired sorting order by start time and inconvenient when webex-ing with limit screen real estate as it makes it harder to see the latest test query status without scrolling to get past these cancelled queries.",design_debt,non-optimal_design
impala,242,comment_0,"I bet this is fixed by Ishaan's shell cancellation patch, IMPALA-243 (previously, the query might not be removed from the in-flight list because the session isn't closed properly). Let's reopen if we see this again after that and IMPALA-124 go in.",non_debt,-
impala,254,summary,Shell: connect without any params should also use fqdn.,non_debt,-
impala,254,description,None,non_debt,-
impala,259,summary,Debug webpage does not render query plans properly (slot refs are blank),non_debt,-
impala,259,description,Debug webpage does not render query plans properly (slot refs are blank). These appear fine in the log. I believe this is because they contain pointy brackets: <,non_debt,-
impala,259,comment_0,Fixed in b7aad78,non_debt,-
impala,265,summary,[api] Support GetLog(),non_debt,-
impala,265,description,This is blocking Impala users in Hue. Hive Server 2 API.,non_debt,-
impala,265,comment_0,"Impala returns error logs from Beeswax's get_log rpc call. For HS2, Impala will also return error logs from GetLog().",non_debt,-
impala,265,comment_1,review:,non_debt,-
impala,278,summary,Expose integer division as QUOTIENT,non_debt,-
impala,278,description,"We've implemented the int divide compute function but we currently expose in sql as '/'. This doesn't work since we automatically promote the types to the correct resolution, in this case double and there is no way to call the int_divide version. We need an explicit function name (QUOTIENT) which is mapped to the int divide compute fn.",non_debt,-
impala,278,comment_0,"We expose int divide from SQL as ""div"" like MySQL. For example ""select 10 div 2"".",non_debt,-
impala,278,comment_2,Fixed in time for 1.0.1?,non_debt,-
impala,278,comment_3,"Yes, looks like I made it in time for 1.0.1!",non_debt,-
impala,290,summary,Improve error message when trying to use LZO compression on file formats other than text,design_debt,non-optimal_design
impala,290,description,"A user ran into a problem when they tried to use LZO compressed RC file. Instead of displaying a clear ""Not Yet error, the query failed with an Unknown Codec message. It would be good to improve this.",design_debt,non-optimal_design
impala,290,comment_0,"Branch: refs/heads/master Home: Commit: Author: Nong Li <nong@cloudera.com Date: 2013-04-25 (Thu, 25 Apr 2013)",non_debt,-
impala,321,summary,Add millisecond granularity to start / end times in profile summary,non_debt,-
impala,321,description,In the summary node in the profile the info strings for start / end times are specified with second granularity. It would be nice to have them specify milliseconds.,design_debt,non-optimal_design
impala,321,comment_0,Fixed in 1085983:,non_debt,-
impala,322,summary,Provide a mechanism for getting error / warning information for queries in the profile,non_debt,-
impala,322,description,"It would be nice to get able to get information about errors the query encountered, like corruptions, missing block metadata, integer overflows, and anything you guys think would be interesting, in the profile so it's all centralized.",design_debt,non-optimal_design
impala,322,comment_0,"We should output the query status to the runtime profile on error, the other stuff is more work for later.",non_debt,-
impala,322,comment_1,"git: This adds the query status to the profile, we can open new JIRAs for other errors.",non_debt,-
impala,324,summary,Max Query Time implementation,non_debt,-
impala,324,description,"It's occurred to me that, without a current global query view, situations will likely occur where a runaway query sent to another query planner goes unnoticed in administration but users may complain of poor cluster performance and aren't able to see why unless they somehow know to specifically go to the other query planner node which that runaway query was submitted to. One could suggest the best practice of only having clients connect to one node to submit queries and keep that effectively the only query planner in the cluster but it doesn't prevent the problem if they fail to heed such advice, which some users almost certainly will. Also some more advanced admins may load balance the query planners for HA or load distribution. I propose that there is, if not already, a max query time that a query planner will allow a query to run before self terminating the query across all impalad nodes. This will prevent the runaway query from continuously affecting performance and leaving a hard to trace problem. Given that Impala queries should execute in seconds, or minutes at the worst, maybe 300 or 600 seconds would be a reasonable default max query execution time (wall time). This would need to be configurable by the admin/user both globally for the node's query planner default via --switch/flags file and also per session override eg. in impala-shell some kind of ""set max_query_time="" command. Even if the global query view implementation I've asked for happens down the road, this max query time would also alleviate administrator burden of having to kill runaway user queries manually.",design_debt,non-optimal_design
impala,324,comment_0,"Might be useful to have this in the future, but for now admins are able to cancel the queries through CM.",non_debt,-
impala,326,summary,Parquet supported encodings validation is too strict,non_debt,-
impala,326,description,"The validation for supported encodings for parquet is too strict. We also need to allow RLE and BIT_PACKED in the enums check. Also, the enums should print the strings in the error message, not the numerical values.",code_debt,low_quality_code
impala,326,comment_0,"Branch: refs/heads/master Home: Commit: Author: Nong Li <nong@cloudera.com Date: 2013-05-05 (Sun, 05 May 2013)",non_debt,-
impala,330,summary,Break up lock in,non_debt,-
impala,330,description,"Web page requests hold for the duration of the callbacks that render the page. This isn't terrible for short-lived requests, but anything that takes a while to render will block other requests. I tried the obvious thing, and Mongoose started behaving very weirdly (rendering the wrong page, rejecting a lot of concurrent requests).",code_debt,multi-thread_correctness
impala,330,comment_0,"Pushing to 1.2 - would be nice to fix, but seems like it might be tricky.",non_debt,-
impala,330,comment_1,Fixed in,non_debt,-
impala,332,summary,impala-shell does not handle multi-line commands that have an open string literal ending in a semi-colon,non_debt,-
impala,332,description,impala-shell does not handle multi-line commands that have an open string literal ending in a semi-colon.,non_debt,-
impala,332,comment_0,I've added a new function to verify whether a semicolon character is a statement terminator or not. I did some tests like below.,non_debt,-
impala,344,summary,StatsMetrics may write unparseable JSON when a value is NaN or inf,non_debt,-
impala,344,description,"JSON doesn't allow {{nan}} or {{inf}} as valid double literals. There's no accepted way to represent nonfinite values in JSON, so the convention is apparently to use null and lose the ability to distinguish between nan and inf. That's what we'll do for now. This affects a {{StatsMetric}} before any updates have been made to it, since it's mean is NaN.",non_debt,-
impala,344,comment_0,Example json output that fails to get decoded: Thanks for filing this Henry.,non_debt,-
impala,344,comment_1,Fixed in ff282e1,non_debt,-
impala,346,summary,Update last_DdlTime after performing dynamic partition insert (if partitions are created),non_debt,-
impala,346,description,None,non_debt,-
impala,364,summary,Cancelled queries sometimes aren't removed from the inflight query list,non_debt,-
impala,364,description,"If I run the query: ""select count(*) from sample_07"" from the shell and cancel it immediately using ctrl-C it never disappears from the /inflight_query_ids list. I'm not sure if this is relevant, but the table sample_07 has 823 rows. This repros consistently for me.",non_debt,-
impala,364,comment_0,"Also, if it's helpful, here's the top of the profile: Query Summary: Start Time: 2013-05-22 13:58:37 End Time: Query Type: QUERY Query State: CREATED Impala Version: impalad version 1.1 RELEASE (build User: root Default Db: default Sql Statement: select count(*) from sample_07",non_debt,-
impala,364,comment_1,"This seems to be due to the cancellation path not calling The shell is actually calling {{Close}} if it gets a cancel signal _after_ it has started returning rows, which means that serendipitously it is unregistering the query itself. Should be a pretty easy fix.",non_debt,-
impala,364,comment_2,Fixed in 99a6b1,non_debt,-
impala,367,summary,"For secure HS2 connection, Impala needs to populate TSessionState with username info",non_debt,-
impala,367,description,"For secure HS2 connection, Impala needs to populate TSessionState with correct client username. Currently, the username field is not set at all. This is because HiveServer2 sessions are a bit different from Beeswax sessions. We should be able to get the username from the sasl transport layer, but need to investigate more what needs to happen to properly support this.",non_debt,-
impala,367,comment_0,CR:,non_debt,-
impala,370,summary,Add HDFS read metrics to runtime profiles,non_debt,-
impala,370,description,CDH4.3 added additional metrics for the read path. We should use these metrics and we can once and for all know for sure things like short circuit reads are enabled.,non_debt,-
impala,370,comment_0,"Branch: refs/heads/master Home: Commit: Author: Aaron Davidson Date: 2013-06-11 (Tue, 11 Jun 2013)",non_debt,-
impala,401,summary,Impala Avro doesn't support Avro schema evolution,non_debt,-
impala,401,description,Impala avro scanner doesn't use Avro ResolvingDecoder so that it cannot handle Avro schema evolution such as * add a field * delete a field * rename a field * change data type of a field Check details and an example here:,non_debt,-
impala,401,comment_0,"git: b1763ac723d4fa This fix includes support for Avro schema resolution minus default values. I've created a new jira, IMPALA-441, to track default value support.",non_debt,-
impala,415,summary,INSERT OVERWRITE should not delete hidden files in the table/partition location,non_debt,-
impala,415,description,INSERT OVERWRITE should not delete hidden files in the table/partition location. Hidden files may be used by other agents to work out the same data directory. Repro:,non_debt,-
impala,415,comment_0,"After some discussion, we decided to limit this just to hidden files / directories in the root directory. The cost of walking a potentially enormous table (in terms of the number of files) to find the hidden files and then deleting each non-hidden file one-at-a-time outweighs the benefit of preserving them. We already do this for non-partitioned tables (which are usually smaller by filecount) to avoid deleting subdirectories, so there's no cost there.",non_debt,-
impala,415,comment_1,Fixed in df4b80d,non_debt,-
impala,418,summary,"""translate"" UDF exists in Hive but not in Impala",non_debt,-
impala,418,description,"""translate"" is used in hive to replace characters in strings. Works in Hive but not in Impala: Query: select ""/"", ""-"") from asthma_acute ERROR: AnalysisException: translate unknown",non_debt,-
impala,418,comment_0,"Branch: refs/heads/master Home: Commit: Author: Nong Li <nong@cloudera.com Date: 2013-06-15 (Sat, 15 Jun 2013)",non_debt,-
impala,442,summary,Test HDFS communication test at startup,non_debt,-
impala,442,description,"Once we've connected to metastore, we can find one HDFS table and see if Impala can read list the dir.",non_debt,-
impala,442,comment_0,review:,non_debt,-
impala,443,summary,Calculated field in where clause causes crash when codegen is enabled,non_debt,-
impala,443,description,"On our cluster, when codegen is enabled, queries like work fine, while queries such as and cause impalads to segfault (presumably because the latter require code generation while the former doesn't). All queries finish correctly when and I've attached a few relevant log files.",non_debt,-
impala,443,comment_0,"FWIW, this seems to be fixed for us in 1.1.1...",non_debt,-
impala,443,comment_1,Thanks for letting us know. We made a bunch of fixes in 1.1.1.,non_debt,-
impala,448,summary,Hive queries fail against table when Impala INSERT is in progress on same table,non_debt,-
impala,448,description,"Hive queries fail if any unexpected subdirectories are found under a table/partition directory. When doing an insert via Impala, Impala will create a temp staging directory for the insert results. This causes any concurrent Hive queries on the table to fail.",non_debt,-
impala,448,comment_0,CR:,non_debt,-
impala,457,summary,Add more process stats to debug webpage,non_debt,-
impala,457,description,"It would be useful to add more process state information to the debug page. Things that come to mind are: number of threads (/proc/status) cgroup stuff (/proc/cgroup) io (/proc/io) scheduling stuff (/proc/sched) # open files (/proc/fd) There's a lot of information in there, we probably just want to pull out the counters that are must useful.",non_debt,-
impala,457,comment_0,"I guess here we want to get information about the impalad. For example, impalad is pid is 100, we want to get information by read /proc/100/io, /proc/100/fd, /proc/100/status. am I right?",non_debt,-
impala,457,comment_1,"- Exactly. I think the first thing to do is to list the useful information you can get out of {{/proc}} on this JIRA, and we'll choose the best information to display.",non_debt,-
impala,459,summary,Failed DCHECK during failpoint tests: Check failed: > 0 (0 vs. 0),non_debt,-
impala,459,description,"{{ CANCEL | target_node: ('HDFS_SCAN_NODE', [2, 4]) | exec_option: {'disable_codegen': False, 'batch_size': 0, 'num_nodes': 0} | table_format: parquet/none | location: OPEN] }}",non_debt,-
impala,459,comment_0,"I have only seen this hit once, moving out of v1.1 for now.",non_debt,-
impala,459,comment_1,Let's investigate whether this can happen in the non-test code paths as well.,non_debt,-
impala,459,comment_2,Resolving since we haven't since this hit in a long time.,non_debt,-
impala,504,summary,right outer join and full outer join do not return row with Null value for the second table,non_debt,-
impala,504,description,When you run select * from a right outer join (or full outer join) b on a.column1=b.column1 row from b will not be returned if b.column1 is null,non_debt,-
impala,504,comment_1,"The problem can be seen in 2.2.0-cdh5.4.7. When doing a full outer join, it behaves like a left outer join.",non_debt,-
impala,509,summary,automatic type conversion,non_debt,-
impala,509,description,To reproduce the issue Same error is reported for FLOAT (expecting DOUBLE). This also affects build-in/aggregate functions. Example,non_debt,-
impala,509,comment_0,"We should consider adding more signatures for out built-in functions, e.g., coalesce() When inserting data the implicit type promotions may force users to use explicit down-casting to make the expr result compatible with the target table.",non_debt,-
impala,530,summary,Could ASCII string sorting be faster?,code_debt,slow_algorithm
impala,530,description,"I was experimenting with queries on year / month / day fields stored as strings in a TEXT data file. In addition to the need to CAST them to INT to get correct sorting during ORDER BY, I noticed that the sorting of the CAST-to-INT values was 2x faster than the original string values (this is with about 500M rows): select distinct year, month, day from raw_data_ymd order by year, month, day limit 7500; ... Returned 5124 row(s) in 102.45s select distinct year, month, day from raw_data_ymd order by cast (year as int), cast (month as int), cast (day as int) limit 7500; ... Returned 5124 row(s) in 41.87s But all the strings are very short, 1-2 bytes for the day and month fields, and 4 bytes for the year field. I would expect these could be sorted in about the same time as the corresponding integers, especially since the integer sorting in the above query has the overhead of 3 CASTs per row. I wonder if there is some latent UTF-8 processing slowing down the string sorting even though all the values are ASCII, or if the unknown-in-advance length of the strings makes the string sort harder to optimize. Could there be some special casing when the string data is known to be ASCII (i.e. like currently), and Impala could infer a binary collation? E.g. the strings could be compared 2, 4, or 8 bytes at a time as if they were integers. I first thought of this as an optimization for short string columns where the data is of consistent length, but if the strings are null-terminated and held in a buffer with a size rounded to an even number, I think the technique would apply for strings of different or unknown-in-advance lengths.",code_debt,slow_algorithm
impala,530,comment_0,I bet you're right - we could do better with string sorting - but we'll track that through the general perf roadmap.,non_debt,-
impala,563,summary,Improve error message on querying an uninitialized Catalog,design_debt,non-optimal_design
impala,563,description,"Catalog will fail to initialize when there's a metastore connection issues (mostly misconfiguration). Impala logs the exception, but no one really reads the log. When a user (or admin) connects to Impala, they see an empty database and wouldn't know what wrong. To improve the experience, Impala should return an exception for all queries submitted (except invalidate metadata) if the catalog wasn't initialized. The exception should also contain the original exception message from the failed metastore connection.",design_debt,non-optimal_design
impala,563,comment_0,this will be resolved as part of the catalog service changes. CR:,non_debt,-
impala,565,summary,Support user impersonation for authorization requests,non_debt,-
impala,565,description,"Impala should support user impersonation for authorization requests. This will enable a more fine-grained authorization story for apps such as Hue. This is only for authorization requests, not for HDFS-level impersonation. This can be done by using the Hive Server 2 ""configuration"" property to specify the user to impersonate. For example a TOpenSessionReq would look like: Impala would just need to verify the user is in the list of authorized proxy users (users who can impersonate other users).",non_debt,-
impala,591,summary,Impalad SIGSEV on GROUP BY query with Avro data 7fa2526910f4,non_debt,-
impala,591,description,"When performing SELECT with GROUP BY on Avro data (no compression): Connected to Server version: impalad version 1.0.1 RELEASE (build Welcome to the Impala shell. Press TAB twice to see a list of available commands. Copyright (c) 2012 Cloudera, Inc. All rights reserved. (Shell build version: Impala Shell v1.0.1 (df844fb) built on Tue Jun 4 08:08:13 PDT 2013) Query: describe tweets2 Query finished, fetching results ... | name | type | comment | | created_at | string | from deserializer | | tweet | string | from deserializer | | id | bigint | from deserializer | | | bigint | from deserializer | | in_reply_to_user_id | bigint | from deserializer | | user_id | bigint | from deserializer | | user_screen_name | string | from deserializer | | geo_lat | double | from deserializer | | geo_long | double | from deserializer | | geo_type | string | from deserializer | Returned 10 row(s) in 1.02s Query: select count(*), user_screen_name from tweets2 group by user_screen_name Query finished, fetching results ... Error communicating with impalad: TSocket read 0 bytes [Not connected] SIGSEGV (0xb) at",non_debt,-
impala,591,comment_0,"Hi Tristan, thanks for the bug report. I see you hit this bug using Impala v1.0.1. Would it be possible for you to see if it reproduces on the Impala v1.1.1 release?",non_debt,-
impala,591,comment_1,"Yes I will try to reproduce. Is there a guide I can use to upgrade? Also will this require an upgrade to the other cdh components? Edit: Just found the link, will get started now.",non_debt,-
impala,591,comment_2,I have now upgraded to v1.1 and cannot reproduce the issue.,non_debt,-
impala,596,summary,INSERT cancellation tests sometimes cause HEAPCHECK failure when Impala cannot close the file,non_debt,-
impala,596,comment_0,I actually do not think the leak is related to the HDFS close error now that I look at the log timestamps.,code_debt,low_quality_code
impala,596,comment_1,Impala 2.1.1 The same result.,non_debt,-
impala,601,summary,Shell should optionally read its configuration from a file,non_debt,-
impala,601,description,'.impalarc - set of commands that get run on shell invocation (e.g. a connect command and SET options)' See for a solution using ConfigParse.,non_debt,-
impala,601,comment_0,IMPALA-276 can be folded into JIRA.,non_debt,-
impala,601,comment_1,"As part of this fix, we should be sure the default host to connect to can be configured. Currently, if ""-i"" isn't specified the shell always connects to the local machine's FQDN.",non_debt,-
impala,601,comment_2,"Abdullah, can you take a look?",non_debt,-
impala,605,summary,HBASE_CACHING default incorrectly displayed,non_debt,-
impala,605,description,"In the impala-shell, the default value of _HBASE_CACHING_ is shown as *0*, when in fact it's *1000* (I think). This can be pretty confusing since it's implicitly using a different value. In my case, it was reading my HBase table which has *very* wide rows, and it was causing the region servers to fall over.",non_debt,-
impala,605,comment_0,"0 means default and default value is 1k. Impala already adjust the hbase_caching according to the estimated stats. But if there are a few outliner that are HUGE, then the hbase_caching value will still be too big.",design_debt,non-optimal_design
impala,605,comment_1,"Is it possible for Impala to send the default values to the shell to display? There isn't an easy way, other than consulting the manual to know the the default hbase_caching (or any other value). I changed the default for hbase_caching to 100 (on the daemons), and the shell is correctly displaying it now, so it seems there is a way to send default config values to the client upon connection.",non_debt,-
impala,605,comment_2,"Yes, it's possible, but I think it's more valuable if we can display the comment rather than the value. Because for some other ""query options"", the value is determined by Impala at run time. For those query options, there is no meaningful value to be displayed.",non_debt,-
impala,606,summary,"fails with error ""terminate called without an active exception""",non_debt,-
impala,606,description,The fails often with the error:,non_debt,-
impala,606,comment_0,I disabled this test for now.,non_debt,-
impala,606,comment_1,"I'm certain this is an error in the test (that is, the way a gets torn down), rather than the SSL logic.",non_debt,-
impala,606,comment_2,Fixed in f27ad4,non_debt,-
impala,608,summary,Make PROFILE more precise for memory figures,non_debt,-
impala,608,description,"In experimenting with the MEM_LIMIT query option, I found that memory figures in the PROFILE output are less precise than other numbers. For example: - PeakMemoryUsage: 8.16 MB - 275.774us - RowsRead: 512.00M (512000000) - 799.49K (799493) In this example, I'm doing a SELECT COUNT(*) from a Parquet table, which I expect to allocate 8MB for the scan buffer plus a little more for overhead. The times, rows, and other figures are very precise, either with several digits of precision of the exact figure in parenthesis. For memory, there's only this rounded figure where it's not clear if MB = 1000 * 1000 bytes or 1024 * 1024. Could we print the exact memory figure in parenthesis as with the number of rows or context switches? When I do queries with MEM_LIMIT intentionally set to right around the 8 MB mark, the cutoff doesn't seem to quite match the reported peak memory figure, and the error message again shows imprecise rounded figures: [localhost:21000] MEM_LIMIT set to 8600000 [localhost:21000] Query: select count(*) from billion_parquet Query aborted, unable to fetch data Backend 0:Memory Limit Exceeded Query Limit: memory limit exceeded. Limit=8.20 MB Consumption=8.20 MB Fragment Consumption=52.00 KB AGGREGATION_NODE (id=3): Consumption=44.00 KB EXCHANGE_NODE (id=2): Consumption=0.00 DataStreamMgr: Consumption=0.00 Fragment Consumption=8.15 MB AGGREGATION_NODE (id=1): Consumption=44.00 KB HDFS_SCAN_NODE (id=0): Consumption=8.09 MB DataStreamSender: Consumption=16.00 KB [localhost:21000] MEM_LIMIT set to 8700000 [localhost:21000] Query: select count(*) from billion_parquet +--+ | count(*) | +--+ | 512000000 | +--+ Returned 1 row(s) in 11.17s I did get a combination of wrong result (0) + out-of-memory error at one point, which seems anomalous: [localhost:21000] MEM_LIMIT set to 8090000 [localhost:21000] Query: select count(*) from billion_parquet +-+ | count(*) | +-+ | 0 | +-+ ERRORS ENCOUNTERED DURING EXECUTION: Backend 0:Memory Limit Exceeded Query Limit: Limit=7.72 MB Consumption=120.00 KB Fragment Consumption=52.00 KB AGGREGATION_NODE (id=3): Consumption=44.00 KB EXCHANGE_NODE (id=2): Consumption=0.00 DataStreamMgr: Consumption=0.00 Fragment Consumption=68.00 KB AGGREGATION_NODE (id=1): Consumption=44.00 KB HDFS_SCAN_NODE (id=0): Consumption=0.00 DataStreamSender: Consumption=16.00 KB Returned 1 row(s) in 0.27s",non_debt,-
impala,608,comment_0,I guess another way to let users stop thinking about how many bytes in a megabyte or gigabyte would be to make SET MEM_LIMIT accept numbers with an MB or GB suffix. Currently that raises an error: [localhost:21000] MEM_LIMIT set to 6MB [localhost:21000] Query: select count(*) from billion_parquet ERROR: Failed to parse mem limit from '6MB'.,non_debt,-
impala,608,comment_1,try set mem_limit=6m,non_debt,-
impala,608,comment_2,"For intuitive behavior, it would be great if the format of the mem figures printed in PROFILE output matched up with the way the user has to enter it. I think my suggestion is really 3 parts: 1) In PROFILE output, spell out the precise number of memory bytes in parenthesis, like is done with number of rows. 2) Allow SET MEM_LIMIT=...MB | GB in addition to M or G, so if someone sees max memory usage of 8.2 GB in PROFILE, they could do SET MEM_LIMIT=8.5GB and expect it to work. 3) If MEM_LIMIT is set to some unparseable value, throw an error immediately after the SET command, don't wait for a subsequent query. Since those (2) and (3) are probably more under control of impala-shell, why don't I open a separate JIRA with those 2 items and we just consider the suggestion (1) for PROFILE output under this JIRA.",non_debt,-
impala,608,comment_3,Fixed with another patch.,non_debt,-
impala,617,summary,Impala crashes unexpectedly when inserting sparse rows into parquet (insert into .. values ..),non_debt,-
impala,617,description,"If you create a table backed by Parquet and you attempt to perform an INSERT INTO with literal values, but leave out some of the columns, the Impala daemon will crash unexpectedly. *How to re-produce* 1. Create parquet table 2. Insert row, only define 1 column Logs show the following *Expected Behavior* Here is an example of doing the same thing, expect with a _TEXTFILE_ Attaching GLOG2 logs (courtesy of Keith from Pulse), not much info in it unfortunately. I can supply a corefile if needed, but this is pretty easy to reproduce.",non_debt,-
impala,617,comment_0,The issue is that the planner is picking TYPE_NULL as the output expr type for the missing column. This is not good for parquet since we need to pick a type to write in the parquet metadata. I think the planner should pick a type based on the table schema column type.,non_debt,-
impala,620,summary,"A StateStore subscriber registering with the same network address as an existing subscriber should succeed, overwriting the existing registration",non_debt,-
impala,620,description,"The StateStore should allow for a subscriber to register with the same network address as an existing subscriber. The new registration request should overwrite the old registration. The current behavior is to block the registration with an error saying that there are duplicate subscribers with the same address. This is a common scenario where a subscriber has failed and tries to reconnect, but it cannot because the statestore has not yet determined the subscriber has failed.",non_debt,-
impala,620,comment_0,"As part of fixing this, let's make sure to figure out whether the subscriber eventually reconnects. It sounds like it doesn't, but it seems like that's a bug as well. The statestore should eventually fail the subscriber that's reconnecting, but I wonder if it's actually sending updates to the new instance of the subscriber, thinking it's the old instance. The new instance should reject those updates, and I bet it doesn't.",non_debt,-
impala,620,comment_1,Fixed in 2fbaabb,non_debt,-
impala,626,summary,LibraryCache will crash if function is dropped and recreated while a UDF is running,non_debt,-
impala,626,description,"If a user is running a UDF and then a Drop then Create on the same binary is issued, we might crash. The running (local) binary will be overwritten by the new create.",non_debt,-
impala,626,comment_0,"Branch: refs/heads/master Home: Commit: Author: Nong Li <nong@cloudera.com Date: 2014-02-20 (Thu, 20 Feb 2014)",non_debt,-
impala,636,summary,Failed DCHECK during 10-Node TPC-H stress run (50 clients) in Check failed:,non_debt,-
impala,636,description,Failed DCHECK during 10-Node TPC-H stress run (50 clients) in Check failed: !entry- Logs and core dump:,non_debt,-
impala,636,comment_0,"When we process topic deletion, we just remove an entry from the backend list without removing the entry from backend_map_. We should remove the entry from backend_map_ if the list is empty.",code_debt,low_quality_code
impala,638,summary,"Impala service fail when submitting query with ""non"" valid regex",non_debt,-
impala,638,description,A query: HUe response: Cloudera manager impalad daemons health after submitting query: impalad log: And then all daemons are restarted automatically.,non_debt,-
impala,638,comment_0,The query now just fails but the daemons are okay.,non_debt,-
impala,661,summary,Explain plan should characterize the cost of evaluating predicates,non_debt,-
impala,661,description,"Explain plan is an effective tool for tuning query. However, it doesn't give much inside into the (cpu) cost of expression evaluation. For complex predicates, it'll greatly affect the runtime of the query. For example, complex regex is very costly to evaluate. Right now, if a complex join predicate cause the join to slow down, it's not very easy to tell. If explain plan can annotate the cost of predicate evaluation (roughly), then it can guide our user to identify and tune the predicates.",design_debt,non-optimal_design
impala,661,comment_0,Closing some JIRAs that have useful suggestions but limited interest. We can reopen if there is more interest.,non_debt,-
impala,690,summary,Impala hit DCHECK,non_debt,-
impala,690,description,F1206 19:19:52.380424 19492 Check failed: == __null || fn == __null,non_debt,-
impala,690,comment_0,Duplicate of IMPALA-561,non_debt,-
impala,710,summary,Cleanup log spew from Planner changes,code_debt,low_quality_code
impala,710,description,"Some top offenders. I'm fine removing these myself, but I'm not sure which we would be useful to keep around as either debug or info messages:",non_debt,-
impala,710,comment_0,These are already debug messages: post-agg selectListExprs: resultExprs: 8 post-analysis AggregateInfo,non_debt,-
impala,713,summary,failing on HBase with wrong results in exhaustive test runs (possibly due to varying batch sizes),non_debt,-
impala,713,description,failing on HBase with wrong results in exhaustive test runs (possibly due to varying batch sizes). This happens consistently. The last known good githash was: commit / Mon Dec 9 20:13:41 2013 -0800,non_debt,-
impala,713,comment_0,This has been fixed.,non_debt,-
impala,719,summary,Query on view gives error Failed to parse view-definition statement of view:,non_debt,-
impala,719,description,View creation works but query on view gives exception,non_debt,-
impala,719,comment_0,Thanks for filing this issue! The repro works like charm. Will fix.,non_debt,-
impala,720,summary,Impala cannot read Parquet files with multiple row groups,non_debt,-
impala,720,description,"Our fix for IMPALA-694 made it so we can't read Parquet files with more than one row group. While Impala will only produce files with a single row group, parquet-mr will write files with multiple row groups.",non_debt,-
impala,720,comment_0,git:,non_debt,-
impala,730,summary,NULLIF function,non_debt,-
impala,730,description,Returns NULL if the two arguments are equal and the first argument if they are not equal per:,non_debt,-
impala,777,summary,Compute stats need to use quotes with identifiers that are Impala keywords,non_debt,-
impala,777,description,Compute stats will fail when there are columns identifiers that are Impala keywords because it does not quote them.,non_debt,-
impala,777,comment_0,Fixed in:,non_debt,-
impala,782,summary,change in backends list format in statestore metrics breaks CM backend list parsing,non_debt,-
impala,782,description,"Impala 1.3 has a change to the backends list metric from the the following: To the following: This breaks the CM 4.x parsing of this list. If we do not revert this change for CDH4, then users that upgrade their Impala to 1.3 but stay on existing CM 4.x will see CM complain about Impala Daemon StateStore connectivity. I have pushed a change to deal with the new format in C5, and can for future CM 4.x releases, but existing CM 4.x releases are stuck with the existing parsing code.",non_debt,-
impala,782,comment_0,"We should revert the change for CDH4, but keep it on the CDH5 branches since it is much more readable.",non_debt,-
impala,782,comment_1,Fixed:,non_debt,-
impala,796,summary,Impalad Crashes while issuing a kinit,non_debt,-
impala,796,description,"This happened while issuing a LOT of create table statements (at least 10,000 sequentially for a single impalad). Henry was kind enough to take a look, and knows what the problem is. Backtrace:",non_debt,-
impala,796,comment_0,Fixed in:,non_debt,-
impala,800,summary,Impalad coredumps executing functional tests in @,non_debt,-
impala,800,comment_0,coredump is @ the matching binaries were deleted while I was getting the backtrace. This seems to be causing many of the failures in out test runs.,non_debt,-
impala,800,comment_1,Might be related to IMPALA-795?,non_debt,-
impala,800,comment_2,"I think we have a pretty good handle on this. This is a bug exposed/exacerbated by The sequence of events that causes the problem is: 1. Drop Function fn to impalad1 This synchronously removes the function from the local catalog and removes the lib cache entry 2. Create Function fn to impalad1 This synchronously adds the function to the local catalog 3. Select fn(). This adds the entry in the lib cache. Halfway through 3 (after the entry is added but before the query terminates), the statestored heartbeat containing the catalog deltas is processed. This includes the delta from 1). The current code does not do proper version checking so removes the entry from 3). This causes the executing function to have a ptr to a dlclosed .so. There are two issues here. The metadata consistency and crashing if a drop happens during execution (IMPALA-626)",non_debt,-
impala,800,comment_3,"Another issue is that the lib cache cleanup happens *before* executing the ""drop command"". This seems problematic because 1) we don't know what catalog version the drop was actually applied in. 2) it means the cache will be cleared even if the drop fails for some reason. I think the cleanup should only happen after the drop has completed successfully. The result of executing a ""drop function"" DDL cmd against the catalog server will provide you with a catalog version. This version can be used to distinguish between a dropped and re-created function.",non_debt,-
impala,834,summary,CREATE TABLE LIKE fails if source is a view,non_debt,-
impala,834,description,"Sometimes it's convenient to refine a schema design by iteration through multiple variations on a view, then in the end create a table with the same column definitions as that view. Or make a view + an identical table to serve as a materialized view. However, CREATE TABLE <table_name (Shell build version: Impala Shell v1.2.3 (1cab04c) built on Fri Dec 20 19:39:39 PST 2013) [localhost:21000] [localhost:21000] [localhost:21000] [localhost:21000] ERROR: Failed to load metadata for table: t2 CAUSED BY: null After this point, the metastore seems to be in an inconsistent state. You can't create a table with the same name as the failed CREATE TABLE LIKE statement, because it's stated to already exist, but you also can't drop it because it's not found: [localhost:21000] ERROR: InternalException: Table t2 already exists [localhost:21000] ERROR: AnalysisException: Table does not exist: view_on_regexp.t2 That's what happens with the original 1.2.3 bits. I thought the problem might be fixed with more recent 1.2.3 bits, but it seems to have mutated instead. CREATE TABLE LIKE doesn't throw an error but there's still something wrong with the metadata afterward so you can't access the table created via CREATE TABLE LIKE: (Shell build version: Impala Shell v1.3.0-INTERNAL (b0ad015) built on Sun Feb 16 22:03:26 PST 2014) [localhost:21000] [localhost:21000] [localhost:21000] [localhost:21000] ERROR: AnalysisException: Failed to load metadata for table: CAUSED BY: Failed to load metadata for table: t2 CAUSED BY: null After Impala starts giving the error 'Failed to load metadata for table: t2', doing 'INVALIDATE METADATA t2' does not appear to help, the error persists.",non_debt,-
impala,843,summary,Impalad's crash while running join on string column + union,non_debt,-
impala,843,description,"I only tried this on master, the imapalad log says I've tried other variations, such as no union, no join, join on int type, and they all succeeded.",non_debt,-
impala,843,comment_0,The issue is that the hdfsscannode is being closed while batches above the scan node still hold IoBuffers.,non_debt,-
impala,843,comment_1,Another repro came up. It also happens if you reference a string column in the select (the join is still needed but can be on any type),non_debt,-
impala,844,summary,Not all Thrift exceptions are caught,non_debt,-
impala,844,description,"We idiomatically do this for Thrift RPCs: Thrift can throw as well (see e.g. {{recv_*}} for any method). We don't catch it, and therefore can abort on the rare occasion it gets thrown. Instead we should catch {{TException}}.",code_debt,low_quality_code
impala,902,summary,Hide scary log spew warning from Hadoop when checking resource pool access,code_debt,low_quality_code
impala,902,description,"We use YARN's to check if a user has access to a resource pool. If the user is not on the remote system, (Hadoop) will write a scary looking message and stack to the log. This is not actually a functional problem, but this spew may appear in impalad logs and should be hidden. e.g. when I connect to a remote impalad where I'm not a user on the local system, our logs will show:",code_debt,low_quality_code
impala,902,comment_0,"Talked with Tucu and he suggested not worrying about this because it represents a misconfiguration, i.e. the users should be on the local system.",non_debt,-
impala,910,summary,Admission control should allow users to clamp very large estimates,non_debt,-
impala,910,description,"The admission control pool mem limits will currently reject requests that have memory limits that are higher than the pool mem limit. Instead of this behavior, we should offer a flag to cap mem estimates as a fraction of the pool mem limits. This will allow requests to be admitted that may be rejected just because their estimates are unreasonably high.",non_debt,-
impala,910,comment_0,Let's discuss this first before adding yet another flag. It would be preferable to come up with a solution that works without yet more user input.,non_debt,-
impala,910,comment_1,", is this something you plan to address in the AC memory work underway (perhaps without adding new configuration)?",non_debt,-
impala,910,comment_2,I don't think this is as relevant anymore. I'll re-open if we change our minds.,non_debt,-
impala,926,summary,Support more timestamp formats in casts,non_debt,-
impala,926,description,"Currently Impala only supports casting strings of the form 'yyyy-MM-dd HH:mm:ss SSSSS' to timestamps. Also add some other common patterns, such as 'yyyy/MM/dd'.",non_debt,-
impala,926,comment_0,Using to_timestamp() with the suggested mask will solve the stated issue.,non_debt,-
impala,946,summary,Loading the metadata of wide tables takes a long time (roughly linear in the number of columns).,code_debt,slow_algorithm
impala,946,description,"I ran a simple experiment to test the performance of various FE steps and noticed that loading the table metadata takes a ""long"" time, roughly linear in the number of columns. I performed the following series of actions for every data point: I ran this test for various N and the table below shows the total query time as reported by the Impala shell (I ran with impala-shell.py -f): A simple script to generate the SQL as above is attached.",code_debt,slow_algorithm
impala,946,comment_0,"I think this is a duplicate of IMPALA-428. The time is being spent loading the table metadata from the Hive Metastore. I believe most of the time is spent loading column stats, but I need to re-run experiments to confirm.",code_debt,slow_algorithm
impala,946,comment_1,"IMPALA-428 probably shouldn't be ""product backlog"" then?",non_debt,-
impala,946,comment_2,agree - bumped to v1.5,non_debt,-
impala,963,summary,Repeated inserts cause Impala to crash in ClearResultCache(). Possible race with tearing down mem trackers.,non_debt,-
impala,963,description,"User ""Krusty"" on the user list reports (rephrased): On my CDH5 cluster when repeatedly do inserts like the following my impalad crashes. Here's the stack of the crashed impalad:",non_debt,-
impala,963,comment_0,The user is performing inserts via the Hue UI.,non_debt,-
impala,963,comment_1,Srinath has tracked it,non_debt,-
impala,963,comment_2,"The query mem tracker is accessed via a coordinator instance in A null check is made on the coordinator instance. For this query, the coordinator is non-null (because it's a DML query). But the coordinator has a NULL executor and therefore returns a NULL memtracker, causing the crash.",non_debt,-
impala,977,summary,AsyncTimer runtime profile counter breaks QueryMonitoring,non_debt,-
impala,977,description,This counter has a slightly different structure than the existing ones causing the CM tool to parse this to have issues.,non_debt,-
impala,977,comment_1,"This compatibility issue breaks CM's query monitoring of Impala. It only impacts Impala v1.3.1/CDH4, Impala v1.3.1/CDH5 should not be affected. The workaround is to upgrade to CM 5.0.1+ or CM 4.8.3+. A fix for backwards compatibility of Impala with older CM versions on CDH4 will be available in a future Impala release.",non_debt,-
impala,979,summary,Abort service if statestore subscriber heartbeat_server_ fails to start successfully,non_debt,-
impala,979,description,If there is an error starting the statestore subscriber BE (heartbeat server) an Impala service should abort. This helps make it easier to diagnose issues such as port conflicts. This currently doesn't happen because we ignore the Status returned by the heartbeat_server_- From,design_debt,non-optimal_design
impala,979,comment_0,Fixed in,non_debt,-
impala,991,summary,Casting a boolean to decimal produces an analysis exception.,non_debt,-
impala,991,description,"We should be able to cast booleans to decimal (unless I'm missing something). Currently, it runs into an AnalysisException.",non_debt,-
impala,991,comment_0,This is explicitly disabled and sensible to me. Compatibility might trump sensibility though...,non_debt,-
impala,991,comment_1,I think the current behavior is sensible. We can fix this if someone needs it.,non_debt,-
impala,991,comment_2,"It means I will have to state an exception in the docs though. You can cast BOOLEAN to other numeric types but not DECIMAL: [localhost:21000] | cast(true as int) | | 1 | [localhost:21000] | cast(true as float) | | 1 | [localhost:21000] ERROR: AnalysisException: Invalid type cast of TRUE from BOOLEAN to DECIMAL(9,0)",documentation_debt,low_quality_documentation
impala,991,comment_3,I am using this opportunity to fill in some holes in the doc discussion of casting to/from BOOLEAN. BOOLEAN -BOOLEAN <-> TIMESTAMP treats false as the epoch date (1970-01-01 00:00:00) and true as 1 second later (1970-01-01 00:00:01).,documentation_debt,low_quality_documentation
impala,1008,summary,impalad crashes with a dcheck while trying to insert data into a decimal partiion,non_debt,-
impala,1008,description,Stack:,non_debt,-
impala,1008,comment_0,This is a duplicate of another bug that's been fixed.,non_debt,-
impala,1013,summary,is unreasonably slow,code_debt,slow_algorithm
impala,1013,description,This function is very simple and goes through the HS2 interface to retrieve a few functions. I believe this used to be instantaneously and now takes hundreds of seconds making this likely unusable.,code_debt,slow_algorithm
impala,1013,comment_0,I see postgres being hammered pretty good so I think something is going weird with the catalog. Didn't dig into it at all though.,non_debt,-
impala,1013,comment_1,HMS is being hammered too.,non_debt,-
impala,1013,comment_2,"Alan, could you investigate?",non_debt,-
impala,1014,summary,Disable Decimal in CREATE * on CDH4,non_debt,-
impala,1014,description,We should probably disable CREATE TABLE with decimal types on CDH4 to ensure data compatibility with the rest of CDH. We should probably also disable CREATE FUNCTION as well.,non_debt,-
impala,1014,comment_0,"This might make testing a bit tricky. Should we consider adding a query option that will allow us to enable ""unsupported"" functionality?",non_debt,-
impala,1014,comment_1,"On CDH4, can you add an analysis warning on create decimal tables?",non_debt,-
impala,1022,summary,Handle cases where in parquet the expected number of rows per metadata is not equal to the actual number of rows in the file,non_debt,-
impala,1022,description,"rows_read < rows_in_file) { We should detect the case where doesn't actually equal the number of rows in the file. If abort_on_error is true, this should fail the query, otherwise we should log something via scan_node_-Such handling did not exist before. Need to decide whether we will read at most as many rows as the metadata or continue reading until there are no more rows in the file. We will need to add tests with parquet files whose metadata is not correct.",test_debt,lack_of_tests
impala,1027,summary,Row count overflow in CTAS summary,non_debt,-
impala,1027,comment_0,"this is only for CTAS, right?",non_debt,-
impala,1027,comment_1,I was running CTAS. Didn't try with insert.,non_debt,-
impala,1027,comment_2,Fixed in,non_debt,-
impala,1043,summary,Instability running queries on a cluster with low mem limit,non_debt,-
impala,1043,description,"I ran one of the tpcds queries with a very low me limit which makes it cancel every time. After about 10K runs, it caused some of the nodes to fail: The failed nodes were Core dumps found! Core dumps found! Core dumps found! and the core dumps are at Each node has a different stack and it looks like some objects were deleted while being used.",non_debt,-
impala,1064,summary,Using a new empty table yields ArithmeticException,non_debt,-
impala,1064,description,"Inserting the result of a JOIN and UNION query using a new table can yield an STEPS: Query: create table new_table (id bigint, value string) Returned 0 row(s) in 0.04s Query: create table other_table (id bigint, value string) Returned 0 row(s) in 0.04s Query: create table destination_table (id bigint, value string) Returned 0 row(s) in 0.03s Query: explain INSERT OVERWRITE TABLE destination_table select * from new_table UNION select t2.* from new_table t1 JOIN other_table t2 ON t1.id = t2.id ERROR: / by zero Inserting a record to ""new_table"" solves the error: Query: insert INTO new_table (id, value) VALUES (1, 'test value') Inserted 1 rows in 0.25s Query: explain INSERT OVERWRITE TABLE destination_table select * from new_table UNION select t2.* from new_table t1 JOIN other_table t2 ON t1.id = t2.id | Explain String | | Estimated Per-Host Requirements: Memory=2.16GB VCores=3 | ...omitted... Also, if ""new_table"" is emptied (ex. removing files from HDFS and issuing REFRESH), it continues to function normally. It only fails when first created.",non_debt,-
impala,1064,comment_0,"The raised the ArithmeticException in line 75 because ""numNodes"" is zero.",non_debt,-
impala,1064,comment_1,I can't reproduce this anymore. Must have been fixed by another patch.,non_debt,-
impala,1072,summary,Parquet writer is miscounting the byte estimate of the current file.,non_debt,-
impala,1072,description,"This results in files that are smaller than the target size by roughly 2x. In practice, this is not a huge deal as the 1GB size might be on the larger size anyway. We should consider in the same change, lowering the default file size.",non_debt,-
impala,1072,comment_0,"From IMPALA-1252: In we increment bytes_added both in the call to EncodeValue() [which will be an uncompressed size] and by the return value from [which will be the final compressed page size]. So, our file size estimate is off by at least a factor of two, and potentially much more (when using compression). And indeed, experiments seem to confirm this. Using the default configuration, when creating a multi GB size table from tpch.lineitem data, I see parquet files created with size ~360MB. If I turn off dictionary encoding, file sizes are ~250MB (because compression ratio will be higher). If I disable compression and encoding, I get files with size 511MB. I believe our intention with the current heuristic is to always create parquet files sized around 1 GB.",non_debt,-
impala,1082,summary,Fatal error,non_debt,-
impala,1082,description,We are seeing one of our nodes crash with the attached log repeatedly. Not positive as to the query causing it but believe its the one below.,non_debt,-
impala,1082,comment_0,"Note that after shutting down impala on the node that was crashing, the above query runs but returns no results (same if I remove all where and sort clauses). Note that it does return data when executed from Hive.",non_debt,-
impala,1082,comment_1,"Please try this against 1.3.1, this may well have been fixed.",non_debt,-
impala,1082,comment_2,We are running CDH 4.4 and would need to upgrade to a later build. This appears to be occurring on only 1 or 2 of our nodes on the cluster and thus I believe is data centric. Are they any workarounds or ways to identify/cleanup the data?,code_debt,low_quality_code
impala,1082,comment_3,I don't know what you're running into. Before we can look into it we'd need you to reproduce it on the latest version.,non_debt,-
impala,1118,summary,Incorrect plan after reordering predicates (inner join following outer join),non_debt,-
impala,1118,description,"The plan for the query below has the join conditions refactored but the result is not correct. Alex looked at this and thinks its a new bug. The query should return 56 rows, but it returns 40,733.",non_debt,-
impala,1118,comment_0,Rewriting query to use the functional database:,non_debt,-
impala,1118,comment_1,Here is another test case The original query was,non_debt,-
impala,1118,comment_2,Fixing this would make query gen testing easier.,design_debt,non-optimal_design
impala,1120,summary,Fetch column stats in bulk using new (Hive .13) HMS APIs,non_debt,-
impala,1120,description,"We should update our code that fetches and updates column stats to use the new bulk APIs introduced in Hive .13. Instead of fetching a single column at a time, we can now fetch stats for a list of column names.",architecture_debt,using_obsolete_technology
impala,1120,comment_0,Fixed in,non_debt,-
impala,1147,summary,View compatibility tests need to be updated to run against Hive .13,test_debt,lack_of_tests
impala,1147,description,The view compatibility tests are disabled and need to be updated to run against Hive .13.,test_debt,lack_of_tests
impala,1147,comment_0,"I have committed a patch, Commit: that resolves this issue. Some tests that were previously set to fail in Hive now pass due to new functionality added to Hive .13",non_debt,-
impala,1211,summary,Query should be unregistered if returns error status,non_debt,-
impala,1211,description,"Once completes with ok status, then all errors following this point need to unregister the query. Otherwise, the query will hang around forever (until the session/connection is closed).",non_debt,-
impala,1211,comment_0,Similar bug in ExecuteMetadataOp. Will fix that too.,non_debt,-
impala,1259,summary,impalad crashes during cancellation in partitioned aggregation node,non_debt,-
impala,1259,description,Running the query: select * from tpch.lineitem order by l_orderkey bt,non_debt,-
impala,1259,comment_0,Coredump and binary in:,non_debt,-
impala,1259,comment_1,This should be fixed in a patch im gvming right now.,non_debt,-
impala,1259,comment_2,Addresses the stack seen in this issue. I think this should resolve it and it passed the run last night.,non_debt,-
impala,1266,summary,tpcds-q6 500gb gets aborted with an out of memory error even with enabled.,non_debt,-
impala,1266,description,"tpcds-q6 on the 10-node. Nong says this is not supposed to happen, so assigning it to him.",non_debt,-
impala,1266,comment_0,The memory doesn't seem to be coming from the block mgr. Somewhere we are using the mem pool incorrectly.,non_debt,-
impala,1277,summary,"MAX_ERRORS query option ignored for [-1, 0]",non_debt,-
impala,1277,description,When the query option {{max_errors}} is set to 0 or -1 its values is ignored during the query execution. It would be great if {{max_errors=0}} would allow to suppress all errors and {{max_errors=-1}} allows to report all errors.,non_debt,-
impala,1277,comment_0,"This may not be a shell issue, fyi.",non_debt,-
impala,1277,comment_1,"In order to avoid impald overload in case of parsing errors, it must be avoided to retrieve all errors.",non_debt,-
impala,1290,summary,AnalyticEvalNode should have results ready after Open(),non_debt,-
impala,1290,description,If an AnalyticEvalNode is the root of a plan tree it should have results ready when Open() returns so that clients do not time out once the query is in a ready state and expect to be able to fetch results. This only happens for unpartitioned analytic evaluation. Should test this in,test_debt,lack_of_tests
impala,1293,summary,Crash running analytic with window ROWS BETWEEN UNBOUNDED PRECEDING AND <large> PRECEDING,non_debt,-
impala,1293,description,"In the query below there is something special about the 11 in ""11 PRECEDING"". The query runs fine with a smaller value, but at 11 and beyond there is a crash. I noticed as the end bound gets further back (larger), the result contains more and more NULLs. I suspect 11 is the point at which all resulting values are NULL. stack trace crashing code",non_debt,-
impala,1293,comment_0,I'm going to disable generating queries with start -> end of UNBOUNDED PRECEDING -> PRECEDING or PRECEDING -> PRECEDING,non_debt,-
impala,1326,summary,Codegen: Crash running agg with LEFT JOINS (incorrect result w/out codegen),non_debt,-
impala,1326,description,With codegen disabled the query runs but the result is wrong. With codegen there is a crash. Let me know if a separate issue should be opened for the incorrect result. If no aggregation is done or the joins are INNER the query runs without crashing. Postgresql says the result should be 8 I tried getting a good stack trace several times but it is always hs_err may be more helpful,non_debt,-
impala,1326,comment_0,"Ishaan saw that if stats are not available, the results are correct and there is no crash with or without codegen.",non_debt,-
impala,1326,comment_1,"Casey, do you have the plans?",non_debt,-
impala,1326,comment_2,Here is the plan with stats. I'm loading data for c5 now. I'll try to get a plan without stats when that is done.,non_debt,-
impala,1360,summary,"TPC-DS Query (16,94,95) - FE Exception ( BETWEEN Expression)",non_debt,-
impala,1360,description,The following query fails with exception. The schema generation script for load-data.py is attached. Feel free to close or move if inappropriate. Query,non_debt,-
impala,1360,comment_0,"Removing the between expression for casted timestamps lets the query run. Same for changing it to a simpler comparison expression. Thus, my assumption is something goes wrong the rewriting of the between predicate.",non_debt,-
impala,1360,comment_1,A simpler query that triggers the same issue:,non_debt,-
impala,1360,comment_2,"Commit: Author: Dimitris Tsirogiannis Date: 2014-10-29 (Wed, 29 Oct 2014)",non_debt,-
impala,1361,summary,TPC-DS Query 51 - FE Exception (With Clause),non_debt,-
impala,1361,description,The following query fails with exception. The schema generation script for load-data.py is attached. Feel free to close or move if inappropriate.,non_debt,-
impala,1361,comment_0,The query can be executed when split into several parts. When I create temporary tables for the both with clauses and then run the query on top of these everything works fine.,non_debt,-
impala,1361,comment_1,The problem seems specific to using WITH and BETWEEN,non_debt,-
impala,1361,comment_2,"Query 64 same problem which used to work in earlier build 1.3/1.4 with cs_ui as (select cs_item_sk as as refund from catalog_sales ,catalog_returns where cs_item_sk = cr_item_sk and cs_order_number = cr_order_number group by cs_item_sk having as (select i_product_name product_name ,i_item_sk item_sk ,s_store_name store_name ,s_zip store_zip b_street_number ,ad1.ca_street_name b_streen_name ,ad1.ca_city b_city ,ad1.ca_zip b_zip c_street_number ,ad2.ca_street_name c_street_name ,ad2.ca_city c_city ,ad2.ca_zip c_zip ,d1.d_year as syear ,d2.d_year as fsyear ,d3.d_year s2year ,count(*) cnt s1 ,sum(ss_list_price) s2 ,sum(ss_coupon_amt) s3 FROM store_sales ,store_returns ,cs_ui ,date_dim d1 ,date_dim d2 ,date_dim d3 ,store ,customer cd1 cd2 ,promotion hd1 hd2 ,customer_address ad1 ,customer_address ad2 ,income_band ib1 ,income_band ib2 ,item WHERE ss_store_sk = s_store_sk AND ss_sold_date_sk = d1.d_date_sk AND ss_customer_sk = c_customer_sk AND ss_cdemo_sk= cd1.cd_demo_sk AND ss_hdemo_sk = hd1.hd_demo_sk AND ss_addr_sk = ad1.ca_address_sk and ss_item_sk = i_item_sk and ss_item_sk = sr_item_sk and ss_ticket_number = sr_ticket_number and ss_item_sk = cs_ui.cs_item_sk and c_current_cdemo_sk = cd2.cd_demo_sk AND c_current_hdemo_sk = hd2.hd_demo_sk AND c_current_addr_sk = ad2.ca_address_sk and = d2.d_date_sk and = d3.d_date_sk and ss_promo_sk = p_promo_sk and = and = and < i_color in and i_current_price between 6 and 6 + 10 and i_current_price between 6 + 1 and 6 + 15 group by i_product_name ,i_item_sk ,s_store_name ,s_zip ,ad1.ca_street_name ,ad1.ca_city ,ad1.ca_zip ,ad2.ca_street_name ,ad2.ca_city ,ad2.ca_zip ,d1.d_year ,d2.d_year ,d3.d_year ) select cs1.product_name ,cs1.store_name ,cs1.store_zip ,cs1.b_streen_name ,cs1.b_city ,cs1.b_zip ,cs1.c_street_name ,cs1.c_city ,cs1.c_zip ,cs1.syear ,cs1.cnt ,cs1.s1 ,cs1.s2 ,cs1.s3 ,cs2.s1 ,cs2.s2 ,cs2.s3 ,cs2.syear ,cs2.cnt from cross_sales cs1,cross_sales cs2 where and cs1.syear = 1999 and cs2.syear = 1999 + 1 and cs2.cnt <= cs1.cnt and cs1.store_name = cs2.store_name and cs1.store_zip = cs2.store_zip order by cs1.product_name ,cs1.store_name ,cs2.cnt limit 100",non_debt,-
impala,1361,comment_3,"Commit: Author: Dimitris Tsirogiannis Date: 2014-10-29 (Wed, 29 Oct 2014)",non_debt,-
impala,1372,summary,Query incorrectly returns NULLs,non_debt,-
impala,1372,description,The following query incorrectly returns NULLs.,non_debt,-
impala,1372,comment_0,Here is the text_types definition,non_debt,-
impala,1372,comment_1,Aas fixed in the 2.0. Definitely works on master now.,non_debt,-
impala,1382,summary,Wasted space in in presence of many NULL tuples,design_debt,non-optimal_design
impala,1382,description,We currently preallocate the null tuple indicator bitstring in each tuple stream assuming that very few tuples will be NULL. That can lead to lots of wasted space in the buffer if there are many NULL tuples in the stream. A possible solution is to use a slotted page (buffer) with NULL indicators growing from the end of the page (buffer).,design_debt,non-optimal_design
impala,1382,comment_0,I'm looking at reworking BufferedTupleStream so I'll take ownership of this.,non_debt,-
impala,1382,comment_1,"Fixing IMPALA-2138 may actually solve this issue, since one solution for IMPALA-2138 is to convert the nullness of tuple into a slot. This would avoid the BTS having to know about tuple nullness.",non_debt,-
impala,1382,comment_2,"The new code that will be enabled with IMPALA-3200 won't have this problem, since the null indicators are prefixed to each row.",non_debt,-
impala,1383,summary,Add summary data to /catalog,non_debt,-
impala,1383,description,It would be useful to augment each table in /catalog with: * Number of columns * Table format * Perhaps size of metadata,non_debt,-
impala,1383,comment_0,Also see this from IMPALA-490:,non_debt,-
impala,1383,comment_1,Feel free to reassign as a ramp-up task (and mark for 2.1).,non_debt,-
impala,1383,comment_2,Best tracked in IMPALA-4886.,non_debt,-
impala,1384,summary,Show table stats test failing,non_debt,-
impala,1384,description,I'm not sure why the expected return type of the first col is string.,non_debt,-
impala,1384,comment_0,Fixed in:,non_debt,-
impala,1396,summary,Query cancelled from Impala shell displayed as in-flight in Web UI.,non_debt,-
impala,1396,description,"Reproduction: 1. Run this query from the Impala shell. 2. Cancel the query with Ctrl+C 3. Go to Impala Web UI and observe that the query is in the EXCEPTION state, but still in the in-flight list. Instead, the query should be moved to the completed queries list. From observing the metrics, it appears that the query is at least not consuming resources anymore.",non_debt,-
impala,1396,comment_0,"The shell isn't calling close() after cancel() (you can see this by watching /rpcz). The bug is in {{_execute_stmt()}} in the shell, which probably leaves early through an exception before getting the {{close()}}. Looks like the signal handler should set a flag indicating the query was cancelled, and then close the query after cancellation in the signal handler. {{_execute_stmt()}} just has to do nothing if that flag is set when it hits the exception handler. I'm going to assign this to Ishaan to get it triaged, but this is a fix that should get into 2.1.",non_debt,-
impala,1396,comment_1,Would you mind taking a look?,non_debt,-
impala,1396,comment_2,This was fixed in Commit:,non_debt,-
impala,1414,summary,Slow query without codegen,code_debt,slow_algorithm
impala,1414,description,"The following query on a relatively small input takes ~70s with codegen and ~35 minutes without codegen, which seems unreasonably long. We should make sure we're not doing anything crazy in the non-codegen eval path. Executing with codegen: The query profile when running without codegen is attached. I also used perf (linux profiling tool) to collect a profile (sampled stacks) and printed the aggregated callstacks to the attached file The summary without codegen. Most of the time (28mins) is spent in the hash joins:",code_debt,slow_algorithm
impala,1414,comment_0,"I tested against master and it's much faster than reported. It looks like the original query was run against a DEBUG build, which may explain why the interpreted path was so slow.",non_debt,-
impala,1414,comment_1,", this JIRA is associated with a disabled test in analytic-fns.test. Are you suggesting we should enable that test again? Or perhaps we should rephrase this JIRA?",non_debt,-
impala,1430,summary,"Codegen all aggregate functions, including UDAs",non_debt,-
impala,1430,description,"Currently codegen is disabled for the entire aggregation operator if a single aggregate function can't be codegen'd. We should address this by making it so all aggregate functions can be codegen'd, including UDAs. For UDAs in .so's, the codegen'd function will call into the UDA library. This also affects aggregation operator on timestamp. This perf hit can be especially bad for COMPUTE STATS which is heavily CPU bound on the aggregation and because there is no easy way to exclude the TIMESTAMP columns when computing the column stats (i.e., there is no simple workaround). Even if the portions involving TIMESTAMP cannot be codegen'd it would still be worthwhile to come up with a workaround where codegen for the other types is still enabled. *Workaround* If you are experiencing very slow COMPUTE STATS due to this issue, then you may be able to temporarily ALTER the TIMESTAMP columns to STRING or INT type before running COMPUTE STATS. After the command completed, the columns can be altered back to TIMESTAMP. Note the workaround is only apply to text data, not parquet data. parquet require compatibles data type. TIMESTAMP is INT96, it's not compatible with STRING or BIGINT.",design_debt,non-optimal_design
impala,1443,summary,descriptive message when query cancelled,non_debt,-
impala,1443,description,"If I cancel a running query from the :25000/queries web UI, the result in impala-shell looks like: Could we have a message that says the query was cancelled, or if there are multiple potential reasons for a 'TException' at least mention cancellation as a possibility? 'TException' seems like too much of an implementation detail to expose to end users in this case.",non_debt,-
impala,1443,comment_0,We now get: which seems reasonable-ish. The query handle is no longer valid to fetch on because it was cancelled.,non_debt,-
impala,1446,summary,Create view statement throws an error when containing a subquery,non_debt,-
impala,1446,description,Create view statements containing a subquery throw an error.,non_debt,-
impala,1446,comment_0,"Commit: Author: Dimitris Tsirogiannis Date: 2014-10-17 (Fri, 17 Oct 2014)",non_debt,-
impala,1466,summary,Excessively long table metadata loading time when processing concurrent request of the same table,code_debt,slow_algorithm
impala,1466,description,"The catalog cache is cold. Loading the table metadata when I've a single request is fast (within 2sec). However, when I've 100+ concurrent queries all requesting the same table metadata, the loading time extremely long (in 10s of seconds+). This shouldn't be the case. The first request should load the metadata and all subsequent request (even though they're concurrent) should be a no-op.",code_debt,slow_algorithm
impala,1466,comment_0,I believe that should be fixed by IMPALA-4765. Please reopen if your encounter this again.,non_debt,-
impala,1475,summary,Impala cannot find unmangled symbols in UDF shared libraries,non_debt,-
impala,1475,description,"When creating a UDF via the CREATE FUNCTION command, Impala will always attempt to mangle the functions symbol in a .so file, even if the symbols aren't mangled. This leads to errors like:",non_debt,-
impala,1475,comment_0,git:,non_debt,-
impala,1481,summary,Query causes Impala crash,non_debt,-
impala,1481,description,QUERY: STACK:,non_debt,-
impala,1481,comment_0,IMPALA-1493,non_debt,-
impala,1484,summary,Query causes Impala crash,non_debt,-
impala,1484,description,QUERY: STACK:,non_debt,-
impala,1484,comment_0,I think the tuple nullable ones are dups. Having additional repro queries would be helpful.,non_debt,-
impala,1484,comment_1,IMPALA-1483,non_debt,-
impala,1487,summary,Create query option for more detailed error reporting,non_debt,-
impala,1487,description,"It would be useful to have a query option to return more detailed error information to the shell/client. For example, including dlerror strings and stack traces. We don't do this by default because in many cases it makes errors needlessly verbose and confusing.",design_debt,non-optimal_design
impala,1487,comment_0,This isn't a bad idea but is not useful to track without more specific details or use case. I'll close for now and let people open more targeted JIRAs.,non_debt,-
impala,1493,summary,Crash due to unhandled exception in timestamp functions,non_debt,-
impala,1493,description,QUERY: STACK:,non_debt,-
impala,1493,comment_0,is the resulting date is invalid.,non_debt,-
impala,1493,comment_1,"It looks like almost all boost timestamp functions can throw. We can either try catch each of those or do it at a higher level up, e.g. scalar fn call. Putting it higher up is a bit tricky since we codegen scalar fn call.",design_debt,non-optimal_design
impala,1493,comment_2,git:,non_debt,-
impala,1504,summary,Allow non-delimited-text default file formats,non_debt,-
impala,1504,description,It would be helpful if tables could be created in a desired file format (...Parquet) without specifying STORED AS X each time. This is especially true for data analysts who are often repeatedly creating and dropping tables in their work.,non_debt,-
impala,1504,comment_0,"In addition it would be nice to have the option to create a 'default' file format. This would help with pushing standards/best practices at organizations, like snappy+parquet.",non_debt,-
impala,1504,comment_1,"I've taken the ""newbie"" label off of this ticket since new feature design requires coordinating with the Impala community and getting agreement, which is more complex than other typical ""newbie"" tickets.",non_debt,-
impala,1504,comment_2,IMPALA-7645 would seem to cover this AFAIK,non_debt,-
impala,1528,summary,Crash:,non_debt,-
impala,1528,description,QUERY: STACK: DB: Functional File Format: Text/None git Hash: 05c3cc5,non_debt,-
impala,1528,comment_0,"Skye, the issue here is that the tuple is null subtree is constant so we evaluate it with a NULL tuplerow. the does not like that. Can you work with alex to figure out what the semantics are for that predicate when it is const?",non_debt,-
impala,1528,comment_1,This causes Impala to crash. Marking it as a block for 2.1 for now.,non_debt,-
impala,1543,summary,"Positive and negative zero floats hash and compare as unequal, although they should be equal.",non_debt,-
impala,1543,description,*Problem* Negative and positive zero hash to different values although they should hash and compare equal. *Workaround* Casting the floating point numbers to decimal fixes the problem: *Query Gen Details* Impala Query: Postgres Query: Number of rows returned by Impala: 16 Number of rows returned by Postgres: 15 DB: Functional File Format: Text/None git Hash: 09363ef,non_debt,-
impala,1543,comment_0,Negative and positive zero hash to different values although they should hash and compare equal. Smaller repro:,non_debt,-
impala,1543,comment_1,Not tracked as blocker even though it's a correctness issue based on low impact.,non_debt,-
impala,1543,comment_2,I'll close this duplicate since IMPALA-6660 discusses the join and agg cases separately.,non_debt,-
impala,1552,summary,Improve info shown in the OOM message,code_debt,low_quality_code
impala,1552,description,"When running out of memory in PAGG the error message does not make it clear why we ran out of memory. In the example below we see that the limit is 100MB and the consumption is only 8MB, but we ran out of memory. In this particular case, the reason is that PAGG tries to allocate the initial min mem for each partition and fails (each partition needs ~8MB and we have 32 of them). It would be good to improve the error message with some more useful info, e.g. how much more memory was needed. Backend 0:Memory Limit Exceeded Limit: Limit=100.00 MB Consumption=8.03 MB Fragment Consumption=8.00 KB EXCHANGE_NODE (id=5): Consumption=0 DataStreamRecvr: Consumption=0 Block Manager: Consumption=0 Fragment Consumption=4.01 MB SORT_NODE (id=2): Consumption=0 AGGREGATION_NODE (id=4): Consumption=4.00 MB EXCHANGE_NODE (id=3): Consumption=0 DataStreamRecvr: Consumption=0 DataStreamSender: Consumption=4.00 KB Fragment Consumption=4.01 MB AGGREGATION_NODE (id=1): Consumption=4.00 MB HDFS_SCAN_NODE (id=0): Consumption=0 DataStreamSender: Consumption=4.00 KB",code_debt,low_quality_code
impala,1560,summary,Crash:,non_debt,-
impala,1560,description,QUERY: STACK: DB: Functional File Format: Text/None git Hash: a95e7ec,non_debt,-
impala,1560,comment_0,IMPALA-1559,non_debt,-
impala,1577,summary,Improve from_utc_timestamp perf which is unpredictable and slow,code_debt,slow_algorithm
impala,1577,description,"The performance of from_utc_timestamp, besides being extremely poor compared to that of other builtin functions such as from_unixtime, depends greatly on the input timezone. Given that evaluating this function can dominate the total query time, it would be good to at least make its performance more consistent, and even better to improve its performance overall.",code_debt,slow_algorithm
impala,1577,comment_0,"I've just signed on to the Impala issue tracker and was about to report the exact same problem as described above. We had to add timezone conversions to our queries lately and the query performance decreased massively when we started using the from_utc_timestamp function. Like you said it's not only slow, but the performance is inconsistent, it seems to get worse the more rows are processed. Unfortunately (at least for us ;-)) this issue doesn't seem to get much attention. It would be great if you could let me know whether this will be addressed sometime soon or if you have any other news on this issue. Thanks in advance! P.S. We're using Impala 2.2 with CDH5.4.2.",code_debt,slow_algorithm
impala,1577,comment_1,Thanks for the feedback. I just looked at the code a little and didn't see any easy improvements. The main perf problem is most likely due to the function not being codegen'd. There's a note in the code about some LLVM limitations. Our LLVM version is about 2 years old so an upgrade might help. An upgrade is on our todo list but I couldn't say when that would be done. Also the function heavily relies on boost so that could be an issue.,architecture_debt,using_obsolete_technology
impala,1577,comment_2,"IMPALA-3307 replaced the timezone implementation, which became generally faster and should be more consistent. The old implementation looked up timezone aliases much slower than canonical timezone names, while there shouldn't be any difference in the new one (many aliases were removed, while some were added to a map for fast lookups).",code_debt,slow_algorithm
impala,1584,summary,TPCH-Q1 crashes impala when mem_limit is 125m,non_debt,-
impala,1584,description,"After IMPALA-1507 was delivered, TPC-H Q1 fails gracefully when mem_limit is set to 100m, and it runs ok when mem_limit is 150m. But when the mem_limit is set to 125m then impalad crashes.",non_debt,-
impala,1584,comment_0,"You don't have to do it this time, but next time (and in general) I think we should add the source code snippet of the failed DCHECK along with the relevant backtrace to bugs. That will make it easier to both review the fix (since it'll be easier to understand what went wrong), and also easier to identify duplicates (for all of dev, cce, and support).",code_debt,low_quality_code
impala,1587,summary,SQL Support for Cached Replicas,non_debt,-
impala,1587,description,Impala should support increasing the cache replication factor for tables and partitions. The currently proposed way of doing this is {{[set] cached in 'pool' with replication = xxx}},non_debt,-
impala,1587,comment_1,The WITH REPLICATION syntax will be in the next doc refresh for the appropriate release(s).,documentation_debt,outdated_documentation
impala,1589,summary,Unable to disable codegen with UDFs or vararg (such as IN-clause),non_debt,-
impala,1589,description,None,non_debt,-
impala,1596,summary,Builtin decimal UDFs should switch on byte size,non_debt,-
impala,1596,description,Some the builtin decimal functions assume they can always use the val16 union field of a DecimalType. This isn't actually a valid assumption though. We should only initialize and use the smaller fields (val4/val8) for performance if possible.,code_debt,low_quality_code
impala,1596,comment_1,We forgot a few:,non_debt,-
impala,1596,comment_2,", is this fixed now?",non_debt,-
impala,1596,comment_3,yes,non_debt,-
impala,1598,summary,Add error codes to user-facing msgs and use them to de-duplicate high-volume output,non_debt,-
impala,1598,description,"Some error conditions cause a high volume of output that gets sent to the users, particularly if each backend produces several messages that are aggregated by the coordinator. An example is the Parquet error message when a file spans several blocks: you get one warning per file, all of which are communicating roughly the same thing. Unfortunately we can't just do simple string-matching to de-duplicate the messages, because they're all slightly different (filename changes). We should add an error code to each user-facing message, and use that as the unique key with which to aggregate the messages when there are lots of them.",design_debt,non-optimal_design
impala,1618,summary,Impala server should always try to fulfill requested fetch size,non_debt,-
impala,1618,description,"The thrift fetch request specifies the number of rows that it would like but the Impala server may return fewer even though more results are available. For example, using the default row_batch size of 1024, if the client requests 1023 rows, the first response contains 1023 rows but the second response contains only 1 row. This is because the server internally uses row_batch (1024), returns the requested count (1023) and caches the remaining row, then the next time around only uses the cache. In general the end user should set both the row batch size and the thrift request size. In practice the query writer setting row_batch and the driver/programmer setting fetch size may often be different people. There is one case that works fine now though - setting the batch size to less than the thrift req size. In this case the thrift response is always the same as batch size. Code example:",non_debt,-
impala,1618,comment_0,"Probably the best fix here would be to add additional buffering in PlanRootSink so that the hand-off between the fetching thread and the producing thread was not synchronous. We would need to think carefully about the design - e.g. how much buffering, how should the buffered rows be stored, etc.",code_debt,multi-thread_correctness
impala,1618,comment_1,"I'm going to close this, this has been implemented in IMPALA-8819. The fix is specific to result spooling = true), but I think that is okay.",design_debt,non-optimal_design
impala,1618,comment_2,I ran the test script in the JIRA description as well and confirmed that when result spooling is enabled it always returns the requested number of rows.,non_debt,-
impala,1629,summary,Column stats for CHAR and VARCHAR not populated by COMPUTE STATS.,non_debt,-
impala,1629,description,"Even if COMPUTE STATS is run, the column stats for CHAR/VARCHAR column remain unpopulated. Follow these steps for a repro:",non_debt,-
impala,1647,summary,Spinlock should be lock compatible with Boost / STD,non_debt,-
impala,1647,description,"Currently, we use our homegrown ScopedSpinlock class that is not compatible to using boost::lock_guard / std::lock_guard and such. To be able to quickly switch / upgrade between lock types our Spinlock should be boost / std lock compatible.",non_debt,-
impala,1651,summary,CREATE TABLE LIKE should not copy caching directives of the source table.,non_debt,-
impala,1651,description,"Currently, CREATE TABLE LIKE 'blindly' copies the source table, including the table's and partitions' parameter maps. The parameter maps may contain information about caching directives that do not apply to the new table. I can see two acceptable behaviors (choose one) if the source table is cached: 1. the new table is also cached; we need to issue new caching directives and store them in the params of the new table as appropriate 2. the new table is not cached and must be cached explicitly",design_debt,non-optimal_design
impala,1651,comment_0,"I think we should separate caching from table creation, ie, go with choice 2. Users should be able to create tables freely, but global caching should be done by an admin.",non_debt,-
impala,1651,comment_1,"Bharath, let me know if you don't have time to work on it for 2.5.",non_debt,-
impala,1651,comment_2,@Dimitris:,non_debt,-
impala,1659,summary,Netezza compatibility functions: metadata,non_debt,-
impala,1659,description,Metadata functions for improved Netezza compatibility: * {{CURRENT_CATALOG}} * {{CURRENT_SCHEMA}} * {{CURRENT_USER}} * {{CURRENT_SID}} - current session ID * {{SESSION_USER}} - effective user after impersonation See attachment for links for more information.,non_debt,-
impala,1659,comment_0,"The description mentions an attachment containing more information, but I don't see any attachments. Edit: I found the attachment in",non_debt,-
impala,1659,comment_1,"As discussed in the code review, we did not add the current_schema() function, because Impala does not have schemas (at least not in the sense of a collection of objects). All other functions have been added.",non_debt,-
impala,1665,summary,varchar being returned as string in thrift packet,non_debt,-
impala,1665,description,The schema description of a varchar column in the thrift packet is being returned as a string not as a varchar.,non_debt,-
impala,1665,comment_0,"Can you say what operation is returning this? When you say 'schema description', do you mean the {{GetSchemas()}} call?",non_debt,-
impala,1665,comment_1,"Yes the GetSchema call. The code works fine, it just returns the type as string not varchar.",non_debt,-
impala,1665,comment_2,Duplicates: IMPALA-1799 Also related: IMPALA-1666,non_debt,-
impala,1673,summary,Hive created avro table throws exception in impala COMPUTE STATS,non_debt,-
impala,1673,description,"Hive created avro table throws following exception in impala COMPUTE STATS ERROR: AnalysisException: Cannot COMPUTE STATS on Avro table 'doctors' because its column definitions do not match those in the Avro schema. Missing column definition corresponding to Avro-schema column 'number' of type 'INT' at position '0'. Please re-create the table with column definitions, e.g., using the result of 'SHOW CREATE TABLE' Step1: Create avro table in hive like here Logging initialized using configuration in hive OK Time taken: 1.344 seconds Step2: Compute stats on that table in impala [x.x.x.x]# impala-shell Starting Impala Shell without Kerberos authentication Connected to Server version: impalad version 2.2.0-cdh5-INTERNAL RELEASE (build Welcome to the Impala shell. Press TAB twice to see a list of available commands. Copyright (c) 2012 Cloudera, Inc. All rights reserved. (Shell build version: Impala Shell (c8b469c) built on Mon Jan 12 22:31:12 PST 2015) [x.x.x.x:21000] Query: show tables +--+ | name | +--+ | applicant_merge | | sample_07 | | sample_08 | +--+ Fetched 3 row(s) in 0.01s [x.x.x.x:21000] Query: invalidate metadata Fetched 0 row(s) in 3.97s [x.x.x.x:21000] Query: compute stats doctors ERROR: AnalysisException: Cannot COMPUTE STATS on Avro table 'doctors' because its column definitions do not match those in the Avro schema. Missing column definition corresponding to Avro-schema column 'number' of type 'INT' at position '0'. Please re-create the table with column definitions, e.g., using the result of 'SHOW CREATE TABLE'",non_debt,-
impala,1673,comment_0,This is a Hive bug and not an Impala bug. There is nothing Impala can do on its side to resolve this problem. The best we can do is issue that error that you already see.,non_debt,-
impala,1673,comment_1,"Btw, you will find that Hive's ANALYZE TABLE also does not work on such Hive-created Avro tables.",non_debt,-
impala,1676,summary,"Unix times have ""y2k38"" problem",non_debt,-
impala,1676,description,See below Impala inherited this bug from boost Using the native linux functions is very easy.,non_debt,-
impala,1676,comment_0,Also IMPALA-1579,non_debt,-
impala,1676,comment_1,commit,non_debt,-
impala,1691,summary,Excessive Memory Usage in Catalogd (without stats),design_debt,non-optimal_design
impala,1691,description,"It used to be the case that the entire Catalogd's memory usage won't go over 1~2GB. However, a single table with 1000 columns and 8000 partitions used up 1.3GB of memory in the catalogd. The attached script created such a table. To see the memory usage, start catalogd without background loading. Observe the initial memory usage. Then run ""describe"" to load the table and observe the memory usage again.",design_debt,non-optimal_design
impala,1691,comment_0,"It could be a regression caused by Hive. The HdfsPartition contains the Hive Partition object and the Hive Partition object now contains the table schema info (and therefore, repeating the column name for each partition).",non_debt,-
impala,1691,comment_1,We should get rid of the cached {{Partition}} object inside {{HdfsPartition}}. I'm pretty sure we can reconstruct it from the partition itself when needed (i.e. when sending updates to the HMS).,design_debt,non-optimal_design
impala,1691,comment_2,I just looked at a catalogd instance with a large table loaded in a profiler. This seems to bear out the idea that the arrays of {{FieldSchema}} in the take a lot of memory. take up about 1.1GB of heap even with a relatively small catalog.,design_debt,non-optimal_design
impala,1697,summary,SHOW TABLES should also show type of table: table or view,non_debt,-
impala,1697,description,Now in order to recognize type of table(table or view) we have to run DESCRIBE FORMATTED %table%,non_debt,-
impala,1697,comment_0,"With our current metadata/loading infrastructure this seemingly simple change is actually tricky to implement. Since the initial catalog update only contains the names of all databases and tables, but not their types implementing SHOW TABLES would force us to load all the table metadata from the catalogd for those tables. Since table metadata loading is rather expensive, the SHOW TABLES command could have a very high latency which is probably incomprehensible to users. As an alternative, we could display the type as ""UNKNOWN"" for tables that have not yet been loaded, but again, this seems incomprehensible to users. Needless to say, we should consider this use case when making architectural changes to the catalog/metadata infra.",design_debt,non-optimal_design
impala,1699,summary,HdfsPartition's compareTo() method breaks contract,non_debt,-
impala,1699,description,During {{SHOW COLUMN STATS}} you might occasionally see: {{'ERROR: Comparison method violates its general contract!'}} The stack trace is: The culprit is {{HdfsPartition}}'s {{compareTo()}} method.,non_debt,-
impala,1699,comment_0,Fixed by,non_debt,-
impala,1723,summary,Minimum Required Memory For TPCH-Q9 is inconsistent,non_debt,-
impala,1723,description,"Environment: 3 Node mini cluster (local) Dataset: TPCH File Format: parquet Git Commit: 429c689 When mem_limit is set to 1289m, sometimes the query completes and sometimes it terminates with ""Memory limit exceeded"" error.",non_debt,-
impala,1723,comment_0,"I don't know how serious this bug is. Afaict, the whole variation is approx 20MBs.",non_debt,-
impala,1723,comment_1,I don't think there's anything interesting to track here beyond the JIRAs that are already linked.,non_debt,-
impala,1774,summary,Allow querying Parquet tables with complex-typed columns as long as those columns are not selected.,non_debt,-
impala,1774,description,"Today, users with existing Parquet data containing nested types are unable to query them via Impala due to some limitations we have in the backend's table column -> Parquet column resolution. It should be possible to query those tables as long as no complex-typed columns are referenced.",non_debt,-
impala,1774,comment_0,"Does the * in SELECT COUNT(*) count as 'referencing' all the columns? I presume not, but I also presume that SELECT DISTINCT * and SELECT * do, so I wonder if these different contexts for * are distinguished in terms of columns affected.",non_debt,-
impala,1774,comment_1,"John, to answer your question:",non_debt,-
impala,1774,comment_2,"This is fixed in however, I'm waiting for us to rebase on the latest Hive bits in order to commit a test for it",test_debt,lack_of_tests
impala,1790,summary,FetchResults() sometimes returns very few resuts,non_debt,-
impala,1790,description,"Hue has found that the {{FetchResults()}} RPC sometimes returns as few as 2 results, when it really should be waiting for a full batch if possible.",non_debt,-
impala,1790,comment_0,Raising the priority of common Hue issues that affect the product experience.,non_debt,-
impala,1790,comment_1,Could be IMPALA-1618,non_debt,-
impala,1805,summary,"Impala's ACLs check do not consider all group ACLs, only checked first one.",non_debt,-
impala,1805,description,After enabling HDFS synchronization with Sentry and set proper acl permission. hdfs dfs -getfacl # file: # owner: hive # group: hive user::rwx user:hive:rwx group:: group:hive:rwx group:daisuke:rwx mask::rwx other::--x all insert queries to those tables fails. insert into movie2 select 1; Query: insert into movie2 select 1 ERROR: AnalysisException: Unable to INSERT into target table (default.movie2) because Impala does not have WRITE access to at least one HDFS path:,non_debt,-
impala,1805,comment_0,This is because Impala's acl check doesn't go over all acls but only the user's one.,non_debt,-
impala,1805,comment_1,"Just commenting to say that despite the title, this issue isn't exclusively linked to the use of Sentry HDFS plugin for synchronising ACLs. Any ACLs manually added would trigger this error also.",non_debt,-
impala,1805,comment_2,Fixed by,non_debt,-
impala,1817,summary,Codegen-related crash in PAGG::Open() when running TPC-H Q16,non_debt,-
impala,1817,description,TPC-H Q16 is crashing in PAGG::Open() when I set mem_limit. When codegen is disabled it is running ok. To repro: hs_err.log has the following:,non_debt,-
impala,1817,comment_0,"Skye, assigning it to you for load balancing. Feel free to re-assign. I have the coredump if needed.",non_debt,-
impala,1817,comment_1,I tested it with the latest bits,non_debt,-
impala,1839,summary,Introducing virtual columns for auto partition pruning,non_debt,-
impala,1839,description,"It is my first message in the community and I don't know if I should write here directly. In the organisation where I work, we have developed a new feature for impala that we needed which consists you don't have to specify the partitioning columns in your queries in order to prune partitions. I don't know if it is useful for the project but for us is a good advantage and is working very well. To create a table which use this feature, just is necessary name the partitioning columns with a specific format. The format is the following: where: - COLUMN is the name of the normal column which use for pruning - part, mandatory subfix to use this feature - FUNCTION is the name of the function to apply to the COLUMN (e.g. year, month, day, module, ...) - [_ARGS] some functions (as module) need some extra arguments, these should be placed at the end of the name. You can find the implementation here: Sorry for disturb and thanks a lot for your reading.",non_debt,-
impala,1839,comment_0,"Hi  - Thanks for taking the time to describe your feature, and to post the source code! I'm very glad you were able to develop this inside Impala and have it be useful for your use case. We plan to implement a more general form of dynamic partition pruning in the future, so we won't be able to integrate your patch into Impala itself, but this JIRA will always be available for anyone looking for a similar feature who wants to add it to Impala themselves. Thanks, Henry",non_debt,-
impala,1846,summary,Add subscriber total to statestore's subscriber web page,non_debt,-
impala,1846,description,None,non_debt,-
impala,1847,summary,Redact sensitive information from graphical representation of plan tree in Web UI.,non_debt,-
impala,1847,description,"The graphical representation of the plan tree could contain sensitive information that also needs to be redacted. For example, the hash exprs of a partitioned join or a grouping aggregation are displayed. They could contain sensitive information.",non_debt,-
impala,1847,comment_0,Fixed in commit,non_debt,-
impala,1869,summary,JDBC driver can't cancel query with predicate which include function,non_debt,-
impala,1869,description,Impala JDBC driver 2.5.16 JDBC driver can't cancel query with predicate which include function. For example: When invoke driver's Statement.cancel() API Impala query still running. But this query gets cancelled successfully:,non_debt,-
impala,1869,comment_0,Also query with NDV function in SELECT can not be cancelled. For example:,non_debt,-
impala,1869,comment_1,"Hi, Can you please provide the JDBC driver logs? Please add the following two parameters at the end of your connection string to generate driver logs: to a folder to generate log files>",non_debt,-
impala,1869,comment_2,Ping!,non_debt,-
impala,1869,comment_3,This issue is fixed in latest driver version:,non_debt,-
impala,1870,summary,Support users containing commas in,non_debt,-
impala,1870,description,"expects a schema like this: The parsing breaks if 'proxy1' contains a comma, as is likely if the delegated user is an LDAP compound name. We can't change the separator without breaking users, but maybe we can support an alternate form or a way to change the separator.",non_debt,-
impala,1870,comment_0,Fixed in,non_debt,-
impala,1907,summary,In ~MemTracker() Crash: DCHECK: == 0 (435728179558 vs. 0),non_debt,-
impala,1907,description,"While investigating IMPALA-1867* we tried to move the check that the consumption() of the mem-tracker is 0 at the mem-tracker destructor, according to the diff below. However, moving it there exposed a problem in the See an example stack below. On the sending side: * IMPALA-1867 was hitting the 0) in exec-node::Close().",non_debt,-
impala,1907,comment_0,"The receiver case just needs a simple fix -- cancel the receiver before closing it. The sender case is caused by accessing invalid memory. The consumption value is a counter from the runtime profile. The profile has already been destroyed by the time ~MemTracker is called. Since nothing else in ~MemTracker accesses the runtime profile, for 2.2 the DCHECK should just be removed. Here is the log with additional info added",code_debt,low_quality_code
impala,1907,comment_1,"What do we want to do about this? Probably wait until we rework the buffer management / mem tracking code, no?",non_debt,-
impala,1907,comment_2,The current code has the same check 0)) in ExecNode::Close() and Adding the DCHECK in the ~MemTracker() is not correct as it involves more mem-trackers than previously.,non_debt,-
impala,1918,summary,redactor*-test fails on ASAN builds,non_debt,-
impala,1918,description,"Both and {{redactor-test}} fail under ASAN, and are disabled if ASAN is enabled for now.",non_debt,-
impala,1918,comment_0,I think Henry already fixed this.,non_debt,-
impala,1918,comment_1,I just disabled them on ASAN:,non_debt,-
impala,1918,comment_2,Ah thanks for the correction. I think this is some sort of clang bug. My notes from before were that most tests pass when compiling with -O0 and that the tests should be tried again after upgrading clang.,non_debt,-
impala,1918,comment_3,I'm pretty sure Martin fixed this.,non_debt,-
impala,1927,summary,Shell hangs on certain queries against CSV table,non_debt,-
impala,1927,description,"I have a CSV table with 14 columns, about 44 million rows. I found that with SELECT * and certain conditions in the WHERE clause, the shell would seem to output all the results (I would see the ++ line at the bottom of the result table) but then would hang and never return to the impala-shell prompt. If I SELECT COUNT(*) with the same WHERE clause, it works. If I SELECT DISTINCT <one of the columnsIf I do a CTAS into a Parquet table with SELECT * from the original table (no WHERE clause), it works. However, if I do the same query via 'impala-shell -q' or with delimited results via 'impala-shell -B -q', it hangs the same way. When I hit Ctrl-C, the resulting message is: ^C Cancelling Query Failed to reconnect and close: ERROR: Cancelled Cancelling Query which is different than what I saw when cancelling other SELECT or INSERT statements that were still in progress. This seems like the query has finished but the shell doesn't close it properly. The equivalent COUNT(*) and DISTINCT queries finish in 1-2 seconds. I let the SELECT * run for 3 minutes or more and it stays stuck. It's possible there is some anomaly somewhere in the CSV files, but like I say I can CTAS the whole contents of the table. It's only outputting in the shell that stalls. Schema, stalled query, actual data, profiles, logs all available on the Cloudera network for diagnosis. (Ping me for the location.)",design_debt,non-optimal_design
impala,1927,comment_0,- just wondering if you still have the CSV file?,non_debt,-
impala,1927,comment_1,"Let us know if this is still a problem, or if you have a repro file we can use.",non_debt,-
impala,1929,summary,Crash because tries to use a NULL hash_tbl,non_debt,-
impala,1929,description,"While running the mem leak test with 4 concurrent clients using the release bits an impalad crashed at with the following stack trace. This can happen in case of RIGHT_OUTER, RIGHT_ANTI and FULL_OUTER joins when the hash table of the partition is NULL. The code has a dcheck that the hash_tbl is not null, which we didn't hit because we were using the release bits.",code_debt,low_quality_code
impala,1929,comment_1,"FYI, this fix was only partial. The issue was fixed more completely with IMPALA-2168.",non_debt,-
impala,1934,summary,impala-shell to support reading password from command line when LDAP authentication is used,non_debt,-
impala,1934,description,"When LDAP authentication is enabled in Impala, impala-shell always prompts for a password when trying to connect to an Impala daemon, which makes it very hard to use impala-shell in a shell script (where to store the password?) Please make impala-shell support reading password from a command line parameter or from a password file, just like what beeline does.",design_debt,non-optimal_design
impala,1934,comment_0,Fixed in,non_debt,-
impala,1946,summary,Outer joins on a view that have ON-clause predicates only referencing the view can cause a crash.,non_debt,-
impala,1946,description,"This bug can happen in the following scenario: 1. There is an outer join on a catalog view or inline view 2. The outer join has a predicate in the On-clause only referencing the view 3. The view definition references multiple tables 4. The On-clause predicate returns a non-NULL value even when all its column values are NULL A possible indicator of this bug is when there is a ""TupleIsNull"" inside a predicate of a scan node (see explain plan below). The stack of the crash will look similar to this: Here's a query to repro: The explain plan hints at the problem: The problem here is that the checks the tuples 1 and 2. However, scan node 01 does not materialize tuple 2.",non_debt,-
impala,1951,summary,Analysis fails when qualifying a built-in function with Impala's _impala_builtins DB.,non_debt,-
impala,1951,description,Repro:,non_debt,-
impala,1951,comment_0,Works for me on trunk (2.2+).,non_debt,-
impala,1951,comment_1,"Something still looks wrong, now using the db explicitly causes a failure",non_debt,-
impala,1951,comment_2,This seems to only affect DECODE(),non_debt,-
impala,1951,comment_3,Fixed in commit,non_debt,-
impala,1954,summary,Query Planner incorrectly reports,non_debt,-
impala,1954,description,"W.r.t. to the attached file: `profile.txt` The query fragment `F00:00` reports it needs 1.01TB of memory per host, which appears to be computed using the following (row-size * cardinality / 10^12) TB = (11869695950 * 85 / 10^12) = 1.01TB. This does not account for the distribution of the query fragment amongst the nodes. The value should by i.e. 16GB, not 1TB",non_debt,-
impala,1954,comment_0,"The estimated memory consumption for the aggregation is proportional to the number of distinct values of the grouping keys per host. There are not necessarily fewer distinct values per host than globally, since each distinct value may be present on each host.",non_debt,-
impala,1963,summary,Impala Timestamp ISO-8601 Support,non_debt,-
impala,1963,description,"We have several data sources that return dates in ISO-8601 format. Following IMPALA-648, we are closer to having these convertable to timestamp without effort, but time zones are still not supported: Neither format with a time zone specifier will parse even though they are valid based on my understanding of ISO-8601 and For now we have worked around this by converting with substring to cut off the time zone identifier, but I'm not sure which time zone the dates will be interpreted in. If I cut off the Z, will my dates be parsed in server local time? Ideally, we should be able to cast dates with time zone specifiers to timestamps with cast easily.",code_debt,low_quality_code
impala,1963,comment_0,"Juan, let's try to fix this by 2.2.2. This issue affects Tableau/Impala usability.",non_debt,-
impala,1963,comment_1,"Hi Alan, do you need full ISO-8601 support? or just Time zone designators support? Silvius, full ISO-8601 support will need more time and testing.",non_debt,-
impala,1963,comment_2,"For my current project, I only really need it to handle the Z properly. Anything else (other timezones, full support) would be a bonus. In terms of Tableau/Impala usability - this problem will come up for anyone with a format that doesn't convert automatically, so the more coverage we get the better. However, in terms of release schedules, I would love to have time zones or at least Zulu time working sooner.",design_debt,non-optimal_design
impala,1963,comment_3,Fixed by Please note Impala handles it the same way as postgres.,non_debt,-
impala,1967,summary,"On Sentry enabled impala 2.1.3, one join order throws ERROR: User 'xx' does not have privileges to access: databse_yy",non_debt,-
impala,1967,description,"The following query returns results 1. select h.dealer_code from view1 h left outer join view2 i on h.dealer_code = i.dealer_code limit 5; But the same query with reversed tables order, throws authorization error 2. select h.dealer_code from view2 h left outer join view1 i on h.dealer_code = i.dealer_code limit 5; ERROR: User 'xx' does not have privileges to access: databse_yy On checking the logs, it's seen that impala is trying to get write access on the source database databse_yy on which these views are built. Impala should not need write access to database to just join the views.",non_debt,-
impala,1967,comment_0,Can you please provide detailed instructions on how to reproduce this issue?,non_debt,-
impala,1967,comment_1,"Additional information on the setup 1. Views view1 and view2 belong to database databse_of_views where user xx has ALL access. 2. tables t1,t2 belong to databse_yy on which views view1 and view2 are built. User xx has read-only access on databse_yy 3. Database databse_yy has custom UDFs defined which are called in view1",non_debt,-
impala,1967,comment_2,"Definition of views here: CREATE VIEW view1 AS SELECT c.dealer_code, CASE WHEN IS NULL THEN NULL WHEN s.cust_indicative = 'Y' THEN ELSE AS STRING) END CASE WHEN s.cust_indicative = 'Y' THEN ELSE '' END c.division_code, c.division_name, c.lst_updt_ts, c.lst_updt_by_id, c.industry_code, CASE WHEN c.prim_industry_ind IS NULL THEN NULL WHEN = 'true' THEN 'Y' ELSE 'N' END PRIM_INDUSTRY_IND, c.gen_id CUST_GEN_ID FROM database_yy.t1 S INNER JOIN database_yy.t2 C ON s.dealer_code = c.dealer_code AND s.customer = 'Y' AND s.consumer_group = 'hpddwrogpps' WHERE 1 = 1 AND c.sub_table_type = 'INDUSTRY' AND c.cust_updt_ind != 'D' CREATE VIEW view2 AS SELECT DISTINCT WHEN = 't' THEN 't' FROM database_yy.t1 S JOIN database_yy.t2 where c.cust_updt_ind != 'D'",non_debt,-
impala,1967,comment_3,Fixed by IMPALA-1519,non_debt,-
impala,1984,summary,WARNINGS: Could not list directory - On overwriting on to an empty table from another empty table,non_debt,-
impala,1984,description,"CDH 5.4 Impala 2.2.0 create TABLE EmptySrc ( F1 INT, F2 STRING ) ; create TABLE EmptyTgt ( F1 INT, F2 STRING ) ; insert OVERWRITE EmptyTgt SELECT * FROM EmptySrc ; Query: insert OVERWRITE EmptyTgt SELECT * FROM EmptySrc WARNINGS: Could not list directory: This warning is unnecessary when there are empty tables involved and should be removed.",code_debt,low_quality_code
impala,1984,comment_0,IMPALA-2008,non_debt,-
impala,2004,summary,"Implement ""SHOW CREATE"" for functions",non_debt,-
impala,2004,description,"Request is to implement ""SHOW CREATE"" for user defined functions. This originated as a customer request.",non_debt,-
impala,2004,comment_0,Is there any way I can help out on this Jira. I'm going to check out the code today and spend the next couple days studying out a design. Just let me know how I can be helpful.,non_debt,-
impala,2004,comment_1,Fixed at commit,non_debt,-
impala,2068,summary,TOpenSessionResp should contain coordinator host,non_debt,-
impala,2068,description,"It is currently difficult for clients connected via load balancers to know which coordinator they're connected to. The TOpenSessionResp has a map of configuration settings which are returned, and we can return the connected coordinator in that map. Clients such as Hue can use that information to find the coordinator and thus the debug web UI pages for submitted queries over that session.",design_debt,non-optimal_design
impala,2072,summary,== 0) on test_spilling during repeated-runs,non_debt,-
impala,2072,description,Some times repeated runs fail at test_spilling. It seems that Impalad crashes. The error from .FATAL: This is in: The last query executed:,non_debt,-
impala,2072,comment_0,I think we should assume this one is also a dup of IMPALA-2245. Thoughts?,non_debt,-
impala,2072,comment_1,Looks like a duplicate of IMPALA-2245.,non_debt,-
impala,2076,summary,ExecSummary does not report Network Transmit time,non_debt,-
impala,2076,description,"*Problem Statement* The ExecSummary does not report the time spent on the network. The time reported on the EXCHANGE node is misleading and does not count network time. This makes it impossible to identify the performance bottleneck. *Cause* The ExecSummary reports the non-waiting time spent in the EXCHANGE as the network time. Obviously, the active time spent in the EXCHANGE is zero (or close to zero). It should be replaced by the corresponding Data Stream Sender total time instead. *Workaround* Examine the corresponding time in the Data Stream Sender instead.",design_debt,non-optimal_design
impala,2076,comment_0,The DataStreamSender's total time includes a lot of things that don't necessarily contribute to network bottlenecks: # Time taken to serialize batches # Time taken waiting for concurrent RPCs to finish (in random case) # Time taken blocked waiting for RPC response when receiver hasn't yet arrived (after IMPALA-1599) What precisely is it that the EXCHANGE node's time should measure? Should it just be processing time + time spent in {{read()}} calls in the underlying socket? Is this what you mean by 'active' time? I think we could add instrumentation to Thrift to measure the latter (by subclassing {{TSocket}}).,documentation_debt,low_quality_documentation
impala,2096,summary,ImportError: No module named query,non_debt,-
impala,2096,description,Builds are failing with: 07:58:00 in <module07:58:00 from import 07:58:00 in <module07:58:00 from import ImpalaTestSuite 07:58:00 in <module07:58:00 from tests.common.query import Query 07:58:00 E ImportError: No module named query e.g.:,non_debt,-
impala,2096,comment_0,", is this resolved?",non_debt,-
impala,2099,summary,BufferedBlockMgr DCHECK == 0 failure w/ RM enabled,non_debt,-
impala,2099,description,"On a 6 node cluster with RM (Yarn+Llama) enabled, running tpcds q46 results in a DCHECK failure. The query completes fine in the same cluster with RM disabled. (profile attached). The core dump shows the following stack (missing symbols so not much else):",non_debt,-
impala,2099,comment_0,"You may not want to spend too much time debugging this at the moment as this class basically needs a full rewrite anyway. Also, maybe a dup of IMPALA-2072? I don't know what the stack trace in that case looked like though.",code_debt,low_quality_code
impala,2099,comment_1,"I'm not sure if the stack trace will necessarily show this is a dupe of IMPALA-2072 or not, I think the question to figure out is why/where was incremented previously. It looks like IMPALA-2072 happens occasionally while this happens every time.",non_debt,-
impala,2099,comment_2,"Pretty sure it's the same as IMPALA-2245, using Casey's newer JIRA since there's a bit more discussion there.",non_debt,-
impala,2112,summary,Support primary key/foreign key constraint as part of create table in Impala,non_debt,-
impala,2112,description,"These would be advisory, ie, Impala would not attempt to enforce them. However, they could be used for cardinality estimation during query planning. To be compatible with Hive: * We neither enforce or validate integrity constraints. Hence, DISABLE and NOVALIDATE options are mandatory. * RELY/NORELY is optional. The CBO is expected to use this information when a user specifies RELY. The default is NORELY. * Since we do not yet have UNIQUE in Hive, the FK mentioned must be Primary Key column in parent table. Support create table syntax like hive does: * {{create table pk(id1 integer, id2 integer, }}{{primary key(id1, id2) DISABLE NOVALIDATE);}} * {{create table fk(id1 integer, id2 integer, }}{{foreign key(id1, id2) references pk(id2, id1) DISABLE NOVALIDATE);}} * {{create table T1(id integer, name string, primary key(id) DISABLE NOVALIDATE RELY}}",non_debt,-
impala,2128,summary,Repartition instead of trying to allocate HT >1GB,non_debt,-
impala,2128,description,The current partitioned HJ and Agg implementation does not check what's the size of the hash table that it is going to be created. There are cases where the hash table of a partition to be needed to be larger than 1GB. In that case due to IMPALA-1619 we have to fail the query (for more info look the work-around IMPALA-2065). Instead of failing the query we may try to repartition that large partition in an effort to create HTs smaller than 1GB. For example in,design_debt,non-optimal_design
impala,2128,comment_0,Shouldn't we simply fix 1619?,non_debt,-
impala,2128,comment_1,"Obviously fixing 1619 is the ultimate solution. But fixing 1619 requires some effort, Martin has a better estimate. Avoiding to create a HT if it is very large is very simple as we know exactly the size. This is just an improvement suggestion.",non_debt,-
impala,2128,comment_2,"I looked the code a bit better. Now that IMPALA-2065 is in and we do not crash in ConsumeMemory() but instead return false in large (>2^31 allocations), we handle the case correctly and will attempt to spill the partition and eventually repartition it.",non_debt,-
impala,2130,summary,verification of file version is not correct,non_debt,-
impala,2130,description,The latest version of clang found this as a warning.,non_debt,-
impala,2140,summary,Add llama expansions to query profile,non_debt,-
impala,2140,description,"It would be useful to record llama expansions in the query profile. This might include #mem expansions, #cpu expansions, and total mem/cpu requested+received.",non_debt,-
impala,2140,comment_0,llama is no longer supported.,non_debt,-
impala,2174,summary,Improve the calculation of the,non_debt,-
impala,2174,description,"Whenever we serialize a row batch, even a row batch with 0 materialized slots, we always allocate an array of tuple_offsets per tuple. That means that there is a serialization overhead of 4B per tuple (per row). Currently we do not consider this overhead when we calculate the and consequently the avgRowSize_ which is used for example when we decide which input to We should take into account this overhead. Such a change may affect plans of queries with small avgRowSize_ or multiple tuples (joins).",design_debt,non-optimal_design
impala,2174,comment_0,", would you be able to finish this off (when you're back from PTO, of course)?",non_debt,-
impala,2178,summary,Impala returns incorrect value when group by multiple CHAR column,non_debt,-
impala,2178,description,"When group by contains multiple CHAR columns which have size <=2, Impala returns incorrect values for some columns. Step to reproduce: 1. create a data file and put it into hdfs 2. create table that contains multiple char columns by using this data file. 3. run a group by query. the ""col_id"" in resultset doesn't exist in original data. however, the row count and rest columns contain right value.",non_debt,-
impala,2178,comment_1,- that patch doesn't have query tests. Is there a simple repro query that we can add?,test_debt,lack_of_tests
impala,2178,comment_2,"Another symptom of the problem is, when debug build is active, you can hit a DCHECK like below in expr.cc",non_debt,-
impala,2178,comment_3,I think I was running the repro in the description.,non_debt,-
impala,2208,summary,Ignore Parquet binary stats in Impala,non_debt,-
impala,2208,description,PARQUET-251 fixed a bug where binary statistics were corrupted by Parquet MR. We need to ignore min/max stats for data written by affected versions in Impala. The logic to determine affected versions is shown here: Consider backporting to 2.2.x .,non_debt,-
impala,2208,comment_0,"I don't think Impala takes any notice of Parquet statistics at the moment. , can you confirm?",non_debt,-
impala,2208,comment_1,"Up until now Impala does not use the min/max metadata of Parquet so we don't need to backport it. But it is in our roadmap to exploit them, should that happen we need to be very careful and consider PARQUET-251.",design_debt,non-optimal_design
impala,2208,comment_2,Currently Impala does not use Parquet's min/max stats. So we are not affected by PARQUET-251. Should we start using the min/max we need to be very careful.,non_debt,-
impala,2212,summary,Beeline output doesn't align properly when accessing impala,design_debt,non-optimal_design
impala,2212,description,"When beeline is accessing the impala, the output is misaligned.",design_debt,non-optimal_design
impala,2212,comment_0,This works fine in Impala 2.1 and earlier version but not 2.2. Not sure if there is any incompatible changes in HiveStatement (hive 1.1.0) that Impala should make corresponding changes.,non_debt,-
impala,2212,comment_1,This is caused by workaround is to set silent option to true,non_debt,-
impala,2216,summary,"fails with ""Illegal reference to unmaterialized slot""",non_debt,-
impala,2216,description,See,non_debt,-
impala,2216,comment_0,"Also breaks the rhel7 build, which runs in exhaustive mode:",non_debt,-
impala,2218,summary,S3: Incorrect results in TPCH Q11,non_debt,-
impala,2218,description,See,non_debt,-
impala,2218,comment_0,I think that is duplicate of the NLJ bug (IMPALA-2192).,non_debt,-
impala,2218,comment_1,"Agreed, thanks Ippo. I thought NLJ just gave us crashes, not wrong results!",non_debt,-
impala,2242,summary,V_CPU_CORES query option does not override impalad process arg,non_debt,-
impala,2242,description,set V_CPU_CORES=x; does not override the process wide value set by,non_debt,-
impala,2242,comment_0,The query option no longer has any effect,non_debt,-
impala,2244,summary,HdfsScanNode.java computeNumNodes() can be slow,code_debt,slow_algorithm
impala,2244,description,"This function now loops through each scan range and when there are many, is noticeably slow. For example, an EXPLAIN on a query against 1M blocks and 1000K partitions went from around 450 ms to 700 ms with this change.",code_debt,slow_algorithm
impala,2244,comment_0,"I think this fix works and in my testing, fixes the issue.",non_debt,-
impala,2244,comment_1,"Dan, can you have a look? Feel free to re-assign.",non_debt,-
impala,2244,comment_2,"The proposed fix isn't quite right because of block replica. It can underestimate numLocalRanges (and overestimate numRemoteRanges - though maybe the final math works out). We could come up with some other early termination that still preserves meaning, though. But, what are you measuring and what was the before/after perf numbers? I'm surprised this matters considering already did this hashing (and additional work). Also, backend simple-scheduler will do it as well.",non_debt,-
impala,2253,summary,setting request_pool query option should not override query to pool rules,non_debt,-
impala,2253,description,"Setting the query option request_pool should not override the query to queue rules specified in the fair-scheduler.xml config. For example, if the fair-scheduler.xml has only the rule ""user"" rule specified, the queue should always be ""root.<user>"", but if the ""request_pool"" query option is set, that will be used.",non_debt,-
impala,2253,comment_0,"I think this was a Llama issue, I filed a Llama JIRA",non_debt,-
impala,2275,summary,S3: failure due to stale,non_debt,-
impala,2275,comment_0,"Branch: Home: Commit: Author: Dimitris Tsirogiannis Date: 2015-09-02 (Wed, 02 Sep 2015)",non_debt,-
impala,2276,summary,"The Isilon, S3, and localFS builds should not attempt to do a full data load if presented with a stale snapshot.",non_debt,-
impala,2276,description,"Currently, the builds which runs Impala tests against Isilon/S3 attempt to do a full data load if the available test data snapshot is stale. Since we don't run Hive on the Isilon and S3 builds, they should instead fail early with a clear message.",non_debt,-
impala,2276,comment_0,This causes the S3 and Isilon builds to fail any time the nightly data load fails until we get a new valid snapshot.,non_debt,-
impala,2282,summary,Impala should use krb5 defaults for kerberos client properties.,non_debt,-
impala,2282,description,"Currently, Impala overrides krb5's renew_lifetime parameter to ensure that tickets are renewed in time ( see We should remove the the -r flag and rely on krb5 defaults set by CM. For this to happen, we need CM to dynamically set the based on the renew_lifetime.",non_debt,-
impala,2282,comment_0,"Let's be sure to test this by lowering the reinit, ticket lifetime and max renew lifetime and verifying that impala still works over several max renewal lifetime periods.",non_debt,-
impala,2282,comment_1,"Here are the contents of manual testing: Case 1: renew_lifetime unset (or 0) ticket_lifetime = 15 minutes. reinit_interval = 5 minutes. Result: impalad fails to create a renewable ticket. Case 2: renew_lifetime = 30 minutes ticket_lifetime = 15 minutes. reinit_interval = 5 minutes. Impala starts up fine. Case 3: renew_lifetime = 1dt ticket_lifetime = 5 minutes reinit_interval = 1 minute Impala starts up and is able to renew its tickets. Case 4: renew_lifetime = 1d ticket_lifetime = 1 minutes. reinit_interval = 5 minutes. Impala's ok, but all queries will fail because after a minute of renewal because the ticket is dead ( this is expected, and a bad config), but the error could be better. you simply get something like:",non_debt,-
impala,2282,comment_2,"In case 2, let's also verify that between ticket_lifetime but before renew_lifetime, queries still work. And that after renew_lifetime, queries still work.",non_debt,-
impala,2290,summary,BTrim() with non constant second argument is not thread safe.,requirement_debt,non-functional_requirements_not_fully_satisfied
impala,2290,description,I think the fix is just to use THREAD_LOCAL.,non_debt,-
impala,2290,comment_0,Fix in the [following,non_debt,-
impala,2295,summary,ignores collection slots,non_debt,-
impala,2295,description,doesn't deep copy collections. This leaves the destination Tuple with pointers into potentially invalid memory.,design_debt,non-optimal_design
impala,2295,comment_0,"It looks like we can reproduce this with this query: -- Serializes collections over the network select straight_join t1.id, m.key from complextypestbl t1, complextypestbl t2, t2.int_map m where t1.id = t2.id It seems to crash one impalad reliably.",non_debt,-
impala,2310,summary,Add PURGE option to DROP TABLE,non_debt,-
impala,2310,description,"Prior to using HDFS Encryption to create an encryption zone that is used to hold data files for an Impala table we would see the expected behaviour of DROP TABLE removing the data files for the table. Now that encryption is used the drop table command succeeds, however, the data files are not removed and the Hive Metastore log shows ""can't be moved from an encryption zone."" Attempting to remove the data files using ""hdfs dfs -rm"" produces the same message as above, however, adding -skipTrash allows the files to be removed. On the basis of the above, adding a PURGE option to DROP TABLE (and ALTER TABLE ... DROP PARTITION) that results in -skipTrash been used for the deletion of the data files would be very useful.",non_debt,-
impala,2336,summary,Failure to execute SQL scripts with trailing comments (including comment-only scripts),non_debt,-
impala,2336,description,"With following input SQL file: $cat test.sql -- a comment only sql file -- ; Impala-shell just complains errors and return with non-zero error code: $impala-shell.sh -f test.sql Starting Impala Shell without Kerberos authentication Connected to XXX:21000 Server version: impalad version 2.3.0-cdh5-INTERNAL RELEASE (build Query: -- a comment only sql file -- ERROR: AnalysisException: Syntax error in line 2: -- ^ Encountered: EOF Expected: ALTER, COMPUTE, CREATE, DESCRIBE, DROP, EXPLAIN, GRANT, INSERT, INVALIDATE, LOAD, REFRESH, REVOKE, SELECT, SET, SHOW, TRUNCATE, USE, VALUES, WITH CAUSED BY: Exception: Syntax error Could not execute command: -- a comment only sql file -- This is not a big issue, however, I think it's better to keep similar behavior as Hive, that Hive could just smoothly pass this case.",non_debt,-
impala,2336,comment_0,Duplicates IMPALA-2195,non_debt,-
impala,2336,comment_1,"Xiaomin, I'm not convinced this is a bug. The Impala shell passes through comments to Impala's frontend for analysis, which is by design. It sees a malformed query (only comments, no text) and complains that there's a syntax error, which is the right thing to do.",non_debt,-
impala,2339,summary,Check failed: == TYPE_STRING (15 vs. 10),non_debt,-
impala,2339,description,"The query below runs fine on the stress test cluster when no mem limit is set but if the mem limit is 192M, then it crashes. (It ran 3 times in a row without a mem limit.) This is using the randomness database on Fatal log Backtrace",non_debt,-
impala,2339,comment_0,The core file is at there is also an impalad binary impalad-impala-2339 in the same dir.,non_debt,-
impala,2339,comment_1,I was recently looking at this code. Looks like the DCHECK is hit when there's a VARCHAR slot - the bug is probably in the DCHECK condition.,non_debt,-
impala,2341,summary,NPE with a left-outer-joined correlated inline view without an On clause.,non_debt,-
impala,2341,description,"Ideally, we should make this query work. Alternatively, the error message should be improved. Stack:",code_debt,low_quality_code
impala,2349,summary,when planning query,non_debt,-
impala,2349,comment_0,"Tim, can you also post the stack trace for this exception? Sometimes it helps to identify duplicates.",non_debt,-
impala,2355,summary,Impala query taking time in unregister query or on Client Fetch Wait time,non_debt,-
impala,2355,description,"I am running a select query that is fetching only 3 rows but taking 15 mins to provide the resultset. I have gone through the Profile and observed that majority of the time is spend on unregister query or Client fetch wait time. I just want to deep dive to find the exact issue. Can someone please help me in the same. Query Info Duration: 15m, 2s Rows Produced: 3 Bytes Streamed: 182.8 MiB Client Fetch Wait Time: 15.0m Client Fetch Wait Time Percentage: 100 Query Timeline Start execution: 81.43us (81.43us) Planning finished: 27ms (27ms) Ready to start remote fragments: 29ms (2ms) Remote fragments started: 724ms (694ms) Rows available: 1.16s (440ms) First row fetched: 1.76s (594ms) Unregister query: 15.0m (15.0m)",code_debt,slow_algorithm
impala,2355,comment_0,"this timer keeps track of how much time passes from when the first result row is available and when the client has called the fetch RPC to retrieve the results. So, for some reason it appears that your client is not calling fetch timely. Hope that helps.",non_debt,-
impala,2386,summary,Run Isilon stress test on RC,non_debt,-
impala,2386,description,We need to run the Isilon stress test on the RC.,test_debt,lack_of_tests
impala,2386,comment_0,"Isilon stress was run manually. Separate issue, IMPALA-2387 to document what was done. Future Isilon test coverage is to be tracked in a test plan.",test_debt,lack_of_tests
impala,2435,summary,Sorter::AddBatch does not check the Status of,non_debt,-
impala,2435,description,"While I was reviewing Dimitris's IMPALA-2369 patch, I realized that in AddBatch() we call Run::Init() without checking the return status of it. That means that Dimitris's patch probably won't do much good :) At Instead of creating a different patch, I will let Dimitris add it to his IMPALA-2369 patch.",design_debt,non-optimal_design
impala,2435,comment_0,One more place: at Sorter::Reset():,non_debt,-
impala,2435,comment_1,"Branch: Home: Commit: Author: Dimitris Tsirogiannis Date: 2015-09-29 (Tue, 29 Sep 2015)",non_debt,-
impala,2457,summary,PERCENT_RANK() returns NaN for row group with 1 row,non_debt,-
impala,2457,description,"According to the math behind the PERCENT_RANK() value, a row group containing a single row yields a division by zero error and therefore Impala returns NaN in this case. In this example, there is only 1 'Reptile' row and that line in the result set has a NaN: Most RDBMS examples on the web use data with multiple rows per group, so they don't illustrate what's supposed to happen in this case. But I suspect the right return value in this case might be zero. Cf. ""The first row in any set has a PERCENT_RANK of 0."" That statement suggests the first row (and any tied rows I presume) could be special-cased and no calculation performed. Cf. The analytic example doesn't show all of the relevant output, but there are two DEPARTMENT_IDs in the output that have only one relevant row, and in both those cases the PERCENT_RANK result is shown as 0. I didn't find any PERCENT_RANK() in MySQL to try. I don't have a PostgreSQL instance handy.",code_debt,low_quality_code
impala,2457,comment_0,Commit in:,non_debt,-
impala,2517,summary,Check failed: Validate(),non_debt,-
impala,2517,description,After ~4 hrs and ~10k queries a stress run crashed saying I'll post the location of the logs/core dump when it is available. The cluster will be up until tomorrow if anyone wants to look now. The crashed node was,non_debt,-
impala,2517,comment_0,"I actually ran into this just now running my own stress tests. For some reason, it repros very quickly for me (like after a few queries are run).",non_debt,-
impala,2517,comment_1,"I'm seeing: The check it hits is that we have a non-I/O sized buffer in free_io_buffers_, which shouldn't be possible. It seems to be happening when the buffered tuple stream of an empty partition of the hash join is closed.",non_debt,-
impala,2517,comment_2,"If you want them, the logs/core are at",non_debt,-
impala,2520,summary,Memory limit exceeded,non_debt,-
impala,2520,description,Build: It looks like the following query is failing:,non_debt,-
impala,2520,comment_0,IMPALA-2484,non_debt,-
impala,2576,summary,Continual breakage of,non_debt,-
impala,2576,description,The build was last green on September 9. The most recent failure looks Sentry related: 22:09:58 [tpch300gb_parquet Thread 0]: 22:09:58 INNER EXCEPTION: <class MESSAGE: 22:09:58 CAUSED BY:,non_debt,-
impala,2576,comment_0,"This broke again today. Since this job is consistently red, perhaps we should just disable it. It's part of a serious signal-to-noise ratio problem in our Jenkins builds.",non_debt,-
impala,2576,comment_1,This was last green on November 14.,non_debt,-
impala,2576,comment_2,- please triage for 2.5.0.,non_debt,-
impala,2576,comment_3,"This is problem with someone's infrastructure, not an Impala bug.",non_debt,-
impala,2632,summary,Link LLVM bytecode into impalad binary,non_debt,-
impala,2632,description,"Currently impalad needs external LLVM IR files to be compiled separately from the binary and distributed with the binary. This slows down builds, make it easy for the code to get out of sync in dev environments, and complicates distribution. Instead, we could simply link the IR data into the binary file, so impalad is more self-contained. These IR files are also loaded from disk by codegen for each fragment. Linking the IR into the binary would avoid this step, saving some overhead. Could also address IMPALA-799 (.bc file extension instead of .ll extension) at same time.",design_debt,non-optimal_design
impala,2642,summary,Potential deadlock in statestore error path,non_debt,-
impala,2642,description,"I just noticed this while reading the statestore code: {{OfferUpdate()}} takes if the update queue is full, but in one place that lock has already been taken by the caller: It's not as scary as it sounds, because if the update queue has > 10000 entries there's something wrong anyway, but we should fix this.",code_debt,multi-thread_correctness
impala,2642,comment_0,This reproduces easily if is lowered to a small value.,non_debt,-
impala,2657,summary,Add total memory used per query for chargeback,non_debt,-
impala,2657,description,None,non_debt,-
impala,2657,comment_0,", I thought CM was going to get this from the profile. What additionally is needed? Also, can you clarify what exactly you want impala to report (is it peak memory used per query per fragment? or is this MBseconds? or something else?).",non_debt,-
impala,2657,comment_1,"I think they should get this from the profile, but I think we need to provide it. I was thinking we would add a counter in MBseconds. We can have query memtracker keep a running value w/ a timestamp. When the mem usage changes, we add (prev_mem_used * (current_ts - prev_ts)) to the value and update the timestamp.",non_debt,-
impala,2657,comment_2,"Turns out CM already computes this based on the sampling counters already in the profile. The ""MemUsage"" profile timeseries counter keeps a max of 64 samples, downsampling after 64 are collected to make room for more. While this isn't really a perfect calculation of memory accrual, it's probably more than sufficient. We can reopen this if we find we need something more sophisticated.",design_debt,non-optimal_design
impala,2659,summary,ODBC driver not relocatable,non_debt,-
impala,2659,description,"Impala's ODBC driver only seems to work when the driver binary is placed in the default installation directory I tried to change the location of the driver by * Copying the {{*.so}} file to a different folder * Symbolically linking the {{*.so}} file from a different folder In both cases, the copied/linked driver works for simple cases - provided {{export is executed before using the driver. However, it seems to ignore the content of the environment variable. This influences a number of things: * Activating driver-side logging is not possible * Switching to {{UTF-16}} mode for unixODBC support is not possible * Specifying the option is not possible, rendering all error messages issued by the driver into {{The error message XXX could not be found in the en-US locale}} h2. Cross-checks I tried to make sure that I did not mess up on a trivial level (misspelling things etc.). Hence, I played around with the environment variable and the driver installed at its original location. It turns out all settings in the file specified by are fully respected. This means I can relocate the error message files (copying them somewhere else, even changing messages), fix unixODBC unicode compatibility, and enable logging for debugging. The only change from this well-documented situation to the one above is that I use the driver via a copied binary or a symbolic link. h2. Why this bug is important for me In our company we mainly rely on Python wheels to let users install binary software in Python virtual environments without requiring root access. In the context of ODBC drivers, this procedure worked well so far, allowing us to package ODBC drivers for MySQL, PostgreSQL, Oracle, Teradata, MonetDB, etc. Impala's ODBC driver is the first to show this most confusing and annoying behavior. h2. System environment I toyed around with the Impala ODBC driver (v 2.5.30) for Debian (64bit) on my Ubuntu 12.04 machine. Asking a system administrator I installed the provided {{.deb}} package into my system.",non_debt,-
impala,2659,comment_0,Small comment: It seems like the ODBC driver is not open source. Is this correct?,non_debt,-
impala,2659,comment_1,"I did some further digging. It seems to be okay to relocate the driver as long as the companion files and {{cacerts.pem}} are relocated to the same directory as well. If is missing, you get curious error messages and you need to use the {{LD_PRELOAD}} hack. A better error message in this case ({{File SimbaImpalaODBC.did could not be found}}) would have helped a lot.",design_debt,non-optimal_design
impala,2707,summary,Add FindOrInsert method to hash table to avoid unnecessary probe in aggregation,design_debt,non-optimal_design
impala,2707,description,"For each input row the aggregation node uses HashTable::Find() followed by HashTable::Insert() if the grouping key isn't already present in the table. Both of these methods probe the hash table to find the same bucket. If we added a FindOrInsert() method to the hash table that returned a modifiable iterator pointing to the bucket, we could save a significant number of hash table probes. There is already a TODO in the code for this, so I'm creating a JIRA to track the issue. This could speed up aggregations with large output size significantly, e.g. TPC-H query 13 (see IMPALA-2470).",code_debt,slow_algorithm
impala,2719,summary,fails with assert ['NULL'] == ['10485760'],non_debt,-
impala,2719,description,"has been broken since Nov 11. One of the them is a newly added test Bharath, would you mind taking a first look ?",non_debt,-
impala,2724,summary,test_analytic_fns: Memory limit exceeded,non_debt,-
impala,2724,description,"test_analytic_fns fails intermittently in cdh5-exhaustive runs. This is similar to IMPALA-2530 It appears that the expected failure of ""Memory limit exceeded"" didn't occur, leading to the assert in the code below to fire; The first failure for some reasons causes the memory limit to remain set at 150m, causing other tests to fail.",non_debt,-
impala,2724,comment_0,"Dan, you seem to be the last person who touched this test. It appears the flakiness of the test had to do with the general non-determinism of impala memory usage. I believe that's the same case for IMPALA-2728.",test_debt,flaky_test
impala,2724,comment_1,"I looped this 2000 times and wasn't able to reproduce it. Also tried setting and still wasn't able to repro. Please reopen if it shows up again on the builds, but otherwise I don't think we should adjust the mem_limit unless we can verify.",non_debt,-
impala,2730,summary,skipif decorator logs wrong message,non_debt,-
impala,2730,description,"When multiple SkipIf decorators are used and the test is skipped due to one of them, the message that is logged is the message of the first SkipIf decorator (even if the condition of that decorator is false). Example: $ export $ run-tests.py -k This will output ""Hive not tested with Isilon"" to Changing the order of the decorators of TestHmsIntegration in changes the log message.",non_debt,-
impala,2730,comment_0,Seems nice to fix for 2.5.0.,non_debt,-
impala,2730,comment_1,This is a pytest bug. IMPALA-2943 filed to consider upgrading pytest.,non_debt,-
impala,2771,summary,Impala DDL metadata synchronize delay,non_debt,-
impala,2771,description,"impala-shell -i bd-131 <<SQL create table census1 (name string, census_year int) partitioned by (year int); insert into census1 partition (year=2010) values insert into census1 partition (year=2011) values insert into census1 partition (year=2012) values SQL impala-shell -i bd-132 <<SQL /* refresh census1 ;*/ alter table census1 drop partition (year=2012); insert into values SQL impala-shell -i bd-133 <<SQL /* refresh census1 ;*/ refresh census1 ; select * from census1 where year=2012; SQL when not use refresh table clause, script 3 query the table make wrong result , no rows selected. script 2 appear partition not exist error. these three script execute together , through CTR-C/CTR-V to the impala-shell termial,you will find the problem. seem to metadata synchronize between different nodes delay cause problems refresh table clause is necessary? but prior to impala 2.1no this problem.",non_debt,-
impala,2771,comment_0,The description of this issue is not clear. Feel free to provide more information and reopen this JIRA.,non_debt,-
impala,2771,comment_1,"- are you wondering whether the refresh in {{bd-132}}'s script is necessary, or the one in {{bd-133}}'s script? Or both? What results do you see with {{bd-133}}?",non_debt,-
impala,2771,comment_2,"when not use refresh table clause, script 3 query the table make wrong result , no rows selected. script 2 appear partition not exist error. these three script execute together , through CTR-C/CTR-V to the impala-shell termial,you will find the problem. seem to metadata synchronize between different nodes delay cause problems",non_debt,-
impala,2771,comment_3,"This sounds similar to an issue I've experienced where not all nodes were seeing the same table metadata. The solution to my issue was to enable the SYNC_DDL option. Information on this option are available here: To be more specific, I updated the setting Impala Daemon Query Options Advanced Configuration Snippet in Cloudera Manager with the value: SYNC_DDL=1",non_debt,-
impala,2795,summary,Report spilling time per operator,non_debt,-
impala,2795,description,"There's a lump sum ""disk write time"" for the entire query, but not for individual operator. A per operator spilling time would be more useful.",non_debt,-
impala,2795,comment_0,Also bytes spilled.,non_debt,-
impala,2799,summary,Query hang up if remote impalad hosts shut down,non_debt,-
impala,2799,description,"I test impala2.3 in a 5 hosts clusterand 3 of them running impalad. Sometimes when I shut down 2 impalad hosts, the query hang up. This situation is rarely seen. By checking the impalad log and tcp connection information (through lsof), I found that when I shut down the 2 remote impalad hosts, the local impalad, i.e. the impalad accepting the query request, disconnected tcp connection with one of the 2 remote impalad, but still had tcp connection with the other one of the 2 remote impalad, and the query hang up. Every time the query hang up, the execution state is 'STARTED', and the last event is 'Ready to start remote fragments', and I cannot cancel the query. BTW, I modified default tcp keepalive parameters, include setting and This means if the tcp server is unreachable, keepalive settings guarantee the tcp client disconnecting the tcp connection actively after 30+3*10=60 seconds, but it seems it do not. Following is the log related to the hang up query.",non_debt,-
impala,2799,comment_0,I just added a method called in thrift TSocket class and called it in CreateClient method to trigger timeout after tcp keepalive sent to fix this issue.,non_debt,-
impala,2799,comment_1,Dup of IMPALA-3875,non_debt,-
impala,2833,summary,Memory limit exceeded is reported as an IO error in some rare cases,non_debt,-
impala,2833,description,This error appeared in the error log for a stress test run. We shouldn't be reporting this error associated with a temporary file.,non_debt,-
impala,2911,summary,rand() is not a constant expr,non_debt,-
impala,2911,description,"Query contains functions like ""rand(), if()"" could crash Impala. *Workaround:* turning logging down or off should fix it",design_debt,non-optimal_design
impala,2911,comment_0,can you please add a concrete steps to reproduce?,non_debt,-
impala,2911,comment_1,fixed by The fix also fix a a bug in Expr::DebugString() You could see thousands of calls for DebugString in callstack.,non_debt,-
impala,2957,summary,"Query Profile: Incorrect ""#Hosts"" count in ExecSummary for SCAN NODE",non_debt,-
impala,2957,description,"*Problem:* For scan node, even if it's scanning only one file, one block (but 3 replica), the ""#Hosts"" count should be one, but the ExecSummary showed ""3"" instead.",non_debt,-
impala,2957,comment_0,Duplicate of ?,non_debt,-
impala,2962,summary,redactor: Rule assignment operator infinite loop,code_debt,low_quality_code
impala,2962,description,Caught by an ASAN compile:,non_debt,-
impala,2962,comment_0,"Oops, I filed a duplicate but duping this one since I already assigned the other: IMPALA-2970",non_debt,-
impala,3008,summary,Improve printing of filter routing table,non_debt,-
impala,3008,description,"The filter routing table printed by the coordinator can be slightly improved: # Don't print it if there's no global filtering happening # Use the {{TablePrinter}} class to format it neatly # Print if filters are broadcast, or if they are partition only.",design_debt,non-optimal_design
impala,3008,comment_0,Fixed in,non_debt,-
impala,3017,summary,Concurrent runs of can cause impala to get OOM killed on exhaustive build,non_debt,-
impala,3017,description,I saw a private build where an impalad appears to get OOM-killed while executing the below tests. I believe what is probably happening is that a bunch of large allocations happen at the same time.,test_debt,expensive_tests
impala,3017,comment_0,"If UDFs use TryAllocate() instead of Allocate(), we may be able to avoid situations like this but I don't think we can do that until IMPALA-2756 and the like are fixed.",non_debt,-
impala,3017,comment_1,I also see a bunch of this in the impalad INFO logs:,non_debt,-
impala,3017,comment_2,"I think the UDF suggestion solves the problem for sensible configurations, but it won't help that much with the tests, since we're running 3 impalads and each has a process mem limit of 80% of system memory.",non_debt,-
impala,3070,summary,test_udfs.py fails in local filesystem mode,non_debt,-
impala,3070,description,See:,non_debt,-
impala,3070,comment_0,"One more build broken by this test :( . This requires a to skip this test in the local mode. Unfortunately due to lib-cache behavior, this test can't be rewritten using single impalad.",non_debt,-
impala,3070,comment_1,when do you plan to fix this? It's listed as a release blocker currently.,non_debt,-
impala,3070,comment_2,"Trivial fix, just submitted a CR. Should be committed soon.",non_debt,-
impala,3077,summary,Runtime filters should be retained even when spilling,non_debt,-
impala,3077,description,"Because of a quirk in the runtime filters implementation, we currently have to disable them when a join spills. The only reason this is necessary is that the filters are constructed as part of the hash table build. But there is no reason that we need to construct the filters at that point: we could instead construct the filters when doing initial processing of the build input, at which point we see all build-side input rows regardless of whether they are spilled or not. This might actually perform better since it would move some of the CPU work from the CPU-intensive hash table build to the less CPU-intensive construction of the input stream.",design_debt,non-optimal_design
impala,3077,comment_0,I'm working on this right now.,non_debt,-
impala,3099,summary,should use a thread pool,design_debt,non-optimal_design
impala,3099,description,"makes one RPC for every fragment instance, serially. There are no dependencies between fragment instances, so we should use to issue the cancellation requests.",design_debt,non-optimal_design
impala,3099,comment_0,I think we should hold off on this one - the {{rpc_pool}} is going to be mostly superseded by async rpcs when we switch to KRPC.,defect_debt,uncorrected_known_defects
impala,3099,comment_1,Will be implicitly solved through KRPC,non_debt,-
impala,3103,summary,Improve efficiency of BloomFilter Thrift serialisation,code_debt,slow_algorithm
impala,3103,description,"{{TBloomFilters}} have a 'directory' structure that is a list of individual buckets (buckets are about 64k wide). The total size of the directory can be 1MB or even much more. That leads to a lot of buckets, and very inefficient deserialisation as each bucket has to be allocated on the heap. Instead, the {{TBloomFilter}} representation should use one contiguous string (like the real {{BloomFilter}} does, so that it can be allocated with a single operation (and deserialized with a single copy).",code_debt,slow_algorithm
impala,3103,comment_0,Without this fix we see a spike to nearly 100% sys time when deserializing large bloom filters.,non_debt,-
impala,3103,comment_1,The following fix improves serialisation efficiency by 20x (!),code_debt,slow_algorithm
impala,3144,summary,Support inserts to a multi-partitioned table that lives on multiple filesystems,non_debt,-
impala,3144,description,"Currently our INSERT logic assumes that the table we're writing to lives only on one filesystem. However, the table could have different partitions on different filesystems. Adjust the logic in so that this becomes possible.",design_debt,non-optimal_design
impala,3144,comment_0,This is taken care of as a part of IMPALA-1878. No need to track it separately anymore:,non_debt,-
impala,3144,comment_1,Let's keep this open. The fact that this might get fixed in one patch doesn't mean it's not a separate feature.,non_debt,-
impala,3144,comment_2,Fixed as a part of IMPALA-1878.,non_debt,-
impala,3146,summary,Runtime filters sometimes not attached to coordinator fragment,non_debt,-
impala,3146,description,"A query that should produce a filter from the coordinator fragment sometimes has the following filter routing table: Note that src and target are both 0, which makes no sense. In fact, the source should be 2, per this plan: The query is: The failure is intermittent, and not apparently dependent on file format.",non_debt,-
impala,3201,summary,Implement basic in-memory buffer pool,non_debt,-
impala,3201,description,"This is a subtask of IMPALA-3200. The first milestone we want to hit is a usable buffer pool that does not support spilling. This would include the actual buffer pool, minus the logic for flushing unpinned blocks to disk (if it runs out of buffers, we can just terminate queries). Initially we should just allocate memory from TCMalloc. It would also include the initial work to port over exec nodes to the new buffer pool: enough to run some queries using them.",non_debt,-
impala,3232,summary,Allow table refs in subqueries to refer to WITH-clause view in parent block.,non_debt,-
impala,3232,description,"A ""not exists"" function fails if it is not a correlated subquery. Please update Impala to accept these types of queries. Example Error: Your query has the following error(s): AnalysisException: Unsupported uncorrelated NOT EXISTS subquery: SELECT 1 FROM another_table",non_debt,-
impala,3252,summary,Configuration file server.xml from shouldn't be checked in,non_debt,-
impala,3252,description,There seems to be a configuration file checked in in git that changes every time we do a build. The file is This file should be removed from git.,build_debt,build_others
impala,3252,comment_0,These files were never present in Apache Impala.,non_debt,-
impala,3252,comment_1,", thanks a lot for nailing this; it has been a nuisance for ages.",code_debt,low_quality_code
impala,3258,summary,Unblock queries on scalar columns in SequenceFiles with complex types,non_debt,-
impala,3258,description,Commit 2b4d7e blocked queries on file formats that doesn't have nested type support yet. IMPALA-3194 relaxes the logic a little for the case of RCFile when issuing a SELECT against scalar columns. Some customers are relying on querying scalar columns in SequenceFile and that needs to be unblocked as well. So this jira plans to allow all queries on SequenceFile as long as they refer to only scalar types.,non_debt,-
impala,3258,comment_0,", are you going to take a look?",non_debt,-
impala,3263,summary,Microbenchmarking - SORT,non_debt,-
impala,3263,description,Sorting is required for window functions. We need to create a micro-benchmark for sorting.,non_debt,-
impala,3263,comment_0,We currently have which does an order by followed an analytical sort,non_debt,-
impala,3263,comment_1,Test cases are already covered here,non_debt,-
impala,3276,summary,does not handle pin failure correctly,non_debt,-
impala,3276,description,PrepareForRead() allows callers to use it in two different modes. In one of the modes it does not report pin failures. The code was written assuming that it will have reservations. This can result it hitting a DCHECK or going down some strange code paths:,code_debt,low_quality_code
impala,3276,comment_0,"The reservation behaviour will change with the new buffer pool, but we will still have to have some error handling for scenarios like this in the caller, so it's worth fixing.",non_debt,-
impala,3329,summary,log rotation policies can rotate out logs during test runs,non_debt,-
impala,3329,description,"I believe the {{impalad}} log rotation policy is causing logs to rotate out that we don't have access to. {{impalad}} defaults to keeping 10 log files in the log directory. This means if {{impalad}} restarts 11 times, the first logs will be rotated out. Many of the custom cluster tests restart {{impalad}}, though. This means {{impalad}} logs in the custom_cluster logs subdirectory are missing. To prove this is happening as a result of the log rotation policy, and it's not our inability to properly collect logs, I ran locally, and to get timestamps to show more about when tests were being run, I used this: (0) This gets us timestamps when there otherwise wouldn't be any. Here's the time range for our custom cluster tests: The earliest timetstamp log files are for 20:58. Contrast this to the end-to-end tests' log directory, which only has a handful of logs and span the 2 hours it took to run the test. The quick fix is to set the custom cluster tests to run with a limitless log rotation policy. The more detailed fix is to probably plumb things through such that we don't ever rotate logs. This is safe in Jenkins since we deleted the logs directory before every run (in code): we won't have a workspace accumulating logs. (0) Yup, you need {{gawk}} for {{strftime}} which {{mawk}}, the default {{awk}} on at least Ubuntu, doesn't have.",design_debt,non-optimal_design
impala,3329,comment_1,This is making it difficult to debug,non_debt,-
impala,3329,comment_2,"I rebased yet again to GVM once more. If it fails again, I'll point you to the log directory and manually hit the Verified button.",non_debt,-
impala,3338,summary,Factor out Impala-lzo build logic,non_debt,-
impala,3338,description,"The Impala-lzo build depends on the Impala build scripts and headers, but we don't need to do an entire Impala build to produce the Impala-lzo .so. Factor out the build logic from buildall.sh so it can be built independently if needed.",build_debt,build_others
impala,3338,comment_0,"The Impala-lzo build logic in buildall.sh is already pretty simple, so doesn't make sense to change it at this point.",non_debt,-
impala,3344,summary,Simplify and document invariants in Sorter,code_debt,low_quality_code
impala,3344,description,"The sorter code does not have its internal invariants well-documented and can be tricky to understand when working on it. E.g. what are the valid states for merged versus initial runs, how are end-of-block cases handled. This issue is to clean up the sorter code while documenting important invariants as preparatory work for porting the sorter to the new buffer pool.",code_debt,low_quality_code
impala,3344,comment_1,"If this is backported, IMPALA-3670 should also be included",non_debt,-
impala,3344,comment_2,"If this is backported, IMPALA-1346 should probably also be included.",non_debt,-
impala,3344,comment_3,We should consider backporting this despite the complexity.,non_debt,-
impala,3352,summary,Analytic function eval order changed for no reason in planner test,non_debt,-
impala,3352,description,I think the plan is equivalent but I'm not sure why this failed.,non_debt,-
impala,3352,comment_0,"I've seen this before where ""hosts=2"" leads to a different plan. Maybe the tests should check that the number of hosts is as expected.",test_debt,low_coverage
impala,3352,comment_1,Casey is correct. We've seen these kind of issues in the past and they are basically a race with HDFS while its trying to replicate all blocks.,non_debt,-
impala,3352,comment_2,Duplicates IMPALA-3887,non_debt,-
impala,3387,summary,Drop table can succeed in Impala but fail in Kudu,non_debt,-
impala,3387,description,"As has been reported by a couple of users, a drop table from the Impala shell can partially succeed, resulting in the Kudu table remaining in the Kudu DB but the table being gone from Impala. This requires users to use the kudu-admin command to delete the table manually on the Kudu side. We should make ""drop table"" more atomic.",non_debt,-
impala,3387,comment_0,Using IMPALA-3424 instead,non_debt,-
impala,3399,summary,Define DITA build procedure for docs,non_debt,-
impala,3399,description,"Use the Derby instructions (even if some Derby details are obsolete) as a basis. What XML-related software to download, how to run the transform for different output formats.",non_debt,-
impala,3399,comment_0,"We've got that in place in the master branch now, instructions + toolchain mods and Makefile to make the process smooth. Tested out with both developer-oriented users and doc-focused authors.",non_debt,-
impala,3403,summary,Rework Impala installation instructions to be generic,documentation_debt,low_quality_documentation
impala,3403,description,"Currently, most of the verbiage around installation concerns the Cloudera Manager paths for packages or parcels. Probably there will be some new details in the context of an ASF release. Perhaps some of this info can be delegated to build instructions, README types of files, ancillary to the main docs containing the SQL Ref and such. That might be easier for maintenance where there are fast-changing version numbers, package names, etc. (Just an idea.)",documentation_debt,low_quality_documentation
impala,3403,comment_0,"I'm changing the ""Fix Version/s"" to 2.9 as part of a bulk edit. If you think this is wrong, please set the version to the correct one.",non_debt,-
impala,3435,summary,Showing the location of a database,non_debt,-
impala,3435,description,"Short of examining table details, there doesn't seem to be a way to show the location of a database. Maybe a ""SHOW DATABASE"" or ""SHOW DATABASE STATS"" is needed?",non_debt,-
impala,3435,comment_0,IMPALA-2196,non_debt,-
impala,3443,summary,Replace BOOST_FOREACH with ranged for() loops,non_debt,-
impala,3443,description,"Replace all uses of var_name, container)}} with {{for (type var_name: container)}}.",non_debt,-
impala,3443,comment_0,Fixed in,non_debt,-
impala,3461,summary,create table does not accept upper case hash keys,non_debt,-
impala,3461,description,"Copied from CREATE TABLE test1 ( Col1 SMALLINT, Col2 BIGINT ) distribute by hash (Col1) INTO 10 BUCKETS TBLPROPERTIES ( ERROR: Error creating Kudu table CAUSED BY: Server[Kudu Master - 172.23.48.30:7051] 4]: unknown column: name: ""Col1""",non_debt,-
impala,3461,comment_0,I don't think this is an issue anymore,non_debt,-
impala,3487,summary,stress test didn't fail on hash mismatch errors,non_debt,-
impala,3487,description,"That's a green build, but there are messages like these in the console log: In this particular run there were 10 such. A quick glance suggests we need to have 10 so-called ""successive"" errors to fail a build if this happens.",non_debt,-
impala,3487,comment_0,Bug scrub: dupe.,non_debt,-
impala,3488,summary,test_ddl.py failure on LocalFS run,non_debt,-
impala,3488,description,"On LocalFS runs, we usually skip any test that uses the hdfs_client because the hdfs_client cannot access the local filesystem. However, we've had a bug in test_ddl.py for a long time where even though we skipped tests which used the hdfs_client, the setup_method() and teardown_method() used the hdfs_client. The only reason this never failed before was because of the way we used to check if files existed in Since we only caught a generic exception, this function always returned 'True' and passed. It actually threw a ConnectionError exception. As a part of IMPALA-1878, I added a function which did the right thing by only catching 'FileNotFound' Exception. This change exposed this bug on localFS runs causing them to fail.",non_debt,-
impala,3488,comment_0,Fixed in the following commits:,non_debt,-
impala,3537,summary,Impala uses _c# aliases for functions when a query is run against a view,non_debt,-
impala,3537,description,"Impala uses _c# aliases for functions when a query is run against a view, while it does not use it when the same query is run against the real table. This is preventing applications to run transparently on the view. Impala should not use _c# aliases, unless necessary.",non_debt,-
impala,3537,comment_0,Thanks for reporting this bug!,non_debt,-
impala,3548,summary,Prune runtime filters based on RUNTIME_FILTER_MODE in the frontend,non_debt,-
impala,3548,description,"Currently, the FE generates a number of runtime filters and assigns them to plan nodes without taking the value of RUNTIME_FILTER_MODE into account. In the backend, the filters are removed from the exec nodes based on the target node types (local or remote) and the value of the RUNTIME_FILTER_MODE option. This may cause some confusion to users because they may see runtime filters in the output of explain that aren't applied when the query is executed. This operation should be performed in the FE.",design_debt,non-optimal_design
impala,3642,summary,"impala exceptions, status reports should include server address, not only instance_id",non_debt,-
impala,3642,description,mem limit error report exception status and so on,non_debt,-
impala,3642,comment_0,"addressed mem limit errors with IMPALA-4858. Another host-specific resource that produces errors is scratch disk I/O, so let's look at those to see if adding a hostname to those errors would be helpful.",non_debt,-
impala,3642,comment_1,I did this for some scratch IO errors at .,non_debt,-
impala,3647,summary,Report runtime filter memory consumption when memory limit is exceeded,non_debt,-
impala,3647,description,"Runtime filters can consume signficant amounts of memory (up to 16mb per filter). We should track this memory so that if a memory limit is exceeded, we can understand how much can be attributed to runtime filters.",non_debt,-
impala,3652,summary,Fix resource transfer in subplans with limits,non_debt,-
impala,3652,description,"There is a tricky corner case in our resource transfer model with subplans and limits. The problem is that the limit in the subplan may mean that the exec node is reset before it has returned its full output. The resource transfer logic generally attaches resources to batches at specific points in the output, e.g. end of partition, end of block, so it's possible that batches returned before the Reset() may reference resources that have not yet been transferred. It's unclear if we test this scenario consistently or if it's always handled correctly. One example is this query, reported in IMPALA-5456:",design_debt,non-optimal_design
impala,3652,comment_0,The above query seems to work now but I can cause problems with this query:,non_debt,-
impala,3652,comment_1,"The required fix is for Reset() to take a RowBatch as an argument, to which memory referenced by previously returned rows can be returned. The logic for Reset() attaching memory needs to be similar to the handling of ReachedLimit(), e.g. this example in NLJNode::GetNext():",design_debt,non-optimal_design
impala,3661,summary,Metastore failed to start on exhaustive,non_debt,-
impala,3661,description,This happened in this exhaustive release build: There is nothing unusual in the logs. Contents of hive-metastore.out:,non_debt,-
impala,3661,comment_0,No info to go on here. Let's see if it happens again.,non_debt,-
impala,3661,comment_1,Not reproducing reliably.,non_debt,-
impala,3661,comment_2,Let's just reopen if we reproduce.,non_debt,-
impala,3671,summary,Add query option to limit scratch space usage,non_debt,-
impala,3671,description,"The immediate motivation for this is to enable better testing of graceful failures when spilling is disabled (e.g. IMPALA-3670). Currently we only have control over this via startup options, so we have to implement these as custom cluster tests, but there's no reason in principle we need to start a fresh cluster. The idea would be to add a query option 'scratch_limit' or similar, that limits the amount of scratch directory space that can be used. This would be useful to prevent runaway queries or to prevent queries from spilling when that is not desired.",design_debt,non-optimal_design
impala,3671,comment_1,- Docs work required.,documentation_debt,outdated_documentation
impala,3680,summary,Reset the file offset after failed cache reads,non_debt,-
impala,3680,description,"Currently we don't reset the file read offset if ZCR fails. Due to this, when we switch to the normal read path, we hit the eosr of the scan-range even before reading the expected data length. This results in re-issuing the whole set of scan ranges and hence a severe performance hit. This has been observed while debugging IMPALA-3679, where zero-copy reads don't work with encryption zones. However this situation can apply to any general case where the ZCRs fail.",non_debt,-
impala,3718,summary,Improve functional testing on Impala Kudu,non_debt,-
impala,3718,description,The functional test coverage for Impala on Kudu can be significantly improved by doing the following: * Run TPC-H and TPC-DS against Kudu tables * Add an alltypes test table that covers all the supported Kudu data types and expand existing functional tests to use this table * Add more complex queries in the planner tests that query both Kudu and Hdfs tables * Add more complex expressions (e.g. conditional functions) in the SET portion of UPDATE statements in functional query tests * Improve the test coverage for using and computing stats in Kudu tables,test_debt,low_coverage
impala,3721,summary,Simplify creating external Kudu tables and add DROP DATABASE CASCADE,non_debt,-
impala,3721,description,None,non_debt,-
impala,3725,summary,Support Kudu UPSERT in Impala,non_debt,-
impala,3725,description,"Add UPSERT to Impala. This also includes adding/updating relevant functional tests, stress tests, and query generator tests. It may impact our statistics story too (TBD, stats work tracked by IMPALA-2830). This should add syntax which works like {{INSERT}} for Kudu tables, but specifying that Kudu perform an UPSERT rather than an INSERT.",non_debt,-
impala,3734,summary,Replace boost::shared_ptr with its std:: equivalent,non_debt,-
impala,3734,description,None,non_debt,-
impala,3734,comment_0,"Need to be a little careful about doing a blind find / replace because many Thrift APIs rely on While we are doing this, it's not a bad idea to audit pass-by-value uses of {{shared_ptr}} to see if pass-by-reference would suffice.",non_debt,-
impala,3734,comment_1,"As  suggested, we can make sure to use {{make_shared<T>}} where appropriate.",non_debt,-
impala,3742,summary,INSERTs into Kudu tables should partition and sort,non_debt,-
impala,3742,description,"Inserts into Kudu tables should be partitioned (i.e. rows hashed using the same hash partitioning as the Kudu table) and, at the table sink, sorted on the primary key. This would significantly improve performance. This will require a local sort (IMPALA-2521), and support from Kudu to provide the partitioning.",code_debt,slow_algorithm
impala,3742,comment_0,"Is there a Jira for the second part (""...support from Kudu to provide the partitioning"")?",non_debt,-
impala,3742,comment_1,I filed If you're interested in doing that work on the Impala side we'll need to talk to the Kudu folks.,non_debt,-
impala,3780,summary,Uncompressed text scanner is slow when reading strings that significantly exceed the HDFS block size,code_debt,slow_algorithm
impala,3780,description,"I observed after adding logging that the scanner was issuing many small 1024-byte scan ranges: This appears to be based on the constant in the text scanner, which is used to try and find the end of a field that extends into the next HDFS block. We could avoid this in a couple of ways: * Increase the constant. It's unclear why it's so low: I think the cost of reading additional data is probably negligible, at least up to 10's or 100's of KB or so. * Ramp up the read size, e.g. recursive doubling up to 8MB.",code_debt,slow_algorithm
impala,3780,comment_0,"After changing the local so that it ramps up the read size, it's improved dramatically. So the issue does appear to be the small reads. Before: After:",non_debt,-
impala,3805,summary,Error on predicate pushdown to Kudu on an INT col with a slot ref value,non_debt,-
impala,3805,description,Nick Yaculak reported on slack the following bug. This query would fail: With the error: But that the following cases work fine:,non_debt,-
impala,3805,comment_0,I wasn't able to reproduce that on cdh5-trunk using the Impala shell. Can you ask the user to try it through the shell and also post any error messages from the Impalad logs?,non_debt,-
impala,3805,comment_1,The error was produced on CHD 5.5.1,non_debt,-
impala,3828,summary,Planner : Add transformation rule that traverses the single node plan to swap the build and probe sides if necessary,non_debt,-
impala,3828,description,"Today the planner creates left deep trees where joins are ordered based on selectivity. The proposal is to add a rule in the planner that traverses the plan after and flip the build and probe sides if the cardinality suggests so, which produces a bushy plan. This optimization should improve queries against normalized schemas with selective joins and multiple fact tables, more ""efficient"" runtime filters will be created as a result of the plan change The query below generates a left deep plan, where are a bush plan should be created query Plan",design_debt,non-optimal_design
impala,3828,comment_0,Tried a prototype and was able to produce the correct plan shape but hitting a dcheck when executing the query,non_debt,-
impala,3859,summary,will log table data,non_debt,-
impala,3859,description,"is designed to log the data around a particular scanner parsing error. However, that log output may include sensitive table data, and should be avoided, particularly for uncompressed text formats.",design_debt,non-optimal_design
impala,3859,comment_0,Fixing all the error tests requires fixing IMPALA-3895.,non_debt,-
impala,3859,comment_1,Fixed in,non_debt,-
impala,3867,summary,tool to kill runaway queries,non_debt,-
impala,3867,description,"We already have a [good for killing idle queries. In some situations it would be nice to have a tool that kills active queries based on configurable rules, e.g. runtime or memory consumed goes over a limit. A first approach could be a script using CM API calls impalaQueries and It would be nice though if the user who originally started the query would see a custom error message that describes why and who killed the query. This should also be stored in the profile.",non_debt,-
impala,3867,comment_0,"Good idea - best place to start is probably a script that can call the HTTP cancel endpoint, after authenticating itself with the debug webserver.",non_debt,-
impala,3867,comment_1,"We've been building a lot of this stuff into Impala, which seems overall better than an external tool. I'm sure there are cases where people might want to do something different, but the requirements are unclear - I think we need more concrete use cases for this to be worth tracking.",non_debt,-
impala,3906,summary,LLVM fails to resolve symbols in release builds of Impalad,non_debt,-
impala,3906,description,"With lazy materialization of the LLVM module's bitcode, it's assumed that functions referenced by global variables can be resolved to native code in the Impalad binary so they are safe to be removed if they are not directly or indirectly referenced by IR functions used in the query. However, it appears that gcc may actually optimize some of the functions away (e.g. via inlining) so LLVM may fail to resolve certain unmaterialized IR functions as externally defined functions in Impalad when compiling the module. This failure only manifests in release builds:",non_debt,-
impala,3906,comment_0,Fixed at,non_debt,-
impala,3915,summary,Read access to views in the presence of WHERE-clause subqueries,non_debt,-
impala,3915,description,"Impala by-passes Sentry authorization for views if the query or the view itself contains a subquery in any WHERE clause. This gives read access to the views to any user that would otherwise have insufficient privileges. The underlying base tables of views are still protected. Queries that do not have subqueries in the WHERE clause are still protected (unless the view itself contains such a subquery). Other operations like accessing the view definition or altering the view are still protected. Example reproduction: 1. Setup as a user with sufficient privileges Notice the IN subquery in the WHERE clause. Other types of subqueries like NOT IN, EXISTS and NOT EXISTS also trigger this bug. 2. Log in as a user with insufficient privileges (e.g. no roles at all)",non_debt,-
impala,3915,comment_0,"The problem is that the authorization request on the catalog view is ""lost"" when rewriting the subquery as a join. I believe this code snippet from FromClause.reset() is the culprit:",non_debt,-
impala,3958,summary,had a test failure but the build succeeded,non_debt,-
impala,3958,description,"This bug is about the fact the data loading job can have a test fail, not produce any data load snapshot artifacts, and still be green. This bug is not about the failing test itself (separate bug). There's not all that much to talk about. A kudu-related backend test failed here: Much further down, we see logic that the build failed, but then didn't? Things to fix: 1. The failed kudu test should have resulted in a red build 2. The inability to generate artifacts should have resulted in a red build, regardless of failure of the kudu test",non_debt,-
impala,3958,comment_0,The green build but with no data load artifacts is the cause of the severe redness of,non_debt,-
impala,3958,comment_1,Thanks for triaging this. The root cause seems to be a script returning 0 even when it fails. I'll put out a review momentarily.,non_debt,-
impala,3958,comment_2,The fix was to change the Jenkins job script to fail the job if the data load failed. Reran this and it looks to be healthy.,non_debt,-
impala,3961,summary,unable to allocate byte array with Integer.MAX_VALUE size,non_debt,-
impala,3961,description,"Our limitation for serializing catalog update size is not 2GB, which has been commonly believed so. The observed behavior is Looking at on my system, the line that triggers this oom is: Note, how the second line above that line {{newCapacity = tries to prevent this from happening, but ironically, it wont necessarily work. I tried to do and it OOMs the same way right away. (it succeeds at {{byte[] test = new -1 also ooms. This puts an limitation of *1GB* update size on us because the default size from Thrift is 32bytes(*power of 2*), and it will fail when its capacity grows from 1GB to 2GB. (if the update is 1GB, it will OOM.) *it is guaranteed to hit 1GB, since it grows by doubling and 1GB is also a power of 2. so essentially, capacity is halved.* To expand our by x2, I think we can patch Thrift to use a ""better"" initial size that grows more friendly when it doubles to avoid this limitation at 1GB. (assume jvm is beyond our control) This will bring impala to IMPALA-2648.",non_debt,-
impala,3961,comment_0,regarding I think solution would be to use the patched jdk.,non_debt,-
impala,3961,comment_1,", I think this should be at least ""Critical"" priority. This makes a huge impact on our catalog->statestore capacity.",non_debt,-
impala,3969,summary,stress test: provide ability to disable codegen,non_debt,-
impala,3969,description,"The stress test framework doesn't have the ability to by told ""disable codegen"". We need that for IMPALA-3962. It seems like this should be simple: add another option to and set it in the appropriate places, proxied through via Jenkins if needed.",non_debt,-
impala,3969,comment_0,"We should probably allow for a way to set any query option, rather than do something codegen specific. And also impalad startup options (though that's less pressing).",non_debt,-
impala,3969,comment_1,Correct  and in fact this is what was decided in the code review,non_debt,-
impala,3969,comment_2,Sounds great.,non_debt,-
impala,4004,summary,failing due to select query querying nested types,non_debt,-
impala,4004,description,- I assigned this to you as the git history looks like you added tpch_nested_parquet to the query and I thought you might have an idea whats going on here; feel free to find another person or assign back to me if you're swamped. Here's the relevant error message:,non_debt,-
impala,4004,comment_0,Looks similar to IMPALA-3047.,non_debt,-
impala,4004,comment_1,Yes. Looks like nested type is not supported with the legacy agg and join. Let me fix up the test.,non_debt,-
impala,4009,summary,document setting up oracle for use as qgen reference database,non_debt,-
impala,4009,description,"We cannot distribute Oracle binaries with Impala, but it will be useful to explain to someone how to get Oracle set up (server, client) so that he may use it as a reference db for the query generator.",documentation_debt,low_quality_documentation
impala,4020,summary,Catalog update can fail due to database creation/deletion in Hive.,non_debt,-
impala,4020,description,"Create a hive script that creates and drops databases: for a in {1..100}; do echo ""create database blah$a;"" run this continuously: beeline -u -u hive -p hive -f hive_script.sql In another session invalidate metadata: for a in {1..100} ; do impala-shell -q ""invalidate metadata""; done Eventually you will see the following in the catalog server logs: E0817 01:10:53.680232 19081 E0817 01:10:53.703266 19081 CatalogException: Error initializing Catalog. Catalog may be empty. This is a corner case and this is why it happens. Impala reset() function in catalog invokes ""invalidate metadata"" and hive creates and drops the tables at the following times as shown in the comment MetaStoreClient msClient = // create database test; done in hive try { for (String dbName: { for (String javaFn: ""*"")) { // This call fails and throws an exception because this database does not exist in HMS now javaFn)); } msDb = Db db = new Db(dbName, this, msDb); // Restore UDFs that aren't persisted. Db oldDb = Also recreated it by deliberately putting a breakpoints and parallely deleting the databases through hive.",non_debt,-
impala,4020,comment_0,", are you currently working on this?",non_debt,-
impala,4027,summary,Memory leak with ExprCtxs not free,code_debt,low_quality_code
impala,4027,description,Do not free exprContext filter_expr_ctxs_ but refree probe_expr_ctxs_. File: Function: Status state),non_debt,-
impala,4027,comment_0,I have made a patch and commit it. Waiting for review now.,non_debt,-
impala,4027,comment_1,I think this should be pretty rare - running the old hash join with runtime filters.,non_debt,-
impala,4027,comment_2,"Despite this is pretty rare, I mean that variable ""probe_expr_ctxs_"" is added twice while variable ""filter_expr_ctxs_"" is not called by function AddExprCtxsToFree",code_debt,low_quality_code
impala,4037,summary,appears to violate lock ordering,non_debt,-
impala,4037,description,The lock order is - * holding * calls * calls * calls * calls which grabs This could lead to deadlock if another thread called GetQueryExecState() on the same ImpalaServer instance (i.e. HS2/beewax) for the same QueryExecState at the same time.,non_debt,-
impala,4037,comment_0,"fix locking during query cancellation * Refactor the child query handling out of QueryExecState and clarify locking rules. * Avoid holding while calling or which can both do RPCs or acquire * Fix a potential race between and where the cancelling thread did an unlocked read of the 'coordinator_' field and may not have cancelled the coordinator. Testing: Ran exhaustive build, ran local stress test for a bit.",non_debt,-
impala,4037,comment_1,Code review can be found here:,non_debt,-
impala,4098,summary,DCHECK in because the context has not been opened.,non_debt,-
impala,4098,description,Failed build: Stack: This is the DCHECK from the Impala logs: Relevant info from Jenkins logs:,non_debt,-
impala,4110,summary,Get Apache RAT working on 'git archive'-created tarballs,non_debt,-
impala,4110,description,"Following Kudu's setup, we should configure and run RAT: I have tried this on 2.7.0rc1, but it has been running for 24 hours, using 100% CPU, and 10G of memory.",non_debt,-
impala,4145,summary,Automated load regression testing for admission control and catalog,non_debt,-
impala,4145,description,"Umbrella feature for first phase of load testing to regress basic functionality of Admission Control and Catalog/DML. Goals: * Create a few basic automated end to end cluster tests for admission control that show resource limits (memory, queue) are obeyed, and bound over admission in lower stress situations. * Create a few basic concurrent catalog operations test in the same framework to validate correctness under load * Enable additional tests to be more easily added in the framework created * Enable self-service as well as regular regression",non_debt,-
impala,4145,comment_0,This is leftover from a previous test plan. We still want to do this kind of testing but need to rescope it.,test_debt,lack_of_tests
impala,4162,summary,Extensive logging in HDFS NameNode during metadata load when,non_debt,-
impala,4162,description,"I noticed that during metadata loading or after running ""invalidate metadata"" is that there is an extensive amount of CPU spent and memory allocated When I checked the NameNode log I found thousands of entries with the error message below By default is false. When I enabled the warning and INFO messages stopped and metadata loading after ""invalidate metadata"" was 5-10% faster. This is the call stack for the exception Query timeline with ACLs enabled Query timeline with ACLs disable",design_debt,non-optimal_design
impala,4162,comment_0,is this fixed by your patch that checks the ACL bit?,non_debt,-
impala,4162,comment_1,I think so.  please reopen if that is not the case.,non_debt,-
impala,4173,summary,Stats computation on fixed length char columns still tries to compute Max & Avg column length,non_debt,-
impala,4173,description,"For fixed length char column the query issued to compute the statistics tried to calculate Max and Average column length which is already known from the column data type. Query issued Table schema Table statistics, not clean why the Avg size is not 11, I guess an overflow might be happening somewhere as row count is 18 Billion.",non_debt,-
impala,4173,comment_0,"CHAR doesn't currently work properly, and it's very likely that in the future CHAR will be implemented as VARCHAR/STRING under the covers (with the appropriate blank padding). In that case, we still want to know the actual average length, excluding trailing spaces (because those won't be materialized during join and agg processing).",design_debt,non-optimal_design
impala,4182,summary,Display which ImpalaD is connected to through load balancer,non_debt,-
impala,4182,description,"It would be nice to have a function that returned the name of the current coordinator so that users could locate the webui that has their running queries. Problem statement: As more and more big installations use a load balancer between impala and users/hue, there is a need for users to be able to get to the query profiles for debugging. But there are a few problems with the current approaches: # Many users will not have access to CM # Profile; in shell does not help Hue users, and often the profile output is truncated on screen (via putty) so users have to exit the shell and pipe to file # Users cannot find the coordinator for the webui if they connect through a load balancer and do not have CM access",design_debt,non-optimal_design
impala,4210,summary,Impala uses an older version of httpcore which breaks KMS integration,architecture_debt,using_obsolete_technology
impala,4210,description,None,non_debt,-
impala,4222,summary,Make --quiet flag suppress warning messages in impala-shell,non_debt,-
impala,4222,description,None,non_debt,-
impala,4231,summary,Performance regression in TPC-H Q2 due to delay in filter arrival,code_debt,slow_algorithm
impala,4231,description,"This commit: ""IMPALA-3567 Part 2, IMPALA-3899: factor out PHJ builder"" caused a regression in TPC-H Q2 on some systems of up to 2x. I was able to reproduce a regression of ~2s to ~3s locally on TPC-H scale factor 20 with 3 Impala daemons in my minicluster. I spent some time looking at profiles and the key difference seems to be that runtime filters arrived later in the scans so were ineffective at reducing the size of the join. The arrival time went from slightly under 1s to slightly over 1s. The regression goes away if I set: set",non_debt,-
impala,4231,comment_0,"Should check the # Codegen time in the fragment producing the Runtime filters # Hash table build time in the join producing the filter Very likely it is one of either. FYI TPC-H Q2 is notoriously sensitive to runtime filter arrival time, not the first time to see and issue with filters arriving late.",non_debt,-
impala,4231,comment_1,Regression appears to be originating from increase in code-gen time After Before,non_debt,-
impala,4231,comment_2,"There are a few things that changed with the codegen'd code in the patch that may be relevant. * A couple of hash functions that were previously shared between the build and probe codegen are generated separately. The final IR should be the same since they're copied during inlining, but there may be some redundant optimisation done before inlining happens. * The function signature for AppendRow() changed so that it returned a status",design_debt,non-optimal_design
impala,4232,summary,RQG-on-Hive: Hive does not support aggregates inside specific analytic clause,non_debt,-
impala,4232,description,"Hive does not support aggregates inside: [AVG, COUNT, MAX, MIN, SUM] ... OVER, FIRSTVALUE, LASTVALUE, LEAD, LAG clauses The RQG needs to be modified so that this pattern is not generated for Hive queries.",non_debt,-
impala,4232,comment_0,Gerrit Review:,non_debt,-
impala,4232,comment_1,Review merged,non_debt,-
impala,4245,summary,UDF docs refer to inconsistent GitHubs for samples,documentation_debt,low_quality_documentation
impala,4245,description,"The doc page for Impala UDFs contains multiple links to the sample UDF repo published by Cloudera. However, these links are inconsistent, they point to multiple locations. In the order of occurrence: 1. Just above section (below the code block) there is a link to This is the master head; subject to change. 2. The last paragraph of section points to the UDF sample repo at this is probably the intended location. 3. Typo: The second half of section contains links to the files uda-sample.h and uda-sample.cc; here the links are reversed compared to the file names (possibly a copy-paste mixup). The links point to the UDF sample repo, which I assume is the right location. 4. The last line of the same section contains a link to Cloudera's internal github. This probably wanted to be 5. Section again points to teh sample UDF repo which I assume is the correct location.",documentation_debt,low_quality_documentation
impala,4245,comment_0,The links do not exist in the Apache Impala docs. Will address where applicable.,documentation_debt,outdated_documentation
impala,4267,summary,We need an owner for Impala docker builds,non_debt,-
impala,4267,description,"Currently no-one seems to own the Impala docker builds. We need to find a new owner for the scripts and tools involved. There is also IMPALA-4085, which needs to be addressed.  - Can you find someone?",non_debt,-
impala,4267,comment_0,"Are the scripts and tools in the ASF repository? If not, this seems more like a Cloudera-specific issue, and we should move discussion to a Cloudera forum.",non_debt,-
impala,4267,comment_1,"The issue came up in the review for IMPALA-4047 here: More discussion in PS1: Indeed we (Impala) could just not publish any docker images, but Cloudera (and anyone else) could.",non_debt,-
impala,4267,comment_2,", I think some scripts are in the ASF repo. If they are not, some should be - it will ease new contributor onboarding to have well-maintained Docker images available.",build_debt,build_others
impala,4278,summary,Don't abort Catalog startup quickly if HMS is not present,non_debt,-
impala,4278,description,"If the catalog daemon can't contact the HMS on startup, it will fail out of the {{Catalog}} constructor in We might consider not doing so, but instead retry for a longer time to allow the catalog and the HMS to be started concurrently.",non_debt,-
impala,4278,comment_0,"After the switch to in commit [1], I think we can tune [2,3] [1] [2] [3]",non_debt,-
impala,4278,comment_1,"I don't want to do that for all connection attempts - if the HMS fails, we need users to get feedback quickly (rather than have long retry times). During start-up it's a different story, because the HMS may not have failed, but just be starting concurrently.",non_debt,-
impala,4278,comment_2,Fixed in,non_debt,-
impala,4287,summary,EE tests fail to run when,non_debt,-
impala,4287,description,On a system where Kudu is not supported and thus running tests fail to collect the test dimensions:,non_debt,-
impala,4291,summary,Lazy creation of IRFunction::Type to Function* mapping,non_debt,-
impala,4291,description,"For simple queries with one or two functions to codegen, we spent about 23ms or more out of 40ms of codegen time in preparation. We can save some time by not eagerly populating the map of to {{llvm::Function*}} map. This should help with the fixes for IMPALA-3638 when we create the LLVM module unconditionally.",code_debt,slow_algorithm
impala,4291,comment_0,"IMPALA-4291: Reduce LLVM module's preparation time Previously, when creating a LlvmCodeGen object, we run an O(mn) algorithm to map the IRFunction::Type to the actual LLVM::Function object in the module. m is the size of IRFunction::Type enum and n is the total number of functions in the module. This is a waste of time if we only use few functions from the module. This change reduces the preparation time of a simple query from 23ms to 10ms. select count(*) from where l_orderkey > 20;",code_debt,slow_algorithm
impala,4304,summary,Refactor cmake_modules to not use thirdparty/,non_debt,-
impala,4304,description,"Some of the files still try to find_path() at Since the source tree does not have that path anymore, it should be changed to point to toolchain/ where necessary instead. The consequences of this could be using pre-installed libraries that may be of an incompatible version with Impala.",non_debt,-
impala,4304,comment_0,CC:,non_debt,-
impala,4309,summary,Introduce expr rewrite phase.,non_debt,-
impala,4309,description,To address issues like IMPALA-1286 we should introduce a new expr rewrite phase where analyzed exprs can be transformed with rules. The new phase could be similar to the subquery rewrite phase where we transform exprs in-place and then reset() and analyze() the whole statement again.,non_debt,-
impala,4320,summary,Use gold linker by default,non_debt,-
impala,4320,description,We should switch Impala to build using the gold linker by default to speed up builds and improve the experience of new developers. We already had a discussion on the dev@ mailing list where people seemed to agree with the idea.,design_debt,non-optimal_design
impala,4328,summary,"Remove ""cloudera"" from",non_debt,-
impala,4328,description,"The config variable has hostnames that are private to Cloudera. Can you make those environment variables or configured at the command line or something, rather than hardcoding ""Cloudera"" into the script?",code_debt,low_quality_code
impala,4351,summary,query generator random profile options for INSERT,non_debt,-
impala,4351,description,"The random query generator's QueryProfile should be extended to support deciding the following for INSERT operations: * The table into which to perform the INSERT * Whether the INSERT will have a SELECT clause or a VALUES clause * Whether the INSERT SELECT, if chosen, will have a column set, or if we are inserting into the whole table * Choosing the columns if a column set is needed * If a VALUES clause is chosen, choosing the number of rows added and those rows values",non_debt,-
impala,4385,summary,Left join doesn't work properly for array,non_debt,-
impala,4385,description,"There is a bug with left join for array. *Problem is that left join doesn't distinguish 'where' and 'on' statement.* Let's consider equivalent information which is saved first in common SQL structure, second in array. Left join should return same result, but it doesn't. Please let me know if anything is not clear *Compare results* RESULTS: 1,2,3,4,5,6,7 vs. 1,2,3,4,5,6,7 RESULTS: 2,3,7 vs. 2,3,7 *RESULTS: 2,3,7 vs. 1,2,3,4,5,6,7* Input data and tables",non_debt,-
impala,4385,comment_0,"Thanks for this excellent bug report! It was very easy to follow your examples and train of thought. Thankfully, this issue has already been fixed in IMPALA-3084. I confirmed that the correct results are returned on master.",non_debt,-
impala,4387,summary,Avro scanner crashes if the file schema has invalid decimal precision or scale,non_debt,-
impala,4387,description,Haven't seen this before; may be a flaky test.,test_debt,flaky_test
impala,4387,comment_0,Looks like a product bug DCHECK is: Backtrace is,non_debt,-
impala,4387,comment_1,I was able to reproduce this by editing an avro file with a decimal column to have a bogus schema. The schema is just JSON so it's possible to edit the binary file in VIM.,non_debt,-
impala,4397,summary,Codegen for compute stats query on 1K column table takes 4 minutes,non_debt,-
impala,4397,description,Codegen of compute stats aggregation takes 4 minutes for a table with 1K columns,non_debt,-
impala,4397,comment_2,After fix,non_debt,-
impala,4410,summary,may crash if Prepare() fails,non_debt,-
impala,4410,description,calls {{desc_tbl()- Easy fix is to only call that method if {{desc_tbl()}} is not {{nullptr}}.,non_debt,-
impala,4412,summary,Per operator timing in profile summary is incorrect when mt_dop > 0,non_debt,-
impala,4412,description,From the summary |Operator|| #Hosts|| Avg Time|| Max Time|| #Rows Est.|| #Rows|| Peak Mem|| Est. Peak Mem|| Detail| |03:AGGREGATE |1 |8.668ms |8.668ms |1 |1 |7.79 MB |-1.00 B |FINALIZE| |02:EXCHANGE |1 |4.914ms |4.914ms |480 |1 | 0 |-1.00 B |UNPARTITIONED| |01:AGGREGATE |480 |1s024ms |21s191ms |24 |1 |14.04 MB |-1.00 B | | |00:SCAN HDFS |480 |949.025ms |20s143ms |900.74M |18.00B |136.28 MB |-1.00 B The slowest fragment in the query,non_debt,-
impala,4412,comment_0,Review in:,non_debt,-
impala,4412,comment_1,The # Rows column is also incorrect.,non_debt,-
impala,4412,comment_2,"Yes, that will be fixed too.",non_debt,-
impala,4412,comment_3,Commit in:,non_debt,-
impala,4423,summary,Wrong results with several conjunctive EXISTS subqueries that can be evaluated at query-compile time.,non_debt,-
impala,4423,description,"Queries with several AND-ed EXISTS subqueries in the WHERE clause may produce incorrect results if some of the subqueries can be evaluated at query compile time. Repro with wrong plan: Same query as above but flipping the order of subqueries gives the correct plan: The underlying problem is that we substitute out the subqueries with constant literals using an but the Subquery.equals() function is not implemented properly, so the second subquery is replaced with whatever boolean literal corresponds to the first subquery.",requirement_debt,requirement_partially_implemented
impala,4441,summary,Divide-by-zero in,non_debt,-
impala,4441,description,is 0 in the following code:,non_debt,-
impala,4441,comment_0,Commit in:,non_debt,-
impala,4460,summary,Investigate and support Kudu basic authentication,non_debt,-
impala,4460,description,"Kudu is adding basic authentication via kerberos (which is the first stage of a much broader pipeline of security features). It sounds like the Kudu client will use kerberos/sasl internally, possibly requiring Impala to make a call on the Kudu client, but we don't have the details yet. Once we have details about what changes will occur, we need to: # Evaluate the impact for Impala. This includes considering the impact of the kudu client interacting with kerberos/sasl in the Impala process (e.g. will this interact with / break Impala's existing kerberos mechanisms). # Make code changes to enable Kudu authentication # Testing",non_debt,-
impala,4460,comment_0,See,non_debt,-
impala,4485,summary,Shift metadata locking level from Table to Partition wide,non_debt,-
impala,4485,description,"Currently table metadata changes are locked at the table level for DML. For tables with partitions, it would help performance if concurrent inserts if locking was based at the partition level based on changes to the partition(s) affected, instead of locking the entire table.",design_debt,non-optimal_design
impala,4485,comment_0,Subsumed by IMPALA-1628.,non_debt,-
impala,4510,summary,pytest command line args need to be passed to test verifiers for remote tests,non_debt,-
impala,4510,description,"We apparently made a decision to not pass command line args when calling at the end of each test run. But we need command line args like --hive_server2, --metastore_server, or --impalad for remote cluster tests. Without them, these verifiers will always fail.",non_debt,-
impala,4510,comment_0,Patch submitted for review:,non_debt,-
impala,4542,summary,Use-after-free in various backend tests,non_debt,-
impala,4542,description,", is this related to commit or is it some sort of latent bug ? Didn't look too much into why the destructor of test_env_ was invoked twice. I thought it was single threaded.",non_debt,-
impala,4542,comment_0,"The problem I think is that accesses however the {{TestEnv}} d'tor deletes the singleton instance so the next test will hit use-after-free. Fixing this problem is easy, since {{RuntimeState}} actually has its own {{ExecEnv}} pointer. But maybe a better approach is a testing-only 'install new exec env' call that replaces the singleton. I know that there's an upcoming change where we remove completely, perhaps we can defer that change until then.",non_debt,-
impala,4542,comment_1,Fixed in,non_debt,-
impala,4544,summary,Data loading without snapshot fails with ASAN,non_debt,-
impala,4544,description,Setting and loading test data from scratch fails: From,non_debt,-
impala,4544,comment_0,Just curious if this is a regression ? Has it ever worked before ?,non_debt,-
impala,4544,comment_1,I do not know.,non_debt,-
impala,4544,comment_2,"I don't know for sure whether this ever worked. However, having to run a full data load happens all the time. I'd be very surprised if this was not a regression and we're seeing ASAN run with a full data load for the ""first"" time.",non_debt,-
impala,4544,comment_3,"I'm not sure which commit or Jenkins change fixed this, but it's fixed now:",non_debt,-
impala,4548,summary,should wait for completion of async build thread,non_debt,-
impala,4548,description,"As shown in IMPALA-4532, the build async thread can still be running after the query has completed. This may lead use-after-free issue in IMPALA-4532. A more appropriate design is for to wait for the completion of the build thread.",code_debt,multi-thread_correctness
impala,4548,comment_1,As a reminder: 2.9 hasn't been released (or even proposed + branch cut) yet.,non_debt,-
impala,4548,comment_2,Oops..sorry for the wrong version.,non_debt,-
impala,4580,summary,Crash when FETCH_FIRST exhausts result set cache,non_debt,-
impala,4580,description,"The following sequence can lead to a crash: # Client sets result cache size to N # Client issues query with #results < N # Client fetches all results, triggering {{eos}} and tearing down # Client restarts query with {{FETCH_FIRST}}. # Client reads all results again. After cache is exhausted, is called to detect {{eos}} condition again. # {{GetNext()}} hits {{DCHECK(root_sink_ != nullptr)}}. Either the QES or the coordinator need to remember that {{eos}} was once true, and turn {{GetNext()}} into a no-op if so.",non_debt,-
impala,4580,comment_0,Fixed in,non_debt,-
impala,4590,summary,Kudu test failure; several error messages changed,non_debt,-
impala,4590,description,Several error messages changed which break Impala tests. For example: There is also a change to : Testing w/ native-toolchain & build: Updating the toolchain to the above version will be blocked on this.,non_debt,-
impala,4593,summary,"kudu-python is built with the system C++ compiler, which may not be ABI-compatible with the toolchain C++ compiler",non_debt,-
impala,4593,description,I ran into this issue just now: importing kudu fails because of an unresolved symbol. The issue seems to be that my system C++ is much newer than gcc-4.9.2 and they have different C++ ABIs (or something along those lines). One workaround is to manually override the C compilers.,non_debt,-
impala,4593,comment_0,Just to clarify: You failed to import the Kudu Python client right? And this is on Ubuntu 16.04? I have this problem too.,non_debt,-
impala,4593,comment_1,I forgot I even filed this. Yeah it looks like this is the issue you ran into.,non_debt,-
impala,4593,comment_2,This is an example of the error:,non_debt,-
impala,4612,summary,in logs when dropping function,non_debt,-
impala,4612,description,I'm seeing a lot of error messages in the log when running test_udf.py along the lines of: I haven't seen any adverse effects of these messages but it's adding a lot of noise.,design_debt,non-optimal_design
impala,4612,comment_0,This is the symptom of IMPALA-4795's underlying cause.,non_debt,-
impala,4617,summary,Remove duplication of isConstant() and IsConstant() in frontend and backend,code_debt,duplicated_code
impala,4617,description,"Currently Expr.isConstant() and Expr::IsConstant() in the frontend and backend have duplicate logic to do exactly the same analysis. They need to be kept exactly in sync to avoid problems. We should plumb through the value of isConstant() from the frontend to avoid this duplication. We need to be a little careful about how we do this in the frontend: Alex Behn mentioned that storing state in Exprs was risky, and also that naively calling isConstant() for each expr node was problematic since it could result in traversing the Expr tree many times.",code_debt,duplicated_code
impala,4639,summary,Add a pytest skip marker for tests that can't pass if run on remote clusters,non_debt,-
impala,4639,description,"Some tests are unable to run on remote clusters because of test setup or infrastructure reasons, and not because of product failure. We should be able to selectively skip tests that can't be set up properly to run against a remote cluster.",design_debt,non-optimal_design
impala,4639,comment_0,In review:,non_debt,-
impala,4652,summary,Add crcutil to toolchain,non_debt,-
impala,4652,description,Kudu's utility library depends on We need to add the most recent version to the toolchain.,architecture_debt,using_obsolete_technology
impala,4652,comment_0,Toolchain commit:,non_debt,-
impala,4657,summary,"""drop database cascade"" command is not deleting KUDU tables",non_debt,-
impala,4657,description,None,non_debt,-
impala,4657,comment_0,The operation works as expected on the latest Impala/Kudu bits.,non_debt,-
impala,4657,comment_1,Nick you may be on an older beta KImpala. The master branch does have a fix for this and it'll be in Impala 2.8.,non_debt,-
impala,4671,summary,Replace kudu::ServicePool with one that uses Impala threads,non_debt,-
impala,4671,description,"Kudu's {{ServicePool}} uses Kudu's {{Thread}} class to service RPC requests. While this works well, the threads it creates aren't monitored by Impala's {{ThreadMgr}} subsystem. Eventually we'd like to standardise on one thread class between projects, but for now it would be useful to reimplement {{ServicePool}} in terms of our thread class. The reactor threads and acceptor threads will still be Kudu threads, but those are less likely to do substantial work, so having them missing from monitoring isn't a big problem.",design_debt,non-optimal_design
impala,4671,comment_0,"Just replacing {{kudu::Thread}} with {{impala::Thread}} works fine, along with a couple of minor changes to the join code.",non_debt,-
impala,4671,comment_1,"We can also add some {{MemTracker}} integration here, if warranted, since we know the size of the payloads sitting in the queue.",non_debt,-
impala,4671,comment_2,An alternative approach is to make the kudu::ThreadMgr report to our webserver as well. This way we can keep a track of all Kudu threads as opposed to just the ServicePool threads.,non_debt,-
impala,4671,comment_3,Commits are in:,non_debt,-
impala,4678,summary,Set up query mem tracker in QueryState,non_debt,-
impala,4678,description,We should hang the query MemTracker off QueryState instead of relying on a static map. This will be required for IMPALA-3200 in order to set up the query's reservation in the QueryState,non_debt,-
impala,4706,summary,Impact of REFRESH statement,non_debt,-
impala,4706,description,"Hi, Can I please get some clarification on some points regarding the refresh command. 1) when a REFRESH/invalidate metadata <table 2) What happens in terms of the performance when a refresh/invalidate metadata table statement is issued on a table which is currently being queried ? e.g. If I am running a query select * from table1 a inner join table2 b on a.id = b.id If there is a refresh or an invalidate metadata command issued when query is executing how does it impact the performance ?",non_debt,-
impala,4706,comment_0,Could you please direct questions like this to the user mailing list:,non_debt,-
impala,4713,summary,Java backtrace should be logged for all frontend errors,non_debt,-
impala,4713,description,"I'm trying to debug failure queries where an is thrown wrapping a from an ""invalidate metadata <table The error message shows up in impala-shell but it doesn't appear that the Java backtrace is logged anywhere in the logs. This makes it impossible to determine where the NPE has come from.",code_debt,low_quality_code
impala,4713,comment_0,"- Didn't you have this issue last week, too? Can you post a description of where you saw this and how to repro it?",non_debt,-
impala,4713,comment_1,"I had a similar problem while running EE tests that needed custom cluster It turned out that I was looking at the wrong logs, as the real ones were created in /tmp/, and those contained the full stacktrace.",non_debt,-
impala,4713,comment_2,"I think this is currently too vague to be useful. I think this specific bug might have been because the JVM doesn't always create stack traces for NPE: Anyway, we should file JIRAs for more specific issues as we encounter them.",non_debt,-
impala,4723,summary,msgF0104 02:03:27.458724 8951 impalad-main.cc:78] ThriftServer 'backend' (on port: 22000) did not start correctly,non_debt,-
impala,4723,description,Error Description: Log file created at: 2017/01/04 02:03:12 Running on machine: quickstart.cloudera Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg E0104 02:03:12.211601 8951 logging.cc:118] stderr will be logged to this file. E0104 02:03:27.162784 8951 Running without spill to disk: could not use any scratch directories in list: /tmp. See previous warnings for information on causes. E0104 02:03:27.458483 9034 ThriftServer 'backend' (on port: 22000) exited due to TE xception: Could not bind: Transport endpoint is not connected E0104 02:03:27.458673 8951 ThriftServer 'backend' (on port: 22000) did not start co rrectly F0104 02:03:27.458724 8951 impalad-main.cc:78] ThriftServer 'backend' (on port: 22000) did not start corr ectly . Impalad exiting. Check failure stack trace: @ 0x1b5934d (unknown) @ 0x1b5bc76 (unknown) @ 0x1b58e6d (unknown) @ 0x1b5c71e (unknown) @ 0xae00fc (unknown) @ 0x7d92d3 (unknown) @ 0x3da381ed5d (unknown) @ 0x7d8f8d (unknown),non_debt,-
impala,4723,comment_0,Can you check that nothing is using port 22000?,non_debt,-
impala,4723,comment_1,Closing this as there was no response to our comments and this looks like a configuration/setup issue and not a bug. Please feel free to reopen if the problem persists.,non_debt,-
impala,4723,comment_2,"Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg E0914 10:58:10.457620 94112 logging.cc:121] stderr will be logged to this file. W0914 10:58:10.467237 94112 LDAP authentication is being used with TLS, but without an file, the identity of the LDAP server cannot be verified. Network communication (and hence passwords) could be intercepted by a man-in-the-middle attack E0914 10:58:13.220167 94268 ThriftServer 'backend' (on port: 22000) exited due to TException: Could not bind: Transport endpoint is not connected E0914 10:58:13.220221 94112 ThriftServer 'backend' (on port: 22000) did not start correctly F0914 10:58:13.221709 94112 impalad-main.cc:89] ThriftServer 'backend' (on port: 22000) did not start correctly . Impalad exiting. Can any one help me?",non_debt,-
impala,4728,summary,materialize expressions for window sorts vs lazy expression evaluation,non_debt,-
impala,4728,description,"Currently Impala uses lazy evaluation for expressions. This can result in a performance overhead when using or reusing expressions in things like a window function order by vs having the expression materialized as a projection from the underlying relation, especially if the expression is used in multiple places.",design_debt,non-optimal_design
impala,4762,summary,RECOVER PARTITIONS should send new partitions in small batches to HMS,non_debt,-
impala,4762,description,"HMS cannot handle more than 32k partitions in one call. When adding large amount of partitions, Impala should send the new partitions in smaller batches. otherwise HMS could go OOM.",non_debt,-
impala,4778,summary,Document web page non-responsivness while planning,non_debt,-
impala,4778,description,The issue in IMPALA-1972 doesn't appear to be actively worked on and should be noted in the documentation unless and until it is fixed.,documentation_debt,low_quality_documentation
impala,4778,comment_0,"I opened this gerrit review: I added the new known issues item under the subcategory ""Crashes and Hangs"".",non_debt,-
impala,4778,comment_1,Gerrit review was finished but I overlooked closing this JIRA at the time.,non_debt,-
impala,4778,comment_2,"Added label ""documentation"" for easier filtering.",non_debt,-
impala,4778,comment_3,", is using the ""Docs"" component for filtering insufficient for your needs?",non_debt,-
impala,4801,summary,Heap use-after-free in expr-test,non_debt,-
impala,4801,comment_0,The backtrace looks eerily related to that in IMPALA-4800. Could they have the same root cause ?,non_debt,-
impala,4801,comment_1,"Yeah I figured out what this is. RuntimeProfile does some cleanup in it's destructor, which includes unregistering some callbacks that periodically touch MemTrackers. So if the MemTracker gets destroyed before the profile, we have a problem. Ideally we should move that out of the RuntimeProfile destructor, but we can fix the immediate problem by fixing the lifetime of the MemTracker.",design_debt,non-optimal_design
impala,4801,comment_2,Another stress test run hit this.,non_debt,-
impala,4831,summary,Clients can violate BufferPool invariants by calling ReservationTracker methods directly.,non_debt,-
impala,4831,description,"If a client unpins some pages, then calls it can leave too many dirty unpinned pages in memory. Similarly, a client can cause mischief by calling AllocateFrom() and ReleaseTo() or by creating child",design_debt,non-optimal_design
impala,4833,summary,Use scheduling information to make per-node memory reservation tight,non_debt,-
impala,4833,description,"Following on from IMPALA-3748, we should consider combining the resource estimates with the schedule to accurate compute the minimum buffer requirement per daemon. Different daemons may have different resource requirements because not every fragment runs on every daemon (e.g. unpartitioned fragments, fragments with fewer scan ranges than daemons).",design_debt,non-optimal_design
impala,4833,comment_0,There's some relevant code in the Scheduler and Coordinator. The Scheduler does a pass over all fragment instances to determine the set of hosts here: The coordinator also does something similar: It would maybe be helpful if the scheduler produced backend_params_map_ itself to make it easier to compute the per-host aggregates.,design_debt,non-optimal_design
impala,4862,summary,Planner's peak resource estimates do not accurately reflect the behaviour of joins and unions in the backend,design_debt,non-optimal_design
impala,4862,description,"In the following example the way the peak resource estimate is computed from per-node estimates is wrong. It should be 476.41MB, because the scan node is Open()ed in the backend *while* the concurrent join builds are executing. Another example is this one, where in the backend the aggregations can execute concurrently with the join builds The behaviour for unions also is not accurate - branches of unions within the same fragment are execute serially, but anything below an exchanges is executed concurrently.",design_debt,non-optimal_design
impala,4871,summary,"Log entry like ""loading block md for 'table-x' file 'file-1'"" needs more file details logging",design_debt,non-optimal_design
impala,4871,description,"Catalog when fails(or passes) to load block metadata of specific table files, throws following messages in logs. I0128 01:54:33.537742 22702 HdfsTable.java:345] load block md for table-x file 000066_0 I0128 01:54:59.373677 22702 Cancelled while waiting for datanode x.y.z.215:50020: I0128 01:54:59.373868 22702 Cancelled while waiting for datanode x.y.z.148:50020: These logs will be more useful if the entire hierarchy of file location is logged including all levels of partitions. Since just file name like ""000066_0"" can be present for thousands of partitions under same table, it is difficult to isolate the problematic file with a just above log entry.",design_debt,non-optimal_design
impala,4871,comment_0,This has been addressed by IMPALA-4768,non_debt,-
impala,4901,summary,"Hive supports recursive reading, so impala should support as well.",non_debt,-
impala,4901,description,None,non_debt,-
impala,4901,comment_0,"This is not a different feature request than the parent Task, IMPALA-1944",non_debt,-
impala,4916,summary,"Missing, redundant or non-evaluable predicates due to buggy equivalence classes.",non_debt,-
impala,4916,description,"Impala's equivalence class computation has a subtle bug which can lead to: 1. omitting predicates 2. adding redundant predicates 3. adding predicates that are non-evaluable at that point in the plan In most queries, the bug has no effect on the final plan. However, in case (1) incorrect results may be returned, and in case (3) a crash will occur. Unfortunately, it is extremely difficult to determine from a query when this bug is being hit because the bug may or may not trigger depending on the specific implementation of Java's HashMap which has a tendency to slightly change across JVM versions. It also depends on the total number of columns (including virtual view columns) in the query. For queries hitting this bug, even minor changes that do not affect the end result are enough to make them not hit this bug (e.g., changing a '*"" to an explicit list of fewer columns). The root cause is a bug in Impala's DisjointSet implementation which is used for computing equivalence classes. *Workaround* Even minor query modifications that do not affect the query result might be enough to fix a query. For example, changing a '*' to an explicit list of (fewer) columns may be enough. Likewise, adding column references in places where they are not needed, e.g., in a EXISTS or NOT EXISTS subquery may fix the problem.",non_debt,-
impala,4933,summary,Force thrift to initialize SSL on process startup,non_debt,-
impala,4933,description,"Thrift currently initializes OpenSSL lazily, when the first connection is created. However, the OpenSSL initialization must happen before the Kudu library is used (because Kudu will not be doing the initialization itself, and it will check that the OpenSSL initialization already occurred), so thrift needs to do the initialization on startup.",design_debt,non-optimal_design
impala,4955,summary,Insert overwrite into partitioned table started failing with null,non_debt,-
impala,4955,description,Exception from Catalog log HMS log at time of failure Repro The test started failing after version 2.9.0-SNAPSHOT RELEASE (build from CDH.,non_debt,-
impala,4955,comment_0,Query attached is what triggers the failure.,non_debt,-
impala,4956,summary,Page on EXPLAIN plans refers to outdated mnemonics,documentation_debt,outdated_documentation
impala,4956,description,"[This refers to the old mnemonics of EXPLAIN_LEVEL, when {{0}} meant {{normal}} and {{1}} meant {{verbose}}. It should be updated to [the new  - I'm assigning this to you thinking you may know best who can take this on. Please let me know if you want me to find someone else.",documentation_debt,outdated_documentation
impala,4956,comment_0,The docs are now in the {{master}} branch. Anyone can fix this:,non_debt,-
impala,4967,summary,Metadata distribution should be incremental,non_debt,-
impala,4967,description,"When running REFRESH table partition (x=1) the metadata distribution contains all partitions, not just the single partition that was refreshed. This creates additional load and delays refreshes. To solve metadata updates might have to be serialized (assign unique #, and apply in order), but this behavior would help the timeliness of data and reduce load on the cluster.",design_debt,non-optimal_design
impala,4967,comment_0,"For refresh or query after invalidate most of the time is usually spent fetching block metadata and partitions from HMS, around 10-15% of the overall time is in serializing the table. The bigger benefit from partition wise refresh is that memory overhead is reduced.",design_debt,non-optimal_design
impala,4967,comment_1,"Thanks for that info, I'm less concerned about serialization costs and more about network. On larger clusters the Catalog Service will need to ship large amounts of metadata to every node on the cluster, which is a single point of congestion. If you have less nodes or fast network you wouldn't run into that as a problem in your testing. The CS becomes a single hot spot especially on busy large clusters.",design_debt,non-optimal_design
impala,4970,summary,Record identity of largest latency RPC per query,non_debt,-
impala,4970,description,"Although we retain the histogram of fragment instance startup latencies, we don't record the identity of the most expensive instance, or the host it runs on. This would be helpful in diagnosing slow query start-up times.",design_debt,non-optimal_design
impala,4976,summary,Impala+Kudu tests fail against a remote cluster for various reasons.,non_debt,-
impala,4976,description,"The resolution of IMPALA-4934 corrected some SSL-related issues for Impala + Kudu, and Impala e2e tests that were not able to run at all before are now running. However, new errors have presented themselves as a result, e.g.: Another error: I'm totally sure yet whether these are product errors, or test configuration errors.",non_debt,-
impala,4976,comment_0,Found a potential config issue. Rerunning tests now to see it that was it.,non_debt,-
impala,4976,comment_1,"The SSL warning should be unrelated. That will go away when you upgrade the Python library to use a 1.3 build instead of 1.2 from PyPi. (right now you're using the 1.2 client with the 1.3 .so, which, while it should work fine, will generate that warning). It seems from the error that it can't connect to 127.0.0.1. Are you sure the master is running on 127.0.0.1?",non_debt,-
impala,4976,comment_2,"Thanks for the info on v1.4 versus v1.2 Yeah, the loopback address was slipping past in one place. That's wrong, and that's the config issue I was referring to. I spinning up a new test right now and will update the bug as soon as I know.",non_debt,-
impala,4976,comment_3,Turns out this was a config issue. Cluster kudu tests are passing.,non_debt,-
impala,4989,summary,Improve filtering based on parquet::Statistics,non_debt,-
impala,4989,description,This is an umbrella JIRA to track various related issues around writing and reading parquet::Statistics,non_debt,-
impala,4989,comment_0,should we resolve this?,non_debt,-
impala,4989,comment_1,I think it's still useful to collect issues for improvements related to parquet::Statistics and it has 7 unresolved children (you may need to click expand). Preferably we could leave it open until those are resolved.,non_debt,-
impala,4989,comment_2,By now all but one child have been resolved and we should close this. The only one left (IMPALA-4986) does not really address filtering but aggregate evaluation.,non_debt,-
impala,4989,comment_3,Setting Fix Version/s to the last one that contains changes from here.,non_debt,-
impala,4996,summary,Single-threaded KuduScanNode,non_debt,-
impala,4996,description,"Since we will multi-thread query execution at the fragment level, we should rework KuduScanNode to only use a single thread (the one that's executing the fragment).",code_debt,multi-thread_correctness
impala,5002,summary,Toolchain build flags should be associated with builds,non_debt,-
impala,5002,description,"As identified in , it is not easy to know how (i.e. build flags) a toolchain build was produced. The build scripts are versioned in the native-toolchain repo, but that is not easily associated with generated toolchain builds. We should have some way to determine this information later. The simplest way to handle this may be to store the native-toolchain githash in the produced toolchain build.",design_debt,non-optimal_design
impala,5002,comment_0,toolchain commit: 272f049,non_debt,-
impala,5015,summary,Run with mt_dop != 0,non_debt,-
impala,5015,description,None,non_debt,-
impala,5020,summary,Query against large cluster with SSL + Kerberos enabled failed with RPC client failed to connect: Couldn't open transport for foo:22000 (Could not resolve host for client socket.),non_debt,-
impala,5020,description,"None of the Impalads crashed, yet there is the following errors in the log",non_debt,-
impala,5020,comment_0,"From the analysis: The tickets expired at around 18:41 [systest@va1026 ~]$ grep ""Mar 02 18:46:36"" | wc -l 1536 [systest@va1026 ~]$ grep ""Mar 02 18:46:37"" | wc -l 1622 [systest@va1026 ~]$ grep ""Mar 02 18:46:38"" | wc -l 1143 [systest@va1026 ~]$ grep ""Mar 02 18:46:39"" | wc -l 75 [systest@va1026 ~]$ grep ""Mar 02 18:46:40"" | wc -l 763 [systest@va1026 ~]$ vi [systest@va1026 ~]$ grep ""Mar 02 18:46:41"" | wc -l 1069 [systest@va1026 ~]$ grep ""Mar 02 18:46:42"" | wc -l 816 So N^2 connections were being made that time, causing a lot of connections to fail because the KDC was responding slowly and the tickets were expired by then causing all the queries to fail for ~5 minutes The cluster was started at around 6 PM on Mar 1. So it makes sense that the tickets expired around 24 hrs later. We need to stagger ticket renewal over a period of time. I'm not yet sure how that would work, since service ticket establishment happens directly from the SASL layer. I need to look into it more.",non_debt,-
impala,5020,comment_1,", we're seeing this more and more. At what scale did you hit this bug? Isn't this just a different symptom for IMPALA-3189? The 5 minute period you describe is in line with the reinit interval Impala sets at Do you think spreading that out more evenly would help with this?",non_debt,-
impala,5021,summary,COMPUTE STATS hang while RowsRead of one SCAN fragment winds down,non_debt,-
impala,5021,description,"I created a table copied from another table, but sorted, partially using Impala and partially using Hive. I invalidated the table metadata. When I run COMPUTE STATS on the table the SELECT COUNT(*) query runs for awhile and then stops making progress. After that point, comparing snapshots of the profile reveals that only one SCAN fragment seems to be active, but its RowsRead counter is moving backwards, and even goes negative. When I kill the query and try again, the problem reproduces, hanging at the same point in the scan progress.",non_debt,-
impala,5021,comment_0,I copied the table.COMPUTE STATS succeeded on the new table.,non_debt,-
impala,5021,comment_1,"Seems likes condition is never met, and the DCHECK should be firing.",non_debt,-
impala,5021,comment_2,The problem is in Impala. Here's the problematic code from,non_debt,-
impala,5042,summary,"Loading metadata for partitioned tables is slow due to usage of an ArrayList, potential 4x speedup",code_debt,slow_algorithm
impala,5042,description,"Loading metadata for partitions with custom paths is 4x slower compared to partitions without custom paths, the slow down is due to an N2 lookups to check if a partition already exists. The List should ideally be replaced with a Set. From From Java mission control",code_debt,slow_algorithm
impala,5043,summary,Admission control error messages don't hint that information is stale when disconnected from statestore,non_debt,-
impala,5043,description,"When (for whatever reason) one or more daemons are disconnected from the statestore the admission control data held on the daemon goes stale. This can lead to the daemon accepting queries when there is not capacity or rejecting queries when there is capacity. For example, a pool somepool has a limit of 10 concurrent queries and is at that limit when a daemon is disconnected from the statestore. Even when other queries in somepool finish and the pool is now empty the disconnected daemon will report the following when new queries are executed: ERROR: Admission for query exceeded timeout 60000ms. Queued reason: number of running queries 10 is over limit 10 Could we have some warning to say that the admission control data is stale here?",non_debt,-
impala,5043,comment_0,"I'm thinking we can also report the last time a statestore update was received, or whether the daemon considers itself disconnected from the statestore.",non_debt,-
impala,5062,summary,Populate distinct_count in parquet::statistics in the parquet table writer,non_debt,-
impala,5062,description,None,non_debt,-
impala,5062,comment_0,why would we want to do this or should we resolve as won't fix?,non_debt,-
impala,5062,comment_1,"After some discussion in the Parquet community, this doesn't seem to be too helpful to answer queries. If may help in the future to pick internal data structures while reading Parquet files, but I don't think it's worth the effort. I'll close it for now.",non_debt,-
impala,5070,summary,Some attachments were not translated properly,code_debt,low_quality_code
impala,5070,description,Issues like IMPALA-92 with multiple attachments with the same name did not have all of their attachments translate properly.,code_debt,low_quality_code
impala,5070,comment_0,"To see which ones were not translated correctly, use --json from",non_debt,-
impala,5070,comment_1,Fixed it by hand.,non_debt,-
impala,5072,summary,fails on S3,non_debt,-
impala,5072,description,See the following error:,non_debt,-
impala,5072,comment_0,want to mark this resolved? People typically copy/paste the commit message or link to the Gerrit review.,non_debt,-
impala,5084,summary,Support variable-length rows in sorter,non_debt,-
impala,5084,description,See IMPALA-3208 for the context. Sorter::Run changes: * We can use a similar approach to that used for BufferedTupleStream as described in IMPALA-5085 Testing: Needs end-to-end tests exercising all operators with large operators,test_debt,low_coverage
impala,5084,comment_0,"As a temporary solution, it may be reasonable to just increase the default buffer size in the sorter to match the maximum row size, given that the sorter's buffer requirements are more modest than the joins and aggs, and there are typically fewer sorts in a plan.",design_debt,non-optimal_design
impala,5084,comment_1,This wouldn't offer enough benefit to offset the complexity. We currently only need 6 regular-sized buffers to execute a spilling sort with var-len data. To support rows larger than the regular page size we'd still need 4 max-sized buffers and 2 regular-sized buffers (since we need to keep a read and write buffer in memory at the same time). A bigger memory reduction could be achieved by different means. E.g. packing fixed and variable-length data in merged runs into the same buffers.,design_debt,non-optimal_design
impala,5130,summary,is not thread-safe,requirement_debt,non-functional_requirements_not_fully_satisfied
impala,5130,description,"can run concurrently with There is no synchronisation between the two. I saw a crash with this stack on a development branch, which I believe is caused by this: on this line: This method is not currently used in query execution, but we need to fix this before switching on the buffer pool for query execution.",code_debt,multi-thread_correctness
impala,5150,summary,Uneven load distribution of work across NUMA nodes,non_debt,-
impala,5150,description,"When doing concurrency testing as part of the competitive benchmarking I noticed that it is very difficult to saturate all CPUs @100% Below is a snapshot from htop during a concurrency run, state below closely mimics the steady state, note that CPUs 41-60 are less busy compared to 1-20. Then I ran the command below which dumps the threads and processor associated with each, reference. for i in $(pgrep impalad); do ps -mo -p $i;done From the man page for ps : The output showed that a large number of threads are running on core 61, not surprisingly the 1K threads are all thrift-server threads, so I am wondering if this is skewing the kernel's ability to evenly distribute the threads across the cores or something. I did a followup experiment using by profiling different core ranges on the system : Run 80 concurrent queries dominated by shuffle exchange Profile cores 01-20 to foo_01-20 Profile cores 41-60 to foo_41-60 Results showed that : Cores 01-20 had 50% more instructions retired Cores 01-20 show significantly more contention on pthread_cond_wait, and __lll_lock_wait Skew is dominated by DataStreamSender ScannerThread(s) also show significant skew",design_debt,non-optimal_design
impala,5150,comment_0,Assigned it back to Mostafa to confirm that IMPALA-4923 solves it,non_debt,-
impala,5221,summary,Fix TSaslTransport negotiation order leading to crash in,non_debt,-
impala,5221,description,Query Stack Seems like m is invalid,non_debt,-
impala,5221,comment_0,Do you have the logs for this node by any chance?,non_debt,-
impala,5221,comment_1,Commit in:,non_debt,-
impala,5238,summary,Support transferring reservation between ReservationTrackers,non_debt,-
impala,5238,description,"In order to support claiming reservation atomically (IMPALA-3748) and distributing it to different execution nodes, we need to add a transfer operation to The idea is that we would claim all the reservation in a single shared ""initial reservation"" tracker, then individual nodes could claim their share from there. When a node is closed, and other node that executes after it depends on getting its reservation, it would have to deposit its initial reservation back into the ""initial reservation"" tracker (or transfer it directly to the other node). This operation should",non_debt,-
impala,5257,summary,fails in local file system build,non_debt,-
impala,5257,description,", the newly introduced test doesn't seem to work on local file system. Would you mind taking a look and marking it as skipped for local filesystem (or non-HDFS) if necessary ?",non_debt,-
impala,5273,summary,StringCompare is very slow,code_debt,slow_algorithm
impala,5273,description,"Replacing StringCompare (which uses SSE4.2 instructions) with a call to glibc's memcmp results in a memcmp on my machine mainly uses sse4.1's ptest, after detecting at run-time that I have sse4.1 instructions available. The StringCompare benchmark is 5 years old and likely out-of-date by now. To replicate:",code_debt,slow_algorithm
impala,5273,comment_0,Patch available here:,non_debt,-
impala,5321,summary,impalad crash when i search sum data from table,non_debt,-
impala,5321,description,"when i search sum data from a partition of the table one node impalad crash An error report: # Problematic frame: # C [impalad+0x8c3405] impala::RowBatch*, bool*)+0x3f5 # # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again #",non_debt,-
impala,5321,comment_0,"Did the impalad write a minidump file (*.dmp)? Usually it should say so in the logs. Can you attach the schema, testdata, and query you are using to the JIRA? Can you also attach the impalad logfiles from the time of the crash?",non_debt,-
impala,5321,comment_1,"What version of Impala are you using. I noticed you set ""Fix Version"" to 2.3. Does that mean the issue does not occur in 2.3?",non_debt,-
impala,5321,comment_2,"This is a duplicate of IMPALA-2829 - the fix is in Impala 2.3.2, which I believe is distributed in CDH5.5.2 (although I'd strongly recommend you update to the latest maintenance release in the 5.5.x series, which has other fixes)",non_debt,-
impala,5338,summary,Fix Kudu timestamp default values,non_debt,-
impala,5338,description,"While support for TIMESTAMP columns in Kudu tables has been committed (IMPALA-5137), it does not support TIMESTAMP column default values. It turns out to be a bit tricky in the catalog. In addition to lacking the ability to specify the default values in DDL (both CREATE and ALTER columns), this also means tables with timestamp default values created outside of Impala (e.g. via the Kudu python client) cannot be loaded by Impala: then in Impala: This is tricky in the catalog because the {{KuduColumn}} class loads the column metadata from Kudu, and it contains the default value as a LiteralExpr, but Kudu represents the timestamp as a bigint unix time micros. Impala should convert that value to a TimestampValue, which isn't hard to do in the backend but isn't easy in the catalog. Unless the catalog were to call into BE code, the KuduColumn class would need to store the default value as a bigint and then all code that then uses the default value later would need to know that it isn't the same type as the column.",non_debt,-
impala,5341,summary,File size filter in planner tests also filters row-size,non_debt,-
impala,5341,description,"In our planner tests we want to ignore minor differences in reported file sizes to avoid flaky tests, see IMPALA-2565. However, the introduced filter also matches ""row-size="" which we do not want to filter. Relevant code is in TestUtils.java:",test_debt,flaky_test
impala,5341,comment_0,I posted a walkthorugh of how to get started with this ticket:,non_debt,-
impala,5341,comment_2,"Jinchul, people usually leave comments like this with the link to the gerrit review so reviewers can jump straight to it. I don't think it's possible to do that with just the Change-Id.",non_debt,-
impala,5341,comment_3,"Sorry for the inconvenience, let me leave a direct link a next time.",non_debt,-
impala,5364,summary,Number of fragments reported in the web-ui is incorrect,non_debt,-
impala,5364,description,"Number of fragments perf backend reported in the web-ui is incorrect. It appears to be displaying the number of queries instead, during that snapshot there was +1K fragments running on the cluster.",non_debt,-
impala,5401,summary,CTAS does not allow creating tables as SEQUENCEFILE even with,non_debt,-
impala,5401,description,Repro:,non_debt,-
impala,5401,comment_0,Removed support,non_debt,-
impala,5408,summary,SIGSEGV destructing,non_debt,-
impala,5408,description,"impalad LOG, minidump, and symbolized minidump attached. 0 + 0x49 rax = 0x000000000277a8f0 rdx = 0x000000000277a910 rcx = 0x0000000000000002 rbx = 0xbde6c92de4f6e926 rsi = 0x00007fe2b58504e0 rdi = 0x00000000209c59a8 rbp = 0x30e8a803fc9b2bf2 rsp = 0x00007fe2082d02d0 r8 = 0x0000000000016d58 r9 = 0x000000000426e000 r10 = 0x0000000000000000 r11 = 0x0000000000000206 r12 = 0x0000000000c98a90 r13 = 0x0000000000c99450 r14 = 0x00000000209c59a8 r15 = 0x00007feab279cc98 rip = 0x0000000000c98f79 Found by: given as instruction pointer in context 1 + 0x1d7 rbx = 0x00000000209c5ff0 rbp = 0x00007feab85e0460 rsp = 0x00007fe2082d0320 r12 = 0x00007fede92627a0 r13 = 0x0000000000c99450 r14 = 0x00000000209c5990 r15 = 0x00007feab279cc98 rip = 0x0000000000c998c7 Found by: call frame info 2 + 0x124 rbx = 0x00007feab279ca80 rbp = 0x00007feab3d88c00 rsp = 0x00007fe2082d0380 r12 = 0x00007fe2082d0730 r13 = 0x0000000011f4e800 r14 = 0x00007fe2082d0450 r15 = 0x00007fe2082d04a0 rip = 0x0000000000c99cd4 Found by: call frame info 3 + 0x9 rbx = 0x00007feab279ca80 rbp = 0x00007fea23555000 rsp = 0x00007fe2082d03b0 r12 = 0x00007fe2082d0730 r13 = 0x0000000011f4e800 r14 = 0x00007fe2082d0450 r15 = 0x00007fe2082d04a0 rip = 0x0000000000c99e19 Found by: call frame info 4 std::alloca$ rbx = 0x00007feac491aa00 rbp = 0x00007fea23555000 rsp = 0x00007fe2082d03c0 r12 = 0x00007fe2082d0730 r13 = 0x0000000011f4e800 r14 = 0x00007fe2082d0450 r15 = 0x00007fe2082d04a0 rip = 0x0000000000c6098d Found by: call frame info 5 + 0xbb9 rbx = 0x0000000000000000 rbp = 0x00007fea23555000 rsp = 0x00007fe2082d0650 r12 = 0x0000000000000000 r13 = 0x00007fe2082d0850 r14 = 0x00007fe2082d07d0 r15 = 0x00007feac491aa00 rip = 0x0000000000c626c9 Found by: call frame info",non_debt,-
impala,5408,comment_0,"This is the Apache Impala bug tracker, not the Cloudera bug tracker. Issue reports will be most useful to the Impala community if they include as much material as possible to help describe the issue. If there are things that are proviate or proprietary to your employer that you wish to share with your co-workers, please use the appropriate forum for that, rather than the Apache Impala bug tracker.",non_debt,-
impala,5408,comment_1,"This looks like another symptom of IMPALA-5407, and it occurred under identical circumstances.",non_debt,-
impala,5428,summary,update external hadoop ecosystem versions,non_debt,-
impala,5428,description,We need to update the external hadoop ecosystem versions.,non_debt,-
impala,5438,summary,Union with constant exprs inside a subplan returns inconsistent results,non_debt,-
impala,5438,description,"The follow queries returned wrong results This should have also included the following rows in the output - 2, 100 2, 200 2, 300 This query works correctly sometimes and returns the 22 rows instead of 19 rows like above. The results are inconsistent because it also returns correct values sometimes",non_debt,-
impala,5438,comment_0,"I think the problem has to do with the handling of the constant-exprs in unions. Those will only be executed by the first fragment instance. However, when the union is inside a subplan, we should always execute the constant-exprs regardless of which fragment instance the union is in.",non_debt,-
impala,5455,summary,test infra cannot always contact TLS-enabled CM,non_debt,-
impala,5455,description,"The {{CmCluster}} class is set up to provide an abstraction for a CM-based cluster, but it can't connect to a CM-based TLS cluster. It needs a few changes to the underlying CM API call: to propagate the port and the TLS flag.",non_debt,-
impala,5481,summary,"RowDescriptors should be shared, rather than copied",non_debt,-
impala,5481,description,"One of the {{RowBatch}} c'tors copies the row descriptor into the row batch. This leads to a lot of allocation churn since {{RowDescriptor}} contains some vector members, and since the descriptor is usually the same the copies are unnecessary. Instead, we should consider allocating the {{RowDescriptor}} once from an object pool, and sharing it amongst all row batches that need that descriptor. In some tests, {{RowDescriptor()}} shows up as 20% of the tcmalloc allocation time.",code_debt,slow_algorithm
impala,5481,comment_0,"An experimental build that stored a {{RowDescriptor&}} in the row batch saw the contribution to {{tcmalloc}} pressure disappear in a highly concurrent workload. It looks very safe to do this as well, based on the lifetimes of the row descriptor (which are ultimately tied to the runtime state for a fragment instance).",non_debt,-
impala,5489,summary,Improve Sentry authorization for Kudu tables,non_debt,-
impala,5489,description,"In IMPALA-4000 we added basic authorization support for Kudu tables, but it had several limitations: * Only the ALL privilege level can be granted to Kudu tables. (Finer-grained levels such as only SELECT or only INSERT are not supported.) * Column level permissions on Kudu tables are not supported. * Only users with ALL privileges on SERVER may create external Kudu tables. It looks like we could make the following work: * Allow column-level permissions * Allow fine grained privileges SELECT and INSERT for those statement types. However, would require ALL because Sentry doesn't have fine grained privilege actions for those types yet (work is planned though). So Impala can do this work, probably without much effort, but the question is whether or not it makes sense to implement this short-term solution in the context of the mid-to-longer term Kudu, Sentry, and Impala authorization plans. Kudu is currently figuring out what their authorization story will look like. Sentry is also poised for some large upcoming changes.",requirement_debt,requirement_partially_implemented
impala,5489,comment_1,cc:  in case there is some impact on the main Kudu docs. cc:  in case updates are needed to the Sentry privilege table(s) for this subset of Impala tables that has slightly different rules.,non_debt,-
impala,5499,summary,session-expiry-test failed because of conflicting ephemeral ports,non_debt,-
impala,5499,description,I suspect this is a rare flaky test failure. Filing a JIRA so that we can see if it reoccurs. This test failed and everything else passed. It looked like for some reason the port was occupied and the process then crashed on teardown. Info log: Error log:,test_debt,flaky_test
impala,5499,comment_0,Hmmmm...... it looks like it's trying to start two services on the same port.,non_debt,-
impala,5499,comment_2,Ran into a similar issue on a recent run.,non_debt,-
impala,5499,comment_3,Can you open a new JIRA with all the relevant info so we can determine if it's the same issue?,non_debt,-
impala,5514,summary,"When only with ldap_password_cmd option, has invalid parameter, impala-shell runs successfully",non_debt,-
impala,5514,description,"When using impala-shell only with ldap_password_cmd option, has invalid parameter, it runs successfully as following: impala-shell Starting Impala Shell without Kerberos authentication Connected to Server version: impalad version 2.5.0-cdh5.7.1 RELEASE (build Welcome to the Impala shell. Copyright (c) 2015 Cloudera, Inc. All rights reserved. (Impala Shell v2.5.0-cdh5.7.1 (27a4325) built on Wed Jun 1 16:06:09 PDT 2016) You can run a single query from the command line using the '-q' option. The processing of only with ldap_password_cmd option ignores this parameter just like ldap_password_cmd is absent. The ldap_password_cmd option is in the use of ldap environment to obtain ldap password. We should use ldap_password_cmd option when impala-shell using LDAP to authenticate with Impala. So, ldap_password_cmd option will only appear if ldap option appears.",non_debt,-
impala,5514,comment_0,I have submit my code on Please review it. Thanks.,non_debt,-
impala,5525,summary,Extend TestScannersFuzzing to test uncompressed parquet,non_debt,-
impala,5525,description,"TestScannersFuzzing currently tests compressed parquet but does not test uncompressed parquet. Uncompressed parquet would help with test coverage because there's more potential to corrupt the actual data in interesting ways, not just the headers. To do this we'd probably need to set do a create table as select to write out some parquet data, then do the fuzz testing on that.",test_debt,low_coverage
impala,5527,summary,Create a nested testdata flattener for the query generator,non_debt,-
impala,5527,description,"In order to use the query generator to test nested types, we need a way to convert a nested dataset into an equivalent flattened one that can be loaded into Postgres. Maps and Arrays should be converted into tables that can be joined with the original table to simulate the nesting.",non_debt,-
impala,5535,summary,Ensure test coverage for spilling and no-spilling cases,test_debt,low_coverage
impala,5535,description,"Make sure that we have test coverage for large null-aware anti joins. * The streams start off unpinned, so can spill even when we don't go down the normal spilling path. I think my current iteration of the BufferPool patch doesn't test NAAJ when spilling is enabled",test_debt,low_coverage
impala,5535,comment_0,I guess I filed this twice.,non_debt,-
impala,5600,summary,Small cleanups left over from IMPALA-5344,code_debt,low_quality_code
impala,5600,description,None,non_debt,-
impala,5612,summary,Join inversion should avoid reducing the degree of parallelism,design_debt,non-optimal_design
impala,5612,description,"The degree of inter-node parallelism for a join is determined by its left input, so when inverting a join the planner should be mindful of how the inversion affects parallelism. For example, the left join input may have been reduced by joining with several dimension tables so much that it becomes smaller than the right hand-side (another small dimension table). By inverting such a join the degree of parallelism may be reduced to one or very few nodes, based on how many nodes the right-hand size is executed on.",design_debt,non-optimal_design
impala,5612,comment_0,Here's a spreadsheet with some sample inputs and the output of the proposed formula:,non_debt,-
impala,5616,summary,Add a new flag --enable_minidumps,non_debt,-
impala,5616,description,We should have an explicit flag to disable minidump support to make it easier to disable the breakpad integration. [Kudu has such a too.,non_debt,-
impala,5618,summary,Performance regresses on buffer pool dev branch for high-ndv aggregations,non_debt,-
impala,5618,description,The extra time appears to be in AddRowCustom() via I think allocating the boost::function object is causing the slowdown.,code_debt,slow_algorithm
impala,5618,comment_0,Is this a case where moving from a method like to one like and using C++11 lambdas as the parameter would eliminate the heap allocation?,non_debt,-
impala,5618,comment_1,yeah I *think* that could defer construction of the boost::function until the non-templated slow path function is called. I'm thinking though that the performance is very hard to reason about and it may be better as a matter of policy to avoid lambdas with captured variables in perf-critical code.,design_debt,non-optimal_design
impala,5636,summary,Impala writer claims that file uses BIT_PACKED encoding when it doesn't,non_debt,-
impala,5636,description,"The Parquet writer always adds the BIT_PACKED and RLE encodings even though (I'm pretty sure) we never write out the BIT_PACKED encoding for rep or def levels. The BIT_PACKED encoding is deprecated according to the Parquet specification: and it is not clear that Impala can even read it correctly: IMPALA-3006 One way of seeing that Impala claims to need the BIT_PACKED encoding is to write a parquet file with Impala then inspect it with parquet-tool. BIT_PACKED is never written by Impala's parquet writer - the definition levels are always written using an RleEncoder and reported as Encoding::RLE. Weirdly, the repetition levels are reported as ""BIT_PACKED"", but this encoding has no effect since we don't actually write out repetition levels.",documentation_debt,outdated_documentation
impala,5636,comment_0,"I think just replacing 2 occurrences of with Encoding::RLE makes sense. I've manually tested with parquet-tools and it seems to work. Since parquet-tools is not in our distribution, I'm not sure how to write a test. Any suggestion?",test_debt,lack_of_tests
impala,5636,comment_1,I updated the patch. Is here a way to link Jira to Gerrit? I refereed the issue id in commit message and nothing happens here.,non_debt,-
impala,5636,comment_2,We don't have that Gerrit/JIRA integration set up unfortunately.,non_debt,-
impala,5640,summary,Enable test coverage for Parquet gzip inserts was disabled,test_debt,low_coverage
impala,5640,description,There's a comment in the code saying that it was disabled because of IMPALA-424. This test coverage seems useful - we should add it back,test_debt,low_coverage
impala,5665,summary,fails with,non_debt,-
impala,5665,description,Seen in an exhaustive Jenkins build: From catalogd.INFO:,non_debt,-
impala,5665,comment_0,assigning to you because its catalog related. Feel free to reassign.,non_debt,-
impala,5665,comment_1,Looks like IMPALA-5288.,non_debt,-
impala,5688,summary,Speed up a couple of heavy-hitting expr-tests,test_debt,expensive_tests
impala,5688,description,"Two tests ({{LongReverse}} and the base64 tests in run their tests over all lengths from 0..{{some length}}. Both take several minutes to complete. This adds a lot of runtime for not much more confidence. If instead we pick a set of 'interesting' (including powers-of-two, prime numbers, edge-cases) lengths, we can get a similar amount of confidence while significantly reducing the runtime of expr-test.",test_debt,expensive_tests
impala,5703,summary,fails intermittently in GVO,non_debt,-
impala,5703,description,"For example: 30 | 0 | exec_option: {'batch_size': 0, 'num_nodes': 0, 5000, 'disable_codegen': False, 'abort_on_error': 1, 0} | table_format: text/none | True] FAILED}}",non_debt,-
impala,5703,comment_0,Another instance:,non_debt,-
impala,5703,comment_1,"Ah, please ignore the above comment. Didn't realize  linked the same GVO URL.",non_debt,-
impala,5703,comment_2,Dupe of IMPALA-5702,non_debt,-
impala,5730,summary,am using impala 2.7.0 version and date is not supporting i have loaded data using timestamp and i want only date alone no need hh:mm:ss how can i achive this,non_debt,-
impala,5730,description,am using impala 2.7.0 version and date is not supporting i have loaded data using timestamp and I want only date alone no need hh:mm:ss how can I achieve this Eg:2017-07-27 08:03:15 I want only 2017-07-27 for complete table data . Please assist me.,non_debt,-
impala,5730,comment_0,Please ask questions on the user list:,non_debt,-
impala,5732,summary,flakiness on exhaustive run,test_debt,flaky_test
impala,5732,description,"failed on an exhaustive integration jenkins run. I'm not sure if the test is flaky (is it safe to check {{row_regex: .*Rows rejected: 2.43K .*}} - will that be deterministic?) or if there was some other problem, e.g. missing stats and the plan had the wrong join order.",test_debt,flaky_test
impala,5732,comment_0,"Looks like the filter only processed ~95% of the rows read. I confess it's not clear why - the filter showed up after ~400ms in the scan nodes. Best guess is a race where the initial scan ranges got issued before the filter arrived for some reason. Let's keep an eye on it, but I don't think this is a critical bug if all the other runtime filter tests continue to pass.",non_debt,-
impala,5732,comment_1,"This Jira is old, has little information, and there have been several recent JIRAs filed for similar flakiness in this test, so there's not really any value to keeping this around.",test_debt,flaky_test
impala,5763,summary,Setting logbufsecs to 0 causes Impala to spin on one core,non_debt,-
impala,5763,description,"Starting impalad with -logbufsecs 0 makes it spin in the thread that tries to delete old logfiles. ,  - Is this intended? If not, one fix would be to sleep for {{std::max(1, FLAGS_logbufsecs)}} in Another option would be to require a value >0 for logbufsecs.",non_debt,-
impala,5763,comment_0,"Maybe we should just improve the documentation and discourage setting it to that value? This behaviour actually kind of makes sense to me, since the flag is controlling a trade-off between the cost of flushing log files and the delay. Setting it to 0 means to me ""I don't care about how expensive it is, just minimise the delay."" I'm sure we could optimise it and get the same frequency of flushing with less cost, but it seems a really niche optimisation. It does call sleep(0) so will yield to another thread if one is running so it won't eat up a full core on a busy system.",documentation_debt,low_quality_documentation
impala,5763,comment_1,Making it more clear in the docs sounds good to me.  - What do you think?,documentation_debt,low_quality_documentation
impala,5774,summary,may read one byte beyond a string's extent,non_debt,-
impala,5774,description,"The following may read if no ',' is found. (This was discovered by poisoning mempool data from IMPALA-5666).",non_debt,-
impala,5779,summary,Add test with spillable buffer size > --read_size,non_debt,-
impala,5779,description,We do not have any end-to-end tests where we attempt to spill buffers larger than the I/O size. I have tested manually but we need some automated testing.,test_debt,lack_of_tests
impala,5780,summary,Add missing test coverage for,test_debt,low_coverage
impala,5780,description,We do not have any end-to-end test coverage with = true. We should add basic tests for the success and OOM cases where querying a table with no stats with = true.,test_debt,low_coverage
impala,5806,summary,Expose post-scheduling reservation info in the query plan,non_debt,-
impala,5806,description,"After IMPALA-4833, Impala started computing per-host minimum reservations using the scheduled plan. While that change included some profile improvements to show what the reservations are per host, it's not possible to get that information without executing the query. It would be helpful if one of the more verbose explain levels also showed the plan annotated with the reservation computations.",non_debt,-
impala,5806,comment_0,This doesn't make sense now since the executor group changes.,non_debt,-
impala,5812,summary,Query hits in FE,non_debt,-
impala,5812,description,Query: FE Exception:,non_debt,-
impala,5849,summary,Don't disable TLS configuration at compile-time even with OpenSSL 1.0.0,non_debt,-
impala,5849,description,"IMPALA-5800, IMPALA-5775 and IMPALA-5743 added TLS configuration to Impala and Squeasel. Since Impala is often built against different versions of OpenSSL (with different TLS capabilities), we used compile-time definitions to avoid using symbols from OpenSSL 1.0.1 that weren't available. This works great if we can ensure that the machine on which Impala is built is the same environment as the one on which it executes, but we have discovered that the installed version of OpenSSL can vary between minor releases of Linux distributions. It appears possible to write the support for TLS1.1+ in terms of symbols that are available in OpenSSL 1.0.0 only. The only downside is that Impala can't then tell whether or not the runtime supports TLS 1.2, and so the error messages won't be quite as clear. However, the benefit of a single binary and Thrift toolchain dependency for all supported versions of OpenSSL is well worth it.",code_debt,low_quality_code
impala,5923,summary,We're printing a binary ID in,non_debt,-
impala,5923,description,We should not print a binary ID in,non_debt,-
impala,5923,comment_0,"As far as I understand, you want the unexpected information(i.e. opid) will not be printed. If so, the line 132 is removed and then refine the logging message. I think it's too simple. Please leave more detailed description if there is any missing.",design_debt,non-optimal_design
impala,5923,comment_2,- This has already been fixed a while ago but I had forgotten to close the JIRA. Apologies for the confusion.,non_debt,-
impala,5952,summary,Query waiting indefinitely for table metadata to arrive,non_debt,-
impala,5952,description,"Impala queries may hang indefinitely while waiting for the metadata of a deleted table to arrive through a statestore topic update. You will see many messages like this in the log of the impalad coordinating the hung query: If one of the tables mentioned in those log messates has been deleted, then you may be hitting this issue. This code in clearly shows the bug: Getting into this hung state requires an elaborate series of events, for example: * impalad A requests table T to be loaded and gets into the wait loop * impalad B issues a ""DROP TABLE T"" * catalogd loads the metadata for table T * statestored requests topic update from catalogd; update includes T * statestored sends update to impalad B * impalad B completes the ""DROP TABLE T"" operation * statestored requests topic update from catalogd; update includes deletion of T * statestored sends update to impalad A which includes the deletion of table T * impalad A is still in the wait loop; the metadata for T will never arrive because T has been dropped Notice how impalad A may ""skip"" the first update for T which includes the metadata for T. This typically only happens on very busy clusters where the statestore has trouble sending all catalog snapshots to all subscribers in a timely fashion (i.e. some subscribers skip some snapshots). *Workaround* * Re-create tables with the same name as the deleted ones (schema and format do not matter, only the dabatase and table name must match) * Might need to run ""invalidate metadata <table* Once the hung queries finished (failed or succeeded), the re-created tables can be dropped again",non_debt,-
impala,5952,comment_0,"Looking at the code more carefully, this is not a bug after all. getMissingTbls() will not return the dropped table and so query analysis will proceed and should report ""Table not found""",non_debt,-
impala,5963,summary,Extend Catalog metrics to list tables that are being loaded along with the ones in the queue,non_debt,-
impala,5963,description,"The Catalog log prints the information below which doesn't clearly show what tables are being loaded, how long is the queue and how far in the queue is a particular table.",design_debt,non-optimal_design
impala,5963,comment_0,I think this is stale. Reopen if you disagree.,non_debt,-
impala,5997,summary,Provide a way to gracefully shutdown an Impala Daemon,design_debt,non-optimal_design
impala,5997,description,"Currently if we shutdown an impalad, queries running on it will all fail. This is bad in a BI system integrating with Impala as its query engine. If we perform maintenance on the impala cluster, we hope users are not aware of it. For example, to decomission impalads in a rack to retire old machines, we hope these impalads not accept new PlanFragments and shutdown when their existing work is done. provides such a way since 0.128: * Presto workers can be instructed to shutdown by submiting a PUT request to /v1/info/state with the body ""SHUTTING_DOWN"". Once instructed to shutdown, the worker will no longer receive new tasks, and will exit once all existing tasks have completed. Hope we can provide a way to do so.",design_debt,non-optimal_design
impala,5997,comment_0,I agree with your analysis - I think this is the same thing as IMPALA-1760,non_debt,-
impala,5997,comment_1,Closing as a duplicate of IMPALA-1760,non_debt,-
impala,5998,summary,Explain on view not allowed with column level privileges on underlying table,non_debt,-
impala,5998,description,"I think there is a bug in the permissions checking for explaining a view. Steps to reproduce: I think the above explain should be authorized because the following equivalent explain is authorized: explain select * from (select firstname, lastname from test.employees) as q;",non_debt,-
impala,5998,comment_0,"The issue is not reproducible, I followed the below steps to reproduce the issue but I did not see the the as reported above. [localhost:21000] Query: grant role psingh to group psingh [localhost:21000] Query: create database test [localhost:21000] Query: create table test.employees (firstname string, lastname string, salary int) [localhost:21000] Query: grant on table test.employees to role psingh [localhost:21000] Query: insert into table test.employees (firstname, lastname, salary) values (""John"", ""Smith"", 12345) [localhost:21000] Query: create view test.employees_view as select firstname, lastname from test.employees [localhost:21000] Query: grant select on table test.employees_view to role psingh [localhost:21000] Query: select * from test.employees_view [localhost:21000] Query: explain select * from test.employees_view | Explain String | | Max Per-Host Resource Reservation: Memory=0B | | Per-Host Resource Estimates: Memory=32.00MB | | WARNING: The following tables are missing relevant table and/or column statistics. | | test.employees | | | | PLAN-ROOT SINK | | | | | 01:EXCHANGE [UNPARTITIONED] | | | | | 00:SCAN HDFS [test.employees] | | partitions=1/1 files=1 size=17B | Fetched 11 row(s) in 0.01s",non_debt,-
impala,6021,summary,FE fails to compile due to incompatible Guava Hasher API,non_debt,-
impala,6021,description,"The version of Hive we are using must have had a revert that went through, because, sigh:",non_debt,-
impala,6021,comment_0,What's the point of a pom.xml if you can't stay somewhat locked in and be unaffected by stuff?,non_debt,-
impala,6021,comment_1,This is related to IMPALA-6009,non_debt,-
impala,6021,comment_2,Yep. We already have the revert going through the pipeline.,non_debt,-
impala,6022,summary,test_kudu_insert failing with unexpected QueryTestResults on RHEL73,non_debt,-
impala,6022,description,"These tests passed on when testing on a live Ubuntu 14.04, but failed on Redhat 7.3. The commit being tested was: *Stack trace:*",non_debt,-
impala,6022,comment_0,"Apparently, this failure occurred because the Kudu client in Impala uses the default read mode (READ_LATEST) which doesn't provide a guarantee of repeatable results. The test would need to set READ_AT_SNAPSHOT to have 100% confirmable results. I'll open a JIRA ticket to follow up.",non_debt,-
impala,6026,summary,"Refresh table failed with null""",non_debt,-
impala,6026,description,"Invalidate metadata ts_part_200; then refresh ts_part_200; it failed with following error. The table has ~1.8K partitions, one file per partition. and it's an s3 table. Note that each partition is under a different location, not all under the same directory.",non_debt,-
impala,6026,comment_0,Do you still have access to this setup or can you repro it locally?,non_debt,-
impala,6026,comment_1,"No, I don't have the cluster any more. And I couldn't reproduce it locally with latest code.",non_debt,-
impala,6026,comment_2,"Thanks, Juan. Do you have the steps to repro this (for an older release)? I recently came across a setup that could repro this on 2.9.0 and we narrowed down the problem to be with the files in the partition mapping to the table root directory. For example '/tmp/foo' is the table root directory and one of the partition directories mapped to '/tmp/foo' (by mistake). We have some weird logic with adding a ""default partition"" with an immutable partition-key list and I'm guessing that the above state has caused a mixup and we tried appending to the Immutable list causing this behavior. In the above setup, we fixed the partition structure to unblock the table operations, fwiw. Interestingly I'm not able to reproduce it locally either. I'll keep this open for a while incase someone else runs into this and we have a more reliable repro.",code_debt,complex_code
impala,6026,comment_3,"I created the table first then added partitions one by one because I want each have a different location, not all under table directory. e.g. ALTER TABLE sales2 ADD PARTITION LOCATION ALTER TABLE sales2 ADD PARTITION LOCATION ALTER TABLE sales2 ADD PARTITION LOCATION The table is good after all Alter commands, I can run query against. After I did invalidate metadata, then refresh, it kept throwing exception. I had to drop the table and recreate it. I tried the same with just a few partitions but didn't have any issues.",non_debt,-
impala,6029,summary,Crash in,non_debt,-
impala,6029,description,Crash occurred while running concurrent TPCDS queries against KRPC branch stack,non_debt,-
impala,6029,comment_0,Did you get a chance to look at this crash?,non_debt,-
impala,6029,comment_1,"The code where it is failing is simple. This is thrift-generated code, primarily default destructors tearing down std::vectors of things. I'm looking at our parquet code, but these parquet thrift structures are initialized via thrift deserialization and read-only after that. If I had to guess, it might be memory corruption.",non_debt,-
impala,6029,comment_2,"Haven't seen this in a long time, even with all the KRPC stress tests running.",non_debt,-
impala,6030,summary,Don't start coordinator specific thread pools if a node isn't a coordinator node,non_debt,-
impala,6030,description,Since we introduced the we've forgotten to disable the coordinator specific thread pools on nodes that have only the executor role.,design_debt,non-optimal_design
impala,6030,comment_0,Commit in:,non_debt,-
impala,6048,summary,Queries make very slow progress and report WaitForRPC() stuck for too long,code_debt,slow_algorithm
impala,6048,description,"When running 32 concurrent queries from TPCDS a couple of instances from TPC-DS Q78 9 hours to finish and it appeared to be hung. On an idle cluster the query finished in under 5 minutes, profiles attached. When the query ran for long fragments reported +16 hours of network send/receive time The logs show there is a lot of messages like the one below, there are incidents for this log message where a node waited too long from an RPC from itself",code_debt,slow_algorithm
impala,6048,comment_0,has plan to investigate this further.,non_debt,-
impala,6048,comment_1,Mostafa will update this JIRA with his findings.,non_debt,-
impala,6048,comment_2,"Issues occurs without KRPC as well, was able to repro on a 7 node cluster without KRPC. Seems to be triggered by heavy spilling and high concurrency. Will continue to investigate.",non_debt,-
impala,6048,comment_3,"So, as explained, this could be triggered by heavy spilling under high concurrency so some nodes are slow to consume row batches, leading to long RPC wait time. This seems to be very similar to what's being tracked in IMPALA-6294.",code_debt,slow_algorithm
impala,6057,summary,Cache Remote Reads,non_debt,-
impala,6057,description,"Reads from local disk might be cached in-memory by the OS. Reads from s3 should be cached when possible, using {{getETag()}} and for",non_debt,-
impala,6057,comment_0,probly needs HADOOP-13282,non_debt,-
impala,6057,comment_1,"We should also consider whether to cache remote HDFS reads, ADLS, etc. Ideally any infrastructure would be reusable for other filesystem implementations.",non_debt,-
impala,6057,comment_2,I threw together a rough description of how we could implement this *within* Impala.,non_debt,-
impala,6057,comment_3,Seems like a duplicate of IMPALA-8341 now.,non_debt,-
impala,6077,summary,remove BIT_PACKED encoding for Parquet levels,non_debt,-
impala,6077,description,"As a follow-on to IMPALA-6076, we should remove support for the encoding in a compat-breaking release so that we no longer have to maintain this code.",code_debt,dead_code
impala,6080,summary,Clean up descriptor table handling in coordinator,code_debt,low_quality_code
impala,6080,description,One of the parts of this patch was cleanup of how descriptor tables were managed. As described in the commit message: That cleanup should be independent of the larger patch so we could get it in separately.,architecture_debt,violation_of_modularity
impala,6080,comment_0,I would prefer not to separate this from the existing patch. What's the reason for not getting the existing patch in?,non_debt,-
impala,6080,comment_1,"I don't think anyone has picked that up due to a lack of time - I agree it would be good to get in but I think parts of that patch require deeper understanding than this subset, which is fairly mechanical cleanup. Separating this out would shrink that patch and I think make it easier to focus on the meat of it.",code_debt,low_quality_code
impala,6106,summary,test_tpcds_q53 extremely flaky because of decimal_v2 not being reset,test_debt,flaky_test
impala,6106,description,"The bad results are a result of running the query with decimal_v2=true. I tracked down the problem to the TPCDS-Q22A query test not resetting decimal_v2 after setting it. In the below test output I'd expect it to reset decimal_v2 at the end of the test. My hypothesis is that the comment interferes with the ""set"" command being parsed.",non_debt,-
impala,6106,comment_0,The bug was latent but triggered by IMPALA-5376,non_debt,-
impala,6131,summary,Track time of last statistics update in metadata,non_debt,-
impala,6131,description,Currently we (ab-)use to track the last update time of statistics. Instead we should introduce a separate counter to track the last update. With that we should also remove all occurrences of from and fall back to Hive's default behavior.,code_debt,low_quality_code
impala,6132,summary,ASAN test fails when trying to move/copy string created by into InitAuth(),non_debt,-
impala,6132,description,"Following from the code review: After moving the string into InitAuth() and storing it in AuthManager (which is a global singleton), the ASAN error still kept showing up. I spent many hours trying to debug this problem and I have no understanding of why it happens. My inclination is that ASAN gets confused by how creates a string. It creates a unique_ptr<char[] For some reason this is seen as a heap-use-after free if it is called multiple times. The only thing that works for now is to leave the code as is in this patch set. I have 2 attempts to move/copy the string into GetExecutablePath() and their corresponding errors here: (copy) (move) If anyone else is able to make progress on this, that would be great.",non_debt,-
impala,6132,comment_0,"I think the use in is incorrect: the destructor for the {{unique_ptr}} calls {{delete}} on the raw pointer underneath when its scope ends at the end of the {{while}} loop. Meanwhile, that pointer leaked to {{std::string * path}}.",code_debt,low_quality_code
impala,6132,comment_1,"Also, you might be able to fix it by calling instead of The former will clear the raw pointer value out of the {{unique_ptr}}, which then cannot call {{delete}}, thus not leaking the memory.",code_debt,low_quality_code
impala,6132,comment_2,"Even I noticed that and tried release(), however, that is not a bug, since string::assign() copies the contents. So, changing it to use release() instead of get() doesn't help here.",non_debt,-
impala,6132,comment_3,Commit in: Explanation in:,non_debt,-
impala,6136,summary,Duration in /queries page is shows a negative value,non_debt,-
impala,6136,description,"On a node that had a query stuck for 2 days, I noticed that the Duration field showed _-12595000.000ns_. I suspect this is related to IMPALA-5599.  - Can you please have a look whether your change could have caused this?",non_debt,-
impala,6136,comment_0,"It's *possibly, but not probably* related to IMPALA-5599. The issue here is that the query is still in a RUNNING state, so the query context object's end_time is not yet set. The 'thing' that's checking the state of the query to get the profile should check the query state, and if the query hasn't completed, it should get the current time and use that to calculate 'how long the query has been running'. Here's a snip of the query profile showing the relevant fields: Query Summary: Session ID: Session Type: HIVESERVER2 HiveServer2 Protocol Version: V6 *Start Time: 2017-10-29 00:11:49.131876 End Time: * Query Type: QUERY * Query State: RUNNING * Query Status: OK Impala Version: impalad version 2.11.0-SNAPSHOT RELEASE (build",non_debt,-
impala,6136,comment_1,"This repro on local dev box. Start a long running query and monitor the duration, it will display what looks like an uninitialized valued",non_debt,-
impala,6136,comment_2,"Ok. Just confirmed that it is indeed a regression due to IMPALA-5599, where we missed getting the current time if end_time is not yet set.",non_debt,-
impala,6184,summary,Check failed: !initialized_ || closed_,non_debt,-
impala,6184,description,A DCHECK was hit in an ASAN build when running From impalad.FATAL:,non_debt,-
impala,6184,comment_0,"Wasn't really sure who to assign this to, so I chose you, Michael. This started happening recently and I couldn't find a relevant commit that could cause this. Maybe this somehow has to do with your recent patch?",non_debt,-
impala,6184,comment_1,Pretty sure this is a genuine bug in ScalarExprEvaluator where it isn't cleaned up on some error path. We've seen a lot of failures with random errors related to UDFs. My suspicion is that IMPALA-6001 (function metadata errors) somehow triggers these errors,non_debt,-
impala,6184,comment_2,"Yes, the bug is here:",non_debt,-
impala,6217,summary,Check failed:,non_debt,-
impala,6217,description,"Hit the following crash during one of these tests: The backtrace:  , assigning this to you as you have touched the parquet column reader recently.",non_debt,-
impala,6223,summary,Gracefully handle malformed 'with' queries in impala-shell,design_debt,non-optimal_design
impala,6223,description,"Impala shell can throw a lexer error if it encounters a malformed ""with"" query. This happens because we use shlex to parse the input query to determine if its a DML and it can throw if the input doesn't have balanced quotes. A simple shlex repro of that is as follows, Fix: Either catch the exception and handle it gracefully or have a better way to figure out the query type, using a SQL parser (more involved). This query also repros it:",design_debt,non-optimal_design
impala,6272,summary,Update external Hadoop ecosystem versions,non_debt,-
impala,6272,description,Let's update the versions of our external Hadoop components to test Impala. (This is similar to IMPALA-5033 and IMPALA-5768; following in those footsteps.),non_debt,-
impala,6275,summary,Successful CTAS logs warning,non_debt,-
impala,6275,description,"I just noticed this on my development branch (branched off master at I'm not sure if it's new behaviour, but it's potentially confusing. I ran a successful CTAS for a table that didn't exist and got a stack trace in the warning log.",non_debt,-
impala,6275,comment_0,"This patch removes the need to have a MetastoreClient for CTAS, which will fix the potential classloader issue in the MetastoreClient when returns null.",non_debt,-
impala,6280,summary,Invalid plan for sorted INSERT with an outer join and null checking,non_debt,-
impala,6280,description,"Kudu DML queries may fail due to an invalid plan in the following circumstances: * Query has an outer join that involves an inline view * The outer-joined inline view contains NULL-checking expressions like and/or constant expressions like literals The following error message will appear in the logs. Further, due to a separate issue, the coordinating impalad may crash (IMPALA-6262). This issue is a regression introduced in Impala 2.10 by the recent addition of partial sorting before Kudu DML. See IMPALA-5498. Reproduction",non_debt,-
impala,6285,summary,Avoid printing the stack as part of DoTransmitDataRpc as it leads to burning lots of kernel CPU,design_debt,non-optimal_design
impala,6285,description,When running on 32 concurrent TPCDS queries against 20 r4.8xlarge some of the RPCs timeout but don't fail the query The status can be changed to expected but it is worth verifying that this timeout can be tolerated.,non_debt,-
impala,6285,comment_0,Why is this all of a sudden a blocker for 2.11.0 ? This is undesirable behavior but why does it show up as a blocker out of the blue ?,non_debt,-
impala,6327,summary,maybe there is a mistake in blocking-queue.h,non_debt,-
impala,6327,description,"hi, i suspect that the function in blocking-queue.h is unsafe, because the 'put_lock_' should be re-acquired when 'notified' is false (which means timeout or any other errors) after 'TimeWait'. But the next 'SizeLocked' hides this mistake. am i right?",non_debt,-
impala,6327,comment_0,"According to the specification of which {{WaitUntil()}} is built on, {{put_lock_}} is always acquired upon return from {{WaitUntil()}}. In addition I can imagine it being a problem if we ever ran into {{ENOTRECOVERABLE}} but then the mutex acquisition in would have failed. So, may be I misunderstood the problem you are alluring to.",non_debt,-
impala,6327,comment_1,"thanks Michael. it is my big mistake on understanding of 'When such timeouts occur, shall nonetheless release and{color:red} re-acquire the mutex{color} referenced by mutex'. What a shame:(",non_debt,-
impala,6327,comment_2,sorry for my big mistake.,non_debt,-
impala,6329,summary,"Wrong results for complex query with CTE, limit, group by and left join",non_debt,-
impala,6329,description,"Impala may generate an incorrect plan for complex query. (see NULL in id and a_id columns) Can get correct result with commented lines (1, 2, 3, 4, 5) Example query and incorrect plan: Result: Plan: Correct result: Result:",non_debt,-
impala,6329,comment_0,", this is not a bug because of the ""limit 3"" used in ""test"". Specifying a limit without an order by means *any* 3 records of ""test"" are correct to be returned. Without the GROUP BY ""test"" happens to produce the records with ids 1,2,3 - but that is implementation defined and not a guarantee provided by SQL. With the GROUP BY Impala does a hash-based aggregation before applying the limit which means you get a different set of ids returned from ""test"". This is perfectly acceptable from a SQL semantics point of view. Ultimately, a limit without order by may produce non-deterministic results. If you want deterministic results you need to add ""order by id"" before the limit in ""test"".",non_debt,-
impala,6329,comment_1,Query contains a limit without order by and so produces non-deterministic results.,non_debt,-
impala,6346,summary,Potential deadlock in KrpcDataStreamMgr,non_debt,-
impala,6346,description,"In [we take the and then call for all contexts. [then which can block if the deserialize pool queue is full. The next thread to become available in that queue will also have to acquire {{lock_}}, thus leading to a deadlock.",non_debt,-
impala,6346,comment_0,Nice catch.,non_debt,-
impala,6350,summary,split-hbase.sh failed during data load for exhaustive test run,non_debt,-
impala,6350,description,"This affects test setup, but not the product.",non_debt,-
impala,6350,comment_0,"This is similar in spirit to IMPALA-1995, which I've linked up.",non_debt,-
impala,6353,summary,Crash in snappy decompressor,non_debt,-
impala,6353,description,"Crash in latest rhel exhaustive run: From the impalad that crashed: F1224 10:25:38.853723 13490 decompress.cc:527] Check failed: input != nullptr int64_t input_len, const uint8_t* input) { DCHECK(input != nullptr); size_t result; if char* input_len, &result)) { return -1; } return result; } Backtrace: #7 0x0000000001a41d18 in (this=0xe6790c00, input_len=0, input=0x0) at #8 0x0000000001a41dec in (this=0xe6790c00, input_length=0, input=0x0, at #9 0x0000000001a324bf in (this=0xe6790c00, input_length=0, input=0x0, at #10 0x0000000001c55576 in (this=0x11ccc3600) at #11 0x0000000001bf4d86 in (this=0xb1e01c0, column_readers=...) at #12 0x0000000001bec14b in (this=0xb1e01c0) at #13 0x0000000001bea957 in (this=0xb1e01c0, at #14 0x0000000001be8f56 in (this=0xb1e01c0) at #15 0x0000000001b73092 in (this=0x88cbc00, filter_ctxs=..., at #16 0x0000000001b724bd in (this=0x88cbc00) at ... Query that was run at the time of the crash: select NULL select * from Several of these were run around the same time: Requesting prioritized load of table(s):",non_debt,-
impala,6353,comment_0,The parquet page header is corrupted: the compressed size is 0 while the uncompressed size is 16. The table being scanned is loaded from a snapshot and is not written by Impala. I checked the content of the page header and everything else in the header makes sense. That means the bug is unlikely to be in our hdfs stream. I think the correct fix here is to remove the DCHECK. Theoretically there could be a compressor that compresses a certain sequence into nothing. The scanner will find the inconsistency between uncompressed size and uncompressed data afterwards. The PageHeader memory dump:,non_debt,-
impala,6355,summary,dcheck failure for decimal asan tests,non_debt,-
impala,6355,description,"asf core asan tests have two new backend failures, likely introduced by either of these recent commits: (for IMPALA-5014) (for IMPALA-6300) Here are the failed tests and their stacktraces: [ RUN ] 22:30:05 Check failure stack trace: 22:30:05 @ 0x3fc73d6 22:30:05 @ 0x3fbddcd 22:30:05 @ 0x3fbf672 22:30:05 @ 0x3fbd7a7 22:30:05 @ 0x3fc0d6e 22:30:05 @ 0x17ad0be @ 0x179dd3c 22:30:05 @ 0x16eed0d @ 0x16b4d81 22:30:05 @ 0x1639ea7 22:30:05 @ 0x1ac4650 @ 0x2c5a86a @ 0x2c65a5a 22:30:05 @ 0x2c60540 22:30:05 @ 0x2c60cba 22:30:05 @ 0x1b1c907 22:30:05 @ 0x1b19cdd 22:30:05 @ 0x1affe6f .... [-] 2 tests from DecimalTest 22:35:55 [ RUN ] 22:35:55 Check failure stack trace: 22:35:55 @ 0x3e5d816 22:35:55 @ 0x3e5420d 22:35:55 @ 0x3e55ab2 22:35:55 @ 0x3e53be7 22:35:55 @ 0x3e571ae 22:35:55 @ 0x13e052e @ 0x13d3a1d ....",non_debt,-
impala,6355,comment_0,This happens reliably on ASAN.,non_debt,-
impala,6355,comment_1,"I was looking at disabling this to unblock test failures but realised that the bug is just that ""bool ovf"" is uninitialised in Mod(). I'll push out a fix shortly.",non_debt,-
impala,6408,summary,"[DOCS] Description of ""shuffle"" hint does not mention changes in IMPALA-3930",documentation_debt,low_quality_documentation
impala,6408,description,"The change in IMPALA-3930 states that if only one partition is written (because all partitioning columns are constant or the target table is not partitioned), then the ""shuffle"" hint leads to a plan where all rows are merged at the coordinator where the table sink is executed. The documentation of the ""shuffle"" hint does not mention this behavior.",documentation_debt,low_quality_documentation
impala,6408,comment_0,"I have found another minor insert hint related issue in ""To use a hint to influence the join order, put the hint keyword /* +SHUFFLE */ or /* +NOSHUFFLE */ (including the square brackets) after the PARTITION clause, immediately before the SELECT keyword."" I do not think that any join order is influenced by SHUFFLE in inserts, and there is also an inconsistency in hint style: "" /* +NOSHUFFLE */ (including the square brackets)""",documentation_debt,low_quality_documentation
impala,6419,summary,Check failed: 0 == (0 vs. 11),non_debt,-
impala,6419,description,Hit this during GVO Repro:,non_debt,-
impala,6419,comment_0,I tried to reproduce this locally but no luck yet. Will revert it and then try to reproduce with the original patch.,non_debt,-
impala,6419,comment_1,It looks like it was the fuzz test triggering the DCHECK: The start of the test runner output XML including the seed was:,non_debt,-
impala,6427,summary,Planner test expected output drops QUERYOPTIONS sections,non_debt,-
impala,6427,description,"The explain output in does not include the QUERYOPTIONS section, so it's not possible to replace the current expected output with the new expected output.",non_debt,-
impala,6442,summary,Misleading file offset reporting in error messages,design_debt,non-optimal_design
impala,6442,description,"has an error message ""File $0 has invalid file metadata at file offset $1."" However, the value reported as ""file offset"" is an offset from the _end_ of the file, not from its _beginning_. This is very misleading, since without explicitly stating that the offset is from the end, it is usually understood to be counted from the beginning. Additionally, although the function name is clearly about a ""footer"", two comments explicitly mention processing the ""header"". This falsely suggests that metadata is at the beginning of the file, when in reality it is at the end.",design_debt,non-optimal_design
impala,6442,comment_0,Thanks for creating this Jira . I will be taking a look and just assigned it to me.,non_debt,-
impala,6442,comment_3,"Many thanks to  for creating the jira, , , and  for the review and commit.. Now it's in 3.1.0.",non_debt,-
impala,6456,summary,Add flag to configure and negotiation thread count in KRPC,non_debt,-
impala,6456,description,"With the fix for KUDU-2228, the was retired in KRPC. We would ideally like to have it configurable, so we can add a flag from the Impala side to control that.",non_debt,-
impala,6456,comment_0,CC:,non_debt,-
impala,6475,summary,Enable running TPCH on Kudu in our nightly tests,non_debt,-
impala,6475,description,"It looks like we do not run TPCH and TPCDS on Kudu at all. With TPCDS, this is intentional because Kudu does not support Decimal. However, we should run TPCH on Kudu.",non_debt,-
impala,6517,summary,fails to recognize lsb_release output from RHEL,non_debt,-
impala,6517,description,It turns out we currently match the `lsb_release -irs` output against this map: We should add,non_debt,-
impala,6601,summary,ASAN in during,non_debt,-
impala,6601,description,"ASAN fails with in during  - Im assigning this to you thinking you might have an idea whats going on here; feel free to find another person or assign back to me if you're swamped. Ive seen this happen in a private Jenkins run. Please ping me if you would like access to the build artifacts. I saw this in builds that also had issues during the e2e tests, so I'm not sure whether this is flaky or reproducibly broken.",test_debt,flaky_test
impala,6601,comment_0,After hitting this a couple more times it looks reproducibly broken.,non_debt,-
impala,6613,summary,Change TEST_KRPC to DISABLE_KRPC,non_debt,-
impala,6613,description,"Now that KRPC is on by default, we should rename the environment variable to reflect that.",code_debt,low_quality_code
impala,6623,summary,Impala 2.12 Doc: Update ltrim and rtrim functions,non_debt,-
impala,6623,description,None,non_debt,-
impala,6623,comment_0,The existing documentation: should be changed to: And for rtrim:,documentation_debt,low_quality_documentation
impala,6647,summary,Add fine-grained CREATE privilege,non_debt,-
impala,6647,description,"Currently ALL privilege is required to create a table or database. We need to have a more fine-grained privilege, i.e. CREATE for database/table creation.",non_debt,-
impala,6656,summary,Metrics for time spent in BufferAllocator,non_debt,-
impala,6656,description,"We should track the total time spent and the time spent in TCMalloc so we can understand where time is going globally. I think we should shard them by CurrentCore() to avoid contention and get more granular metrics. We want a timer for the amount of time spent in SystemAllocator. We probably also want counters for how many times we go down each code path in (i.e. getting a hit immediately in the local area, evicting a clean page, etc down to doing a full locked scavenge).",non_debt,-
impala,6656,comment_0,cc,non_debt,-
impala,6656,comment_1,"There are some interesting metrics Kudu got out of tcmalloc, like the tcmalloc spinlock contention time: Unfortunately, it's no longer supported from tcmalloc 2.6 onwards apparently: In any case, there might be more interesting metrics we can get out of tcmalloc.",non_debt,-
impala,6666,summary,Impala 2.12 & 3.0 Docs: Use AES-GCM for spill-to-disk encryption when CLMUL instruction is present and performant,non_debt,-
impala,6666,description,"This is a performance optimization for spill-to-disk when encryption is enabled. If running with an OpenSSL version with AES-GCM support on a CPU that supports the CLMUL instruction, this faster encryption mode will be used.",code_debt,slow_algorithm
impala,6666,comment_0,Not a customer-facing feature.,non_debt,-
impala,6694,summary,BufferPool appears misaligned in query profile,non_debt,-
impala,6694,description,"It appears that the buffer pool statistics of exec node is sometimes misaligned in the profile. For instance, the aggregation node's buffer pool appears after the exchange node below: cc'ing",code_debt,low_quality_code
impala,6694,comment_0,"I understand what the bug is now. The order of ""Buffer Pool"" and the child ExecNode is preserved correctly in the original profile, but can be swapped in the profile in the coordinator. The issue is in The bug can happen when the coordinator receives a profile with the child ExecNode profile but not the ""Buffer pool"" profile. This can happen because the buffer pool profile is only added when the node is opened, whereaes the child is added when the ExecNode tree is created. In that case the child ExecNode's profile is added to the coordinator's copy of the profile. Then when the coordinator later receives a profile with the ""Buffer pool"" profile added, it does not attempt to reconcile the order of children, but instead just appends the ""Buffer pool"" profile at the end of the 'children_' array.",non_debt,-
impala,6709,summary,Simplify tests that copy local files to tables,code_debt,complex_code
impala,6709,description,"tests/query_test/ contains several tests that use ""hdfs dfs -copyFromLocal"" to fill tables with files that cannot be created with Impala. The current way of doing this looks like ""copy paste"" and adds unnecessary complexity to the test code. The common pattern of ""create a table, copy 1-2 files to it"" could be covered by a single function, or could be moved to the .test files, which support SHELL sections. for an example of SHELL section, see",design_debt,non-optimal_design
impala,6724,summary,Allow creating/dropping functions with the same name as built-ins,non_debt,-
impala,6724,description,"To match the behavior above, the statement below should be allowed. The statement below should trigger an not modify system database."")",non_debt,-
impala,6748,summary,Impala 2.12 & 3.0 Docs: Support more separators between date and time in default timestamp format,non_debt,-
impala,6752,summary,import kudu fails in python on Ubuntu 16.04,non_debt,-
impala,6752,description,IMPALA-6731 broke impala-python for me. It is now failing to import kudu with this error: It looks like the client was compiled with the C++11 ABI whereas libkudu_client.so does not - its version of the symbol is Reverting IMPALA-6731 solves the problem. I have PYPI_MIRROR set locally. This makes it impossible to run Python tests for me.,non_debt,-
impala,6752,comment_0,The bug is that --no-binary requires an argument but we didn't provide one so it swallowed up the next argument.,non_debt,-
impala,6762,summary,encounters an exception doing a,non_debt,-
impala,6762,description,"Problem: In the function while calling an exception is encountered in boost library, which results in a SIGABRT. The probable cause of this issue is that lock has been freed. Note : This problem has been investigated for legacy thrift setup not in a new KuduRPC setup Evidence: We have a minidump for the issue seen; the two suspected threads involved in the issue are listed below. or does not wait for threads inside to finish, that leads to a situation where the ~DataStreamRecv() will be called with thread still in which may sometime result in this crash.",non_debt,-
impala,6762,comment_0,The ExchangeNode holds a shared_ptr to the DataStreamRecvr so ~DataStreamRecvr() can't be called while the fragment instance is still executing (which is the thread that will be calling GetBatch()). So there must be something else going on here.,non_debt,-
impala,6762,comment_1,"After investigating the core and the code, I have couple of observations, I'm listing them below: a) One can see from the logs that DeregisterRecvr() has been done so has been called so is_cancelled_ member field should be set to true. b) checks the is_cancelled_ member field before going to sleep to be notified if cancellation or new batch request arrives. It should not be doing so as is_cancelled_ is true. Since it's in the process of waiting tells that the value of is_cancelled is not correct, which tells that there is likelihood of heap corruption as SenderQueue is allocated from the heap -- Code snip - SenderQueue* queue = SenderQueue(this, Log traces for the issue. I0313 08:18:18.036808 106561 Open(): . . I0313 08:18:18.036655 106560 Open(): . . I0313 08:18:18.040612 36316 sending CancelPlanFragment rpc for I0313 08:18:18.040603 36316 cancelled stream: node_id=4 I0313 08:18:18.040597 36316 cancelling all streams for I0313 08:18:18.040593 36316 Cancel(): I0313 08:18:18.040591 36316 Cancelling plan fragment... I0313 08:18:18.040586 36316 Cancel() I0313 08:18:18.040577 36316 Cancel(): I0313 08:18:18.040566 36316 UnregisterQuery(): I0313 08:18:18.040510 36316 status.cc:111] Session closed . . . I0313 08:18:18.044102 36311 status.cc:111] Session closed . . I0313 08:18:18.044162 36311 UnregisterQuery(): I0313 08:18:18.044172 36311 Cancel(): I0313 08:18:18.048719 36311 DeregisterRecvr(): node=4 I0313 08:18:18.048748 36311 cancelled stream: node_id=4 I0313 08:18:18.048331 36311 status.cc:111] Invalid or unknown query handle . . Wrote minidump to",non_debt,-
impala,6762,comment_2,"I took another look and agree that it doesn't make sense - there's no way it should be referencing invalid memory here. So it's probably a heap use-after-free, which we can't really track down without a repro (in all likelihood it's been fixed)",non_debt,-
impala,6780,summary,have always-true asserts,non_debt,-
impala,6780,description,I discovered in the process of looking at IMPALA-6453 that we have some assertions that the Python 2.6 compiler thinks are always true. This seems like a true test bug.,non_debt,-
impala,6786,summary,List of backends doesn't get updated for dedicated coordinators,non_debt,-
impala,6786,description,"List of backend for Impala daemons running with ""is_executor=false"" doesn't get updated when other new daemons go online or old ones go offline Repro 1) Setup an Impala daemon with dedicated coordinator 2) Run a couple of query // Might not be necessary// 3) Change the state of the Impala cluster by brining more nodes online or taking some offline 4) Check the subscriber page on the statestore, it should match expected state of the cluster 5) Check the backends page for the dedicated coordinator, it won't match the SS subscriber page Number of backed from each Impala daemon page",non_debt,-
impala,6786,comment_0,inconsistency in statestore state and impalad seem to be cause by same root cause.,non_debt,-
impala,6806,summary,TLS certificate with Intermediate CA in server cert file fails with KRPC,non_debt,-
impala,6806,description,"Take 2 certificate files: cert.pem and truststore.pem cert.pem has 2 certificates in it: A cert for that node (with CN=""hostname"", and signed by And the intermediate CA cert (with and signed by truststore.pem has 1 certificate in it: A cert which is the root CA (with self-signed) This format of certificates don't seem to verify on the OpenSSL command line but works with Thrift. This also doesn't work with KRPC. Workaround for this issue w/ KRPC turned on: If we move the second certificate from cert.pem into truststore.pem, then this seems to work. We'll need to dig into whether this is a PEM file format issue, or a KRPC issue. But the above workaround should unblock us for now.",design_debt,non-optimal_design
impala,6806,comment_0,Fixed by cherry-pick:,non_debt,-
impala,6817,summary,Clean up Impala privilege model,code_debt,low_quality_code
impala,6817,description,A follow up for this review:,non_debt,-
impala,6817,comment_1,Is there any user-facing feature change with this story? That needs to be documented?,documentation_debt,outdated_documentation
impala,6817,comment_2,Not for this. This story is just for code clean-up. It doesn't affect the behavior. We do need a documentation for this: As soon as we're done with we need to start documenting it.,documentation_debt,outdated_documentation
impala,6824,summary,Crash in when events_ is empty,non_debt,-
impala,6824,description,When {{events_}} is empty in events_.back() is undefined. This can lead to a crash. The fix is to check for {{events_.empty()}} before calling {{back()}}.,non_debt,-
impala,6824,comment_2,"which commit broke this? If there is one, could you link it to this jira.",non_debt,-
impala,6824,comment_3,This was the commit: I will also link the Jiras.,non_debt,-
impala,6847,summary,Consider adding workaround for high memory estimates in admission control,non_debt,-
impala,6847,description,"A scenario we've seen play out a couple of times is this. 1. An Impala admin sets up memory-based admission control but sets the default query mem_limit to 0. This means that admission control will use memory estimates instead of mem_limit. Typically admins want some protection from large queries consuming excessive memory but can't or don't want to set a single mem_limit because the workload is unknown or unpredictable. This configuration has caveats and we recommend against it, but this happens and often works well enough as long as the workload is comprised of relatively simple queries. The caveats include: * There is no enforcement that a query stays within the memory estimate. This means that a query can fail or force other queries to fail. * Memory estimates are often inaccurate (this is unavoidable since they depend on cardinality estimates, which are commonly off by 10x even with state-of-the-art query planners). This means that runnable queries may be impossible to admit without setting a mem_limit. 2. Something changes about the workload, e.g. a new query is added, stats are computed, data size changes, we make an otherwise-innocuous planner change. Then we run into the second problem above and one or more queries cannot be executed, e.g. ""Rejected query from pool root.foo: request memory needed 1234.56 GB is greater than pool max mem resources 1000.00 GB. Use the MEM_LIMIT query option to indicate how much memory is required per node. The total memory needed is the per-node MEM_LIMIT times the number of nodes executing the query. See the Admission Control documentation for more information. "" The last problem is the problem this JIRA is intending to work around. The preferred solutions, which work most of the time, are: # Configure a default query memory limit for the pool # Set a mem_limit for the query # Disable memory-based admission control and do admission control based on num_queries, which is simpler and easier to understand. It would however, be useful to have a third fallback in cases where the first two options are difficult or impossible to apply. The basic requirement is to allow an administrator to configure a resource pool such that queries with very high estimates can run. We probably need enough flexibility that they can run concurrently with other smaller queries (i.e. the big query shouldn't take over the whole pool) and ideally we would also have a mem_limit applied to the query so that we're still protected from runaway memory consumption. The long-term solution to this is IMPALA-6460. This is a short-term workaround that can tide users over until we have a comprehensive solution.",design_debt,non-optimal_design
impala,6847,comment_0,"I think the preferred solution alternatives should also have: use max concurrent queries rather than max mem, if mem_limits can't be determined. (Not to say we can't also have this escape valve for mem estimates, but for clarity on preference of solutions).",non_debt,-
impala,6850,summary,Print the actual error message to the console when Sentry fails,code_debt,low_quality_code
impala,6850,description,"When Sentry fails to start, this is currently what gets printed into the console.",non_debt,-
impala,6858,summary,Add type information to query profiles,non_debt,-
impala,6858,description,"If query profiles (or structured plans) had table metadata information like types, testing Impala on a pre-existing query profile would be easier, since we could automate generating the table scaffolding.",design_debt,non-optimal_design
impala,6858,comment_0,", I think Phil is asking for something like the test case builder for which we already have a JIRA. Just FYI, not sure if you want to keep these separate or consolidate.",non_debt,-
impala,6858,comment_1,Yes looks like the end goal is the same.  Do you think this is covered by IMPALA-5872?,non_debt,-
impala,6886,summary,Impala Doc: Remove Impala Cluster Sizing doc,non_debt,-
impala,6886,description,Removing per 's request.,non_debt,-
impala,6896,summary,in DESCRIBE FORMATTED on views,non_debt,-
impala,6922,summary,test_kudu_insert on exhaustive build,non_debt,-
impala,6922,description,Error Message Stacktrace Standard Error,non_debt,-
impala,6922,comment_0,"[sorry, thought I saw this again; turns out it's a different bug; going back down to p2 for now]",non_debt,-
impala,6922,comment_1,dup of IMPALA-6812,non_debt,-
impala,6937,summary,"Connection failure to HS2 causes data log to fail with ""Error opening session""",non_debt,-
impala,6937,description,This GVO job failed: That log file has the following error:,non_debt,-
impala,6937,comment_0,- Have you seen this in the past couple of days?,non_debt,-
impala,6937,comment_1,This doesn't look familiar.,non_debt,-
impala,6937,comment_2,As expected a subsequent build passed so I'm marking this one as flaky.,test_debt,flaky_test
impala,6937,comment_3,Hasn't been seen again.,non_debt,-
impala,6946,summary,Hit DCHECK in,non_debt,-
impala,6946,description,The bug comes from conversion between signed and unsigned. I don't believe this will cause a crash on a release build and can only be triggered by a very specific corrupt parquet file. It was introduced by IMPALA-4177,non_debt,-
impala,6993,summary,Don't print status stack trace when propagating thrift status in,non_debt,-
impala,6993,description,"We should use Status::Expected() here, just like it's used above. The stack trace isn't interesting and the error is expected once we get down this path. (I'm not sure why we don't just use {{exec_status}} though, but presumably the prefix was added for a reason).",design_debt,non-optimal_design
impala,6999,summary,Upgrade to sqlparse 0.1.19 in Impala Shell,non_debt,-
impala,6999,description,None,non_debt,-
impala,7049,summary,Scan node reservation calculation seems off,non_debt,-
impala,7049,description,"Running the query TPC-DS Q77a with a memory limit, we ran into the error *HDFS scan min reservation 0 must be According to the code, the reservation for the scan node is supposed to be computed correctly in the FE but this doesn't appear to be the case",non_debt,-
impala,7049,comment_0,"Attached a profile from an affected query. It doesn't really make sense to me - the plan looks like it's from before the below commit, but that's the commit that added the error message:",non_debt,-
impala,7049,comment_1,Turns out that the new build wasn't deployed correctly. Sorry for the confusion.,non_debt,-
impala,7080,summary,DCHECK hit in DelimitedTextParser,non_debt,-
impala,7080,description,Seen in a 2.x exhaustive run: excerpt from impalad.INFO Possibly related to IMPALA-6389,non_debt,-
impala,7080,comment_0,chose you at random. Feel free to re/un-assign if you don't have time,non_debt,-
impala,7080,comment_1,dup of IMPALA-7058,non_debt,-
impala,7161,summary,Bootstrap's handling of JAVA_HOME needs improvement,design_debt,non-optimal_design
impala,7161,description,"installs the Java SDK and sets JAVA_HOME in the current shell. It also adds a command to the to export JAVA_HOME there. This doesn't do the job. tests for JAVA_HOME at the very start of the script, before it has sourced So, the user doesn't have a way of developing over the long term without manually setting up JAVA_HOME. also doesn't detect the system JAVA_HOME. For Ubuntu 16.04, this is fairly simple and if a developer has their system JDK set up appropriately, it would make sense to use it. For example:",design_debt,non-optimal_design
impala,7161,comment_0,"Additional issue: Suppose a user sets JAVA_HOME in their environment like they are supposed to. Suppose they also have JAVA_HOME in like writes. The two can get out of sync. The JAVA would be from the environment JAVA_HOME, but after setting JAVA, value for JAVA_HOME would overwrite the environment variable and JAVA_HOME would be that value. These could point to two different places.",design_debt,non-optimal_design
impala,7161,comment_1,"This all sounds good to me. I think if there's a working java on the path, we should use it. I found the following works for me: I think the {{readlink}} approach is good too.",non_debt,-
impala,7171,summary,Add docs for Kudu insert,non_debt,-
impala,7171,description,"On the page: at the end of the section: ""Impala DML Support for Kudu Tables (INSERT, UPDATE, DELETE, UPSERT)"", we should add text like: Starting from Impala 2.9, Impala will automatically add a partition and sort step to INSERTs before sending the rows to Kudu. Since Kudu partitions and sorts rows on write, pre-partitioning and sorting takes some of the load off of Kudu, and helps ensure that large INSERTs complete without timing out, but it may slow down the end-to-end performance of the INSERT. Starting from Impala 2.10, the hints ""/* */"" may be used to turn this pre-partitioning and sorting off. Additionally, since sorting may consume a lot of memory, users should consider setting a ""mem_limit"" for these queries.",documentation_debt,outdated_documentation
impala,7171,comment_0,"In impala_hints.html, we have: Starting from Impala 2.9, {{INSERT}} or {{UPSERT}} operations into Kudu tables automatically have an exchange and sort node added to the plan that partitions and sorts the rows according to the key scheme of the target table (unless the number of rows to be inserted is small enough to trigger single node execution). Use the{{ /* +NOCLUSTERED */}} and {{/* +NOSHUFFLE */}} hints together to disable partitioning and sorting before the rows are sent to Kudu. Do you want to use the same text in impala_kudu.html?",non_debt,-
impala,7179,summary,Consider changing default,non_debt,-
impala,7179,description,I've seen multiple instances of Impala users being tripped up by this behaviour and zero instances of it being useful (although it's possible that it helped someone and they didn't notice).,non_debt,-
impala,7179,comment_0,"I agree this should be default, this behaviour is what I would intuitively expect.",non_debt,-
impala,7205,summary,Respond to ReportExecStatus() RPC with CANCELLED whenever query execution has terminated,non_debt,-
impala,7205,description,"Currently we respond with CANCELLED only when hitting EOS. It seems a bit more robust to always respond with CANCELLED whenever query execution has terminated. That way, if the cancel RPC from the coordinator to a backend fails, the backend will still cancel if it can send status back to the coordinator later on. Without this fix, the query can hang and/or finstances can continue running (until the query is closed, at which point the response to this RPC will be an error).",design_debt,non-optimal_design
impala,7218,summary,Impala 3.1 Doc: Allow Column Definitions in ALTER VIEW,non_debt,-
impala,7218,description,None,non_debt,-
impala,7232,summary,Display whether fragment instances' profile is complete,non_debt,-
impala,7232,description,"While working on IMPALA-7213, it's noticed that we can fail to serialize or deserialize a profile for random reasons. This shouldn't be fatal: the fragment instance status can still be presented to the coordinator to avoid hitting IMPALA-2990. A missing profile in ReportExecStatus() RPC may result in incomplete or stale profile being presented to Impala client. It would be helpful to mark whether the profile may be incomplete and/or final in the profile output.",non_debt,-
impala,7232,comment_0,I think IMPALA-6741 would help.  would that be a good solution?,non_debt,-
impala,7232,comment_1,We also have IMPALA-5555,non_debt,-
impala,7232,comment_2,"Yes, fixing IMPALA-6741 will definitely help. A missing profile update for a particular instance just results really old timestamp in the profile. We also need a marker in the profile to indicate whether the profile is final in order to tackle situation like IMPALA-5555.",non_debt,-
impala,7232,comment_3,"The fragment instance states are already printed to the profile (IMPALA-6190 / IMPALA-6246) which indicate the profile as being complete when the status propagates to and prints ""ExecInternal Finished"" to the profile",non_debt,-
impala,7234,summary,Non-deterministic majority format for a table with equal partition instances,non_debt,-
impala,7234,description,"The getMajorityFormat method of the FeCatalogUtils currently returns non-deterministic results when its argument is a list of partitions where there is no numerical majority in terms of the number of instances. The result is determined by the order in which the partitions are added to the HashMap. We need more deterministic results which also considers the memory requirement among different types of partitions. Ideally, this function should return the format with higher memory requirements in case of a tie.",design_debt,non-optimal_design
impala,7234,comment_0,"Should this function actually take into account the total byte sizes or counts of ranges or files? In recently looking at this code I couldn't quite make sense of the logic. For example, if we have 10 partitions that are text, each containing one file, and one partition which is Parquet, containing 100 files, maybe it makes more sense to estimate scan range memory usage based on Parquet instead of text?",design_debt,non-optimal_design
impala,7234,comment_1,You are right. We should always consider the partition with the highest memory requirement while estimating the scan range.,non_debt,-
impala,7279,summary,failing: incompatible regex,non_debt,-
impala,7279,description,the regex that parses the time string is incompatible with microseconds in the string. eg. 999us,non_debt,-
impala,7307,summary,Support TABLESAMPLE and stats extrapolation in LocalCatalog,non_debt,-
impala,7307,description,None,non_debt,-
impala,7325,summary,SHOW CREATE VIEW on a view that references built-in functions requires access to the built-in database,non_debt,-
impala,7325,comment_1,This may duplicate IMPALA-2595.,non_debt,-
impala,7349,summary,"Automatically choose mem_limit based on estimate, clamped to range",non_debt,-
impala,7349,description,"We should add admission control support for intelligently choosing how much memory to give a query, based on the memory estimate. Currently we require that you set a single mem_limit per pool. Instead it would be better if we allowed configuring min/max guardrails within which admission control chose an amount of memory that will allow good performance. Initially, I think mem_limit will be the same for all backends. Eventually it could make sense to have different limits per backend depending on the fragments.",design_debt,non-optimal_design
impala,7350,summary,More accurate memory estimates for admission,non_debt,-
impala,7350,description,"For IMPALA-7349, we will be relying more on memory estimates. This is an umbrella JIRA to track improvements to memory estimates where the current estimates are way off and result in over- or under- admission. over-admission is probably the more significant concern.",design_debt,non-optimal_design
impala,7350,comment_0,"All subtasks are closed, we can call this done.",non_debt,-
impala,7362,summary,Add query option to set timezone,non_debt,-
impala,7362,description,"Hive has ""set time zone"" command to set the timezone used for utc<->local time conversions during a session (HIVE-16614). A similar command could be useful in Impala, at least for testing.",non_debt,-
impala,7368,summary,Add initial support for DATE type,non_debt,-
impala,7368,description,"DATE values describe a particular year/month/day, in the form YYYY-MM-DD. For example, DATE '2013-01-01'. Date types do not have a time of day component. The range of values supported for the Date type is 0000-01-01 to 9999-12-31. The initial DATE type support should incluide the following changes: - new internal type - casting between DATE and other types - codegen infrastructure for expression evaluation - ""IS [NOT] NULL"" and ""[NOT] IN"" predicates - common comparison operators - BETWEEN operator - conditional functions - infrastructure changes for builtin scalar functions. - some built-in functions: aggregate functions, analytical functions, math functions. - support partitioning. - text support only. These items are tightly coupled and it makes sense to implement them in one change-set.",non_debt,-
impala,7370,summary,Add DATE type support to Parquet scanner/writer,non_debt,-
impala,7370,description,Implement Parquet DATE type support.,non_debt,-
impala,7376,summary,Impala hits a DCHECK if a fragment instance fails to initialize the filter bank,non_debt,-
impala,7376,description,"While Prepare()-ing a fragment instance, if we fail to initialize the runtime filter bank, we will exit FIS::Prepare() without acquiring a thread token FIS::Finalize() is called *always* regardless of whether the fragment instance succeeded or failed. And FIS::Finalize() tries to even though it might not have gotten acquired: , causing a DCHECK to be hit. This was found while I was adding global debug actions (IMPALA-7046) to the FIS.",non_debt,-
impala,7376,comment_0,CC:,non_debt,-
impala,7388,summary,JNI THROW_IF_ERROR macros use local scope variables which likely conflict,design_debt,non-optimal_design
impala,7388,description,"The THROW_IF_ERROR macros all use a locally scoped variable ""status"". If they're called with a pattern like: then the status variable inside the macro ends up being assigned to itself instead of the outer-scope variable. This makes it not throw or return, which is quite surprising.",design_debt,non-optimal_design
impala,7400,summary,"""SQL Statements to Remove or Adapt"" is out of date",architecture_debt,using_obsolete_technology
impala,7400,description,"""Impala has no DELETE statement."" and ""Impala has no UPDATE statement. "" are not totally true - Impala has those statements but only for Kudu tables. ""For example, Impala does not support natural joins or anti-joins,"" - Impala does support Anti-joins via NOT IN/NOT EXISTS or even explicitly like: ""Within queries, Impala requires query aliases for any subqueries:"" - this is only true for subqueries used as inline views in the FROM clause. E.g. the following works: "" Impala .. requires the CROSS JOIN operator for Cartesian products."" - untrue, this works: ""Have you run the COMPUTE STATS statement on each table involved in join queries"". This isn't specific to queries with joins, although may have more impact. We recommend that users run COMPUTE STATS on all tables. ""A CREATE TABLE statement with no PARTITIONED BY clause stores all the data files in the same physical location,"" - unpartitioned tables with multiple files can have files residing in different locations (and there are already 3 replicas per file by default, so the statement is a little misleading even if there's a single file). I think the latest statement about ""Have you partitioned at the right granularity so that there is enough data in each partition to parallelize the work for each query?"" is also misleading for the same reason. ""The INSERT ... VALUES syntax is suitable for setting up toy tables with a few rows for functional testing, but because each such statement creates a separate tiny file in HDFS"". This advice only applies to HDFS, this should work fine for Kudu tables although the INSERT statements are not particularly fast. ""The number of expressions allowed in an Impala query might be smaller than for some other database systems, causing failures for very complicated queries"" - this doesn't seem right - I don't know why the queries would fail. Also the codegen time isn't really specific to expressions or where clauses. There seems to be a point buried in there, but maybe it's just essentially that ""Complex queries may have high codegen time""",design_debt,non-optimal_design
impala,7406,summary,Flatbuffer wrappers use almost as much memory as underlying data,non_debt,-
impala,7406,description,"Currently the file descriptors stored in the catalogd memory for each partition use a FlatBuffer to reduce the number of separate objects on the Java heap. However, the FlatBuffer objects internally each store a ByteBuffer and int position, so each object takes 32 bytes on its own. The ByteBuffer takes 56 bytes since it stores various references, endianness, limit, mark, position, etc. This amounts to about 88 bytes overhead on top of the actual underlying flatbuf byte array which is typically around 100 bytes for a single-block file. So, we're have about a 1:1 ratio of memory overhead and a 2:1 ratio of object count overhead for each partition. If we simply stored the byte[] array and constructed wrappers on demand, we'd save 88 bytes and 2 objects per partition. The downside is that we'd need to do short-lived ByteBuffer allocations at access time, and based on some benchmarking I did, they don't get escape-analyzed out. So, it's not a super clear win, but still worth considering.",design_debt,non-optimal_design
impala,7406,comment_0,I filed with some measurements about different approaches to avoid the flyweight allocations at access time,non_debt,-
impala,7434,summary,Impala 2.12 Doc: Kudu's kinit does not support auth_to_local rules with Heimdal kerberos,documentation_debt,outdated_documentation
impala,7459,summary,Query with width_bucket() crashes with Check failed: lhs > 0 && rhs > 0,non_debt,-
impala,7459,description,"When running a private exhaustive build, decimal_fuzz_test ran into a case which triggers a DCHECK in",non_debt,-
impala,7459,comment_0,This is impacting GVO and several builds (see duplicate IMPALA-7461). I'm reverting fix for IMPALA-7412.,non_debt,-
impala,7484,summary,Unrecognized hint interpreted as straight_join,non_debt,-
impala,7484,description,The call to setIsStraightJoin() in should be wrapped in an else-clause.,non_debt,-
impala,7484,comment_0,The patch was merged a while back. Hence closing it.,non_debt,-
impala,7485,summary,test_spilling_naaj hung on jenkins,non_debt,-
impala,7485,description,"seemed hung (was running for more than 4 hours), see Core dumps and stack traces of impalad were created and the impalad was killed. The tests continued without failures after impalad was restarted.",non_debt,-
impala,7485,comment_0,"Update: I haven't find any weird thing around impalad callstacks and logs yet, but the statestore log is full of error messages like: These errors start around 23:00:00, which is close to the point where tests started to hang. Tests from were running around this time, but they should have stopped much sooner than they did.",non_debt,-
impala,7485,comment_1,i'd be interested in looking at the callstacks if you could attach them,non_debt,-
impala,7485,comment_2,I have uploaded the (impalad) stacks. Sadly we didn't create stacks for statestore.,non_debt,-
impala,7485,comment_3,There was a cluster of hangs around this time related to IMPALA-7488. I suspect it's related to that.,non_debt,-
impala,7517,summary,Hung scanner when soft memory limit exceeded,non_debt,-
impala,7517,description,"As reported on the mailing list, this is a regression due to IMPALA-7096 The scanner thread has the following code: What if we have the following scenario: T1) grab scan range 1 and start processing T2) grab scan range 2 and start processing T1) finish scan range 1 and see that 'progress_' is not done() T1) loop around, get no scan range (there are no more), so set all_ranges_satrted_ and break T1) thread exits T2) finish scan range 2 T2) happen to hit a soft memory limit error due to pressure from other exec nodes, etc. Since we aren't the first thread, we break. (even though the first thread is no longer running) T2) thread exits Note that no one got to the point of calling SetDone() because we break due to the memory limit error _before_ checking progress_.Done(). Thus, the query will hang forever.",non_debt,-
impala,7519,summary,Support elliptic curve ssl ciphers,non_debt,-
impala,7519,description,"Thrift's SSLSocketFactory class does not support setting ciphers that use ecdh. We already override this class for others reasons, it would be straightforward to add the necessary openssl calls to enable this.",non_debt,-
impala,7519,comment_0,"What's the motivation - is it performance, or security, or . . .",non_debt,-
impala,7519,comment_1,"The motivation is security primarily. There's an Impala user who has business requirements for using this type of cipher. Fwiw, we already support this in the krpc stack, this would just be to extend the same support to the thrift stack. I have a review out here:",non_debt,-
impala,7525,summary,Impala Doc: Document SHOW GRANT USER,non_debt,-
impala,7542,summary,"in impala-gdb.py misses to find the ""root threads""",non_debt,-
impala,7542,description,"uses ThreadDebugInfo from IMPALA-6416 to find the query ids and fragment ids currently being executed of an (probably crashed) impalad. To achieve that it traverses the stack frames of each thread and searches for the stack frame of This function has a parent_thread_info parameter that points to its parent thread's ThreadDebugInfo object. Then, checks what instance id the parent thread is working on. This is all good until the bottom of the thread hierarchy because threads always work on the same fragment instance that their parent work on. However, at the top of the hierarchy there is some ""root thread"" that starts to work on a fragment instance, but its parent doesn't work on any. Since we only check the parent thread's instance id we miss to catch this first thread. also has a local ThreadDebugInfo that contains the proper instance id, so we should use it instead of the parent's.",non_debt,-
impala,7542,comment_0,"Under review: I also want to note that it still doesn't find all threads that work on some fragment instances, because threads that belong to thread pools aren't annotated currently. IMPALA-6417 is tracking this.",non_debt,-
impala,7548,summary,attempts to check asf-site commits,non_debt,-
impala,7548,description,"See for example. The job can be restricted to branches; right now it's running on "".*"", which is nice because it picks up new code branches without any adjusting.",non_debt,-
impala,7548,comment_0,I'm going to switch it to only run on master and 2.x for now.,non_debt,-
impala,7562,summary,Caused by: Communication link failure. Failed to connect to server. Reason: Unknown.,non_debt,-
impala,7562,description,"I encountered a very strange problem. Spring boot configured impala JDBC query. Under normal circumstances, all SQL queries are normal, and the statement has no problem.However, after I restart the impala service, this exception will be reported, or the same error code will be reported for all SQL queries. After I have to restart my spring boot web service, all the queries will be normal again, as I have tried many times.I looked through the server logs and found that I did not seem to have received the request.But why is this problem so low?Is it driven?My impala is three nodes built under CDH. I really can not find out the reason, please help to look.thank you ImpalaJDBC41.jar",non_debt,-
impala,7562,comment_0,"You would probably have to recreate your JDBC connections and implement retry logic within your app. Resolving since Impala itself is working fine, things recover once you restart the web application.",non_debt,-
impala,7567,summary,Implement timezone aware parquet stat filtering for timestamp columns,non_debt,-
impala,7567,description,"Parquet timestamp columns can contain UTC normalized data, which means that the data is stored in UTC but it is expected to be shown in local time (to be consistent with Hive). This is done by converting these timestamp from UTC to local time during scanning. This conversion has to be considered during min/max stat filtering, otherwise some row groups can be incorrectly skipped. For this reason IMPALA-7559 disables stat filtering on UTC normalized timestamp columns. This ticket deals with creating a correct implementation to be able re-enable stat filtering for these columns. DST and historical rule changes add some complexity to this. UTC- The solution I see is to convert min/max of the predicate from local to UTC and resolve ambiguity by choosing the earlier time in case of min, and the later time in case of max. These UTC values can be compared with stats safely. Note the timezone rules can be different in Hive and Impala (especially historical ones), so we cannot ensure that Impala gives exactly the same results as Hive. The goal is to ensure that Impala returns the same rows with and without stat filtering.",non_debt,-
impala,7567,comment_0,Created by mistake.,non_debt,-
impala,7585,summary,Always set user credentials after creating a KRPC proxy,non_debt,-
impala,7585,description,"ctor may fail in for various reason: This resulted in an empty user name being used in With plaintext SASL (e.g. in an insecure Impala cluster), this may result in the following error: While one can argue that Kudu should fall back to some default username (e.g. ""cpp-client"") when fails, it may have non-trivial consequence (e.g. generating an authn token with some random username on one machine while using the real user name on another machine). Therefore, it's best for Impala to explicitly set the user credentials after creating the proxy.",non_debt,-
impala,7588,summary,incorrect HS2 null handling introduced by IMPALA-7477,non_debt,-
impala,7588,description,"reported this issue with the HS2 endpoint: Reproduction: From JDBC client (only tried Hive JDBC client) The bug is with handling of nulls in the conversion functions, specifically output_row_idx isn't incremented.",non_debt,-
impala,7588,comment_0,I pushed in the revert of IMPALA-7477 to the ASF repo:,non_debt,-
impala,7613,summary,Support round(DECIMAL) with non-constant second argument,non_debt,-
impala,7613,description,Sometimes users want to round to a precision that is data-driven (e.g. using a lookup table). They can't currently do this with decimal. I think we could support this by just using the input decimal type as the output type when the second argument is non-constant. Motivated by a user trying to do something like this;,non_debt,-
impala,7613,comment_0,This is consistent with the principle of IMPALA-6230 of using the input type.,non_debt,-
impala,7631,summary,Add Sentry configuration to allow specific privileges to be granted explicitly,non_debt,-
impala,7631,description,Sentry requires a new configuration to specify which privileges are permitted to be granted explicitly: We need to update files with a new configuration.,non_debt,-
impala,7632,summary,Erasure coding builds still failing because of default query options,non_debt,-
impala,7632,description,Two tests fail because the default query options they set were clobbered by the custom cluster test infra:,non_debt,-
impala,7640,summary,ALTER TABLE RENAME on managed Kudu table should rename underlying Kudu table,non_debt,-
impala,7640,description,"Currently, when I execute ALTER TABLE RENAME on a managed Kudu table it will not rename the underlying Kudu table. Because of IMPALA-5654 it becomes nearly impossible to rename the underlying Kudu table, which is confusing and makes the Kudu tables harder to identify and manage.",design_debt,non-optimal_design
impala,7667,summary,does not accept empty value,non_debt,-
impala,7667,description,"supports putting an empty value to mean allow all privileges. However, there's a bug with empty value as described in The workaround is to add a space until the to mean allow all privileges.",non_debt,-
impala,7677,summary,multiple count(distinct): Check failed:,non_debt,-
impala,7677,description,"The random query generator found a query that crashes an executor node. Multiple COUNT(DISTINCT) looks involved, so assigning to . Query that reproduces the crash: From",non_debt,-
impala,7677,comment_0,This simplified query also produces the crash:,non_debt,-
impala,7682,summary,AuthorizationPolicy is not thread-safe,requirement_debt,non-functional_requirements_not_fully_satisfied
impala,7682,description,"The method public Set<String> groups, Set<String> users, ActiveRoleSet roleSet) in AuthorizationPolicy needs to be synchronized.",code_debt,multi-thread_correctness
impala,7685,summary,Connect to impala database via JDBC and the connection is not closed when the query is finished,non_debt,-
impala,7685,description,CDH,non_debt,-
impala,7685,comment_0,"This is the expected behaviour, you have to close the session from the client. Impala will only close idle sessions after see docs for specifics.",non_debt,-
impala,7686,summary,Allow RANGE() clause before HASH() clause for PARTITION BY,non_debt,-
impala,7686,description,"Table creation succeeds this way: However, it fails with a syntax error if we swap the order of HASH() and RANGE(): I think we shouldn't restrict the order of RANGE() and HASH(). Check the relevant accepted syntax here:",non_debt,-
impala,7688,summary,Spurious error messages when updating owner privileges,design_debt,non-optimal_design
impala,7688,description,Failure in updating owner privileges is not an issue since this could mean a Sentry refresh occurred while updating owner privileges and Sentry refresh itself will update all privileges including owner privileges.,non_debt,-
impala,7703,summary,Upgrade to Sentry 2.1.0,non_debt,-
impala,7703,description,Upgrade to Sentry 2.1.0 for various bug fixes related to fine-grained privileges and object ownership.,non_debt,-
impala,7703,comment_0,"Fredy, please include the rationale for tickets you file in the Description box.",non_debt,-
impala,7705,summary,Impala 2.13 & 3.1 Docs: ALTER DATABASE SET OWNER,non_debt,-
impala,7705,description,ALTER DATABASE SET OWNER,non_debt,-
impala,7709,summary,Add options to restart catalogd and statestored in,non_debt,-
impala,7709,description,Currently there exists an option to restart impalad processes. We need similar options to restart catalogd and statestored in which can be useful for testing against catalogd and statestored scenarios.,non_debt,-
impala,7715,summary,"""Impala Conditional Functions"" documentation errata",non_debt,-
impala,7715,description,"Consider the documentation page [Impala Conditional Multiple functions have ambiguous descriptions. For example: The above is confusing, it essentially means: ""Returns true if a Boolean expression is false or not."" This obviously means the function always returns false, which is not accurate. Reword to say: ""Returns true if the expression is false. Returns false if the expression is true or NULL."" Other ambiguous descriptions: Better: ""Returns true if an expression is true. Returns false if the expression is false or NULL."" Others: Better: ""Returns true if the expression is non-null, tase if the expression is null. Same as {{expression IS NOT NULL}}."" Better: ""Returns true if the expression is NULL, false otherwise. Same as {{expression IS NULL}}""",documentation_debt,low_quality_documentation
impala,7716,summary,Failure in,non_debt,-
impala,7716,comment_0,I think this is the same as I'm posting a review today.,non_debt,-
impala,7716,comment_1,IMPALA-7639,non_debt,-
impala,7721,summary,/catalog_object web API is broken for getting a privilege,non_debt,-
impala,7742,summary,User names in Sentry are now case sensitive,non_debt,-
impala,7742,description,Sentry no longer stores user names in lower case,non_debt,-
impala,7748,summary,Remove the appx_count_distinct query option,non_debt,-
impala,7748,description,"With IMPALA-110, we can now support multiple count distinct directly, and the appx_count_distinct query option is no longer needed. Users who want the perf improvement from it can always just use the ndv() function directly in their sql. We'll mark this option as deprecated in the docs starting from 3.1. Removing it can be targeted for 4.0",code_debt,dead_code
impala,7748,comment_0,"After further discussion, we've decided t leave appx_count_distinct as is",non_debt,-
impala,7787,summary,failed because of docker 503 Service Unavailable,non_debt,-
impala,7787,description,This happened a couple of times. Looks like flakiness but unsure if it was just a transient infra issue or something we're doing wrong.,non_debt,-
impala,7787,comment_0,"I added a retry loop (5 retries; sleep 30 seconds between) and separated out the ""docker pull"". We're dependent on the Docker public repos here, but perhaps their windows of failure are small enough that this will help.",non_debt,-
impala,7787,comment_1,"Hasn't happened for a while, maybe Phil's change did the trick.",non_debt,-
impala,7789,summary,Impala 3.1 Doc: Admission Control Status in Impala Shell,non_debt,-
impala,7808,summary,Refactor SelectStmt analyzer for easier debugging,code_debt,low_quality_code
impala,7808,description,"The analysis steps in {{SelectStmt}} and {{AnalysisContext}} are large and cumbersome. There is ample evidence in the literature that simpler, smaller functions are easier to understand and debug than larger, more complex functions. This ticket requests breaking up the large functions in these two cases into smaller, easier-understood units in preparation for tracking down issues related to missing rewrites of the {{WHERE}} and {{GROUP BY}} clauses. One might argue that large functions perform better by eliminating unnecessary function calls. However, the planner is not performance sensitive, and the dozen extra calls that this change introduce will not change performance given the thousands of calls already made. Experience has shown that the JIT compiler in the JVM actually does a better job optimizing smaller functions, and gives up when functions get to large. So, by creating smaller functions, we may actually allow the JIT compiler to generate better code. And, this refactoring is in support of a possible outcome that the planner can handle rewrites without making multiple passes through the analyzer: that savings will far outweigh the few extra calls this change introduces.",code_debt,complex_code
impala,7811,summary,Add flag to count JVM memory against process limit,non_debt,-
impala,7811,description,None,non_debt,-
impala,7841,summary,"Refactor QueryStmt, other analysis code for easier debugging",code_debt,low_quality_code
impala,7841,description,"IMPALA-7808 started the process of refactoring the Analyzer code for easier debugging. It did so by grouping {{SelectStmt}} code into a nested class, which then allowed breaking up a large function into smaller chunks. This ticket continues that process with two changes: * Follow-on refactoring of {{SelectStmt}} to make some of the newly-created functions simpler. (The first change tried to keep code unchanged as much as possible.) * Apply the same technique to the {{QueryStmt}} base class and to its other subclasses.",code_debt,low_quality_code
impala,7841,comment_0,Was needed for a change that won't get made.,non_debt,-
impala,7861,summary,ABFS docs: SSL is now enabled by default regardless of URI scheme,non_debt,-
impala,7861,description,"When it was originally submitted to Hadoop, the ABFS driver disabled TLS when using the URI scheme ""abfs"", and enabled TLS when using the URI scheme ""abfss"". This was reflected in the documentation originally submitted for ABFS: We should update that to reflect that TLS is enabled with either URI scheme unless you disable TLS in configuration by setting in the configuration.",documentation_debt,outdated_documentation
impala,7869,summary,Split up for readability and compile time,architecture_debt,violation_of_modularity
impala,7869,description,suggested reorganising the file to be easier to read on Compile times are also an issue - this file is the longest pole in the Impala compilation at the moment.,code_debt,low_quality_code
impala,7893,summary,Impala shell does not handle Ctrl+C correctly for a non-running query,non_debt,-
impala,7893,description,"Impala uses Ctrl+C to cancel a running query, which works. However, running Ctrl+C for a non-running query does not work. Example: Compare this behavior to Linux shell.",non_debt,-
impala,7902,summary,"Revise NumericLiteral to avoid analysis, fix multiple issues",non_debt,-
impala,7902,description,"The {{NumericLiteral}} class is a leaf node in the Impala FE AST. In order to address a number of pending issues, we must adjust this class to improve its semantics and fix a number of bugs. See the linked JIRA tickets for the issues that this roll-up ticket addresses. The issues are combined because they are tightly related; makes more sense to offer them as a single patch than as a series of disjoint patches.",design_debt,non-optimal_design
impala,7987,summary,Get to start up a usable minicluster,non_debt,-
impala,7987,description,The goal here is to start up an impala minicluster running inside docker containers (process per container) and be able to run queries against the minicluster running on the host (i.e. not in docker).,non_debt,-
impala,7987,comment_0,"A major obstacle here that I'm running into is that ""localhost"" appears in many places in configuration files. Finding those and replacing them with the gateway IP of the docker network isn't sufficient to solve the problem without reloading data, because ""localhost"" makes its way into table definitions and other places (probably sentry?).",design_debt,non-optimal_design
impala,7988,summary,Support loading data into a dockerised minicluster,non_debt,-
impala,7988,description,"This JIRA tracks getting data load to work against dockerised impala daemons. * Fix to wait for cluster to become ready using ImpalaCluster * Fix test configuration to work against all table formats (HDFS, HBase, Kudu)",non_debt,-
impala,8005,summary,Randomize partitioning exchanges destinations,non_debt,-
impala,8005,description,"Currently, we use the same hash seed for partitioning exchanges at the sender. For a table with skew in distribution in the shuffling keys, multiple queries using the same shuffling keys for exchanges will end up hashing to the same destination fragments running on particular host and potentially overloading that host. We should consider using the query id or other query specific information to seed the hashing function to randomize the destinations for different queries. Thanks to  for pointing this problem out.",design_debt,non-optimal_design
impala,8005,comment_0,"It looks like the code for this revolves around EXCHANGE_HASH_SEED in Other code is in the constructor (see init of channels_) as well as HashAndAddRows(), HashRow(), and AddRowToChannel().",non_debt,-
impala,8047,summary,Add support for the .proto file extension to .clang-format,non_debt,-
impala,8047,description,"The .proto file extension is used for the Google Protocol Buffers language. Impala uses this language to specify the format of messages used by KRPC. Add support for this language to .clang-format so that we can have consistent formatting. The proposed support is: {{Language: Proto BasedOnStyle: Google ColumnLimit: 90}} This produces only a few diffs when run against the existing Impala code. Im not proposing to make any changes to .proto files, this is just to show what clang-format will do. Apart from wrapping comments and code at 90 chars, the diffs are mostly of the form {{-syntax=""proto2""; +syntax = ""proto2"";}} {{- message Certificate {}; + message Certificate { + };}} {{- optional bool = 4 [ default = false ]; + optional bool = 4 [default = false];}} {{- UNKNOWN = 999; - NEGOTIATE = 1; - SASL_SUCCESS = 0; - SASL_INITIATE = 2; + UNKNOWN = 999; + NEGOTIATE = 1; + SASL_SUCCESS = 0; + SASL_INITIATE = 2;}} This last change can be configured using true but that creates a different set of diffs.",non_debt,-
impala,8052,summary,Fail to run,non_debt,-
impala,8052,description,Encountered the following test errors from The errors only occur in my local machine. It can't be reproduced in Impala Jenkins. The timezone of my machine is PST.,non_debt,-
impala,8052,comment_0,looks like others have seen this too.,non_debt,-
impala,8073,summary,failed in private build because of socket error,non_debt,-
impala,8073,description,The error in the log (attached) appears to be a connection to the HMS error. Some initial googling suggested that it might be the server closing the connection because of hitting a connection limit.  could you take a look and see if you have any ideas. I wonder if we're leaking HMS connections in this test somehow?,code_debt,low_quality_code
impala,8141,summary,ASAN build failure in,non_debt,-
impala,8141,description,From the build log: From {{impalad.ERROR}}:,non_debt,-
impala,8141,comment_0,The stack trace looks like another duplicate of what we're now tracking in IMPALA-8140. Let's stop opening new Jiras for Asan failures with the same stack trace until that one is fixed.,non_debt,-
impala,8146,summary,Remove,non_debt,-
impala,8146,description,These scripts are thin wrappers around make_impala.sh and don't add much value. We should remove them. I think there are a couple of references in the wiki that we can update to show alternative invocations.,documentation_debt,outdated_documentation
impala,8146,comment_2,"So I got the feedback that we need a better alternative to build both debug and release binaries - some scripts are using the pattern of buildall.sh, then make_release.sh to end up building all artifacts.",non_debt,-
impala,8146,comment_3,"I think we should stage this differently - first make it possible to do everything without make_impala.sh, have an interim period where the script is still present so any downstream users have a chance to migrate their tools, then remove the old scripts.",code_debt,low_quality_code
impala,8150,description,"started to fail recently with the following backtrace. , would you mind taking a look as you seem to have touched this test recently.",non_debt,-
impala,8160,summary,Impala Doc: Doc CAST(... AS ... FORMAT <template>),non_debt,-
impala,8160,comment_1,Hey Unfortunately the code change didn't make it to 3.3. Could you please make sure that the doc change is excluded from the release? Gabor,non_debt,-
impala,8160,comment_2,"Nevermind, I see that the release is already rolled out :)",non_debt,-
impala,8168,summary,On S3 Sentry HDFS sync should be disabled,non_debt,-
impala,8168,description,"Currently we enable Sentry HDFS sync when running on S3 which sometimes can make the authorization tests on S3 flaky. Since HDFS sync is not supported on S3, we should update our build script to only enable Sentry HDFS sync on HDFS and not on S3.",test_debt,flaky_test
impala,8176,summary,Convert tests with trivial main() functions to use a unified executable,design_debt,non-optimal_design
impala,8176,description,"With the initial commit for the unified backend test executable commited (IMPALA-8071), it should be straight-forward to convert another set of tests to use the unified executable. Any test with a simple main() function (such as IMPALA_TEST_MAIN()) should require only modest changes to convert. Command to find such tests: git grep IMPALA_TEST_MAIN It looks like there are dozens of such tests:",design_debt,non-optimal_design
impala,8181,summary,Show abbreviated row counts in DESCRIBE output,non_debt,-
impala,8181,description,"A recent change added plan node cardinality to the DESCRIBE output using a ""metric"" format: 123.46M instead of 123456789. This makes the output easier to read. Also, since all numbers are estimate, having seven or eight digits of precision is not very helpful. This ticket requests that the same formatting be used for the other places where the DESCRIBE output shows row counts: table stats, extrapolated row count, partition row counts, and so on.",design_debt,non-optimal_design
impala,8181,comment_1,"Hey , I see this is submitted. Can this be resolved and the fix version be set to 3.2? 360f88e IMPALA-8181: Abbreviate row counts in EXPLAIN",non_debt,-
impala,8187,summary,UDF/UDA samples should explicitly export entry points,non_debt,-
impala,8187,description,"It's possible for UDF authors to get into a pickle by exporting symbols from UDF/UDA shared objects unintentionally. E.g. if using boost headers, which result in weak symbols in the .so, which then get merged with the (incompatible) symbols in the impalad binary. We should really be keeping symbols hidden by default then exporting the entry points explicitly in the UDF/UDA code",design_debt,non-optimal_design
impala,8189,summary,fails on S3 because 'hadoop fs -cp' fails,non_debt,-
impala,8189,description,"In a parquet file is copied. The first copy succeeds, but the second fails. In the DEBUG output (below) you can see the copy writing data to an intermediate file and then after the stream is closed, the copy cannot find the file. The test failure is",non_debt,-
impala,8189,comment_0,"I'll take a look at this, it looks like this is due to the s3 eventual consistency issues.",non_debt,-
impala,8195,summary,Impala Doc: Impala does support Cartesian join,non_debt,-
impala,8198,summary,Add DATE type support to Avro scanner,non_debt,-
impala,8198,description,Implement Avro DATE type support.,non_debt,-
impala,8247,summary,Backend tests from and are not running,non_debt,-
impala,8247,description,"Several tests in be/src/util were converted to the unified backend test executable in IMPALA-8071. Unfortunately, some tests stopped running as part of this change. Specifically, the tests in and were not included in the test binary and stopped running. The tests changed to use but the files were not included in the UtilTests library. These two files should be included in the test executable and we need to update the validation of the unified test executable to catch this case.",non_debt,-
impala,8247,comment_0,I have a code change to fix this. I am testing it now.,non_debt,-
impala,8247,comment_1,Code review:,non_debt,-
impala,8248,summary,Improve Ranger test coverage,test_debt,low_coverage
impala,8248,description,We have authorization tests that are specific to Sentry and authorization tests that can be applicable to any authorization provider. We need to re-organize the authorization tests to easily differentiate between Sentry-specific tests vs generic authorization tests. This also will improve test coverage for Ranger.,test_debt,low_coverage
impala,8252,summary,Impala writes malformed thrift profiles,non_debt,-
impala,8252,description,"The change for IMPALA-1048 writes unconditionally, even when both its fields are unset. This trips up the Java reader code, which expects to find exactly one type of a union to be set. The resulting error looks like this: (This particular implementation rewrites the Java namespace for compatibility reasons, but the code used the latest master commit's thrift files.)",non_debt,-
impala,8273,summary,Change metastore configuration template so that table parameters do not exclude impala specific properties,non_debt,-
impala,8273,description,CDH Hive has a configuration which gives the ability to exclude certain parameter keys from notification events. This is mainly used as a safety valve in case there are huge values stored in these parameter maps. The template file should make sure that the parameter exclusion is disabled (or atleast configured such that it does not exclude which is needed by this feature.,non_debt,-
impala,8273,comment_0,Adding the gerrit link,non_debt,-
impala,8290,summary,Display constraint information in 'SHOW CREATE' statement,non_debt,-
impala,8290,description,Show create statement should display primary key and foreign key information.,non_debt,-
impala,8290,comment_0,Resolved as part of IMPALA-9158,non_debt,-
impala,8294,summary,Inconsistent updating of BytesRead* counters,non_debt,-
impala,8294,description,Some of the BytesRead* counters in profiles (e.g. BytesReadLocal) are only updated once a query finishes successfully. This leads to confusion because queries that are still running or failed look like they did not read data locally.,design_debt,non-optimal_design
impala,8294,comment_0,"If you look in HdfsScanNodeBase, bytes_read_ follows the good pattern where it gets updated by DiskIoMgr in real time. bytes_read_local_ follows the bad pattern.",non_debt,-
impala,8361,summary,Bound predicates optimization doesn't work for InlineView,non_debt,-
impala,8361,description,Tables for test: Bound predicates optimization (implement in works for this sql: But does't work for this one:,non_debt,-
impala,8361,comment_0,"I think the problem is that we are not creating bound predicates for inline views. In the second case, predicate upper_val='123' is not bounded to any scan node unless it's migrated (pushed down) into the inline view and become upper(test_b.val) = '123'. But we can't migrate it since it evaluate the nullable side of the outer join. Instead, we should duplicate it and put it both inside and outside the inline view. So adding some logics to creating bound predicates for inline view could help on this.",non_debt,-
impala,8416,summary,Impala Doc: Document the system disk I/O throughput added to query profiles,non_debt,-
impala,8429,summary,Update docs to reflect default join distribution mode change,non_debt,-
impala,8429,description,The Query Option' page needs an update to reflect the changes in IMPALA-5120.,documentation_debt,outdated_documentation
impala,8429,comment_0,"] In which was resolved later that IMPALA-5120, the default value is BROADCAST. And in the nightly-build, the default value for Query Option shows as ""BROADCAST."" Could you explain what needs to be updated in the doc? Thanks!",non_debt,-
impala,8429,comment_1,Could you take a look at my question above? Thank you.,non_debt,-
impala,8429,comment_2,"Sorry for the delay. This took me a while to figure out again for some reason. I agree the docs are correct, I don't think I was aware of IMPALA-5381 at the time of submitting this request.",non_debt,-
impala,8432,summary,"ASF-master data load job fails with ""Could not resolve dependencies for project",non_debt,-
impala,8432,description,ASF-master core data load job fails with following:,non_debt,-
impala,8432,comment_0,This is not an upstream issue.,non_debt,-
impala,8435,summary,Prohibit unsupported operations on transactional tables,non_debt,-
impala,8435,description,"For a transactional table prohibit unsupported statements like any access to full-ACID table, compute stats, alter, or write to insert only transactional table.",non_debt,-
impala,8438,summary,List valid writeIds for a ACID table,non_debt,-
impala,8438,description,"Before listing the partitions of a table, fetch and store the list of valid (committed) writeIds for the table. This will be used later during planning/refresh.",non_debt,-
impala,8443,summary,Record time spent in authorization in the runtime profile,non_debt,-
impala,8443,description,Authorization is done as a part of analysis. It helps to have the time spent in authorization recorded as a part of the query profile timeline. Something like as follows. Helps to debug issues when bottleneck is in authorization.,non_debt,-
impala,8469,summary,Admit memory not set in backend descriptor for coordinator-only nodes,non_debt,-
impala,8469,description,"When configuring admission control with dedicated coordinator daemons, queries in pools with memory limits fail with the admission rejections like the following: Tracing this in the code leads us to line 576 of and therefore to suspect that the local for the coordinator node in {{scheduler.cc}} never has {{admit_mem_limit}} set, and thus ends up with the default value of 0. The issue goes away if NO_SPECIALIZATION is used instead of COORDINATOR_ONLY.",non_debt,-
impala,8469,comment_0,"I'm thinking the local coord backend descriptor should call somewhere in the {{Status TNetworkAddress& backend_address, const TNetworkAddress& krpc_address, const IpAddr& ip)}} method of {{scheduler.cc}}.",non_debt,-
impala,8469,comment_1,fyi while you're looking at refactoring this.,non_debt,-
impala,8473,summary,Refactor lineage publication mechanism to allow for different consumers,non_debt,-
impala,8473,description,"Impetus for this change is to allow lineage to be consumed by Atlas via Kafka. h3. Design Proposal Implement a plugin approach (similar to for consuming query event hooks, where downstream users can provide their own hook implementations as runtime dependencies. Keep but deprecate existing lineage event file writing. has provided a fe patch (attached) with suggested mechanism for allowing multiple hooks to be registered with the fe. Hooks would be invoked from the be at appropriate places, e.g. The hooks should all be executed asynchronously, so the current thinking is that this execution should happen in the fe, since the be does not know about what hooks are registered. IOW, the method (see patch) should probably make use of a thread-pool executor service (or something similar) in order to execute all hooks in parallel and in a non-blocking manner, returning to the be asap. h3. Code Review",non_debt,-
impala,8473,comment_0,"Can you define ""ImpalaPostExecHook "" as an interface instead an abstract class? I don't see benefits of defining it as abstract class. On the other hand, defining it as interface allows the implementation to extends from another base class. Thanks! Instead of Can you define",design_debt,non-optimal_design
impala,8473,comment_1,"please keep your current approach and uses abstract class for ""ImpalaPostExecHook "".",non_debt,-
impala,8473,comment_2,", any reason you've changed your mind? Impala team and I actually prefer the interface approach.",non_debt,-
impala,8485,summary,References to deprecated feature authorization policy file need to be removed,code_debt,dead_code
impala,8485,description,Running the command *git grep authz-policy* produces the following output: ** authz-policy.ini = % WAREHOUSE These references to the *authz-policy.ini* should be cleaned up as the authorization policy file feature is deprecated as of *IMPALA-7918.*,code_debt,dead_code
impala,8507,summary,Support DropTable DDL with Kudu/HMS integration in Catalogd mode,non_debt,-
impala,8507,description,"Similar to IMPALA-8504, but for DropTable DDL.",non_debt,-
impala,8524,summary,"Avoid calling ""hive"" via command line in EE tests",non_debt,-
impala,8524,description,"""hive -e SQL..."" without further parameters no longer works when USE_CDP_HIVE=true (it doesn't establish a connection). Some tests used this to load data. These calls can be replaced with which seems like a good idea regardless of the Hive 3 efforts.",non_debt,-
impala,8534,summary,Enable data cache by default for end-to-end containerised tests,non_debt,-
impala,8534,description,"Following on from IMPALA-8121, I don't think we can enable the data cache by default, since it depends on what volumes are available to the container at runtime. But we should definitely enable it for tests.  said already mounts some host directories into the container, so we could either do the same for the data cache, or just depend on the container root filesystem (which is likely to be slow, unfortunately).",non_debt,-
impala,8534,comment_0,"I'm not going to get to this right away, although I'd like to circle back. If you want to turn it on to get some additional testing, feel free to pick it up - I can provide any pointers if you need them.",test_debt,low_coverage
impala,8538,summary,Support hiveserver2 over HTTP,non_debt,-
impala,8538,description,"Impala should provide the option to connect to our hiveserver2 interface over http, to give clients more flexibility in how they would like to connect. This should include support for https and some form of authorization, probably BASIC auth to ldap to start and Kerberos support can be added in a later patch.",non_debt,-
impala,8538,comment_0,IMPALA-8420,non_debt,-
impala,8578,summary,Reduce code in metric headers,non_debt,-
impala,8578,description,"metrics.h and other metric headers are included a lot of places and there is a lot of code in the header that has very few callers. It appears to be pulled into several hundred compilation units, increasing the compile time of each of those and forcing recompilation when the headers are changed. Some ideas: * Move function implementations to .cc files. E.g. ToJson() and ToPrometheus() don't need to be inlined. * Move MetricGroup to its own file * Try to see if we can use forward declarations in more places to avoid including it.",code_debt,low_quality_code
impala,8599,summary,Create a separate Maven module for query event hook API,non_debt,-
impala,8599,description,Impala needs to publish this API into Maven Central so that Atlas can consume it.,non_debt,-
impala,8599,comment_0,will Impala release a new version with this change? Atlas will get the jar from Apache repository with certain versioned snapshot. What version we should use? And what is the timeline this will be done? Atlas update cannot be released until Impala release is done.,non_debt,-
impala,8605,summary,Clean up connection/session management,code_debt,low_quality_code
impala,8605,description,"Following on from IMPALA-8538 and IMPALA-1653, it would be nice to clean up some of the session management logic and add some more tests for the HS2 APIs - we have some testing gaps and a few of the invariants are unclear about what operations are allowed.",code_debt,low_quality_code
impala,8626,summary,JDBC tests don't seem to be using HTTP,non_debt,-
impala,8626,description,"I noticed that the parameterized JDBC tests are passing on the dockerised cluster, which shouldn't be possible because IMPALA-8623 isn't done. The connection strings look identical in both cases: I was looking at related code and saw some misuse of == vs equals() for string comparison here But I don't think that explains what I'm seeing above.",code_debt,low_quality_code
impala,8626,comment_0,"So I don't think that this JIRA is actually correct - if I run the dockerized tests with "", the http tests fail, suggesting that they really are hitting the http server when they're succeeding. Also, if you look at the logging in the build you linked to, everything including timestamps is exactly the same for every test case, so I think there's something wrong with the logging and its doesn't necessarily reflect that the http tests are actually using the binary connection string. For example, see which shows an error message that the http port can't be connected to even though the standard output only shows attempts to connect to the binary port. Of course, that doesn't explain why the http tests are passing in docker without IMPALA-8623. I'm stil investigating that",non_debt,-
impala,8626,comment_1,"Hm, ok, the logging being wrong makes a bit more sense. I would guess that the port mapping added in is sufficient to ensure that the port gets exposed, even if it's not exposed by default by the container config.",non_debt,-
impala,8632,summary,Add support for self-event detection for insert events,non_debt,-
impala,8632,description,"In case of {{INSERT_EVENTS}} if Impala inserts into a table it causes a refresh to the underlying table/partition. This could be unnecessary when there is only one Impala cluster in the system. The existing self-event detection framework cannot identify such events because they are not sending HMS objects like tables and partitions to the HMS. Instead in case of {{INSERT_EVENT}} HMS API only asks for a table name or partition value to fire a insert event on it. We can detect a self-event in such cases if the HMS API to fire a listener event is improved to return the event id. This would be used by EventProcessor to ignore the event when it is fetched later in the next polling cycle. In order to support this, we will need to make a change to Hive as well so that the enhanced API can be used.",design_debt,non-optimal_design
impala,8648,summary,Impala ACID read stress tests,non_debt,-
impala,8648,description,None,non_debt,-
impala,8656,summary,Support for eagerly fetching and spooling all query result rows,non_debt,-
impala,8656,description,"Impala's current interaction with clients is pulled-based: it relies on clients to fetch results to trigger the generation of more result row batches until all the result rows have been produced. If a client issues a query without fetching all the results, the query fragments will continue to consume the resources until the query hits is cancelled and unregistered for whatever reasons. This is undesirable as resources are held up by misbehaving clients and other queries may wait for extended period of time in admission control due to this. The high level idea for this JIRA is for Impala to have a mode in which result sets of queries are eagerly fetched and spooled somewhere (preferably some persistent storage). In this way, the cluster's resources are freed up once all result rows have been fetched and stored in the spooling location. Incoming client fetches can be returned from this spooled locations. cc'ing , , ,",design_debt,non-optimal_design
impala,8656,comment_1,"Closing this and marking as Fixed. All the dev work has been completed, and the feature has been sitting in master for a few weeks and seems to be stable.",non_debt,-
impala,8697,summary,test_query_options fails on EC(Erasure coding turned on) builds,non_debt,-
impala,8697,description,IMPALA-7290 made changes this test but the order expected in the test is different from the one which is actually encountered. Expected: Actual: Notice the order of TIMEZONE and query options is reversed.,non_debt,-
impala,8747,summary,can hit TypeError on HS2 connection setup failure,non_debt,-
impala,8747,description,"If we reach this exception handler in then it will fail with: The problem is that the statement uses ""\{0}"" replacements that work with .format() while LOG.info is using ""%"" replacements. We can fix this by using .format ourselves or changing the message to use ""%"" codes.",non_debt,-
impala,8750,summary,failing when compilation events appear dynamically,non_debt,-
impala,8750,description,"is failing during builds where the query compilation event differs from the specified. This can occur in 2 cases: - When is specified it adds an extra ""Lineage info computed:"" line. - When metadata is not preloaded on the coordinator instead of ""Metadata of all * tables cached:"" it can be multiple lines: We should cover these cases as well.",non_debt,-
impala,8771,summary,Missing stats warning for complex type columns,non_debt,-
impala,8771,description,"We currently don't support column stats for complex typed columns (ingored in `compute stats` statements). However running queries against those columns throws the missing col stats warning which is confusing. We could probably skip the warnings if we detect the missing stats are for complex typed columns, until we support them.",requirement_debt,requirement_partially_implemented
impala,8811,summary,Impala Doc: query option to change default ACID type of new tables,non_debt,-
impala,8855,summary,Impala docs do not mention all places VALUES clause can be used,documentation_debt,low_quality_documentation
impala,8855,description,"The documentation only mentions the values clause in the context of an INSERT statement. It can actually be used anywhere that a SELECT statement could be used, e.g. this is a valid query:",documentation_debt,low_quality_documentation
impala,8857,summary,may fail because client reads older timestamp,non_debt,-
impala,8857,description,"I believe this is a flaky tests, since there's no attempt to pass the timestamp from the kudu client that did the insert to the impala client that's doing the reading.",test_debt,flaky_test
impala,8857,comment_0,", , we may release 3.3.0 soon. should we change the target version to 3.4.0?",non_debt,-
impala,8857,comment_1,"If we haven't seen this recently, let's close it.",non_debt,-
impala,8857,comment_2,"Encountered the issue recently in the following build: Not sure if it's relevant, but a similar test, failed due to network error here:",non_debt,-
impala,8862,summary,Don't ship jetty and ant in Impala containers,non_debt,-
impala,8862,description,These get bundled into the containers but are not be invoked at runtime. We should be able to avoid including them as dependencies entirely.,build_debt,over-declared_dependencies
impala,8884,summary,"Track Read(), Open() and Write() operation time per disk queue",non_debt,-
impala,8884,description,It would be useful for debugging I/O performance problems if we had histogram stats for the time taken for various operations so that we could see if there were slow operations on a particular disk (e.g. because of disk failure) or from a particular remote filesystem.,code_debt,low_quality_code
impala,8884,comment_0,Some WIP -,non_debt,-
impala,8892,summary,Add tools to Docker images,non_debt,-
impala,8892,description,Our docker images lack a bunch of tools that help to implement health checks and debugging issues.,design_debt,non-optimal_design
impala,8901,summary,# links on /catalog page broken,non_debt,-
impala,8901,description,"Due to IMPALA-7935, the database links at the top of the /catalog page which use '#' links to jump to the part of the page corresponding to the particular database no longer work if local catalog is enabled because the <a> tag with the id corresponding to the database name is omitted from the page.",non_debt,-
impala,8912,summary,Avoid calling computeStats twice on HBaseScanNode,design_debt,non-optimal_design
impala,8912,description,"For simple queries on HBase tables that has HBaseScanNode as the root of the SingleNodePlan, will be called twice. Stacktrace for the first call: Stacktrace for the second call: Codes of the second call: Logs for a simple query on an old version of Impala: Such kind of queries are usually point queries and are always expected to return fast. is heavy since it requires RPCs to HBase. We should avoid calling it twice.",design_debt,non-optimal_design
impala,8912,comment_0,"The second call is required since the limit_ may be updated, which may cut the cardinality. I think what we should avoid is sampling twice on hbase.",design_debt,non-optimal_design
impala,8935,summary,Add links to other daemons from webui,non_debt,-
impala,8935,description,"It would be convenient for all of the debug webuis to have links to the other debug webuis within a single cluster. For impalads, it would be easy to add links to each other impalad on the /backends page (from IMPALA-210 it looks like this even used to be the case, but everything has changed a ton since then, eg. we weren't even using templates at the time, so it got lost somewhere along the way). Its also fairly straight forward to add a link to the statestored and catalogd, eg. maybe on the index page or else on the nav bar.",non_debt,-
impala,8935,comment_1,FYI not working for me on my mini-cluster (which is running on an EC2) machine. Whenever I click on the link on my laptop it takes me to,non_debt,-
impala,8935,comment_2,"So the use-case you're talking about, accessing the minicluster webui in a dev environment from another machine, I think is difficult to make work in all cases. Rather than using ip addresses, we could specify hostnames for these links, but that's not guaranteed to always work - I think its common for people to develop on machines that don't have DNS-resolvable hostnames (eg see IMPALA-8917). And its always going to be the case that the machine's hostname resolves to 127.0.0.1 locally due to I don't think this is too big of a deal - it should work in any sort of real, non-development environment, and of course its possible in a dev environment to access the other webuis by manually specifying the right host:port instead of clicking the link, as its always been. One option if you really want this to work in a dev environment is to specify the flag to some public IP on minicluster startup. This flag is currently broken, but I have a patch out to get it working:",design_debt,non-optimal_design
impala,8945,summary,Impala Doc: Incorrect Claim of Equivalence in Impala Docs,documentation_debt,low_quality_documentation
impala,8945,description,"Reported by  The Impala docs entry for the IS DISTINCT FROM operator states: The <= But this expression is not equivalent to A <= (A IS NULL AND B IS NULL) OR ((A IS NOT NULL AND B IS NOT NULL) AND (A = B)) This expression should replace the existing incorrect expression. Another expression that is equivalent to A <= if(A IS NULL OR B IS NULL, A IS NULL AND B IS NULL, A = B) This one is a bit easier to follow. If you use this one in the docs, just replace the following line with: The <=> operator can use a hash join, while the if expression cannot.",documentation_debt,low_quality_documentation
impala,8948,summary,"[DOCS] Review ""How Impala Works with Hadoop File Formats""",non_debt,-
impala,8948,description,"Ref: In the ""Impala Can INSERT?"" column of the file type support matrix for Text, we claim that Impala can insert into a compressed-text table: ""Yes: {{CREATE TABLE}}, {{INSERT}}, {{LOAD DATA}}, and query."" This doesn't appear to be the case as Impala does not support the writing of compressed text in any version at the time of this writing.",non_debt,-
impala,8959,summary,test_union failed with wrong results on S3,non_debt,-
impala,8959,description,Error details Stack trace,non_debt,-
impala,8959,comment_0,"I'm pretty sure that this the same cause - I checked and the query was run via HS2, so we're exposed to the same Impyla bug.",non_debt,-
impala,9013,summary,Column Masking DML support,non_debt,-
impala,9013,description,Review Hive implementation to see if anything special needs to be done for DML. The Hive column masking design doc does not reflect the current code.,documentation_debt,outdated_documentation
impala,9013,comment_0,Covered by IMPALA-9009,non_debt,-
impala,9030,summary,Handle translated external Kudu tables,non_debt,-
impala,9030,description,"In HIVE-22158 HMS disallows creating of any managed table which is not transactional. This breaks the behavior for managed Kudu tables created from Impala. When user creates a managed Kudu table, HMS internally translates such table into a external table with an additional property set to true. The table type is changed to EXTERNAL. Subsequently, when such a table is dropped or renamed, Catalog thinks such tables as external and does not update Kudu (dropping the table in Kudu or renaming the table in Kudu). This is unexpected from the point of view of user since user may think that they created a managed table and Impala should handle the drop and rename accordingly. The same applies to certain other alter operations like alter table set properties on a managed Kudu table which previously was disallowed now goes through after HIVE-22158",non_debt,-
impala,9030,comment_0,We may need to deal with translated external HDFS tables and HBase tables too. May need to review all supported storage systems.,non_debt,-
impala,9070,summary,Include table location information in lineage graphs for external tables.,non_debt,-
impala,9070,description,Atlas needs table location information to establish lineage between table location and newly created external tables in Impala. This information should be sent to backend after successful table creation.,non_debt,-
impala,9119,summary,"failed with ""Memory limit Exceeded.""",non_debt,-
impala,9119,description,Observed during a GVD run here: Part of the error message: Aggregation node id 4:,non_debt,-
impala,9119,comment_0,There wasn't enough information captured to figure out which query it was.,non_debt,-
impala,9119,comment_1,"Since the jenkins runs are short-lived, is it a good idea to attach full logs while creating such JIRAs. I missed capturing the actual query that caused these errors in the JIRA :(",non_debt,-
impala,9119,comment_2,"I would've put more effort into reproducing if it was a high frequency flaky or a product bug, but decided just to move forward here.",non_debt,-
impala,9146,summary,Limit the size of the broadcast input on build side of hash join,non_debt,-
impala,9146,description,"Since broadcast based hash joins are often chosen, we sometimes see very large tables being broadcast, with sizes that are larger than the destination executor's total memory. This could potentially happen if the cluster membership is not accurately known and the planner's cost computation of the broadcastCost vs partitionCost happens to favor the broadcast distribution. This causes spilling and severely affects performance. Although the DistributedPlanner does a mem_limit check before picking broadcast, the mem_limit is not an accurate reflection since it is assigned during admission control (See Given this scenario, as a safety check it is better to have to an explicit configurable limit for the size of the broadcast input and set it to a reasonable default. The 'reasonable' default can be chosen based on analysis of existing benchmark queries and representative workloads where Impala is currently used.",design_debt,non-optimal_design
impala,9150,summary,Restarting minicluster breaks HBase on CDH GBN 1582079,non_debt,-
impala,9150,description,"On the most recent CDH GBN (1582079), restarting HBase using our normal scripts / results in an unusable HBase. Our script use the script: This kills the region servers before the master. On CDH GBN 1582079, the master gets unhappy: Then, when the master starts up again, it remains unhappy: This continues for an indefinite amount of time. Current workaround: Use HBase's bin/stop-hbase.sh script rather than our script. I do not see the problem when using that script, as it does a more graceful shutdown. We should look into changing to use bin/stop-hbase.sh.",non_debt,-
impala,9151,summary,Number of executors during planning needs to account for suspended executor groupa,non_debt,-
impala,9151,description,"When configuring Impala with executor groups, the planner might see a that has no executors in it. This can happen if the first executor group has not started up yet, or if all executor groups have been shutdown. If this happens, the planner will make sub-optimal decisions, e.g. decide on a broadcast join vs a PHJ. In the former case, we should have a configurable fallback cluster size to use during planning. In the latter case, we should hang on to the last executor group size that we had observed.",non_debt,-
impala,9154,summary,KRPC DataStreamService threads blocked in PublishFilter,non_debt,-
impala,9154,description,"I hit this on when doing a single node perf run: tan I noticed that the query was hung and the execution threads were hung sending row batches. Then looking at the RPCz page, all of the threads were busy: Multiple threads were stuck in UpdateFilter() - see It looks like this is a deadlock bug because a KRPC thread is blocked waiting for an RPC that needs to be served by one of the limited threads from that same thread pool",non_debt,-
impala,9154,comment_0,"Given the fix is non-trivial, it may make sense to back out the offending change for now.",non_debt,-
impala,9177,summary,query 18 on Kudu sometimes hits memory limit on dockerised tests,non_debt,-
impala,9177,description,"A few upstream GVO jobs have seen Kudu Q18 hit a memory limit: There have been a few occurrences, so far on dockerised-tests:",non_debt,-
impala,9177,comment_0,"This is probably a straggler issue from IMPALA-8451, which enabled admission control for the dockerised tests. That 256MB limit is coming from Probably the easiest fix is to bump that slightly, e.g. to 300mb. There wasn't anything specific about 256mb, it was just a power of two that felt like it wouldn't starve queries.",non_debt,-
impala,9177,comment_1,"Great, I'll put together a change to bump this to 300mb and see what happens.",non_debt,-
impala,9177,comment_2,I'm running tests on a draft of this.,non_debt,-
impala,9209,summary,is flaky,test_debt,flaky_test
impala,9209,comment_0,Only seen once so far on an exhaustive build.,non_debt,-
impala,9235,summary,Backport Kudu socket stats for /rpcz,non_debt,-
impala,9235,description,This Kudu commit has some nice socket stats that will help diagnose network perf issues: We should backport this and make sure we expose all of the useful per-connection information (kudu exposes more already on its rpcz page because it dumps the protobufs to JSON).,non_debt,-
impala,9237,summary,Make package binutils building work on aarch64,non_debt,-
impala,9237,description,"Run ./buildall.sh on aarch64 instance, errors raised when configure binutils: checking build system type... checking host system type... checking target system type... checking for a BSD-compatible install... /usr/bin/install -c checking whether ln works... yes checking whether ln -s works... yes checking for a sed that does not truncate output... /bin/sed checking for gawk... gawk checking for gcc... checking for C compiler default output file name... error: in configure: error: C compiler cannot create executables See `config.log' for more details. Thread model: posix gcc version 4.9.2 (GCC) configure:4387: $? = 0 configure:4376: -V gcc: error: unrecognized command line option '-V' gcc: fatal error: no input files compilation terminated. configure:4387: $? = 1 configure:4376: -qversion gcc: error: unrecognized command line option '-qversion' gcc: fatal error: no input files compilation terminated. configure:4387: $? = 1 configure:4407: checking for C compiler default output file name configure:4429: -fPIC -O3 -m64 -mno-avx2 conftest.c {color:#de350b}gcc: error: unrecognized command line option '-m64'{color} {color:#de350b}gcc: error: unrecognized command line option '-mno-avx2'{color} configure:4433: $? = 1 configure:4470: result: configure: failed program was: | /* confdefs.h */ | #define PACKAGE_NAME """" | #define PACKAGE_TARNAME """" | #define PACKAGE_VERSION """" | #define PACKAGE_STRING """" | #define PACKAGE_BUGREPORT """" | #define PACKAGE_URL """" | /* end confdefs.h. */",non_debt,-
impala,9246,summary,Make crcutils building work on aarch64,non_debt,-
impala,9246,description,Make crcutils failed on aarch64 platform g++: error: unrecognized command line option '-msse2' g++: error: unrecognized command line option '-mcrc32' g++: error: unrecognized command line option '-msse2' g++: error: unrecognized command line option '-mcrc32' Makefile:856: recipe for target failed make: Error 1 make: Waiting for unfinished jobs.... Makefile:849: recipe for target failed make: Error 1 {color:#de350b}g++: error: unrecognized command line option '-msse2'{color} {color:#de350b} g++: error: unrecognized command line option '-mcrc32'{color} Makefile:842: recipe for target failed make: Error 1,non_debt,-
impala,9246,comment_0,fixed in,non_debt,-
impala,9265,summary,Add support for toolchain Kudu to provide maven artifacts,non_debt,-
impala,9265,description,"If toolchain Kudu provided Java artifacts, then toolchain Kudu could be used standalone without needing a separate Maven repository for Kudu artifacts. This is nice, because it would allow us to use toolchain Kudu for both USE_CDP_HIVE=true and USE_CDP_HIVE=false without any Java artifact version conflicts. Having only a single version of Kudu to build against would simplify Kudu/Impala integration projects. To do this, the native toolchain (which may be a misnomer) would need to push Kudu artifacts to a repository. One option is for it to create a local filesystem repository and then include it in the kudu tarball produced.",design_debt,non-optimal_design
impala,9304,summary,Add script to launch Hive with Ranger in minicluster,non_debt,-
impala,9304,description,"When developing Ranger integrations, I found it useful to play around on Hive with Ranger, which helps to understand the behaviors and underlying implementations. It would be good to add a script to launch Hive using Ranger authorization.",non_debt,-
impala,9363,summary,Add support to skip given table storage formats,non_debt,-
impala,9363,description,"Catalog by default pulls in all the tables from HMS. Impala however, only can work with some table formats. Such tables are shown in the table listing and when they are queries, Impala also loads them. It is only during querying that we throw an error saying its an unsupported table format. It would be good to have a config which can take in list of storage handlers and skip loading such tables.",design_debt,non-optimal_design
impala,9363,comment_0,Hi  Will you be able to take this up? Thanks!,non_debt,-
impala,9363,comment_1,"Given the performance impact is huge for such a change, I think we should defer it until we have a better API from hive metastore. Can you create a Hive JIRA for this? Specifically, I think what we would should see if we can implement something similar to HIVE-19715 for tables.",design_debt,non-optimal_design
impala,9363,comment_2,"We have benchmarked the performance of a patch that calls to load the table metadata. This patch incurs a prohibitively expensive cost. Specifically, for a database consisting of 40,000 tables, it takes around 170 seconds to load the table metadata. The detailed results are provided in",code_debt,slow_algorithm
impala,9363,comment_3,Thanks ! I will try to create a Hive JIRA for this at,non_debt,-
impala,9373,summary,Trial run of IWYU on codebase,non_debt,-
impala,9373,description,I did a trial run and implemented some of the suggestions of IWYU to confirm that it made sense. I'll post a patch with the changes and leave notes here.,non_debt,-
impala,9373,comment_0,"Notes so far: * In many cases it recommends including internal headers instead of the public-facing header * It gets confused by ""using"" statements in headers, e.g. it thinks is needed for references to ""string"" * The recommendations are mostly pretty good, but there were various small misfires, e.g. recommendations that didn't work or match our coding standards",code_debt,low_quality_code
impala,9426,summary,Download Python dependencies even skipping bootstrap toolchain,non_debt,-
impala,9426,description,Download Python dependencies even skipping bootstrap toolchain. Because when you set the python dependencies still need to be downloaded. The toolchain building process will not download the python dependencies autometically,non_debt,-
impala,9427,summary,Add -fsigned-char compile option,non_debt,-
impala,9427,description,"On aarch64 platform, type of char default is unsigned char, here set it to signed-char to be compatible with x86-64",non_debt,-
impala,9431,summary,DOC: Update docs to reflect that Deflate is supported for text files,non_debt,-
impala,9431,description,"IMPALA-8549 added support for scanning deflate text files. The docs however still have following: ""Deflate Not supported for text files."" Update the docs to reflect that scanning deflate text files are supported.",documentation_debt,outdated_documentation
impala,9443,summary,[DOC] Update show table stats output,non_debt,-
impala,9443,description,"The {{SHOW TABLE STATS}} output for HDFS tables is outdated on some doc sites or abbreviated with ellipsis. Additionally, some other tables can be broken as well, for example {{SHOW FILES IN}}. I am aware of the following pages: - -",documentation_debt,outdated_documentation
impala,9443,comment_0,"There are a few sections in the impala_perf_stats doc that mention ALTER TABLE, starting at line 462. During the update to resolve this issue, I hit ALTER TABLE is not supported on transactional (ACID) table: Need to get advice about handling doc problem: cannot test/make doc SHOW TABLE STATS updates that involve ALTER TABLE. Should the docs be hidden?",non_debt,-
impala,9443,comment_1,let me know if I can connect you with someone who knows the transactional table support well.,non_debt,-
impala,9443,comment_2,let me know if I can connect you with someone who knows the transactional table support well.,non_debt,-
impala,9456,summary,Allow disabling kerberos for incoming internal and external connections even if --principal is set,non_debt,-
impala,9456,description,"This would be useful in cases where we need to talk to kerberized services (HMS, etc) and want to keep our TGT up to date using Impala's kinit infrastructure, but don't want to kerberize all connections.",non_debt,-
impala,9467,summary,Impala Doc: Improve Impala shell usability by enabling live_progress in the interactive mode,design_debt,non-optimal_design
impala,9467,description,"We enable shell option live_progress in interactive mode by default. As for in the non-interactive mode, live reporting is not supported. Impala-shell will disable live_progress if the mode is detected. Need to update the doc to reflect the changes.",documentation_debt,outdated_documentation
impala,9530,summary,Allow limiting memory consumed by preaggregation,non_debt,-
impala,9530,description,"In some cases pre-aggregations can balloon up and consume lots of memory, forcing the merge aggregation to spill. This is often OK as long as the preaggregation is reducing the input sufficiently, since it reduces the amount of data shuffled over the network. However in some cases it's preferable to be more conservative with memory and just cap the size of the preaggregation to prevent it ballooning too much. It would be useful to add a query option to directly limit the memory consumption of the preaggregations.",design_debt,non-optimal_design
impala,9543,summary,Reduce duplicate code in thrift CMakeLists.txt,code_debt,duplicated_code
impala,9543,description,"Reduce duplicate code in thrift CMakeLists.txt. And if in future, we change hive to version 4 or higher. This can adapt automatically.",code_debt,duplicated_code
impala,9546,summary,Update after RANGER-2688,non_debt,-
impala,9546,description,"Due to a recent change introduced by we have to add the following configuration in to start the Ranger server properly. In this regard, we need to change how Impala generates for the Ranger server running in Impala's development environment accordingly.",non_debt,-
impala,9546,comment_0,can you update this JIRA to reflect the changes in,non_debt,-
impala,9546,comment_1,Thanks ! I have updated the title and the description of the JIRA accordingly.,non_debt,-
impala,9548,summary,UdfExecutorTest failures after HIVE-22893,non_debt,-
impala,9548,description,"HIVE-22893 added a dependency on to certain UDFs. This causes {{UdfExecutorTest}} to start failing. defines a specific set of classes that a pulled in from the {{hive-exec}} jar, just needs to be added to that list.",non_debt,-
impala,9560,summary,Changing version from 3.4.0-SNAPSHOT to 3.4.0-RELEASE breaks,non_debt,-
impala,9560,description,"When working on the Impala 3.4 release, we changed the version on branch-3.4.0 from 3.4.0-SNAPSHOT to 3.4.0-RELEASE. now fails with the following error: The output is expecting a cardinality of 17.91K, but instead the cardinality is 17.90K. The RELEASE version has one character fewer than the SNAPSHOT version. The version gets embedded in parquet files, so the parquet file is slightly smaller than before. The test is estimating cardinality by looking at the size of the parquet file. Apparently, this is right on the edge. This test should tolerate this difference.",design_debt,non-optimal_design
impala,9610,summary,Crashed statestore causes queries to hang,non_debt,-
impala,9610,description,"The following steps on master cause a query to hang: The query hangs in the ""CREATED"" state. From the coordinator logs: The ""Unable to connect to localhost:24000"" pattern continues indefinitely (presumably until the statestore comes back up). When the coordinator gets a query from the client, it is able to successfully parse / plan it, but hangs when trying to load metadata: The catalogd logs show the following:",non_debt,-
impala,9610,comment_0,"Something weird happens to impala-shell as well: I can't quit the impala-shell, I have to kill the process externally to exit.",non_debt,-
impala,9610,comment_1,"Might be a catalogd issue because if I run the query, kill the statestore, wait for everyone to notice the statestore failure, and then run the query, everything works. My guess is that since the metadata has already been cached on the coordinator, no RPC to the catalogd is necessary, which is why the query succeeds. So this might be a catalogd or interaction issue.",non_debt,-
impala,9610,comment_2,Doesn't happen when I enable the local catalog:,non_debt,-
impala,9626,summary,Use Python 2.7 from toolchain,non_debt,-
impala,9626,description,None,non_debt,-
impala,9626,comment_0,Review:,non_debt,-
impala,9626,comment_2,"I met an issue If user want to use tools by locally, and the path is exists, then the error ""OSError: Cannot call rmtree on a symbolic link"" will be raised when creating python virtualenv first time, right? Of course, maybe we should call but the python package on s3 doesn't work for aarch64 platform. So could you have a look how to fix this, thanks very much.",non_debt,-
thrift,18,summary,warning: deprecated conversion from string constant to 'char*',code_debt,low_quality_code
thrift,18,description,"gcc 4.2 shows a huge amount of warnings ""warning: deprecated conversion from string constant to 'char*'""",code_debt,low_quality_code
thrift,18,comment_0,patch,non_debt,-
thrift,18,comment_1,I just committed this patch (as two separate commits).,non_debt,-
thrift,21,summary,TThreadPoolServer has dangerous thread-safety issue in accepting connections,requirement_debt,non-functional_requirements_not_fully_satisfied
thrift,21,description,"TThreadPoolServer currently accepts incoming connections in threads. This means that at any time, as long as the thread pool is not completely full of running connections, there are multiple threads currently blocking on the #accept call. This is dangerous because the accept syscall is not documented as being thread-safe. The only reason this actually works in ruby is because of the cooperative threading, but if this library is used in any ruby interpreter that supports native threads (e.g. MacRuby, jruby, etc) I would expect it to start manifesting strange bugs.",requirement_debt,non-functional_requirements_not_fully_satisfied
thrift,21,comment_0,"I Googled around for this a bit and found this which states that accept is defined to be thread-safe (which I verified) but that not all OSes comply with this (which I did not verify). If anyone can find an example of an OS that we want to support that does not provide a thread-safe accept, I think we should consider implementing a workaround.",requirement_debt,non-functional_requirements_not_fully_satisfied
thrift,21,comment_1,"The ""standard"" workaround, so to speak, is to do the accepting in the main thread and then hand the socket off to the worker thread. This can be achieved by blocking on select() instead of on the SizedQueue. This is also something that I'm planning on doing with my rewrite, because another goal is to make the threaded server more fair, by using method invocation as the unit of work instead of the lifetime of the client (in other words, after each exchange, the socket will be thrown back into the queue so long-lived client connections won't prevent other clients from connecting).",non_debt,-
thrift,21,comment_2,"I'm going to go ahead and say this is a non-issue. I find it perfectly believable that all modern OS's do in fact implement accept() in a thread-safe manner, and even if that is not the case, ThreadPoolServer is going to be obsoleted by NonblockingServer very shortly.",defect_debt,uncorrected_known_defects
thrift,21,comment_3,"Sounds like this is taken care of. If it's not, we'll reopen.",non_debt,-
thrift,37,summary,add some missing new lines to fprintfs,code_debt,low_quality_code
thrift,37,description,There are some fprintfs in TNonblockingServer that do not have new lines.,non_debt,-
thrift,37,comment_0,"Unless anyone objects, this is the sort of thing that I'm going to be committing without review. (Post-facto reviews are welcome, of course.)",non_debt,-
thrift,37,comment_1,"Since you're not Paul, if you commit it, then one assumes you've read it and think it is reasonable, which constitutes review. In general, if someone besides the author of a patch commits it, that counts as review. Its best for the wisdom of even no-brainer one-line patches to be confirmed by a second set of eyes, and that's all that reviewing means. The best practice is simply not to commit your own patches. That way everything is reviewed without a lot of formal process. However this requires committers who are attentive and don't ignore patches. Every patch proposed as ready-for-commit should either be -1'd or committed within a few days.",non_debt,-
thrift,39,summary,javabean-style generated code is very poorly formatted,code_debt,low_quality_code
thrift,39,description,The code that gets generated when you run thrift -javabean has no indentation in it at all. This makes it a little challenging to read through it.,code_debt,low_quality_code
thrift,39,comment_0,"I've looked this over pretty thoroughly, and I can't seem to figure out what the cause is. Only some of my structs' indentation is being dropped, while others are completely fine. There's no apparent connection between the ones that work and the ones that don't. For the moment, I'm stumped. If anyone else can think of a reason as to why indent() calls in the generator would be getting outright ignored, I'm all ears.",code_debt,low_quality_code
thrift,39,comment_1,"I found it! Here is the patch. The fix required only one line of code. If you are curious, what caused this was that in certain situations a call was made to indent_down(), without a corresponding call to indent_up() somewhere before, giving the private variable indent_ a negative value, and making calls to indent() have no effect.",non_debt,-
thrift,39,comment_2,"Wow, you are my hero.",non_debt,-
thrift,39,comment_3,I think this patch should be committed. Can a committer take a look?,non_debt,-
thrift,48,summary,TServerSocket can work with unix socket,non_debt,-
thrift,48,description,"The patch adds the unix_socket parameter for TServerSocket. The patch fixes an error message, because it's confusing to see the message ""Could not connect to localhost:9090"" if you try to connect to /tmp/unix_test.",code_debt,low_quality_code
thrift,48,comment_0,"Line 74 of the patch, please edit the comment to say ""We need remove the old unix socket if the file exists and nobody is listening on it."" Line 95 of the patch, I personally prefer leaving the parens here. Line 103 of the patch, please use ""is not None"" instead of ""!="" Other than those nitpicks, I'm fine with this. bmaurer, you wrote the original Python Unix-domain code. What do you think of it?",code_debt,low_quality_code
thrift,48,comment_1,"Here is the second version of patch. I've moved close() routine to TSocketBase, but I don't like an extra parentness. If you wish, you can not apply this part of the patch.",non_debt,-
thrift,48,comment_2,Looks good to me.,non_debt,-
thrift,48,comment_3,"David, can it go in?",non_debt,-
thrift,56,summary,Deprecation should skip thrift library code when printing the caller,non_debt,-
thrift,56,description,"When displaying the caller of a deprecated class/module, any thrift library code should be skipped. This really only affects (as it called each_field, which would trigger the deprecation). The given patch is actually 2 patches (suitable for use with git-am), one for this specific issue, the other fixes the specs to handle deprecation warnings a bit better. I bundled the second with this because the issue with the specs only shows up once the first patch is added.",code_debt,low_quality_code
thrift,56,comment_0,In 671984 and 671985. Thanks!,non_debt,-
thrift,62,summary,Merge of for Ruby bindings,non_debt,-
thrift,62,description,"This is the updated patch which adds a C extension to speed up Ruby thrift serialization and deserialization. It's largely been reviewed already, but should still be looked over. The pre-Apache discussions of the bindings, including peer review and modifications happened here: Post that, there have been a few changes prompted by bugs found internally at Powerset, and I fixed the read buffering in This patch also adds automake compatability (make check and such) to the Ruby bindings.",code_debt,slow_algorithm
thrift,62,comment_0,"The C extension, automake stuff, and buffered transport code",non_debt,-
thrift,62,comment_1,"We need setup.rb back for the automake stuff to work, and more generally until the RubyGems work is completed. I'm going to apply this to the tree shortly, but until then you'll need to apply this yourself for testing.",test_debt,lack_of_tests
thrift,62,comment_2,Patch 0003 restores `rake test` functionality in lib/rb,non_debt,-
thrift,62,comment_3,"In talking with Kevin Clark, the benchmark this patch provides demonstrates a significant performance penalty between the old code and the new (new meaning my recent work). We've narrowed it down to two areas: MemoryBuffer's new implementation, and calling #dup on default values. The first issue is addressed by THRIFT-63. The second is still a problem, and I don't know if there's any way we can fix it, because calling #dup is fairly important to protect against destructive modification of default values.",code_debt,slow_algorithm
thrift,62,comment_4,"We definitely need the dup effect. If we really want to try and improve the performance of that code, we could try generating the default value stuff inline rather than storing it in a class-variable map which requires dups.",code_debt,slow_algorithm
thrift,62,comment_5,"What, like have the default value be a block which returns the real default value? Interesting idea, but it doesn't seem like that would necessarily be any better than just calling #dup. We can certainly profile it and try, though.",non_debt,-
thrift,62,comment_6,"The performance problems uncovered aren't created by this patch, and exist independently of it. If there aren't any objections, I'm going to push it. I'll push Patch 0003 at the same time.",non_debt,-
thrift,62,comment_7,"This patch updates the Manifest so `rake install` will include the ext dir. It also provides a pre-install check to ensure thrift hasn't already been installed in site_ruby, so people don't end up with 2 versions of thrift installed.",non_debt,-
thrift,62,comment_8,In and about 674688,non_debt,-
thrift,84,summary,"fingerprint depend on appearence order, not field identifiers",non_debt,-
thrift,84,description,The next two struct produce different fingerprints:,non_debt,-
thrift,84,comment_0,The attached patch solve the issue.,non_debt,-
thrift,84,comment_1,"This is by design. The serialization code serializes the fields in code order, rather than identifier order. For the dense protocol, we have to make sure that we deserialize them in the proper order because we don't include the field tags. Therefore, two structures that define fields in different orders have to have different fingerprints in order for the dense protocol to work. We could modify the code generator to always serialize structures in identifier order instead of code order, but that would break with the dense protocol (if anyone is using structures with backwards-defined fields) and I'm not sure what the benefit would be.",non_debt,-
thrift,84,comment_2,"Dense protocol is experimental, we can add 'old-fp' switch to thrift to use current fingerprint algorithm and raise a warning if 'old-fp' isn't equal to 'new-fp'. What do you think about it? I think it's better to solve the problem while it still experimental. We want to use type 'any' in Rambler apps, and the type depend on fingerprint.",non_debt,-
thrift,84,comment_3,The issue was solved by THRIFT-236.,non_debt,-
thrift,87,summary,Website improvements: more info on index.xml and whoare.xml,non_debt,-
thrift,87,description,I'm putting a little more information about Thrift on index.xml and adding all the current committers to whoweare.xml.,non_debt,-
thrift,87,comment_0,"If someone reviews and approves the attached diffs (Forrest html output also attached), I'll work with kclark to get them committed.",non_debt,-
thrift,87,comment_1,LGTM,non_debt,-
thrift,87,comment_2,Pushed. Site should update soon.,non_debt,-
thrift,92,summary,Hardcoded path to in,non_debt,-
thrift,92,description,"Packaging for Fedora with a spec file that installs to /usr/ instead of /usr/local, ran into this error with rpmlint. Attaching patch that switches the path to ""/usr/bin/env thrift"".",non_debt,-
thrift,92,comment_0,Replace hardcoded path to thrift executable with /usr/bin/env thrift.,non_debt,-
thrift,92,comment_1,How about this?,non_debt,-
thrift,92,comment_2,All the better! :),non_debt,-
thrift,95,summary,TBufferedTransport class initializtion error,non_debt,-
thrift,95,description,Incorrect initialization of TBufferedTransport class in constructor: Class member wBufSize_ is initializing by rsz parameter. Bug was found with -Wall option :),non_debt,-
thrift,95,comment_0,Patch to fix the problem.,non_debt,-
thrift,95,comment_1,"Strange that this didn't trigger a warning for me. What system are you on, and what gcc version are you using?",non_debt,-
thrift,95,comment_2,I'm working with gcc version 3.4.6 on FreeBSD release 6.3.,non_debt,-
thrift,125,summary,OCaml libraries don't compile with 32-bit ocaml,non_debt,-
thrift,125,description,"TBinaryProtocol.ml contains integers outside the range of 32-bit OCaml's native int type (specifically the version_mask and version_1). Additionally, when reading int32s off of the network it returns them as OCaml-native ints. This will behave wrong when an int In TBinaryProtocol 64-bit ints are kept as int64 objects. The same should be done with 32-bit int values.",non_debt,-
thrift,125,comment_0,Does this fix it?,non_debt,-
thrift,125,comment_1,"I've updated the patch for the current SVN version. With this patch, I'm successfully using thrift under 32-bit OCaml with no problems.",non_debt,-
thrift,125,comment_2,I just committed this patch to TRUNK.,non_debt,-
thrift,134,summary,Add simple daemon management to fb303 simple management script,non_debt,-
thrift,134,description,"Add some sort of simple start, stop restart capability.",non_debt,-
thrift,134,comment_0,"We have another issue open to remove fb303, so I'm guessing this issue won't get fixed.",non_debt,-
thrift,137,summary,GWT support for Thrift,non_debt,-
thrift,137,description,"As previously discussed on the mailinglist, using Thrift from Google Web Toolkit (GWT) applications (AJAX) would be nice, as it does not only allow you to consume existing Thrift services from GWT applications, but also means that you now can write GWT-consumable RPC services in any language (and say host them on Google Appengine) that are practically source-code compatible with the official GWT RPC framework. Doing this presents two challanges: 1) The GWT compiler only supports a subset of the JRE libraries (luckily, this is rather easy to work around). 2) As the A in AJAX hints, the only way of doing RPC is asynchronously, something not supported by Thrift, by using the XMLHttpRequest object in the browser. Here's what I've done (an excerpt from the mailing-list): This solution works really well for my problem, but it's half-assed in two ways. 1) It only allows for asynchronous client transports (as in the case of the XMLHttpRequest object) and not on the server side (with messages coming back in a non-sequential order). 2) I'm not sure how to solve the client library issues. Right now, I've moved the core classes (those required on the client (GWT) side of things) into while keeping everything else where they are. This allows the GWT compiler to translate while using the same jar both on the client and server. This is not very elegant for people not using GWT (which I suppose is 99.99% of the audience) but short of maintaining two separate Java client libraries, I'm not sure how to solve this issue. The attached patch is only for the compiler, and does not produce compilable client code without the modified client library. Just wanted to get some input before producing a somewhat committable patch. Comments? Ideas?",design_debt,non-optimal_design
thrift,137,comment_0,"As previously discussed on the mailing list I have also worked to make integrating Thrift structs and services with GWT easier. Here is the path I followed. I patched the Thrift compiler to accept an additional options for Java code generation, namely 'gwt'. When this option is specified, the Java generator generates classes for structs in a 'gwt' subpackage of the specified namespace. Those classes are slightly modified versions of the normally generated classes. The modifications are simply the removal of the TBase inheritance, the removal of any hashcode method (as StringBuilder is not supported by GWT) and the inclusion of the isset field as non final and as part of the constructor. A helper class has been written which using reflection converts from a Thrift TBase to its GWT counterpart. For services, a mirror class of the Thrift one is created in the 'gwt; subpackage, this class has two inner interfaces, one for the regular service, the other one for the Async version. I'll add my patch to this issue so anyone interested can have a look.",non_debt,-
thrift,137,comment_1,Patch to the Java generator to accept a new 'gwt' option that will generate GWT compatible versions of classes.,non_debt,-
thrift,137,comment_2,Helper class to convert back and forth between Thrift and GWT versions of objects.,non_debt,-
thrift,137,comment_3,"Ah, now I better understand what you did ;) However, I'm not sure I understand the use-case here. Why would one want to generate GWT RPC services definitions from Thrift IDLs rather than creating them manually, when it's impossible to produce or consume these services without using the GWT RPC framework (ie. nothing but the GWT client could consume them, and nothing but a Java servlet implementing GWT's could produce them)?",non_debt,-
thrift,137,comment_4,New version of the compiler modifications. Now generates the asynchronous client in addition to the standard one.,non_debt,-
thrift,137,comment_5,"New version of the modifications; now including the client library side of it that wasn't submitted the last time. Compile the Thrift Java library with 'ant compile-gwt dist' to enable the GWT changes. This should now provide fully working Thrift/GWT integration. What remains to be done is rewriting the TGWTJSONProtocol, which is currently a butchered version of the standard TJSONProtocol.",requirement_debt,requirement_partially_implemented
thrift,137,comment_6,"Fourth iteration of the GWT patch. Now included is a TGWTProtocol JSON implementation that uses the native GWT JSON parser (compatible with the standard JSONProtocol implementation), and a TGWTProxyGenerator that provides deferred binding support for Thrift cilents. This means that it's now as easy to use Thrift from GWT as using the standard GWT RPC framework. Also, I made some build changes since the last patch which means that a single JAR can be used for both server and client code, but also that I did not have to modify or move any of the existing classes (almost). I suppose the patch is more or less committable, as it is now feature complete, but I'm sure there's a bunch of bugs in the GWT part of it and could definitely use a pair of extra eyes.....",design_debt,non-optimal_design
thrift,137,comment_7,"And if anybody dare to try it, compile the Thrift Java library with ant compile-gwt dist' and generate your client code with 'thrift --gen java:beans,async example.thrift'. final client = GWT ClickListener() { public void onClick(Widget sender) { try { new public void onFailure(Throwable throwable) { } public void onSuccess(Document result) { ("" + result.getUri() + ""): "" + } }); } catch (Exception e) { } } });",non_debt,-
thrift,137,comment_8,"issue is too old, please reopen or create a new issue and patch if you need this. see",non_debt,-
thrift,141,summary,"If a required field is not present on serialization, throw an exception",non_debt,-
thrift,141,description,"The desired behavior of the ""required"" keyword is to state that a given field MUST be present in the serialized version of a record, or it will fail to deserialize. If we are going to have such functionality, it makes sense for us to also fail to serialize in the first place if a required field is omitted.",non_debt,-
thrift,141,comment_0,Closing as all needed components for this issue are done.,non_debt,-
thrift,147,summary,Ruby generated classes should include class doc strings,documentation_debt,low_quality_documentation
thrift,147,description,"Generated code should include the provided docstrings (from the IDL) as RDoc on the classes. If the fields weren't generated at runtime, they should have RDoc too.",documentation_debt,low_quality_documentation
thrift,147,comment_0,This patch uses the changes in THRIFT-179 for simplicity.,non_debt,-
thrift,147,comment_1,"This version conforms to the way the Java version works. It also includes per-field docstrings mixed in with the generated FIELDS constant, so that people browsing the code for schema info (like my users) can see what the fields are all about.",non_debt,-
thrift,147,comment_2,Looks good. I'll push barring objections.,non_debt,-
thrift,147,comment_3,In 709309,non_debt,-
thrift,163,summary,Create an Plugin Framework so Developers can plug-in arbitrary code-generation features,non_debt,-
thrift,163,description,"It would be really nice to be able to plug in arbitrary code-generation extensions to the Thrift compiler. There are a lot of problems that can be elegantly solved by augmenting the Thrift compiler. For example, in my application, I need to be able to find all instances of an object of a certain type reachable from a Thrift object. (Java reflection is too expensive). I doubt that the community would accept this is a feature within the core compiler, but if I could do it via my own extension that would be ideal. There are a number of steps that need to happen to make this work: 1. Create an interface that extensions implement - this could be as simple as one method called ""generate_body"" which is called within the class definition. This allows extensions to create field declarations and new methods. Possibly we might need a if the extension needs to do anything in the object's constructor. 2. Create a way for the compiler to dynamically load extension modules - I'm not that familiar with how to do this in C++. We might need a configuration file for each module which specifies what language it generates for. I think one of the main objections to this will be robustness - how do we ensure that extensions don't conflict with one another or with the Thrift compiler itself? I would propose that we take the Firefox route to addressing this and don't worry about it. It should be up to the extensions designed to choose sufficiently good method and field names to prevent conflicts. Thoughts?",non_debt,-
thrift,163,comment_0,"I think this is an excellent idea, and its one that we had discussed doing internally even before open sourcing Thrift. Theoretically, all of the language generators could just use this plugin architecture. There will be a bit of heavy lifting in setting up the dynamic loading, but I'm sure there's someone on the project who's already roughly familiar enough with that to start thinking about tackling it. But overall, this would be great to have. I think it'll promote greater sharing of extended functionality that doesn't belong in the core compiler.",non_debt,-
thrift,163,comment_1,"I have some code lying around that implements part of this using libtool. I'll try to dig it up in the next few days. I think the biggest risk is the Thrift compiler having problems loading the generator modules. I think the second biggest risk is build errors on all sorts of platforms (I'm looking at you, OS X).",non_debt,-
thrift,163,comment_2,"This is actually way more primitive than I remembered. If I recall properly, it successfully builds the generators as shared objects. Since this was written, we've modified most of the generators to use a dynamic registry, which was one of the big hurdles to getting this to work. I think the big chunks that remain to be done are first modifying the compiler to load the appropriate shared objects with libltdl and investigating whether it is possible to compile some of the objects directly into the compiler so that we don't have to worry about library search paths.",design_debt,non-optimal_design
thrift,163,comment_3,This one looks interesting,non_debt,-
thrift,163,comment_4,"issue is too old, please reopen or create a new issue and patch if you need this. see",non_debt,-
thrift,186,summary,Erlang Makefile not compatible with Solaris,non_debt,-
thrift,186,description,"The sed lines in weren't happy on solaris. Here's a replacement for them: MODULES = $(shell find . -name \*.erl | sed 's:^\\./::' | sed 's/\.erl//') MODULES_STRING_LIST = $(shell find . -name \*.erl | sed 's:^\./:"":' | sed 's/\.erl/"",/') Marking patch available even though it's not a strict patch. Hopefully that's ok cuz it's like 4 characters ;-)",non_debt,-
thrift,186,comment_0,I think JIRA messed up the formatting. Any chance you can do it as an attachment? Do we need to apply this to any other Makefiles in erl? Does the upstream for this Makefile still exist?,non_debt,-
thrift,186,comment_1,Here's a patch,non_debt,-
thrift,191,summary,Add ability to get a field name from a field id,non_debt,-
thrift,191,description,"It would be nice to be able to get a field name for a field id. For instance, if I have the following struct: struct Foo { 1: optional i32 number; } I should be able to say something like and get back the string ""number"". The use case for this is for debugging/logging.",non_debt,-
thrift,191,comment_0,Sounds reasonable. We had something like this implemented in PHP at one point. I think the best way to do this is as a static map of meta-data in the class.,non_debt,-
thrift,191,comment_1,"I agree, a static map of meta-data would be ideal. This would allow for iterating over the field's as well, and allow for additional meta-data (besides the field name) to be accessible.",non_debt,-
thrift,191,comment_2,It sounds like a map of field id to metadata structure is probably a good idea. We can stick the upcoming annotation data in there as well.,non_debt,-
thrift,191,comment_3,"Here is a first patch. For the time being the metadata structure only contains the field name and is an inner class just like Isset, but unlike Isset this one will be the same for all classes, so may be it should be placed somewhere else. Let me know your thoughts.",code_debt,low_quality_code
thrift,191,comment_4,"I like the map declaration, but I think that the MetaData class should live outside of the generated code. It's not actually a class that will vary by structure, so it should just live in c.f.thrift.",architecture_debt,violation_of_modularity
thrift,191,comment_5,Here is second version of the patch that moves the MetaData class from the generated code to package,non_debt,-
thrift,191,comment_6,I applied this patch and tested it out. It works really well. I think we're ready for commit.,non_debt,-
thrift,195,summary,HTTP Server for Python,non_debt,-
thrift,195,comment_0,"The docs about an apache/php endpoint are sorta facebook specific. Other than that, LGTM.",documentation_debt,low_quality_documentation
thrift,202,summary,Java tests won't compile,non_debt,-
thrift,202,description,There's a compile error in the test code because the constructor signature of TFramedTransport changed.,non_debt,-
thrift,202,comment_0,This should fix it.,non_debt,-
thrift,206,summary,ruby required field validation requires before read instead of after,non_debt,-
thrift,206,description,"There's a glaring bug in the original ruby required patch. The check of required field presence happens BEFORE deserializing, not after. This means unset required fields will always be null.",non_debt,-
thrift,206,comment_0,"I'm going to push, but is there a test that could have caught this that we're missing?",non_debt,-
thrift,206,comment_1,In 714069.,non_debt,-
thrift,208,summary,Thrift doesn't build on Ubuntu 8.10,non_debt,-
thrift,208,comment_0,Anyone want to review this? It is pretty trivial and pretty high priority.,non_debt,-
thrift,208,comment_1,"We built some packages of Thrift for Ubuntu 8.10 and didn't notice any issue. Anyway, I've applied your patch locally and will upload it to our PPA. Let's see if Launchpad can build it.",non_debt,-
thrift,208,comment_2,"Ok, Launchpad built it, see full output log for i386 and amd64:",non_debt,-
thrift,211,summary,"Support ""tethered"" clients for Erlang",non_debt,-
thrift,211,description,"Add a client option that causes clients to monitor their creators and terminate when the creator dies. This makes it possible to prevent client leaks without linking, because the latter causes application code to be killed when a transport error occurs and exits are not trapped.",code_debt,low_quality_code
thrift,211,comment_0,Can this be committed?,non_debt,-
thrift,211,comment_1,"It should be reviewed first. Todd, Chris, or Eugene, want to jump in?",non_debt,-
thrift,211,comment_2,"1/ Unify the setup of {Starter, Opts} so that you don't have to do the monitor keysearch twice. 2/ Consider checking the source of the DOWN message ... having an unverified DOWN message could lead to some very hard-to-trace bugs. Lg otherwise. Also, what do you think about converting all the Erl unit tests to use eunit at some point (now that it's standard as of R12B) ?",design_debt,non-optimal_design
thrift,211,comment_3,"Replaced tabs with spaces (oops). Fixed #1. Is this unified enough to you? It eliminates the double search. It is not super-unified, but unifying them more would be kind of gross. For 2, I did consider it, but who would forge a 'DOWN' message? Verifying it just requires more state. I think converting to eunit would be great.",code_debt,low_quality_code
thrift,211,comment_4,I changed my mind. Now they are more unified.,non_debt,-
thrift,211,comment_5,"This is an approximate incremental diff of the interesting changes. interdiff didn't get the context quite right, but I think the content is right.",non_debt,-
thrift,211,comment_6,"OK. This is probably good enough to check in. I don't remember if erlang:monitor has a monopoly on DOWN messages, so hence there's some reason to at least add a guard clause (is_reference() on the monitor_ref) to make sure that it's in the right format.",code_debt,low_quality_code
thrift,221,summary,Make java build classpath more dynamic and configurable,design_debt,non-optimal_design
thrift,221,description,"The way that the current Java library and test builds are set up, they rely on an external jar that's assumed to be at some specific path. This isn't where the jar resides on my machine, so every time I want to do some building, I have to hand edit the file, and then when I want to generate a diff for posting on jira, I have to revert the changes. We should make it so the class path is configurable in a way that doesn't require checkins. There are a couple different ways to do this, but a properties file with the classpath is probably one of the easiest.",design_debt,non-optimal_design
thrift,221,comment_0,Committed as part of THRIFT-166.,non_debt,-
thrift,228,summary,Ruby version of binaryprotocol.rb has an outdated version of read_message_begin,architecture_debt,using_obsolete_technology
thrift,228,description,"Following exception is raised by the read_message_begin method"" Missing version identifier from `receive_message' Comparing the implementation of the method in the Ruby class to its python counterpart TBinaryProtocol shows that the Ruby method is quite outdated. I have changed the method to be: def read_message_begin version = read_i32 if(version <0) if (version & VERSION_MASK != VERSION_1) raise 'Missing version identifier') end type = version & 0000000ff name = read_string seqid = read_i32 else name = type = read_byte seqid = read_i32 end [name, type, seqid] end This does not raise an exception on the strict read condition in the else clause as is raised by the Python version but can be easily added to.",architecture_debt,using_obsolete_technology
thrift,228,comment_0,"Can you post this change as a patch file? When you attach a file, you're given the opportunity to grant license to the ASF.",non_debt,-
thrift,228,comment_1,This issue was fixed in THRIFT-254.,non_debt,-
thrift,234,summary,Can't assign binary fields default values,non_debt,-
thrift,234,description,"The Thrift compiler treats binary and string fields the same. This means that if you want to give a binary field a default value, you have to give it a string. The code generated from this then tries to assign a byte array variable the value of a string, which doesn't work.",non_debt,-
thrift,234,comment_0,"If this is still an issue, open it as a new bug.",non_debt,-
thrift,234,comment_1,Reopened as Thrift-4932,non_debt,-
thrift,239,summary,Generate @Override annotations for all overrided methods,non_debt,-
thrift,239,description,This makes the code more complete. It's useful when you have your IDE set to show errors whenever the annotation isn't present (like I do).,non_debt,-
thrift,239,comment_0,"For those of us not using an IDE, can you list which methods need the @Override annotation?",non_debt,-
thrift,239,comment_1,I think the only ones are: equals clone hashcode toString,non_debt,-
thrift,239,comment_2,I'm going to commit this patch. I tested it out locally and it appears to work just fine.,non_debt,-
thrift,239,comment_3,Committed.,non_debt,-
thrift,240,summary,TBase should implement Cloneable,requirement_debt,requirement_partially_implemented
thrift,240,description,"Now that we have a deep copy constructor, TBase should implement cloneable.",design_debt,non-optimal_design
thrift,240,comment_0,"Ridiculously tiny patch, so I've committed it. (Take a look at the 'subversion commits' tab if you want to see the change I made.)",non_debt,-
thrift,241,summary,Python __repr__ is confusing and does not eval to the object in question,code_debt,low_quality_code
thrift,241,description,"Having the repr return the repr of __dict__ is confusing, because the object is not a dict and does not act (much) like one. 100% of the 3 python developers I have seen who are new to thrift were confused at first why the repr looked like a dict but obj[attributename] raised KeyError. Additionally, this violates the repr guideline that where possible eval(repr(obj)) == obj. Finally, specifying a __str__ equal to __repr__ is redundant, since str() will use __repr__ if no __str__ is given. Here is a patch: $ diff -u compiler/cpp/sr\  2008-12-24 16:36:54.000000000 +0000 +++ 2008-12-24 16:49:54.000000000 +0000 @@ -614,11 +614,10 @@ // Printing utilities so that on the command line thrift // structs look pretty like dictionaries out << - indent() << ""def __str__(self):"" << endl << - indent() << "" return str(self.__dict__)"" << endl << - endl << indent() << ""def __repr__(self):"" << endl << - indent() << "" return << endl << + indent() << "" L = ['%s=%r' % (key, value)"" << endl << + indent() << "" for key, value in << endl << + indent() << "" return '%s(%s)' % ', '.join(L))"" << endl << endl; // Equality and inequality methods that compare by value",code_debt,low_quality_code
thrift,241,comment_0,Sorry about the messed up formatting. Clean patch attached.,code_debt,low_quality_code
thrift,241,comment_1,+1 The patch looks good.,non_debt,-
thrift,241,comment_2,Was committed as r731685,non_debt,-
thrift,251,summary,Omniprotocol to have multi protocol over single services,non_debt,-
thrift,251,description,"Inspired by the thrift like framework we have in the company i work for, i think it would be really cool to have an omniprotocol that would recognize and dispatch to real underlying protocols the current message. This would allow having a single service able to process request from other services in an efficient binary protocol, but also serve queries coming over AJAX from the browser via HTTP POST + JSON. The output protocol will be the input one to get things easier, even if having an output parameter in the URI (in case of HTTP request) could be possible in the future. We have all the basics to get this feature running, the JSON protocol is fixed (THRIFT-244) and the THttpServer transport now exists for the C++ library (THRIFT-247)",non_debt,-
thrift,251,comment_0,"See for a related, but different, idea that hasn't gained much traction so far because of its broader scope.",non_debt,-
thrift,251,comment_1,"Thanks for the link Michael. This is related but different yeah, but not incompatible. Actually, this is how our framework works, services have a name and are reachable over HTTP with /ServiceName/Method URIs... I'll get things done step by step, but I'll keep a track about it in my fun things to do list.",non_debt,-
thrift,251,comment_2,"I've also been thinking of a similar protocol. I'm all for it. The question is can you efficiently and reliably peek and detect the originating protocol. Also, what languages are you looking to support for this patch (c++ I assume)?",non_debt,-
thrift,251,comment_3,"Detecting the protocol should be quite straightforward since i'll support only the most used protocols (Binary, JSON, and hopefully Compact when a C++ version will be available) and they are totally different from the first byte (well, for binary / compact, i'm not sure because i didn't looked to bryan's implementation yet, so i think that it will be very efficient, only a few bytes to recognize the protocol and then transparent proxying, and reliable. I'll start by the C++ Library implementation in a few days. When it will be rock solid, i'll be able to help other language developpers as far i as can, so Python, Ruby and Java in priority.",non_debt,-
thrift,251,comment_4,Java Implementation =>,non_debt,-
thrift,251,comment_5,"Is the original request here to allow for multiple input protocols to the same TServer so that you could listen on both, say, TBinaryProtocol with SSL/TLS on one port, and https on another port, etc...? Given this issue is fairly old now (about 6 years) I'm not certain there's been any call for multiple input protocols in a while.  is this an issue we should maintain in the backlog?",non_debt,-
thrift,251,comment_6,Jake Farrell is this an issue we should maintain in the backlog?,non_debt,-
thrift,255,description,is a thin wrapper around TFDTransport that simplifies the process of opening a file.,design_debt,non-optimal_design
thrift,255,comment_1,"Make sure to change the namespace from facebook to apache. Otherwise, looks good.",non_debt,-
thrift,255,comment_2,1) const std::string& for path 2) does specifying default params in just the .cpp file really work? seems like the compiler would have trouble understanding that. 3) Document using O_APPEND. Maybe allow overwriting as another option,code_debt,low_quality_code
thrift,255,comment_3,"I agree with Ben Maurer on the first two points. David could you update the patch and switch to the new namespaces ? It's a useful tool and a triviail patch, would be cool to get this commited.",non_debt,-
thrift,255,comment_4,What are we waiting on to get this committed?,non_debt,-
thrift,255,comment_5,"The patch does have little glitches and just does not apply anymore. If David doesn't show up on this one in a couple of days, i'll update it by myself.",non_debt,-
thrift,255,comment_6,"Updated namespaces, moved default args, used a reference for the path, and documented O_APPEND.",non_debt,-
thrift,255,comment_7,"Looks perfect, +1 !",non_debt,-
thrift,275,summary,Remove deprecated classes from Ruby library,code_debt,dead_code
thrift,275,description,"There are a lot of weirdly named files in the Ruby Thrift library in order for backwards compatibility to work. I think this is of dubious value, and is really confusing. The deprecation code is also very complicated. I think we should just get rid of the deprecated stuff. We're pre-version 1.0, so better to get all the big changes in now.",code_debt,dead_code
thrift,275,comment_0,"I'm alright with this, as long as we well document the changes. Right now anyone pulling trunk would be getting the dep warnings, but those waiting for a new release would just see breakage.",non_debt,-
thrift,275,comment_1,"This patch removes all the deprecation stuff and the t*.rb classes that were only placeholders. In addition, I've changed the implementations of some ""abstract"" methods to throw NotImplementedError instead of returning nil, and fixed the test accordingly. Finally, I removed the no longer required borrow and consume methods from all the transport implementations that had them. (Borrow and consume have been supplanted by the thrift_native package.) All the specs and unit test pass, so I think the job is done. It seems like the only documentation that needs to be made is to indicate that when you see you just change it to and your problems more or less go away. (You will have to replace the T with Thrift:: on some declarations, but that's pretty easy too.) I'd love to have this reviewed and get it committed.",documentation_debt,outdated_documentation
thrift,275,comment_2,"Looks great, push it.",non_debt,-
thrift,275,comment_3,Committed. Thanks for the review!,non_debt,-
thrift,277,summary,Abstract Transport in Ruby #read method should throw,code_debt,low_quality_code
thrift,277,description,"It's really an abstract method, so not overriding it is an error. We should throw an error when someone calls it inappropriately. The same can probably be said for #write. #open and #open? might also want to change, though that's not quite as certain.",code_debt,low_quality_code
thrift,277,comment_0,1,non_debt,-
thrift,277,comment_1,This should do it.,non_debt,-
thrift,277,comment_2,Looks good. +1,non_debt,-
thrift,277,comment_3,Thanks for the quick review. Committed.,non_debt,-
thrift,278,summary,#validate exceptions should contain the offending value,code_debt,low_quality_code
thrift,278,description,"If you have an enum field, and its value isn't in the VALID_VALUES of your enum, the exception thrown says that it's not found, but it doesn't say what the value is.",code_debt,low_quality_code
thrift,278,comment_0,1,non_debt,-
thrift,278,comment_1,Patch.,non_debt,-
thrift,278,comment_2,"Does this work? Maybe I'm reading it wrong, but: {{+ indent(out) << ""throw new field '"" << field- Looks like it just prints the field name as the value. Got a test?",test_debt,lack_of_tests
thrift,278,comment_3,"I tested it locally. The generated code looks like this: throw new field 'myField' has been assigned the invalid value "" + myField); and it does display the value and not the name.",non_debt,-
thrift,278,comment_4,Is there any reason for this patch not to be committed?,non_debt,-
thrift,278,comment_5,Looks fine to me.,non_debt,-
thrift,278,comment_6,"Committed, with slight modifications. Thanks for the patch!",non_debt,-
thrift,285,summary,Don't generate _result helper classes for async methods,non_debt,-
thrift,285,description,"Currently, the Thrift compiler generates METHOD_result helper classes for all methods, even async ones.",non_debt,-
thrift,285,comment_0,The attached patch removes _result classes if a method declared as async.,non_debt,-
thrift,285,comment_1,Looks to me as if this issue can be closed? SVN trunk appears to heed 'oneway' (the new old name for 'async') and will not generate _result structs if set.,non_debt,-
thrift,285,comment_2,"This ticket was already marked as fixed since January. Bruce, have you noticed any regressions?",non_debt,-
thrift,298,summary,Exception propagation seems broken for Ruby clients,non_debt,-
thrift,298,description,"Instead of raising the user defined exception, it's `initialize': wrong number of arguments (0 for 1) (ArgumentError) Any thrift service that throws user defined exceptions would get the same exception. Here is an example for the included tutorial: $ ./RubyClient.rb ping() ping() add(1,1) 1+1=2 add(1,4) 1+4=5 calculate(1, Work(comment=None, num1=15, num2=10, op=2)) 15-10=5 getStruct(1) Log: 5 calculate(1, Work(comment=None, num1=1, num2=0, op=4)) `initialize': wrong number of arguments (0 for 1) (ArgumentError) from `call' from from `initialize' from `new' from `read_field' from `handle_message' from `read' from `loop' from `read' from `receive_message' from `recv_calculate' from `calculate' from ./RubyClient.rb:44 I've verified that this is broken in the current trunk (and quite a few commits back as well) This is a blocker for us to release another open source package using thrift.",non_debt,-
thrift,298,comment_0,"Looks like there is a simple one line fix for this problem (see attached patch) Our exceptions now works fine and the tutorial output looks more reasonable: $ ./RubyClient.rb ping() ping() add(1,1) 1+1=2 add(1,4) 1+4=5 calculate(1, Work(comment=None, num1=15, num2=10, op=2)) 15-10=5 getStruct(1) Log: 5 calculate(1, Work(comment=None, num1=1, num2=0, op=4)) InvalidOperation: Cannot divide by 0 zip zip()",non_debt,-
thrift,298,comment_1,Any chance of adding an explicit spec to ruby lib for this?,non_debt,-
thrift,298,comment_2,"I guessed just enough to fix the problem but I don't understand the code enough to write a correct spec for this. I think the original author of the feature (Kevin Ballard (committed by Kevin Clark), I believe, according to the log) and whoever is refactoring ruby protocols (in this case, you, Bryan) should write specs for all the features of thrift.",documentation_debt,outdated_documentation
thrift,298,comment_3,"Well, if you can believe it, I don't get what's going on here, either :). I think this code is a little too complicated. For now, I think I'll just apply your patch, and hopefully one day it won't be a problem, as I refactor my way through the Ruby libraries.",code_debt,complex_code
thrift,298,comment_4,I just committed this.,non_debt,-
thrift,298,comment_5,Thanks!,non_debt,-
thrift,315,summary,Invalid code when Enum is in another package,non_debt,-
thrift,315,description,"Thrift file 1 Thrift file 2 When the java code is generated, the method doesn't fully qualify the type name, leading to a compile error",non_debt,-
thrift,315,comment_0,Turns out this was fixed already as part of another issue. Oops!,non_debt,-
thrift,331,summary,Compact Protocol implementations,non_debt,-
thrift,331,description,The Java compact protocol implementation in THRIFT-110 should serve as a reference implementation for implementations in all our library languages. Let's create sub-issues for each language so we can resolve them as they come in.,non_debt,-
thrift,331,comment_0,Closing parent issue as all children have been closed and no one has touched this since 2010,non_debt,-
thrift,341,summary,Perl libraries should honor INSTALLDIRS,non_debt,-
thrift,341,description,"The Perl libraries should honor the INSTALLDIRS variable (e.g. INSTALLDIRS=site, INSTALLDIRS=vendor)",non_debt,-
thrift,341,comment_0,Here's a trivial fix.,non_debt,-
thrift,341,comment_1,"Any comment? It's a very simple patch and doesn't break anything, AFAIK.",non_debt,-
thrift,341,comment_2,In 749472. Sorry for the wait.,non_debt,-
thrift,341,comment_3,"Sorry, missed this. I've subscribed to the feed so I'll be catch these.",non_debt,-
thrift,341,comment_4,No worries. Thanks!,non_debt,-
thrift,349,summary,Accelerated binary protocol serialization segementation fault,non_debt,-
thrift,349,description,"When using the thrift_protocol PHP extension, serializing an object which doesn't have a _TSPEC static member as a T_STRUCT results in a segfault.. This can happen if you have cached Thrift objects which didn't get constructed in the current session. This happens because the null zval is interpreted as a hashtable. A patch is attached which throws an exception in this case.",non_debt,-
thrift,349,comment_0,is now relative to the project root.,non_debt,-
thrift,349,comment_1,I just committed this patch. Thanks Russ!,non_debt,-
thrift,353,summary,"Service generation needs to make service name capitalized, since Ruby modules need to be constant.",code_debt,low_quality_code
thrift,353,description,"service generator needs to capitalize service names, since services are ruby modules and ruby modules are constants.",code_debt,low_quality_code
thrift,353,comment_0,"Hi Gary, I agree here too, but can't see the patch.",non_debt,-
thrift,353,comment_1,"Ah, scratch that, just beat you to it. I'll take a look.",non_debt,-
thrift,353,comment_2,"Looks good to me, +1",non_debt,-
thrift,353,comment_3,here is the correct version. mispelled function name.,documentation_debt,low_quality_documentation
thrift,353,comment_4,"+1 - We've been running this internally for the better part of a year. I'm pretty sure I've sent it in before, but David didn't like it for some reason.",non_debt,-
thrift,353,comment_5,"Ok, pushed in 750160.",non_debt,-
thrift,353,comment_6,Wow. That was fast. Thanks Kevin.,non_debt,-
thrift,372,summary,Ruby lib doesn't rescue properly from lack of native_thrift extension,non_debt,-
thrift,372,description,"Files that try to load thrift_native need to rescue from LoadError. LoadError isn't a StandardError, so isn't caught by a bare rescue. From current HEAD (757825). Clio:rb kev$ g g thrift_native 'thrift_native' { 'thrift_native' ""thrift_native"" ""thrift_native"" We may want to put the require for the extension in a ruby file, and just rescue there. That way load path stuff is taken care of properly, and we get our rescue.",non_debt,-
thrift,372,comment_0,"How's this? I cleaned up some missed dead code related to deprecation, as well.",code_debt,dead_code
thrift,372,comment_1,Looks good. +1,non_debt,-
thrift,372,comment_2,Committed.,non_debt,-
thrift,390,summary,Cabalize Haskell library code,non_debt,-
thrift,390,description,Use Cabal to build and install the Haskell library code.,non_debt,-
thrift,390,comment_0,Committed. Thanks for the patch!,non_debt,-
thrift,397,summary,remove unnecessary redefinition of generate_program(),code_debt,dead_code
thrift,397,description,This is a holdover from the OCaml generator and is unnecessary in Haskell.,code_debt,dead_code
thrift,397,comment_0,"Cool, in r757824.",non_debt,-
thrift,418,summary,Don't do runtime sorting of struct fields,non_debt,-
thrift,418,description,"Currently the ruby struct code sorts the field ids of each struct every time it is serialized, which is an unnecessary drag on performance.",design_debt,non-optimal_design
thrift,418,comment_0,Attaching patch. There are 2 changes: 1) Modified t_rb_generator.cc to generate a FIELD_IDS constant for each struct + a struct_field_ids() accessor method. 2) Use instead of to iterate through struct fields id-sorted order. Updated the native code as well. All tests pass. Should be targeted for next major release as the change is not code-compatible with 0.6-generated files. Can be re-implement without a compiler change if we want to keep full compatibility between 0.6 gen-rb files and 0.7 library.,design_debt,non-optimal_design
thrift,418,comment_1,"I just committed this patch to trunk. Thanks, Ilya!",non_debt,-
thrift,418,comment_3,Reopening as I've thought about it and a patch which doesn't change the compiler or generated code ends up being much cleaner.,design_debt,non-optimal_design
thrift,418,comment_4,New patch which reverts the previous changes and re-implements the optimization without making any compiler / gen-rb changes.,non_debt,-
thrift,418,comment_5,I committed the newer version.,non_debt,-
thrift,418,comment_6,Resolving.,non_debt,-
thrift,425,summary,numeric_limits is declared in <limits>,non_debt,-
thrift,425,description,doesn't compile without including limits.,non_debt,-
thrift,425,comment_0,Committed. Thanks for the patch!,non_debt,-
thrift,427,summary,print_const_value is broken in the compiler for Java,non_debt,-
thrift,427,description,Broken by the privatization of isset.,non_debt,-
thrift,427,comment_0,System.out.println removed locally.,non_debt,-
thrift,427,comment_1,"The patch looks good, and everything compiles and the tests pass still. It'd be nice if this code was exercised through at least a test .thrift file though.",test_debt,lack_of_tests
thrift,427,comment_2,See THRIFT-356.,non_debt,-
thrift,433,summary,'rake spec' sort of fails,non_debt,-
thrift,433,description,"All the specs actually pass, but the process running the specs crashes with this:",non_debt,-
thrift,433,comment_0,This is Michael Stockton's patch from THRIFT-175.,non_debt,-
thrift,433,comment_1,I just committed this patch. Thanks Michael!,non_debt,-
thrift,447,summary,Make an abstract base Client class so we can generate less code,design_debt,non-optimal_design
thrift,447,description,"The Java generator currently uses the generator to create all of the contents of the myService.Client class, including boring stuff like the constructor and instance variables. It seems like we could just factor this common base stuff out into a BaseClient that lives in the library and simplify the generator accordingly.",code_debt,complex_code
thrift,447,comment_0,"This patch restructures the code generator to rely on some abstract base classes for clients, processors, and process functions. It makes the generated code marginally smaller and a lot cleaner.",code_debt,complex_code
thrift,447,comment_1,I just committed an updated version of this patch.,non_debt,-
thrift,452,summary,Gotcha with Java enums not having a zero value,non_debt,-
thrift,452,description,"In the java library, if you define an enum that does not have a value declared for 0, it seems like you can get into a strange situation. In java, the field for the enum will be set to zero, but isset will be true. Compare to a language like Python where the value will be None, causing isset to effectively be false. When the value is deserialized, Java will check if the enum value is valid, find that it is not, and throw an exception. Any thoughts on the best behavior?",non_debt,-
thrift,452,comment_0,"The isset should only be true if you ""set"" that field's value to 0. If you do that, then the struct should fail to serialize in the first place. Maybe I'm missing the pattern that gets you to this spot; can you elaborate a little?",non_debt,-
thrift,452,comment_1,Writes seem to be done unconditionally:,non_debt,-
thrift,452,comment_2,"That's because you didn't make your foo field optional. If you make it optional, the isset will be checked. Personally, I think the ""default"" requiredness is confusing - this is the problem you end up with.",design_debt,non-optimal_design
thrift,452,comment_3,"Is this still an issue, or should I close it?",non_debt,-
thrift,452,comment_4,"I agree that it is weird that we initialize the enum to an invalid value. Maybe the default value for enums should default to the first declared value, rather than 0?",design_debt,non-optimal_design
thrift,459,summary,Ruby installation always tries to write to /Library/Ruby/site,non_debt,-
thrift,459,description,"The Ruby installation always tries to write into /Library/Ruby/site, and pays no attention to the --prefix configuration setting. I realize that it probably does not make sense to install the Ruby library cide into prefixDir/lib, but I think it does make sense to be able to build it and install it somewhere other than the standard system location. Not everyone that builds Thrift wants to install the build products to the standard locations.",non_debt,-
thrift,459,comment_0,patch to add RUBY_PREFIX similar to JAVA_PREFIX or PY_PREFIX,non_debt,-
thrift,459,comment_1,I just committed this. Thanks for the patch!,non_debt,-
thrift,471,summary,Generated exceptions in Python should implement __str__,code_debt,low_quality_code
thrift,471,description,"When the python generator makes an exception class since THRIFT-241, it does not include a __str__ method. This is problematic since raised exceptions don't include any useful information that might be part of the exception struct. Without __str__: With __str__: Clearly the latter is way more useful. Patch to follow if no one objects.",code_debt,low_quality_code
thrift,471,comment_0,"yeah, the python docs say that __repr__ should be used if there is no __str__ and every other case I have seen follows that rule but the exception printing apparently does not. +1",code_debt,low_quality_code
thrift,471,comment_1,created although that's only of academic interest for existing versions of python :),non_debt,-
thrift,471,comment_2,Patch that implements __str__ only for the case of Exception-type structs by calling through to __repr__,non_debt,-
thrift,471,comment_3,I'll be interested to see how the Python folks respond.,non_debt,-
thrift,471,comment_4,"They responded, ""Exception implements __str__ so this is not a bug.""",non_debt,-
thrift,473,summary,constant structs that contain enum fields don't allow use of enums from external thrift files,non_debt,-
thrift,473,description,"Ok... this one is a little tricky to explain. Take this example Thrift def: In this code, a constant of a struct type that contains an enum field can be declared by using just the constant name that we care about. This works today. Now, take another example: (first.thrift) (second.thrift) If you do this, then you get an error that causes the compiler to quit. I think this is a valid use case, so it should probably be made to work.",non_debt,-
thrift,473,comment_0,"As far as I can tell, what happens is that the ONE in second.thrift doesn't get detected as a constant, so it gets parsed as an ""unquoted string"" (see THRIFT-472). This behavior is usually used to create the left-hand side names of enums and such. I'm not sure where the external constants can be retrieved from in the current organization, so if someone who knows the compiler could chime in, that would be great.",non_debt,-
thrift,473,comment_1,This was fixed incidentally a while ago.,non_debt,-
thrift,475,summary,DeprecationWarning for Thrift-generated code in Python 2.6,non_debt,-
thrift,475,description,Declaring an Exception in a Thrift class and then using it under Python 2.6 generates the following code: DeprecationWarning: has been deprecated as of Python 2.6 self.message = message,non_debt,-
thrift,475,comment_0,Lowering priority,non_debt,-
thrift,475,comment_1,Is this not just:,non_debt,-
thrift,475,comment_2,Aah yes. Didn't realize this affects everybody.,non_debt,-
thrift,475,comment_3,Here is one way to fix this annoying issue:,non_debt,-
thrift,475,comment_4,We do not support python 2.6 any longer.,non_debt,-
thrift,487,summary,errors,non_debt,-
thrift,487,description,"The method and its associated class have errors and race conditions that cause frequent failures in the test. Some failure modes return to the caller, but others just hang the test forever. The main problem is that the test function relies on indirect indications that its tasks have reached certain known conditions. The indications that it sees are actually caused by the ThreadManager's Workers, and this results in a number of race conditions between the test function and the BlockTasks. There are 2 patch files attached. One patches the file to fix the test. The other patches Tests.cpp so that it will run the blockTest in a loop. In my experience, an unpatched version of generally fails within 100 iterations. After being patched, the test will still fail unless a separate patch is applied to the ThreadManager.cpp file. That patch is part of another issue that I haven't entered yet (because it needs to refer to this one). When I know its number I will add a comment. Anyway, if the patch is applied to but not to ThreadManager.cpp, then the test will still fail, generally with an assert, but sometimes with a Bus Error. Test Procedure: 1) Apply the Tests.cpp patch. This makes the thread-manager portion of the test run ONLY the blockTest in an effectively infinite loop. 2) Run a make in lib/cpp in order to rebuild concurrency-test 3) Run concurrency test with the command line argument ""thread-manager"". This will start the blockTest loop. It should fail in a fairly short time. Repeated runs may fail different ways, including infinite hangs. 4) Apply the patch to 5) Run make in lib/cpp to rebuild concurrency-test 6) Run concurrency_test as before. It should probably run for a longer period of time. I have seen it run for an hour or more after beng patched. Eventually it should fail either with an assert in Monitor.cpp while trying to destroy a pthread_mutex_t, or it will get a Bus Error because it tried to execute invalid memory. I have also seen it hang forever. In order to resolve the remaining issues, a patch needs to be applied to ThreadManager.cpp. I will add a comment about that as soon as the issue is filed and the patch is available.",design_debt,non-optimal_design
thrift,487,comment_0,Patch for to fix the blockTest functionality. Patch for Tests.cpp to run the blocxkTest in a loop. This patch is for testing ONLY.,non_debt,-
thrift,487,comment_1,"The ThreadManager patch file is attached to THRIFT-488 If you have followed the test procedure above, then continue by doing this: 7) Apply the patch file from THRIFT-488 to TestManager.cpp 8) Run make in lib/cpp 9) Run concurrency_test as before. It should run without errors.",non_debt,-
thrift,487,comment_2,"This one is probably ok, but until I'd rather hold off on it until we nail down the best approach for",non_debt,-
thrift,487,comment_3,I am resolving this as fixed in version 0.10.0; there were significant ThreadManager changes to stabilize it (see THRIFT-3932).,non_debt,-
thrift,492,summary,Add Timeout Argument to,non_debt,-
thrift,492,description,"does not use a timeout option when calling gen_server:call(), so the default timeout of 5 seconds is used. It would be good to offer that to the user of the module. The attached trivial patch does exports a new function call/4 which does this, and makes the original call/3 function pass in the default 5000 ms timeout value.",non_debt,-
thrift,492,comment_0,Said patch.,non_debt,-
thrift,492,comment_1,Looks good to me,non_debt,-
thrift,492,comment_2,"Sorry, I was too eager to submit that patch. Just realized that it doesn't quite solve the issue the issue completely. Some other calls to gen_server:call() are not still not using timeout. I'm digging deeper... will report back. Thanks.",non_debt,-
thrift,492,comment_3,"I'm using framed transport, and found that making the gen_server:call() timeout infinity for reads fixes the issue for me, i.e. the timeout is determined by the initial so we can allow the underlying transport to take as long as it wants. If you think that this is patch is the way to go, then maybe other transports would need similar fixes as well, but I don't use them so I'm not so comfortable patching them.",non_debt,-
thrift,492,comment_4,"Please ignore the attached patch, I've updated it to work with trunk. It's on my github fork of David's repos here: Could someone please advise how I could help to get this merged into the official code base? IMO it's quite an important issue as the default 5 seconds timeout is too restrictive for many use cases. Thanks!",non_debt,-
thrift,492,comment_5,I'm pretty sure the refactoring means this ticket can be closed. thrift_client is no longer a gen_server that is left up to the application developer. The timeout controls are provided by the underlying erlang connection.,non_debt,-
thrift,492,comment_6,Refactoring made this obsolete.,non_debt,-
thrift,502,summary,typo on incubator page in python client example,documentation_debt,low_quality_documentation
thrift,502,description,# Use the service we already defined service = service.store(up) should have Client with a capital C: # Use the service we already defined service = service.store(up),non_debt,-
thrift,502,comment_0,"r774605 (It looks like site revisions don't appear in the ""all"" tab). Anyone remember how to push this out?",non_debt,-
thrift,503,summary,"""make check""-enabled C++ tests should be in lib/cpp/test",architecture_debt,violation_of_modularity
thrift,503,description,None,non_debt,-
thrift,503,comment_0,"what is the progress on that? It probably makes sense to commit that one before working on THRIFT-406, THRIFT-457, etc.",non_debt,-
thrift,503,comment_1,"It looks like I was working on this on a branch with a number of other build-related issues including THRIFT-500, THRIFT-503, THRIFT-505, and THRIFT-507. I wouldn't be surprised if there are conflicts with work that has been done since. I'm interested in finishing this up, but I'd like to figure out the situation with the Ruby build first. Either get ""make distclean"" working reliably or get it out of the autoconf/automake scripts.",non_debt,-
thrift,503,comment_2,This looks committed. Feel like closing the issue/,non_debt,-
thrift,508,summary,"Services that define a method twice should raise an error (in Python compiler, at least)",non_debt,-
thrift,508,description,"If you define a Thrift service that mentions the same method more than once, the Python compiler generates code for the method more than once. Given that this is (almost?) certainly an error in the Thrift service specification, it should be flagged and, I think, an error should be raised. This will prevent people (like me) from becoming the victim seemingly mysterious Thrift service failures due to silly cut & paste errors when editing specification files.",code_debt,low_quality_code
thrift,508,comment_0,"This is the same as THRIFT-570, which has a fix submitted. Should probably be closed duplicate.",non_debt,-
thrift,508,comment_1,"Duplicate of THRIFT-570, tested and verified that this is resolved.",non_debt,-
thrift,510,summary,segmentation fault in errorTimeWrapper,non_debt,-
thrift,510,description,"During scribe startup, thrift triggers a potential segmentation fault right after the first message that uses errorTimeWrapper: The problem is detected by the Stack Smashing Protection, which has been somehow enabled on my compiler. In errorTimeWrapper uses a 25 character long buffer as a placeholder for a call to ctime_r. According to the man page, ctime_r requires a 26 characters buffer. Changing the buffer size made everything work again.",non_debt,-
thrift,510,comment_0,"Sorry, I thought this got fixed a long time ago.",non_debt,-
thrift,510,comment_1,Hopefully none of those obsd patches broke anything.,non_debt,-
thrift,525,summary,ThriftTest project will not generate c# from .thrift file or generate ThriftImpl.dll,non_debt,-
thrift,525,description,"The C# Library Solution fails to compile because of problems with the PreBuildEvent for the ThriftTest Project - Thrift.exe is called with the outdated -csharp instead of --gen csharp - All of the existing commands in the PreBuildEvent will fail if there's a space in any directory name in the directory tree (such as ""My Documents"" or ""Visual Studio 2008""). - The recursive forced rmdir command will match other directory trees that share a common root (e.g. c:\test\Work\ will be recursively removed if the project is located in c:\test\Work for Thrift\test\csharp) I have attached a patch that replaces the PreBuildEvent in the ThriftTest project file to fix these problems and work with directory trees that contain spaces. As the thrift.exe compiler does NOT accept paths with spaces when surrounded by quotes, my script generates MSDOS 8.3 pathnames to pass to the thrift compiler and it appears to work just fine. This has only been tested on my machine with VS.net pro 2008 and a current thrift checkout",architecture_debt,using_obsolete_technology
thrift,525,comment_0,Initial version of patch described in original bug report,non_debt,-
thrift,525,comment_1,"+1 Patch looks good. Shouldn't effect the Mono/makefile users, but makes it a little easier to manage in Visual Studio. FYI, If you're using a Cygwin-compiled thrift.exe you'll need to make sure Cygwin is on your path (both before and after patch, but something I forgot when testing this, that mikew had to remind me about)",non_debt,-
thrift,525,comment_2,"I just committed this. Thanks for the patch, Michael!",non_debt,-
thrift,540,summary,Have set methods return reference to self,non_debt,-
thrift,540,description,Sometimes its useful to be able to make a single or string of modifications to a thrift object inline. Having set methods return a reference to itself allows for syntactic sugar like this:,non_debt,-
thrift,540,comment_0,I think this should do the trick. What do you think?,non_debt,-
thrift,540,comment_1,1,non_debt,-
thrift,540,comment_2,I just committed this.,non_debt,-
thrift,544,summary,multiple enums with the same key generate invalid code,non_debt,-
thrift,544,description,"The current generator produces multiple -define statements with the same name, which isn't valid erlang code (and also isn't valid semantically if we want two different values). produces: In the patched version, it produces this:",design_debt,non-optimal_design
thrift,544,comment_0,"This fixes the issue, although it does change the names defined, so it may break existing code, but I can't see any other way around this.",non_debt,-
thrift,544,comment_1,"This is an error in C as well, which is what Thrift's enums are based on. So I think the real solution is to throw an error at code generation time.",non_debt,-
thrift,544,comment_2,"I agree with David here - the breaking of backwards compatibility seems like too much to me to be worth it. On the other hand, I might support a patch which changes enum generation in erlang to use atoms like 'A' and 'B' instead of -defines at all. It's more erlangy - the bindings use -defines here for historical reasons. Since I'm no longer an active user of the bindings, though, I'll defer to Eugene, Chris, and David (or anyone else who uses them and wants to pipe up) for whether they think that's a good idea.",non_debt,-
thrift,544,comment_3,Here's a really simple patch that prevents multiple constants with the same name.,non_debt,-
thrift,544,comment_4,"I didn't try it out, but its very simple and looks good to me. +1",non_debt,-
thrift,544,comment_5,"I think the patch is correct, but the argument for correctness is very complicated because constants_[name] is a mutating operation that creates an entry if the ""name"" key is not present. I'd much prefer the condition be != constants.end()""",code_debt,complex_code
thrift,544,comment_6,I made the change that David suggested and committed this.,non_debt,-
thrift,544,comment_7,"This was committed as r982825. It doesn't show up in the ""all"" tab because it was committed with a tag of ""THRIFT-554"".",non_debt,-
thrift,554,summary,Perl improper namespace check for exception handling and writeMessageEnd missing on processor calls,code_debt,low_quality_code
thrift,554,description,"There are two more issues affecting the functioning of a Perl service server: 1. Failure to prepend the Perl namespace to the exception name when checking the exception type from a eval'ed method call. 2. writeMessageEnd() should be present after a method call writes its result. I'm attaching a patch which addresses these issues, in addition to the following more minor changes: 1. Tried to make indentation and line breaks more consistent to ensure readability of the generated code. 2. Added a few best practice ideas to improve the code in minor ways. 3. Added a readAll() function to the as the one found in Thrift::Transport uses a while loop to consume the data, which results in a endless loop.",code_debt,low_quality_code
thrift,554,comment_0,Committed thx.,non_debt,-
thrift,554,comment_1,"Note that r982825 was committed with a tag of ""THRIFT-554"", but it was really for THRIFT-544.",non_debt,-
thrift,568,summary,Thrift build on Ubuntu uses site-packages not dist-packages,non_debt,-
thrift,568,description,"When building thrift python libs on ubuntu, it places the files into site-packages instead of dist-utils",architecture_debt,violation_of_modularity
thrift,568,comment_0,This is available from an extra install option you can pass to the make file. Since debian added a patch to setuptools in python-setuptools (0.6c9-0ubuntu4) jaunty an option called is available which will install packages into the dist-packages directory. From within the lib/python directory run: make install,non_debt,-
thrift,593,summary,Thrift Perl Client Library Performance Improvement patch,design_debt,non-optimal_design
thrift,593,description,"Perl client library for Thrift is bit slow.This implementation can make 24.75 QPS. I wrote a I/O buffering patch for this implementation.This makes 147x faster the echo server. The patch is here: regards,",code_debt,slow_algorithm
thrift,593,comment_0,oops. I found,non_debt,-
thrift,595,summary,in Java,non_debt,-
thrift,595,description,None,non_debt,-
thrift,595,comment_0,I made some slight cosmetic changes to your v6 patch to get this.,design_debt,non-optimal_design
thrift,595,comment_1,I just committed this.,non_debt,-
thrift,597,summary,Python THttpServer performance improvements,code_debt,slow_algorithm
thrift,597,description,"This class was originally meant for functional testing only, so performance wasn't a concern. But now I'm using it for load testing. :) Two patches here. The first enables buffered I/O. The second allows the http server class to be specified, which allows users to use the ThreadingMixin.",code_debt,slow_algorithm
thrift,597,comment_0,I just committed this.,non_debt,-
thrift,597,comment_1,"One query: I want to use THttpServer in my python project but from initial performance review for a single client single request (No ThreadingMixIn required at this point), it shows that HttpServer is always 2 to 4 times slower than NonBlocking server in tutorial Calculator client server app and same was observed in my product testing as well. Is there a way to make the performance equal for THTTPServer as compared to NonBlocking? Heres the code for NonBlocking client and server in python and its performance on ubuntu. single machine.  In my project I observed that on client side, its the generated code send_<api Will really appreciate if someone can shed some light on how to improve performance for THttpServer in thrift 0.8.0.",code_debt,slow_algorithm
thrift,605,summary,error compiling struct.c with ruby 1.9,non_debt,-
thrift,605,description,"When trying to compile a newly checked out version of thrift from svn, after switching ruby to 1.9.1, I get this error: removing -Werror from the Makefile allows the build to finish, although I haven't tested compact_protocol at runtime.",non_debt,-
thrift,605,comment_0,Duplicate of THRIFT-664.,non_debt,-
thrift,612,summary,readFrame being called out of scope,non_debt,-
thrift,612,description,"ERROR:root:global name 'readFrame' is not defined Traceback (most recent call last): iprot.trans, (self.__class__, self.thrift_spec)) File line 301, in cstringio_refill readFrame() NameError: global name 'readFrame' is not defined",non_debt,-
thrift,612,comment_0,+1 Looks good to me. Do you have test so that we make sure there are no regressions in the future?,test_debt,lack_of_tests
thrift,612,comment_1,"How did this happen? It's an obvious bug/fix, but this path shouldn't actually be triggered under normal operation.",non_debt,-
thrift,612,comment_2,That's why I was asking for a test before committing the patch.,test_debt,lack_of_tests
thrift,612,comment_3,Sorry for stepping on your toes. :(,non_debt,-
thrift,612,comment_4,"I wasn't actually complaining at all, sorry if I sounded like that :-) I'm just curious how could that bug be introduced and we never noticed. You're completely right that that path shouldn't execute in normal circumstances, so it's even weirder.",non_debt,-
thrift,612,comment_5,I'll look into it with one of our engineers who ran into the issue. I think it might be related to the fact that we had to use THRIFT-347 recently.,non_debt,-
thrift,612,comment_6,"NP. I know how the bug was introduced: I'm used to writing C++ code, and not needing a ""self."" for each method call. I only tested this code in the context of a normal method call.",non_debt,-
thrift,617,summary,Add EINTR handling to Python socket code,non_debt,-
thrift,617,description,One of my thrift services does special handling upon receiving SIGHUP. This causes spurious exceptions about EINTR to be logged. Attached patch adds EINTR retry logic akin to the C++ implementation to handle this.,code_debt,low_quality_code
thrift,617,comment_0,"This was resolved inside python (if you use python 3.4 or later, I believe):",non_debt,-
thrift,620,summary,"Compact Protocol should call readAll, not read",non_debt,-
thrift,620,description,"TCompactProtocol's current implementation is calling trans_.read instead of trans_.readAll. As a result, if you are using a buffer, or something like a GZipInputStream, you can get an underrun on your read unexpectedly, desynchronizing the stream.",non_debt,-
thrift,620,comment_0,This fixes the problem.,non_debt,-
thrift,620,comment_1,I just committed this.,non_debt,-
thrift,628,summary,Hash code method for _Fields objects does not behave as expected,non_debt,-
thrift,628,description,"It turns out that Enums in Java don't have good hashcode behavior. It uses object ID, which can be inconsistent between different invocations of the same application, which breaks things like Hadoop partitioning. We should use the hash of the actual thrift field id instead of the hash of the enum version of the field.",design_debt,non-optimal_design
thrift,628,comment_0,"This should do the trick. Note that the only change is to Union structs, since that's the only type of struct that uses the field id in the hashcode.",non_debt,-
thrift,628,comment_1,1,non_debt,-
thrift,628,comment_2,I just committed this.,non_debt,-
thrift,635,summary,CamelCase methods in Java code generator,non_debt,-
thrift,635,description,"thrift --gen java:camel doesn't generate idiomatic names for getters and setters. For instance: struct SlicePredicate { 1: optional list<binary 2: optional SliceRange slice_range, } has generated getters and setters: getSlice_range setSlice_range",code_debt,low_quality_code
thrift,635,comment_0,"I like the idea of this patch - harmonizing camel vs nocamel modes - but I'm not sure that we should edit people's field names. My thinking has been that the user is quite capable of choosing a capitalization scheme for their field names on their own, and that the point of the nocamel mode was to stop the compiler from assuming otherwise. Does anyone else think that we should enforce camelCased accessors?",non_debt,-
thrift,636,summary,"Create an ""include"" annotation",non_debt,-
thrift,636,description,"The intention of this idea is to minimize the amount of typing necessary to navigate thrift structures. The idea is to be able to ""include"" a field within a struct, like so (I don't know the syntax of Thrift annotations but this is the idea): struct B { 1: required i32 f1; 2: required i32 f2; } struct A { 1: @include required B b; 2: required i32 field2; } If we have an instance of A named ""a"", we can access the inner B's fields by saying ""a.get_f1()"". There's the obvious problem of name conflicts, but I think it's fine to leave it to the programmer to make sure the code is safe.",design_debt,non-optimal_design
thrift,636,comment_0,So you are proposing that the object structure would stay the same and we would just add extra methods to save you from typing a.get_b().get_f1() (or a.b.f1 if you are not using the beans style)?,non_debt,-
thrift,636,comment_1,"Yes, exactly. And I suppose the setters could automatically create an instance of the underlying object if it doesn't exist already.",non_debt,-
thrift,636,comment_2,"I think this would be somewhat clunky and inconsistent to use. If you'd like to submit a patch, feel free to reopen.",design_debt,non-optimal_design
thrift,639,summary,"Timeouts Interrupting Read Can Leave Data in Read Queue, Corrupting Later Reads",non_debt,-
thrift,639,description,"When a Timeout interrupts a client that is reading a Thrift response, the client may leave unread bytes in the read queue. If this transport instance/queue is reused in a later request, the extra bytes will corrupt that later response. We're currently working around this by having the rescue blocks of our TimeoutExceptions close the transport so that subsequent requests will have to create a new, clean one.",code_debt,low_quality_code
thrift,639,comment_0,What transport are you using?,non_debt,-
thrift,639,comment_1,"I think any of the transports can experience this problem. If you stop reading when there's still data waiting to be read, then go on with writing, there will still be data to be read, but it'll be from the previous response.",non_debt,-
thrift,639,comment_2,"I think you're supposed to throw away your and re-create them when you get an error (and timeouts are a kind of error), because an error can be raised at any time and leave your in an undefined state. At least, that's what Twitter's thrift_client gem (higher-level client on top of Thrift) does and it works pretty well in my experience.",non_debt,-
thrift,639,comment_3,"I guess what I'm saying is, I don't think this is a bug, this behavior is by design. Right?",non_debt,-
thrift,653,summary,Fix toString method for enums,non_debt,-
thrift,653,description,"Now that enums are actually enums, their toStrings are acceptable in the overall struct toString. We should remove the special case we built in for this in the past.",code_debt,dead_code
thrift,653,comment_0,I think this should do it.,non_debt,-
thrift,653,comment_1,I just committed this.,non_debt,-
thrift,654,summary,What is the best way to merge a bunch of code from Facebook?,non_debt,-
thrift,654,description,"Hey guys. I've been kind of lax porting patches developed at Facebook back into the open-source repository, but I'm catching up now. I have a bunch of Apache-ready patches at (long view: It's almost all C++ library changes, with some other misc stuff sprinkled in. What do you guys think would be the best way of merging this stuff into Apache? I'd prefer not to create a separate issue for each C++ patch, especially since they've all been reviewed and production-tested internally. I can do it for the non-C++ stuff if you'd like. Would people be okay if I just create one big patch set for the C++ stuff, attach it here, and commit after a week if there are no objections?",non_debt,-
thrift,654,comment_0,"Is it possible to rebase -i them into a smaller number of cohesive patches? I'd be fine reviewing a single JIRA of ""Miscellaneous bug fixes for python"", but I don't think multiple new features should be conflated into the same JIRA.",non_debt,-
thrift,654,comment_1,I would think that at the very least you should summarize what changes you're making so that interested reviewers know what they're scanning.,non_debt,-
thrift,654,comment_2,Okay. I've created individual issues for everything that isn't C++. I'll create a single issue with a summary of the C++ stuff and we'll see how it looks.,non_debt,-
thrift,654,comment_3,Are you still waiting to merge in these changes?,non_debt,-
thrift,654,comment_4,Yeah. I just asked if anyone cared if I commit THRIFT-665. I was going to check them all in tonight (the others are approved or trivial).,non_debt,-
thrift,654,comment_5,"I'm assuming this has actually been done by now. If that's incorrect, please do it and retag the issue as 0.5.",non_debt,-
thrift,655,summary,Make it possible to typedef a struct type,non_debt,-
thrift,655,description,None,non_debt,-
thrift,655,comment_0,"I assume this is obsolete, since 0.91 will process an IDL file that is using typedefs for struct types? On the other hand, the grammar at does not mention the possibility of referring to a struct type in a typedef, as far as I can see.",non_debt,-
thrift,667,summary,Period should not be allowed in identifier names,non_debt,-
thrift,667,description,"I had an enum with a value named ""BLAH.SOMETHING"", which the compiler happily consumed, outputting invalid Java code. Period should not be allowed in names like this.",non_debt,-
thrift,667,comment_0,"+1, thanks again Jens",non_debt,-
thrift,667,comment_1,Committed.,non_debt,-
thrift,673,summary,Generated Python code has whitespace issues,code_debt,low_quality_code
thrift,673,description,The Python code generator produces code with a number of whitespace issues: - Trailing whitespace at the end of lines. - Multiple blank newlines at the end of files. Both of these issues cause problems if the code is used in a Git repository with `git diff --check' in a hook. The attached patch corrects the code generation to address these issues.,code_debt,low_quality_code
thrift,673,comment_0,Updated patch which catches one more case of trailing whitespace.,code_debt,low_quality_code
thrift,673,comment_1,I just committed this.,non_debt,-
thrift,678,summary,HTML generator should include per-field docstrings,code_debt,low_quality_code
thrift,678,description,"Other docstrings are included in the output, but these aren't for some reason.",non_debt,-
thrift,678,comment_0,"Trivial fix. Unless there objections, I will commit this shortly.",non_debt,-
thrift,678,comment_1,I just committed this.,non_debt,-
thrift,685,summary,Direct buffer access to improve deserialization performance,design_debt,non-optimal_design
thrift,685,description,"After poking around a bit and comparing how Thrift performs versus Protocol Buffers, I think we should change our transports and protocols to support optional direct buffer access behavior. Basically, the way this works is that if the transport is backed by a buffer, then it can give access to that buffer to the protocol. The protocol can then do things like read a byte without instantiating a new one-byte array or decode a string without an intermediate byte[] copy. In my initial testing, we can reduce the amount of time it takes to deserialize a struct by at least 25%. There are probably further gains to be had as well.",design_debt,non-optimal_design
thrift,685,comment_0,"+1. The template code that Chad and I worked on for C\+\+ (almost ready for trunk) gets the complier to do something similar for us, and we saw a really significant boost.",design_debt,non-optimal_design
thrift,685,comment_1,"Here's my initial effort. This produces a pretty noticeable speedup in deserialization time when using the compact protocol and TDeserializer. This patch notably does not yet address the binary protocol implementation. I'd love some people to review it and give me feedback. Ideally, we'd commit what I have here as a starting point and then continue to improve other parts.",design_debt,non-optimal_design
thrift,685,comment_2,I've committed this patch. I'll open follow-on tickets to improve other parts of the codebase.,non_debt,-
thrift,686,summary,TMemoryBuffer missing from Cocoa library,non_debt,-
thrift,686,description,The Cocoa library has no implementation of TMemoryBuffer found in other language libraries.,non_debt,-
thrift,686,comment_0,A patch to add TMemoryBuffer to the cocoa libraries,non_debt,-
thrift,686,comment_1,Patch applied,non_debt,-
thrift,694,summary,Python,non_debt,-
thrift,694,description,Having classes ala Java and Ruby is useful. Here's a patch with an implementation for python.,non_debt,-
thrift,694,comment_0,Submitted it again after I got a server error... check THRIFT-695,non_debt,-
thrift,695,summary,Python,non_debt,-
thrift,695,description,Having classes ala ruby and java is useful. Python should have its own equivalent.,non_debt,-
thrift,695,comment_0,implementation for python,non_debt,-
thrift,695,comment_1,"This seems like a very faithful translation of the Java version, which has benefits and drawbacks. I'm okay committing it, but the patch seems corrupt (""%ld"" where there should be a line number).",design_debt,non-optimal_design
thrift,695,comment_2,"Looks like the macports ""svn diff"" is buggy on snow leopard. I've attached an updated patch with the correct line numbers.",non_debt,-
thrift,695,comment_3,"Here's a simpler to use, more pythonic version.",code_debt,complex_code
thrift,695,comment_4,Can we commit one of these patches? Whichever one you think fits in better with the rest of python thrift is fine with me.,non_debt,-
thrift,701,summary,Generated classes take up more space in jar than needed,build_debt,build_others
thrift,701,description,"In glancing at one of our jars of generated Thrift class files, I noticed that it was pretty huge. Further digging showed that while certainly most of the bulk was attributable to the number of class files we have, part of it was due to lots of unnecessary internal classes. This is because we use a specific syntax for defining some maps inline in structs and enums. Since this is only syntactic sugar, I think it would make sense to write a tiny bit of helper code and avoid the need for a dynamic internal class.",design_debt,non-optimal_design
thrift,701,comment_0,I just committed a fix for this in r926466.,non_debt,-
thrift,702,summary,TUnion's toString method throws NPE if the union is unset,non_debt,-
thrift,702,description,None,non_debt,-
thrift,702,comment_0,This patch adds a small test case and a fix. I'll commit this shortly.,non_debt,-
thrift,710,summary,TBinaryProtocol should access buffers directly when possible,non_debt,-
thrift,710,description,TBinaryProtocol can probably take even more advantage of direct buffer access than TCompactProtocol. Identify the relevant spots and clean it up.,design_debt,non-optimal_design
thrift,710,comment_0,"This patch makes TBinaryProtocol use direct buffer access in the relevant methods. My performance testing was somewhat rudimentary, but I think it may have as much as doubled performance. Obviously your performance boost will be really dependent on the contents of your struct, but this seems pretty great. As a side effect of this issue, I refactored the TCompactProtocol test so that we could exact TBinaryProtocol to the same bevy of test cases.",code_debt,slow_algorithm
thrift,710,comment_1,I just committed this.,non_debt,-
thrift,714,summary,maxWorkerThreads parameter to THsHaServer has no effect,non_debt,-
thrift,714,description,"THsHaServer instantiates its ThreadPoolExecutor with a That behavior is documented in java as: There are three general strategies for queuing: ... 2. Unbounded queues. Using an unbounded queue (for example a LinkedBlockingQueue without a predefined capacity) will cause new tasks to wait in the queue when all corePoolSize threads are busy. Thus, no more than corePoolSize threads will ever be created. (And the value of the maximumPoolSize therefore doesn't have any effect.) This may be appropriate when each task is completely independent of others, so tasks cannot affect each others execution; for example, in a web page server. While this style of queuing can be useful in smoothing out transient bursts of requests, it admits the possibility of unbounded work queue growth when commands continue to arrive on average faster than they can be processed. therefore changing maxWorkerThreads (passed as maximumPoolSize) has no effect. The parameter should probably just be removed and minWorkerThreads renamed to numThreads, since setting minWorkerThreads does have an effect and is a workaround.",design_debt,non-optimal_design
thrift,714,comment_0,"Hm, good catch. I'll work up a fix when I have a moment.",non_debt,-
thrift,714,comment_1,"After reading up on it some more, I think this change is what we actually want. Up until the min number of threads, new ""core"" pool threads will be created. Once we pass this limit, new threads will be created up to the max number of threads, at which point the server will start rejecting executions. After (default) 60 seconds of idleness, the pool will start to kill threads, but go no lower than the min number of threads. This patch is probably incomplete, as if we get a rejected execution exception when trying to queue an invocation, we should respond to the client with an error immediately.",code_debt,low_quality_code
thrift,714,comment_2,"I'd rather have min threads = max threads, than reject requests just because we had a brief spike to more than min (queue size) + max (thread count). Document it and it's a feature. :)",non_debt,-
thrift,714,comment_3,I committed a patch to remove maxWorkerThreads and rename minWorkerThreads to workerThreads. This will keep the behavior the same and remove the ineffectual parameter.,code_debt,dead_code
thrift,716,summary,Field names can conflict with local variables in code for unions,code_debt,low_quality_code
thrift,716,description,"Try creating a union with the field name ""value"", and the code won't compile. In writeFields for the generated class, you'll have something like the following: <code case VALUE: String value = return; </code ""String value"" conflicts with the parameter ""Object value"".",code_debt,low_quality_code
thrift,716,comment_0,"That would happen, wouldn't it? The solution is just to rename the internal value var to something like __value__. Feel like submitting a patch?",non_debt,-
thrift,716,comment_1,"How about ""_value""? Thrift identifiers shouldn't be starting with underscores.",non_debt,-
thrift,716,comment_2,"In looking at this issue, I realized that there's actually no reason to make the value to write a parameter to writeField, since the generated class will have access to the protected value variable in the base class. This patch reflects the removal of the value (and set field) parameters from the signature of writeField and updates to the generator accordingly. What do you guys think?",non_debt,-
thrift,716,comment_3,Looks fine to me if it works.,non_debt,-
thrift,716,comment_4,I just committed this.,non_debt,-
thrift,717,summary,Global variables should not be used for configuration of PHP library,code_debt,low_quality_code
thrift,717,description,"The Thrift PHP library makes gratuitous use of the $GLOBALS array to store basic configuration. Globals in PHP are generally bad practice, so I suggest something else: Use constants. Being immutable, constants are more secure than globals (that could be overwritten in scripts susceptible to injection attacks); they also perform much better, since the $GLOBALS variable is a hash-table, lookups are comparatively expensive. I will attach a patch soon unless anyone has any better ideas.",code_debt,low_quality_code
thrift,717,comment_0,1,non_debt,-
thrift,717,comment_1,Patch to change the global variable to the THRIFT_ROOT constant.,non_debt,-
thrift,717,comment_2,"Added patch to change the THRIFT_ROOT to a constant. It can be defined manually, or intelligently derived by including the base Thrift.php: require_once // or require_once",non_debt,-
thrift,717,comment_3,"Well, I can't accept this patch because it doesn't update every use of THRIFT_ROOT. However, I'm not convinced that this would be a good approach even if the patch were complete. I think the security argument is completely pointless, since any attacker capable of injecting PHP code into your execution would not need to mess with Thrift to take over your server. The performance argument is also not compelling, since the cost of a single global lookup is tiny compared to the cost of including a library file (even with APC (or HipHop)). Finally, I think that leaving the value in a global gives users more freedom when setting up their environment. It is easy to determine if it is already set and easy to fix after-the-fact if you need to hack around something in your sandbox.",design_debt,non-optimal_design
thrift,717,comment_4,I agree with David on all points above. the globals lookup is meaningless for performance next to the multi-millisecond RPC you're about to send out :),design_debt,non-optimal_design
thrift,717,comment_5,"I agree, there is no win here.",non_debt,-
thrift,717,comment_6,"Ok, sure, the performance and security issues are fairly pointless. The biggest problem here is pollution of the global namespace, which is a bit of a show-stopper when integrating in to larger frameworks or libraries. Constants work because they don't pollute the $GLOBALS super-global and are immutable, so other system components can't accidentally modify them (i.e. through naming clashes) without the developer being alerted to it. I thought I'd got all references to I take it some reside outside of lib/php/src/ ?",design_debt,non-optimal_design
thrift,717,comment_7,"Constants pollute a different global namespace, so I don't see the win. Plus, the likelihood of a collision with ""THRIFT_ROOT"" is small. Yes, immutability protects you from another library messing you up by running later, but exposes you to another library messing you up by running sooner, so I don't see the win. Plus, it reduces your flexibility with how you choose to set THRIFT_ROOT. Yes, there are references in the generated code, created by the compiler.",design_debt,non-optimal_design
thrift,717,comment_8,I've been convinced that there's no win to be had here. Closing.,non_debt,-
thrift,728,summary,Make generated Haskell code an instance of Arbitrary,non_debt,-
thrift,728,description,"The patch * Generates Arbitrary instances for Thrift structs, enums and exceptions * Provides Arbitrary instances for GHC's Int64, Data.Map.Map and Data.Set.Set * Makes Thrift enums instances of Bounded and improves the Enum instance declaration Making a type an instance of specifies how to generate random instances of the struct. This is useful for testing. For example, consider the following simple Thrift declaration: With the patch, the following program (import statements elided) is a fuzzer for the log service. In implementing the Arbitrary instances, it was useful to make Thrift enums instances of Bounded and to improve the Enum instance. Specifically, whereas before, would throw an exception, now it behaves as expected without an exception. I consider the patch incomplete. It's more of a starting point for a discussion at this point than a serious candidate for inclusion. If it is of interest, I'd appreciate some direction on testing it as well as style, and I'd welcome any other comments or thoughts.",code_debt,low_quality_code
thrift,728,comment_0,The patch mentioned in the issue body text.,non_debt,-
thrift,728,comment_1,"Include's Iain's feedback and a fix replacing use of the variable ""rec"" with ""record"" to allow compilation on GHC 6.12",non_debt,-
thrift,728,comment_2,looks good,non_debt,-
thrift,728,comment_3,"Aran, I think that Thrift.Arbitraries is missing from your patch.",non_debt,-
thrift,728,comment_4,This is also resolved my 's recent patch. haskell code: thrift: cc:,non_debt,-
thrift,730,summary,configure should check for g++ before checking for boost,non_debt,-
thrift,730,description,"The configure script that's generated fails the ""checking for boost"" part if g++ is not installed. This confuses people into thinking something's wrong with the boost installation, whereas in fact missing g++ is the issue. A g++ check should be moved in front of the boost check.",design_debt,non-optimal_design
thrift,730,comment_0,Something like this should do the trick:,non_debt,-
thrift,730,comment_1,Dupe of THRIFT-381,non_debt,-
thrift,742,summary,Compile error (Thread::id_t not fully qualified),non_debt,-
thrift,742,description,"Compile error, fixed by fully specifying a type. Patch: diff --git index e48dce3..ce411ce 100644  +++ @@ -281,7 +281,7 @@ class { Thread::id_t const { // TODO(dreiss): Stop using C-style casts. - return + return } };",non_debt,-
thrift,742,comment_0,Dupe of THRIFT-214. Already fixed in trunk.,non_debt,-
thrift,750,summary,C++ Compiler Virtual Function Option,non_debt,-
thrift,750,description,"The C++ Compiler currelty emits most functions in the *Client class as non-virtual. This makes it difficult to subclass the generated *Client class and override its functions. In particular, if a subclass overrides the *_send and *_recv functions, it must also override the function itself. Otherwise, the *Client version of the function calls the *Client versions of *_send and *_recv. A workaround is to inherit from the interface class *If, which has virtual functions, and use them to call *Client class member functions. But this can be cumbersome in some situations, and still requires additional functions to be overridden. I propose to add a virtual option to the C++ compiler that emits function declarations as virtual. I have attached a patched version of t_cpp_generator.cc from Thrift 0.2.0 - I can work out how to turn it into a patch file if needed. Is this worth merging into the trunk?",design_debt,non-optimal_design
thrift,750,comment_0,Attached patched t_cpp_generator.cc from Thrift 0.2.0,non_debt,-
thrift,750,comment_1,"You know that the ones inherited from the *If are automatically virtual, even though they are not explicitly declared as such, right?",non_debt,-
thrift,750,comment_2,"Yes, the *If functions are fine. It's the member functions in the *Client class that aren't virtual, meaning that if the *_send or *_recv functions are overridden in a subclass, the original function needs to be overridden in the subclass to call the new versions. I've tried to clarify the description.",non_debt,-
thrift,750,comment_3,"Okay. This seems acceptable. There is a little bit of a performance hit, but it is not major. Can you attach a patch?",code_debt,slow_algorithm
thrift,750,comment_4,Edit: this patch was broken,non_debt,-
thrift,750,comment_5,Corrected patch against thrift-0.2.0 The original patch was broken Please let me know if a patch against svn would be easier.,non_debt,-
thrift,750,comment_6,has been outstanding for 7 years; recommend Won't Fix. We can re-open it as a new item against the current master if needed.,non_debt,-
thrift,758,summary,incorrect deference in exception handling,non_debt,-
thrift,758,description,"This is a nasty error that is triggered during exception, leaving the user clueless about the error since she gets a Perl error instead: ""Can't use string (""0"") as a SCALAR ref while ""strict refs"" in use"" For instance:",non_debt,-
thrift,758,comment_0,Please see patch in my github fork:,non_debt,-
thrift,758,comment_1,"Hi Yann, If you want this patch applied, then you'll need to attach it to the JIRA this is the ASF protocol and will ensure there are no licensing issues. Thanks!",non_debt,-
thrift,758,comment_2,Done! Thanks Jake.,non_debt,-
thrift,758,comment_3,"Oh sorry, could you include a test case for this under lib/perl/test please? Thanks!",test_debt,lack_of_tests
thrift,758,comment_4,"Jake, I unfortunately have no clue how to generate a with the provided ThriftTest. I didn't find a way to run the tests automatically either. I manually executed: perl -Iblib/lib -Ilib -Itest/gen-perl test/processor.t Thanks, Yann",test_debt,lack_of_tests
thrift,758,comment_5,I just committed this patch.,non_debt,-
thrift,771,summary,Publish JMX metrics from Thrift servers,non_debt,-
thrift,771,description,It'd be great if we tracked metrics about the operation of our Thrift servers and published them via JMX. This would allow standard monitoring tools to track Thrift servers.,non_debt,-
thrift,771,comment_0,"Linked to THRIFT-2927, if something happens in this area the link will help people decide what to do.",non_debt,-
thrift,784,summary,make install target broken when erlang bindings enabled,non_debt,-
thrift,784,description,"After thrift-646 was committed, `make install` fails. It looks like the wrong variable is being passed to install.",non_debt,-
thrift,784,comment_0,Sorry about that.,non_debt,-
thrift,788,summary,thrift_protocol.so: does not handle more than 17 keys correctly,non_debt,-
thrift,788,description,thrift_protocol.so does not handle with more than 17 keys correctly. The query time sky rockets from 10ms to a steady 900ms starting with 18 keys. It doesn't matter if you fetch 18 or 50. Solid 900ms... The bug can easily + steadily be reproduced: - standard Keyspace1 / CF Standard1 setup - name: Keyspace1 replication_factor: 1 column_families: - name: Standard1 compare_with: BytesType - insert e.g. 1000 columns email:value / with key = 1...1000 - - $client- - - $client- - - $client- It does return the right columns though. It can easily be fixed: disable thrift_protocol.so ;) This might be a 32bit microseconds issue. Still very strange though.,non_debt,-
thrift,788,comment_0,"Refining my notes bellow according to the comment of original creator of this issue: Mine is happening to SuperColumnFamily with 9 or more column per row. Moving discoveries from CASSANDRA-1199 to here: I am comparing the following * Reading of 100 SuperColumns in 1 SCF row using multiget_slice() with 1 key and 1 column name in 100 loop iterations * Reading of 100 SuperColumns in 1 SCF row using multiget_slice() with 1 key and 100 column names in a single call I always get a consistent result and that is the single call takes more time then 100 calls. After some investigation, it seamed that the time it took to execute multiget_slice with 100 columns is always close to the TSocket- This only happens if is used. I have attached my code to reproduce this issue. You can set the timeouts to see how it affects the read call in multiget_slice. Please investigate. This a timing example for the above scenario with TSocket's default timeouts: 100 Sequential Writes took: 0.4047749042511 seconds; 100 Sequential Reads took: 0.16357207298279 seconds; 100 Batch Read took: 0.77017998695374 seconds;",documentation_debt,low_quality_documentation
thrift,788,comment_1,Thrift code including cassandra schema creation and the scenario calls.,non_debt,-
thrift,788,comment_2,"I had another observation: if you increase your TSocket read buffer size to few KBs, then the TSocket->read will not timeout in the last call.",non_debt,-
thrift,788,comment_3,"It just came to me THRIFT-638. This might be related to the same issue because I have studied the output of stream_get_meta and it stops at some point, then resumes all of a sudden. So, to me it makes sense that when multiget_slice returns large results, the read if TSocket binded to C lib had to read from stream many times and faces the issue in THRIFT-638.",non_debt,-
thrift,788,comment_4,"This sounds like a duplicate of #638 which has been fixed. If you still experience this issue post Thrift 0.7, please re-open with more information.",non_debt,-
thrift,790,summary,Generated makefiles should not be distributed in release tarballs,non_debt,-
thrift,790,description,Our release tarballs include some generated Makefiles because directories using Automake are included in EXTRA_DIST. The proper solution is to make those directories specify their own EXTRA_DIST.,non_debt,-
thrift,790,comment_0,THRIFT-841 is a more complete description of this problem.,non_debt,-
thrift,813,summary,"Getting URL encoded strings back on the server side for example: (""@"" becomes ""%40"")",non_debt,-
thrift,813,description,"Hello, Please help me understand what is going on. I'm using the js client for thrift and making calls to a java-bean thrift server. In the server implementation, I examine the strings that are passed through a service call, and they are URL encoded. For example. hi@bye.com would be hi%40bye.com Where is this happening. I'm guessing it's at the TJSONProtocol. I expect many characters to be percent escaped like this as it's coming across the wire, but once it gets to the server code it should be back in plain ol' string land.",non_debt,-
thrift,813,comment_0,"It could be in Java's TJSONProtocol implementation. I'm not super familiar with that implementation, so I don't know where to look off hand, but maybe you could add a test case that exercises this?",test_debt,lack_of_tests
thrift,813,comment_1,"Bryan, Where would I look in order to debug what's going on? What part of the TJSONProtocol would this be in. Also, I've seen several bugs (I think) and I'd like to learn some about the code base. Are there any ramp up docs for contributors?",documentation_debt,low_quality_documentation
thrift,813,comment_2,"I have no idea - TJSONProtocol.java is a big file, and I didn't write it. personally, I'd just grep around for any occurrence of the @ sign or something that indicates URL-encoding is going on. Sadly, not really. It's a bit of a jungle in there. The very first thing to do in every case is to open a JIRA ticket so that we have a record of the bug. From there, either get out your machete and safari hat and dig in, or try to find someone who can fix it for you.",code_debt,complex_code
thrift,813,comment_3,I wrote TJSONProtocol. There may certainly be some bugs but I don't think this URL encoding issue is coming from TJSONProtocol -- I am pretty sure there is no URL encoding done inside TJSONProtocol -- see JSON spec at www.json.org.,non_debt,-
thrift,813,comment_4,"Hi Jordan, This too is most a browser side encoding issue. The js implementation of writeString uses I suppose this isn't consistent, with the non javascript protocol readers. but the js readString uses We could implement our own encoder to encode only JSON special chars. -Jake",design_debt,non-optimal_design
thrift,813,comment_5,"Jake, What you're saying sound correct, because when I pass the string to the server, and then back to the browser, it's back to the original form. The only problem, is that when the string shows up on the server, its encoded. We need it automatically decoded on the server, which is why I suspected TJSONProtocol. URLEncoding is not part of the json spec (only quote escaping, I believe). It's just that to get the data across the wire in post form, we need to encode it (i think.)",non_debt,-
thrift,813,comment_6,clone of THRIFT-885 which is resolved,non_debt,-
thrift,813,comment_7,"is a clone of THRIFT-885, which is resolved",non_debt,-
thrift,818,summary,Async client doesn't send method args,non_debt,-
thrift,818,description,It turns out there was a stupid mistake in the unit tests for TAsyncClientManager that was preventing us from noticing that method args weren't getting sent to the server. The actual problem turned out to be an order of events issue that's easily fixed.,non_debt,-
thrift,818,comment_0,This patch fixes both the test and the underlying code.,non_debt,-
thrift,818,comment_1,I just committed this.,non_debt,-
thrift,840,summary,Perl protocol handler could be more robust against unrecognised types,code_debt,low_quality_code
thrift,840,description,"The Perl protocol implementation can loop arbitrarily on corrupt data because Protocol::skip() skips nothing if it doesn't recognise a type. It might be nice if it threw an exception instead. For example, the loop to skip a map will keep looking at the same invalid bytes for every iteration if it hits an unrecognised type. If corrupt data has resulted in a bogus huge map size, then the loop goes on for a long while, rather than dying quickly after running out of available data. I recognise that input validation probably isn't a priority for Thrift (usually communications are well-formed!), but wonder if you'd consider the attached patch which dies if skip or skipBinary encounter types that they don't know what to do with. It probably needs more thought than I've given it, as I'm not sure what the correct behaviour should be for valid types not handled by the existing code: in the patch I'm throwing an exception for those saying that they cannot be skipped. An additional TType constant of MAX_TYPE might be helpful in writing a better solution. I know it's a bit weird to be suggesting this; I'm using Thrift for serialisation among other things, and with an data store. This ""infinite"" loop was the only instance in which your existing error handling didn't adequately flag up bad data. Clearly I should also be doing checksums on the data beforehand, but I just thought I'd suggest this to you. Thanks, Conrad",design_debt,non-optimal_design
thrift,840,comment_0,Throw exception instead of silent return on unrecognised type.,code_debt,low_quality_code
thrift,840,comment_1,"I'm no Perl guy, but it seems like there's no reason to have two different kinds of ""die"" cases. The one with the if seems to be subsumed by the one without - am I wrong?",design_debt,non-optimal_design
thrift,840,comment_2,"No, you're exactly right. I separated them out because I thought it might be informative in case it turns out that all those valid types for which there's no skip code should just return 0. I don't know what should be done for them really, but looking at the Java skip code they aren't handled there either. It appears to me that you can only reach the end of that method in error, so any exception will do as far as I'm concerned. Conrad -- The University of Edinburgh is a charitable body, registered in Scotland, with registration number SC005336.",non_debt,-
thrift,840,comment_3,I committed this with the redundant parts removed. Thanks for the patch!,code_debt,duplicated_code
thrift,841,summary,Build cruft,build_debt,build_others
thrift,841,description,"The following files are shipped in the binary release artifacts, (despite being removed in the clean target). This creates problems for packaging where it is typical to import to the binary releases into a vcs alongside the packaging artifacts. Also, the following is generated during a build, but not removed on a clean.",build_debt,build_others
thrift,841,comment_0,"Are generated by yacc, and it's customary to include these in release tarballs so that users building the package don't need the tool installed. I'll take a look at what other packages do with respect to ""make clean"". I'll try to take care of the other files.",non_debt,-
thrift,841,comment_1,"Sorry, I meant to say that thriftl.cc is generated by lex and the other two are yacc. lib/erl/Makefile and lib/php/Makefile should be fixed in trunk. I'm not sure what is the most maintainable way to handle the others.",non_debt,-
thrift,841,comment_2,This has been taken care of in trunk,non_debt,-
thrift,871,summary,Thrift compiler for WIndows (binary distribution),non_debt,-
thrift,871,description,"It would be a great benefit to have a Thrift compiler supporting all Languages running on Windows. Just provide a Download option for a compiler binary on the Thrift Website for each release, ready to use directly on Windows without many dependencies (e.g. cygwin). So, our C# guys would be much faster on board;-)",non_debt,-
thrift,871,comment_0,THRIFT-874 already has some patches attached,non_debt,-
thrift,871,comment_1,I'll be producing these with the cross-compile script starting with 0.5.0,non_debt,-
thrift,873,summary,Java tests fail due to Too many open files,non_debt,-
thrift,873,description,"All of the tests run in the same JVM, and it seems like is leaking sockets. As a quick fix, dropping that to use only 200 clients instead of 500, and changing each unit test to run in its own JVM instead of sharing them. Also allowing the port used for binding the test servers to be configured from the command line",code_debt,low_quality_code
thrift,873,comment_0,"Patched worked for me flawlessly. Tests were failing for me before, and now they all pass.",non_debt,-
thrift,873,comment_1,"I like all of this patch except for the separate JVMs. I know that's its probably a more responsible way to do things, but it makes the test run slower. Are you certain that this is what is necessary in order to make the test pass? Wouldn't it be possible for us just to clean up after ourselves in the test for",test_debt,expensive_tests
thrift,873,comment_2,"On my box it goes a bit slower, but still pretty fast. With forkmode=""once"" I did indeed see failures that went away with the separate JVMs. It also made it harder to figure out which test was actually at fault, since the problem with AsyncClientManager caused a bunch of later tests to fail too.",test_debt,expensive_tests
thrift,897,summary,Don't allow unqualified constant access to enum values,non_debt,-
thrift,897,description,"Through looking at THRIFT-544 and THRIFT-895, it's come to my attention that we currently register each of every enum's values as a global (and scoped) constant. This allows you to do things like: This is handy, insofar as you might want to use the values of an enum in constant or default circumstances. However, this behavior is unstable - if you have two enums with values that have the same name, all constant references will point at the last occurrence of the name. Further, in order to allow this to go on, we must not check if any constant has been declared twice, which means you can get stupid, detectable errors in your IDL very easily. I propose that we stop allowing this method of access, and instead require the enum values referenced in constant context to be prefixed with the enum type's name. For instance:",design_debt,non-optimal_design
thrift,897,comment_0,"I think this is a really simple change, but it will break existing files.",non_debt,-
thrift,897,comment_1,"YIkes. I'll defer to whatever people prefer. Seems we can either: (1) Require strongly-typed references to enums, as you propose (2) Perform enum-lookups in a strongly-typed manner (i.e. when we see ""const MyEnum"", we know the only valid values are those in the MyEnum type) The former seems but the latter is",non_debt,-
thrift,897,comment_2,"What would be the cost of doing type resolution? I think this might be the most convenient (and behavior, but I have no idea if it is feasible in the current architecture of the compiler. This is my proposal:",non_debt,-
thrift,897,comment_3,"For simple constants like the ones in your example, I think it would be reasonably easy. For things like container and structure contants containing enums, it would be harder.",non_debt,-
thrift,897,comment_4,"I think the smart resolution thing is only worthwhile if we really want to keep backwards compatibility. I'm not crazy about breaking people's IDL, but I think that's preferable in the long term to supporting a more convoluted syntax. If we were starting from scratch today, I think that we'd want to use the fully-qualified approach, and the cost of fixing the breakage *should* be reasonably low, especially because you're getting a consistency improvement at the same time. This is certainly my preference.",design_debt,non-optimal_design
thrift,897,comment_5,This patch enforces fully qualified enum value references.,non_debt,-
thrift,897,comment_6,"LGTM. In the main.cc part, you could possibly check the part before the dot matches the name of the enum (possibly with the scope qualifier). Not critical.",code_debt,low_quality_code
thrift,897,comment_7,I just committed this.,non_debt,-
thrift,906,summary,Improve type mappings,non_debt,-
thrift,906,description,"The current haskell type mappings are awkward and error prone to work with: binary -string -byte -i16 -i32 -i64 - This patch updates the mappings to the canonical types of the correct length in Haskell: binary -string -byte -i16 -i32 -i64 - THIS BREAKS EXISTING CODE. It is, however, very arguably broken already. For convenience of patching, this patch is a superset of THRIFT-743. Thoughts?",design_debt,non-optimal_design
thrift,906,comment_0,"For users updating to a newer version of Thrift, what kind of changes will the necessitate?",non_debt,-
thrift,906,comment_1,"For someone starting from scratch, it'd be no more or less annoying that using Int: the natural integral type in Haskell is Integer (multiprecision integral), not Int (or Int16, Int32, ...). To patch existing code... Logically speaking, they'd have to re-type every bit of code using stuff imported from the gen-hs/* files. This is not has horrible as it sounds, you can mechanically get everything to compile again (and I suspect work about as well as before my patch) by liberal use of ""fromIntegral"" for the integral types: myStruct { f_myStruct_field1 = foo, f_myStruct_field2 = bar } would become myStruct { f_myStruct_field1 = fromIntegral foo, f_myStruct_field2 = fromIntegral bar } The problem is that Int in Haskell is only guaranteed to be (at least) 30 bits wide [1], not 16, 32 or 64, so conversions to/from are potentially lossy. Using the fromIntegral trick doesn't fix (or, I believe, worsen) the lossiness. The ""right"" thing to do is to handle bit-width in w\e way is appropriate for your code; how complex is that depends on the code. Handling the binary change (String - [1] see -- short version is ""implementation dependant"" and ""at least able to represent [-2^29 .. 2^29-1]"".",non_debt,-
thrift,906,comment_2,"This is re-synced to current head, and NOT a superset of any other patch (except it includes the import length from Prelude fix, since it's impossible to compile code without it). It also fixes a few uses of ""fromInteger $ toInteger"" in v1, and uses fromIntegral instead.",non_debt,-
thrift,906,comment_3,"That all seems pretty reasonable to me, particularly if it improves things going forward. Any Haskell users want to weigh in?",non_debt,-
thrift,906,comment_4,I just committed this.,non_debt,-
thrift,923,summary,Event-driven client and server support for C++,non_debt,-
thrift,923,description,Support for writing nonblocking event-driven clients and servers in C++,non_debt,-
thrift,923,comment_0,I am trying to use the to connect to a Java/Servlet based server is there any sample code for setting up,non_debt,-
thrift,923,comment_1,Does anyone know if this code is in a usable state for async clients?,non_debt,-
thrift,931,summary,Use log4j for Java tests,non_debt,-
thrift,931,description,"Right now, slf4j-simple is used when the tests are run. Unfortunately, the ""simple"" logger doesn't support debug level messages at all, which is somewhat of a pain. It would be good to switch to slf4j-log4j for the tests so we have debug logs available.",design_debt,non-optimal_design
thrift,931,comment_0,Looks good to me. Go ahead and commit it.,non_debt,-
thrift,959,summary,TSocket seems to do its own buffering inefficiently,code_debt,low_quality_code
thrift,959,description,"I was looking through TSocket today while reviewing THRIFT-106 and I noticed that in TSocket, when we open the socket/stream, we wrap the input/output streams with objects and use those for reading and writing. Two things stand out about this. Firstly, for some reason we're setting the buffer size specifically to 1KB, which is 1/8 the default. I think that number should be *at least* 8KB and more likely something like 32KB would be better. Anyone have any idea why we chose this size? Secondly, though, is the fact that we probably shouldn't be doing buffering here at all. The general pattern is to open a TSocket and wrap it in a TFramedTransport, which means that today, even though we're fully buffering in the framed transport, we're wastefully buffering again in the TSocket. This means we're wasting time and memory, and I wouldn't be surprised if this is artificially slowing down throughput, specifically for multi-KB requests and responses. If we remove the buffering from TSocket, I think we will probably need to add a TBufferedTransport to support users who are talking to non-Framed servers but still need buffering for performance.",design_debt,non-optimal_design
thrift,959,comment_0,"Some investigation shows that actually, you won't have any performance issues with large writes, since Java's buffered stream implementations offer a fast path to bypass the internal buffer if the write or read is big enough to justify that. There still is the question of throughput on smaller writes though - for instance, if all your method calls are 100-byte requests, then you have to do the extra copy only to go straight into a flush.",non_debt,-
thrift,959,comment_1,I just committed a tiny fix for this. I found that removing the buffer improved the distribution of latencies by a small but notable percentage.,design_debt,non-optimal_design
thrift,959,comment_2,This caused a big performance regression see THRIFT-1121,code_debt,slow_algorithm
thrift,959,comment_4,"This was never completed, I am closing as incomplete.",non_debt,-
thrift,962,summary,Tutorial page on our website is really unhelpful,documentation_debt,low_quality_documentation
thrift,962,description,"It's just a weak skeleton. I mean, really, really weak. At the very least, we should point to TRUNK/tutorial/ for where to look at code examples.",documentation_debt,low_quality_documentation
thrift,962,comment_0,"fully agree Bryan! It is really important to have a consistent tutorial across all Languages using the same tutorial.thrift file and possible interaction between these tutorials might be great! Another place is the wiki, some ThriftUsage pages do not use the tutorial example, they have their own",documentation_debt,low_quality_documentation
thrift,962,comment_1,just did the nodejs tutorial an had a look on other missing ones... managing tutorial for the website as copy of existing tutorial source is a maintenance nightmare. I think we should move the website to the git repo within a www folder and reference markdown files within source repo directly. source =let's go for markdown: rename all files such as README to README.md we can still publish the web site via svn. any thoughts? ;-r,documentation_debt,low_quality_documentation
thrift,962,comment_2,"Anthing that helps towards getting better documentation is fine with me. There are some old tutorials in the old wiki and I really would like to see them on the web site. Not sure if git will help with this ;-), but I don't have any problems with it either, as long as it produces less overhead, instead of more.",documentation_debt,low_quality_documentation
thrift,962,comment_3,"Having the website in git will not change that we need to create content nor will it make it any easier for someone to contribute to it. The site is already in markdown and we can pull in the tutorial source files from the repo to keep things up to date. I recently redid the mesos and aurora websites and have an updated version for us as well, i'll see what i can do to automate pulling in the tutorial files from our source repo.",non_debt,-
thrift,962,comment_4,"I would love to have the capability to include source code directly on the web site especially for tutorial, test, IDL's. see here: = People do probably not know how to contribute documentation to our project, having the web site within the source tree can simplify this. What about patches including code and web site fixes?",documentation_debt,low_quality_documentation
thrift,962,comment_5,Brilliant! I was thinking on it but I thought it would be really hard to get done. It'd be awesome to get rid of this redundancy! We would basically be able to compile the web site ;),code_debt,complex_code
thrift,962,comment_6,"In order to publish the website it must be in svn. That doesnt mean the source has to be in svn, but having it included in the main git repo does not buy us any advantages. I'm in favor of keeping the src repo as focused on development and leaving the website content and publish data where it is",non_debt,-
thrift,962,comment_7,"tutorials are now incorporating code from repo /tutorials, this will always be a work in progress to make better and improve what we can provide to the community. closing as we now have something available",documentation_debt,low_quality_documentation
thrift,972,summary,missing in,non_debt,-
thrift,972,description,"Calling stop in another thread doesn't work unless the transport knows to ""be interrupted."" All the big-kid servers cal interrupt(), why not the simpleton, simple one? :)",non_debt,-
thrift,972,comment_0,please provide a patch,non_debt,-
thrift,977,summary,Hex Conversion Bug in C++ TJSONProtocol,non_debt,-
thrift,977,description,The hex conversion functions in TJSONProtocol.cpp contain bugs and return wrong values for hex values > 10.,non_debt,-
thrift,977,comment_0,Patch to fix this bug.,non_debt,-
thrift,977,comment_1,I just committed this patch. Thanks Aravind!,non_debt,-
thrift,978,summary,better error message when pkg-config is missing,design_debt,non-optimal_design
thrift,978,comment_0,"out of curiosity, do any other thrift bindings besides c_glib use pkg-config?",non_debt,-
thrift,978,comment_1,Mono.,non_debt,-
thrift,978,comment_2,patch to check if pkg-config is available,non_debt,-
thrift,978,comment_3,I don't think this is the right change. This problem only occurs when the person who ran bootstrap.sh didn't have pkg.m4. The user running configure doesn't need pkg-config to build the parts of Thrift that don't depend on it.,non_debt,-
thrift,978,comment_4,"adds pkg.m4 and required check's to c_glib and csharp section within configure.ac I think it makes sense to add pkg.m4 to aclocal folder, other people had similar issues, see",non_debt,-
thrift,981,summary,cocoa: add version Info to the library,non_debt,-
thrift,981,description,add version info to the library and update,non_debt,-
thrift,981,comment_0,Roger thoughts on using a #define THRIFT_VERSION in a Thrift.h for this?,non_debt,-
thrift,981,comment_1,committed Thrift.h with ThriftVersion string for this,non_debt,-
thrift,992,summary,Naming convention in C# constructor is not consistent with other fields causes compile errors,code_debt,low_quality_code
thrift,992,description,"While updating my FluentCassandra project using the latest Thrift-0.5.0.exe generator against the 0.7 Beta3 release of cassandra.thrift I experienced a generation problem that caused compiler errors when compiling the C# code. It appears the constructor fields are using the old naming convention for the fields, while every other part of the class seems to be using the new naming convention for fields with underscores. You can see a diff of the files I had to manually edit here: The paths starting with should be the ones that you need to worry about for this bug.",code_debt,low_quality_code
thrift,992,comment_0,The cassandra.thrift file in question,non_debt,-
thrift,992,comment_1,"this bug or feature... was introduced with THRIFT-634 hmm, where should we go now?",non_debt,-
thrift,992,comment_2,"updated compiler [^thrift.exe] Nick, could you please verify this?",non_debt,-
thrift,992,comment_3,The changes you have made in the attached thrift.exe file generate a clean build that can be compiled. Thank you for the quick turn around.,non_debt,-
thrift,992,comment_4,committed the patch!,non_debt,-
thrift,1003,summary,Polishing c_glib code,code_debt,low_quality_code
thrift,1003,description,"attached patch contains following changes: * Added Apache headers to c/h files * Use gtester for running tests. We don't need -wrapper script anymore * Use one-line macros G_DEFINE_TYPE instead of 15-line class definition * Keep formatting closer to glib-like style (one line class definition macroses/remove trailing spaces) Given changes are mostly fixing low hanging fruits. It does not change any logic/api. There are more chages needed, such as * using CLASS_TYPE_new functions instead of * stop using _set_property (aka reflection) in constructors * check more careful about _ref and _unref handling but this requires more careful refactoring so it will be later in a separate patch.",code_debt,low_quality_code
thrift,1003,comment_0,Also the patch fixes *.pc file (adds correct dependencies to glib-2.0),non_debt,-
thrift,1003,comment_1,"patch fails with a minor issue: to following step was required to build successfully if {{--with-cpp}}, I have to build the cpp test libs also by hand It seems, that something in is broken",non_debt,-
thrift,1003,comment_2,my env on Debian Lenny same behavior with new libtool(2.2.6b) from backports {{sudo apt-get -t lenny-backports install libtool}},non_debt,-
thrift,1003,comment_3,Fixed version you can find here And here is the fix for the dependency issue I am not an automake guru (I just restored previous code there) but as I understand should be generated before testdebugproto built. How to make it with automake? Adding to does not make any difference - 'make check' still fails. Is my fix a true fix for the problem? PS Please feel free to optimize/refactor Makefile.am.,non_debt,-
thrift,1003,comment_4,committed!,non_debt,-
thrift,1008,summary,byte[] accessors throw NPE on unset field,non_debt,-
thrift,1008,description,None,non_debt,-
thrift,1008,comment_0,Fixed.,non_debt,-
thrift,1011,summary,Error generating package imports when using classes from other packages,non_debt,-
thrift,1011,description,"If you define multiple thrift files and use a data type from one file in another the generated code is broken because it tries to use the fully qualified class name as object type without adding an import call. While this works in java it does not work in as3. For example, if we have a file types.thrift types.thrift namespace com.xxx.xxx.types struct Type1 { 1: string param1, } and a file service.thrift service.thrift namespace include ""types.thrift"" service Service1 { void method (1:types.Type1 object), } This will generate a method definition like public function onError:Function, but will not generate the import import",non_debt,-
thrift,1011,comment_0,This is the patch which will resolve this issue. The patch was made against thrift revision number: 1039194,non_debt,-
thrift,1011,comment_1,Are there any AS3 people who could give this a review?,non_debt,-
thrift,1011,comment_2,"After changing the ""namespace java"" to ""namespace as3"" in the attached .thrift test cases, and applying the patch (which needs a newline at the end to apply cleanly) .. I can verify that this generates .as files with proper import statements for custom types.",non_debt,-
thrift,1011,comment_3,Updated test-case files with namespaces for as3.,non_debt,-
thrift,1011,comment_4,"I just committed this. Thanks for the patch, Usman, and the review, Rowan!",non_debt,-
thrift,1030,summary,C++ THttpTransport doesn't support chucked transfer encoding,non_debt,-
thrift,1030,description,"With the chucked transfer encoding, the response of HTTP doesn't have ""content-length"", instead it has and send the content by chucks. But the c++ thrift library will treat it as no data, and throw",non_debt,-
thrift,1030,comment_0,"This is caused by response headers with ""transfer-encoding"" instead of the expected ""Transfer-Encoding"" string. Attaching patch to do case-insenstive comparison of headers.",non_debt,-
thrift,1030,comment_1,Thanks for Rowan's patch,non_debt,-
thrift,1030,comment_2,fixed without committing the patch?,non_debt,-
thrift,1030,comment_3,"Thanks, just committed Rowan's patch to trunk.",non_debt,-
thrift,1048,summary,Thrift java library has redundant dependency on commons-lang.,build_debt,over-declared_dependencies
thrift,1048,description,The only class from commons-lang used in Thrift is It is used in and in It can be replaced with like it is done in,code_debt,low_quality_code
thrift,1048,comment_0,"I'd accept a patch for this, though I'm not in any hurry to get it done.",non_debt,-
thrift,1048,comment_1,We still use it for HashCodeBuilder as well if you request a non-stub hashCode.,non_debt,-
thrift,1048,comment_2,"Ah, right. I knew there was some dependency.",non_debt,-
thrift,1048,comment_3,Shouldn't and have consistency in implementation of write(..)?,non_debt,-
thrift,1051,summary,Applying @Override to interface method breaks compile on Java 1.5,non_debt,-
thrift,1051,description,Applying the @Override annotation to an interface method breaks compile on Java 1.5. Only use this on true class method overrides. compile: [javac] Compiling 80 source files to [javac] method does not override a method from its superclass [javac] @Override [javac] ^ [javac] Note: Some input files use unchecked or unsafe operations. [javac] Note: Recompile with -Xlint:unchecked for details. [javac] 1 error BUILD FAILED Compile failed; see the compiler error output for details.,non_debt,-
thrift,1051,comment_0,Patch for fix,non_debt,-
thrift,1051,comment_1,Committed revision 1064911.,non_debt,-
thrift,1055,summary,csharp TServerSocket and TSocket do not disable Nagle via Socket.NoDelay = true like cpp and java do,non_debt,-
thrift,1055,description,"resulted in TFramedTransport being improved so it wasn't necessary to disable Nagle. For the case of TServerSocket and TSocket, a simple server case, performance is still very poor, ~30 msg/sec vs. ~11k msg/sec with NoDelay = true. Java and cpp disable nagle in their TSocket and TServerSocket implementations and performance is MUCH improved with nagle disabled for csharp so I'm proposing that csharp should also disable nagle.",design_debt,non-optimal_design
thrift,1055,comment_0,Prototype patch for the issue.,non_debt,-
thrift,1055,comment_1,"I just committed this. Thanks for the patch, Chris.",non_debt,-
thrift,1062,summary,Problems with python tutorials,non_debt,-
thrift,1062,description,"I'm trying to do some experiments with python and thrift, starting with the tutorials in the source code. Here it seems the problem is that no host has been given, and getaddrinfo fails on that. Giving the listen address explicitly: transport = -makes the server start. Connecting with the client: tutorial/py]# python PythonClient.py ping() 1+1=2 InvalidOperation: why='Cannot divide by 0') 15-10=5 Check log: 5 gives the expected result. So the python client can interact with the python server, but the Looking at the code, I can't see why these two can't interact. BTW, I've tried the Ruby client, and it can work with the python server, but not with the python twisted server. So I guess python twisted server (and so the client) is incompatible with everything else. Which seems pretty bad... Or I'm just on the wrong track somewhere.",non_debt,-
thrift,1062,comment_0,This bug still available in version 0.8 My message in mailing list:,non_debt,-
thrift,1062,comment_1,Looking at the current trunk r1225728 your first two issues have all ready been addressed. both the twisted client and server tutorials are using 127.0.0.1 to connect with and have,non_debt,-
thrift,1062,comment_2,"I pull and install last revision 1225773, and install python thrift library from lib direcory. Then I compile py.twisted and py modules: $ thrift -r --gen py tutorial.thrift $ thrift -r --gen py:twisted tutorial.thrift After it at first console I run server: $ cd py.twisted $ python PythonServer.py And on second console I run python client (not twisted(!)) and get error: $ cd py $ python PythonClient.py TSocket read 0 bytes I see that twisted server does not work correctly with not-twisted clients.",non_debt,-
thrift,1062,comment_3,"@Viacheslav I met this problem too, then I did some inspection. That's because twisted add a 4 bytes INT length in front of the real data. check my blog:",non_debt,-
thrift,1062,comment_4,THRIFT-1735 integrates Python Tutorials into regular build please create a test case for the test suite: - test/test.sh (interoperability) - test/py/ - test/py.twisted,test_debt,lack_of_tests
thrift,1063,summary,Fix Erlang Tutorial Files,non_debt,-
thrift,1063,description,"A user was having issues with the erlang tutorial as it was out of date. I fixed and commited the change, but figured people would like a record of it being done.",documentation_debt,outdated_documentation
thrift,1063,comment_0,"tutorial now generates thrift, compiles, and server starts.",non_debt,-
thrift,1065,summary,Unexpected exceptions not proper handled on JS,code_debt,low_quality_code
thrift,1065,description,Following THRIFT-550 and THRIFT-807: We still get JS errors when any undeclared exception is thrown (if not defined on the service IDL),non_debt,-
thrift,1065,comment_0,Can you provide an example?,non_debt,-
thrift,1065,comment_1,remove some misplaced code (probably copied from another implementation?),code_debt,dead_code
thrift,1065,comment_2,"e.g. if you remove any _throws_ declaration from ThriftTest.thrift: _void testException(1: string arg) throws(1: Xception err1),_ *void testException(1: string arg),* and call the service from the JS client side. The server throws then the ""unexpected"" exception and I was getting something like: Now, with this patch, it should still throw the exception on the JS client site",non_debt,-
thrift,1065,comment_3,"apparently the java server is not encoding undeclared exceptions ""properly"" (it could be my impl., please see patch), but it works fine on C++ Incoming content: [cpp] Outgoing content: is a [java] Outgoing content: Error:This is a",design_debt,non-optimal_design
thrift,1065,comment_4,behavior of Java and CPP Testframework should be the same,non_debt,-
thrift,1065,comment_5,"Yes Henrique, that's the way! I Just committed the following stuff: * qUnit test, modified a bit * JavaServer Handler, modified to the same behavior as C++ (see THRIFT-847) * original JavaScript patch Thank you for making Thrift better! Roger",non_debt,-
thrift,1069,summary,Add command line option to prevent thrift from inserting gen-* directories,non_debt,-
thrift,1069,description,"I maintain a plug-in to execute thrift under maven: The gen-* directories do not add any value for me (I have to add additional code to move the files back up one level after generation - for IDE integration). Would it be possible to add an additional command line option to either: * Suppress the insertion of the gen- directory * or, Allow me to specify the full path to the output directory (without the gen- postfix)",non_debt,-
thrift,1069,comment_0,"Nice plugin. The maven build helper and the eclipse plugins will take care of what you are looking for. I've been using an ant task to accomplish what your plugin does. Here is a the section from the pom which should help you not have to copy files around and provides eclipse integration <!-- Run an ant task to generate the thrift source -- <plugin <groupId <artifactId <version <executions <execution <id <phase <goals <goal </goals <configuration <tasks <property name=""target.dir"" <mkdir <exec <arg line=""-o ${target.dir} --gen java </exec </tasks </configuration </execution </executions </plugin <!-- Include the thrift generated sources in the build -- <plugin <groupId <artifactId <version <executions <execution <id <phase <goals <goal </goals <configuration <sources <source </sources </configuration </execution </executions </plugin <!-- Add generated sources to eclipse build sources list -- <plugin <groupId <artifactId <version <configuration <sourceIncludes <sourceInclude </sourceIncludes </configuration </plugin>",non_debt,-
thrift,1069,comment_1,I'd love to see a patch for this effect.,non_debt,-
thrift,1069,comment_2,Adds a -out option which will set the specified directory as an absolute path for generated files without creating the gen-* folder Options: -o dir Set the output directory for gen-* packages (default: current directory) -out dir Set the ouput location for generated files. (no gen-* folder will be created) Example: [jfarrell]$ mkdir src src_orig [jfarrell]$ thrift -o src_orig --gen java scribe.thrift [jfarrell]$ tree src_orig/ src_orig/ `-- gen-java `-- scribe `-- thrift |-- LogEntry.java |-- ResultCode.java `-- scribe.java [jfarrell]$ thrift -out src --gen java scribe.thrift [jfarrell]$ tree src/ src/ `-- scribe `-- thrift |-- LogEntry.java |-- ResultCode.java `-- scribe.java,non_debt,-
thrift,1069,comment_3,"I just committed this patch. Thanks for the contribution, Jake!",non_debt,-
thrift,1069,comment_4,"Thanks Guys, I have updated the plug-in to use the new -out parameter (works great). The code is committed, but I will wait for your next release before doing the plug-in release.",non_debt,-
thrift,1070,summary,C++ compiler and runtime have 32/64bit problems,non_debt,-
thrift,1070,description,"When compiling thrift with a 64bit G++ and a strict set of warnings, some errors come up. The code assumes that Since most of these problems are related to the wire protocol, which defines a four-byte size, the fixes are straightforward casting. We'll provide a patch shortly.",non_debt,-
thrift,1070,comment_0,Patch to fix the issues raised in thrift-1070,non_debt,-
thrift,1070,comment_1,This looks fine. What were the errors and warnings?,non_debt,-
thrift,1070,comment_2,Here's one clump. We use -Werror so warnings are fatal errors for us. cc1plus: warnings being treated as errors In file included from from In member function virtual const uint8_t* uint32_t*): error: conversion to unsigned int from long int may alter its value In member function void uint32_t*): error: conversion to unsigned int from long int may alter its value In member function uint32_t const: error: conversion to uint32_t from long int may alter its value In member function uint32_t const: error: conversion to uint32_t from long int may alter its value make[2]: Leaving directory make[2]: [applianceUtils.o] Error 1 make[1]: [sdk.build] Error 2 make: [lib.build] Error 2 make[1]: Leaving directory,non_debt,-
thrift,1070,comment_3,1,non_debt,-
thrift,1070,comment_4,Thanks Rich! Committed your patch.,non_debt,-
thrift,1071,summary,SSL/C++: invalid cast breaks build,non_debt,-
thrift,1071,description,THRIFT-151 breaks the build on Ubuntu and Debian Squeeze see,non_debt,-
thrift,1071,comment_0,Changed to c style cast as per David's suggestion in THRIFT-151.,non_debt,-
thrift,1071,comment_1,"Rowan, that's fast! I still have the following build issue: I had to pass *-lssl* by hand on Debian squeeze and it works!",non_debt,-
thrift,1071,comment_2,"committed the invalid cast, thanks Rowan!",non_debt,-
thrift,1071,comment_3,There were a couple configure changes from one of the older ssl patches left out of the more recent versions. Re-running bootstrap.sh and configure did add the correct -lssl flag to LIBS for the cpp Makefile for me on Ubuntu.,non_debt,-
thrift,1071,comment_4,I have added openssl headers and lib to configure.ac =Could you verify that patch?,non_debt,-
thrift,1071,comment_5,That's basically the same as the configure.ac changes I pulled out of the other issue. Checks slightly different ssl headers but I confirmed that it does do the job. (The all-important -lssl is added to lib/cpp/Makefile LIBS).,non_debt,-
thrift,1071,comment_6,bootstrap.sh needs to be re-run after applying the patch,non_debt,-
thrift,1071,comment_7,committed,non_debt,-
thrift,1072,summary,Missing - (id) in,non_debt,-
thrift,1072,description,"- (id) (id<TProcessor is missing in from the Cocoa headers. Should be added, to let user know that they need to init with this method to delegate stuff to their own processor. Can't see other possibility to make server delegate incomming connections to protocol implementing classes. (no Cocoa server examples available!)",design_debt,non-optimal_design
thrift,1072,comment_0,Improvement,non_debt,-
thrift,1072,comment_1,Can you attach a patch instead of a new version of the file?,non_debt,-
thrift,1072,comment_2,Improvement,non_debt,-
thrift,1072,comment_3,"Sorry, patch is attached",non_debt,-
thrift,1072,comment_4,"I was hoping for a unified diff as taken from the base of the Thrift tree. ""svn diff"" produces the correct output.",non_debt,-
thrift,1072,comment_5,"This made it into trunk at some point, closing ticket",non_debt,-
thrift,1091,summary,integrate JavaScript Test into Testsuite via make check,non_debt,-
thrift,1091,description,JavaScript jslint target and QUnit based unit tests should be able to run via *make check* => CI,non_debt,-
thrift,1091,comment_0,jslint executed via unit test,non_debt,-
thrift,1091,comment_1,I forgot to addschanges to lib/Makefile.am here is patch v2,non_debt,-
thrift,1091,comment_2,"no concerns, so I committed this testsuite enhancement.",non_debt,-
thrift,1091,comment_3,It breaks the build...,non_debt,-
thrift,1099,summary,TPhpString: from string,non_debt,-
thrift,1099,description,"I want to serialize thrift instances to save them to disk, serialize them in session, transfer them them to elephant bird, ... Therefore i build a class named TPhpString which extends TTransport and can be accessed like other TTransports. I added additional getters to easy convert in both directions: from string to instance, from instance to string. I provided the class as a patch and hope that it will be integrated, see the examples below: write example: $vector = new =$body = $body); read example: $body = $vector = I build a tutorial in german about it:",non_debt,-
thrift,1099,comment_0,Why a new transport instead of implementing TJSONProtocol for PHP?,non_debt,-
thrift,1099,comment_1,JSON is another dataformat. Why should i convert my data in another data format ;) TPhpString just allows me to write and read the thrift binary data. I can directly transfer the binary data over network with Flume. And I don't need to do any conversion.,non_debt,-
thrift,1099,comment_2,How is this different from TMemoryBuffer?,non_debt,-
thrift,1099,comment_3,"You are right. I am sorry, when i started with thrift, i didn't get TMemoryBuffer running. Now it works :( Then my only suggestion would be to add the public static methods for a really conversion between string and object. Should i patch this in TMemoryBuffer? Or should we leave out these conversion methods from ""core"" ?",non_debt,-
thrift,1099,comment_4,"Torben, check out",non_debt,-
thrift,1099,comment_5,"Hi Thomas, TBinarySerializer looks like the right place! I can see some bugs within these lines, i will provide a patch and add a new ticket for it. My TPhpString should not be included. Ticket closed :)",non_debt,-
thrift,1100,summary,"python TSSLSocket improvements, including certificate validation",non_debt,-
thrift,1100,description,"The python TSSLSocket.py module has TSSLSocket and TSSLServerSocket for outbound and inbound SSL connection wrapping. This ticket is for a patch that makes several improvements: * adds Apache license at top of file * for outbound sockets, SSL certificate validation is now performed by default ** but may be disabled with validate=False in the constructor ** instructs python's ssl library to perform CERT_REQUIRED validation of the certificate ** also checks to make sure the certificate's {{commonName}} matches the hostname we tried to connect to ** raises when the certificate fails validation - tested using google's www.gmail.com (doesnt match) versus mail.google.com (matched cert commonName) ** puts a copy of the peer certificate in self.peercert, regardless of validation status ** sets a public boolean self.is_valid member variable to indicate whether the certificate was validated or not * adds a configurable server certificate file, as a constructor argument {{certfile}} ** allows runtime changing of server cert with setCertfile() on the server, that changes the certfile used in subsequent ssl_wrap() calls ** exposes a class-level variable SSL_PROTOCOL to let the user select ssl.PROTOCOL_TLSv1 or other versions of SSL, instead of hard-coding TLSv1. Defaults to TLSv1 though. * removes unnecessary sys.path modification * adds lots of docstrings In a somewhat unrelated change, this patch changes two lines in TSocket.py where self.handle is compared to None using {{!=}} instead of: {{is not}}.",code_debt,low_quality_code
thrift,1100,comment_0,patch attached: adds lots of code to and touches 2 lines in,non_debt,-
thrift,1100,comment_1,I just committed this patch. Thanks Will!,non_debt,-
thrift,1103,summary,"TZlibTransport for python, a zlib compressed transport",non_debt,-
thrift,1103,description,"New implementation of zlib compressed transport for python. The attached patch provides a zlib compressed transport wrapper for python. It is similar to the TFramedTransport, in that it wraps another transport, implementing the data compression as a transformation layer on top of the underlying transport that it wraps. The compression level is configurable in the constructor, from 0 (none) to 9 (best) and defaults to 9 for best compression. The way this works is that every write() to the transport appends more data to the internal cStringIO write buffer. When the transport's flush() method is called, the buffered bytes are then passed to a zlib Compressor object and flush()ed with zlib.Z_SYNC_FLUSH. Because the thrift API calls the transport's flush() after writeMessageEnd(), this means very small thrift RPC calls don't get compressed well. This transport works best on thrift protocols where the payload contains strings longer than 10 characters. As with all data compression, the more redundancy in the uncompressed input, the greater the resulting compression. The TZlibTransport class also implements some basic statistics that track the number of raw bytes written and read, versus the decompressed equivalent. The getCompRatio() method returns a tuple of where ratio is computed using: (So 10 compression is 0.10, meaning smaller numbers are better.) The getCompSavings() method returns the actual number of which might be negative when the compression of non-compressible data ends up expanding the data. So hopefully, anyone who uses this transport will be able to tell whether the compression is saving bandwidth or not. I will add the patch in a few minutes. I haven't tested this against the C++ TZlibTransport, only against itself.",test_debt,lack_of_tests
thrift,1103,comment_0,"Patch attached. Adds TZlibTransport.py into and adds TZlibTransport into the module's __all__ list. I tested this on python 2.4 and 2.7. The zlib module is present and provides the same API in python 2.4 as 2.7 for our needs. If the patch for THRIFT-1094 is good and can be commited, then it would make it easier for me to extend the code to include testing that exercises the TZlibTransport code. (I did it locally in my copy of thrift-svn/trunk to test this code, but didn't want to submit a patch that requires another patch ( THRIFT-1094 ) which hasn't been approved yet.)",non_debt,-
thrift,1103,comment_1,THRIFT-1094 is committed. Do you want to revise your patch before I evaluate this ticket for commit?,non_debt,-
thrift,1103,comment_2,"Thanks for comitting THRIFT-1094, I'll update the patch this evening (and attach as _v2) to include testing TZlibTransport wrapping in the code (as a cmdline --zlib argument to both scripts). FYI, currently the hudson(jenkins) build seems to be failing on the javascript jslint tasks, stopped the build tests from progressing past the 'test/js' directory...",test_debt,lack_of_tests
thrift,1103,comment_3,"I updated the test suite to include running every valid combination of server, protocol and wrapping transports (both ssl and zlib). For python2.4, this is 30 combinations and runs in about 24 seconds. For python2.7, there is an extra server type (TProcessPool which uses the multiprocessing module) and the SSL transport (unavailable in py2.4), whichadds up to 66 combinations of tests, running in ~95 seconds. The 4 nested for-loops significantly expands the code test coverage. In addition to everything in the _v1 of this patch, the _v2 version also has: Updated test code: * added testing of TSSLServer, an alternate socket transport * added testing of TZlibTransport, a wrapping transport * added a self-signed cert in with a cautionary .readme to allow testing of the TSSLServerSocket (it needs a certificate file) * fixed -q (quiet) and -v (verbose) options to to lower and raise the verbosity Fixed two problems in and one enhancement: * fixed confusing parameters to both client and server constructors, removing the overly ornate \*args and \*\*kwargs which made the constructor behave poorly when used with just (host,port) as arguments. The constructors better match the TSocket and TServerSocket constructor parameters now. * fixed logic in TSSLServerSocket parameter checking, if validate=True and ca_certs=None, now it raises an exception like the docstring claims it should. * made TSSLServerSocket more robust on failed SSL handshake by closing socket connection and returning None from accept() call, which is better than terminating the entire server in some cases I will attach the _v2 patch in a moment.",code_debt,low_quality_code
thrift,1103,comment_4,version 2 of patch attached (obsoletes the v1 patch),non_debt,-
thrift,1103,comment_5,I just committed this. Thanks Will!,non_debt,-
thrift,1105,summary,OCaml generator does not prefix methods of included structs with their type,non_debt,-
thrift,1105,description," Doc.thrift struct Document { 1: string user_name 2: string document_name } typedef Document page_id  Ranking.thrift include ""Doc.thrift"" struct Request { 1: Doc.page_id page }  thrift -gen ocaml Doc.thrift thrift -gen ocaml Ranking.thrift  You will see #set_page (read_document iprot) generated in read_request, instead of the proper #set_page iprot) Generated code fails to compile as a result.",non_debt,-
thrift,1105,comment_0,I just committed this.,non_debt,-
thrift,1121,summary,Java server performance regression in 0.6,code_debt,slow_algorithm
thrift,1121,description,"A user reports a 30% performance regression after upgrading some high-request-rate Java software from Thrift 0.3 to 0.6. After some inspection, it turns out that the changes for THRIFT-959 caused the slowdown. However, even after altering the code to use the TFramedTransport, performance was still only 80% of version 0.3. I believe the problem is that the TFramedTransport must read the length (unbuffered) before reading (only) one message. In one particular workload, sent with oneway streaming, the server is making many more system calls. It wasn't obvious how to compose a Transport that would add back the buffering using existing components. We created our own trivial TServerSocket that adds the socket buffering back. Performance is now back where it was with 0.3.",code_debt,slow_algorithm
thrift,1121,comment_0,"Todd, do you have a test case you can attach which shows the issue and how you where able to resolve it",non_debt,-
thrift,1121,comment_1,Bryan can you revert THRIFT-959? We noticed the same slowdown,code_debt,slow_algorithm
thrift,1121,comment_2,"OK, I reverted THRIFT-959 in trunk.",non_debt,-
thrift,1121,comment_3,"I don't have any test case - the regression showed up in large scale performance tests of a distributed system. Just forwarding along some results for some folks who, at the time, were not permitted to participate in the JIRA.",code_debt,slow_algorithm
thrift,1121,comment_4,"Although this issue is closed and considered fixed, I would like to add my two cents on the combination of TFramedTransport and TSocket. I have worked with Hector, a Cassandra client, which is using Thrift v0.6.1, which is suffering from a large deal of overhead, at least in my analysis. I came to the conclusion that the performance regression is caused - at least in my setup - by overhead on the TCP layer. In my setup I use the binary protocol over the framed transport over the Thrift socket (without buffering in v0.6.1). I discovered, that two TCP segments are being sent for every frame sent. One for the length of the frame and one for the frame itself (or more if the frame is larger than the maximum a TCP segment can hold). With small messages, the overhead is substantial: 56 extra bytes of headers in case of Ethernet + IP + TCP. Also considering that in my setup the PSH flag was raised, causing the 4 bytes length of the frame to be pushed from the TCP stack to the application ... while the data itself is on its way. Now this issue is alleviated by using a buffered output stream in TSocket. However, I do think that this solution causes unnecessary memory overhead in cases where framed transport is used. Then the data is in memory twice. And the data is first written into the framed transport buffer, then written in the buffered output stream of the socket and then written to the TCP stack. The above goes for the writing side of things. As for the reading: it's a system call extra (first read the length and then in a separte call read the frame from the socket). I do not estimate this to account for the big difference in throughput. Of course in cases where the framed transport isn't used, buffered in and output streams are very useful.",design_debt,non-optimal_design
thrift,1121,comment_5,"I can't remember which language I did it for (probably C++) but I changed one TFramed implementation to reserve 4 bytes in the buffer so only one write() was called, which helps to ensure this doesn't happen. It could be done for Java too... shouldn't be that hard.",non_debt,-
thrift,1122,summary,Thrift Compiler for Windows - Snapshot Build,non_debt,-
thrift,1122,description,Let's add a snapshot build for the *Thrift Compiler for Windows*. I've just requested the packages *mingw32 mingw32-binutils mingw32-runtime* for the Ubuntu build slave.,non_debt,-
thrift,1122,comment_1,"here it is: I will add some more Info, like buildnumber, revision.",non_debt,-
thrift,1130,summary,Add the ability to specify symbolic default value for optional boolean,non_debt,-
thrift,1130,description,"When specifying a default value for an optional struct member of the type bool the user now needs to enter a numeric value (0, or 1). It would be nice if they could specify a symbolic value such as 'true' & 'false' Example thrift file: struct { 1: optional bool bDoSomething = true, 2: required string whatever; } With current error",non_debt,-
thrift,1130,comment_0,"Not sure if this is the best fix, but seems to be rather minimalistic By the way - this is my first attempt at contribution here, please let me know if there is some other procedure should be followed.",design_debt,non-optimal_design
thrift,1130,comment_1,Hopefully this is monitored ... I can't find anywhere a way to change my JIRA email - need to change it from to nhed@bamboom.com (username nhed) Thanks -Nevo,non_debt,-
thrift,1130,comment_2,"I haven't tested this patch yet, but my concern is that by adding these tokens, you may possibly have broken string literals or identifiers named ""true"" and ""false"". Can you add some examples to ThriftTest.thrift that exercises these corner cases?",test_debt,lack_of_tests
thrift,1130,comment_3,"This was my concern too, and my threshold for passage was to ensure that it will provide an error, obviously its not the same error as it originally did. At the end of the day, this seems to be pretty much like a c-pre-processor macros: #define true 1 #define false 0 The the items NOT in my negative tests that WOULD fail are attempts to use true/false as field numbers (for example) What you are asking for is a negative test (which I performed locally), I only saw one but it is commented out (so its not in any automated test) Is this how you would like to see it? If not can you point me at an example of a negative test in ThriftTest.thrift? This is what I tried and what I had in mind ...",test_debt,lack_of_tests
thrift,1130,comment_4,"Hm, looks like my concerns were unfounded. Sorry for letting this sit so long. I just committed the patch.",non_debt,-
thrift,1130,comment_5,"Thanks! On Thu, Oct 13, 2011 at 5:35 PM, Bryan Duxbury (Closed) (JIRA) <",non_debt,-
thrift,1135,summary,Node.js tutorial,non_debt,-
thrift,1135,description,It would be great to have a tutorial for the Node.js implementation located at tutorial/nodejs/,documentation_debt,low_quality_documentation
thrift,1135,comment_0,I will upload a patch. Could someone place the code on please (or allow editing of that page)? Thanks.,non_debt,-
thrift,1135,comment_1,Patch added.,non_debt,-
thrift,1135,comment_2,mind reviewing/updating this based on recent updates,non_debt,-
thrift,1135,comment_3,Sure. I will try to post an update in the next hour or two.,non_debt,-
thrift,1135,comment_4,From what I can tell Pierre put together a pretty complete calculator tutorial in another patch which is now committed. I added some minor clean up in the attached patch: The predates the above work and should not be applied. I think 1135 can be safely closed.,code_debt,low_quality_code
thrift,1135,comment_5,Talked with Randy on this and this has all ready been taken care of in other tickets so closing,non_debt,-
thrift,1139,summary,C++ compilation of classes generated for an enum,non_debt,-
thrift,1139,description,"When using the patches from either THRIFT-1123 or THRIFT-1031, I get a compilation error when compiling the class generated for an enum type. The details are available in [this I found that making extend std::pair<int, const char*> >}} solve the issue, but I'm not able to test it on other platforms.",non_debt,-
thrift,1139,comment_0,could you please create a patch? see,non_debt,-
thrift,1139,comment_1,I'm not able to checkout the source from svn ({{svn: access to forbidden}}). The attached patch is made against two local extractions of the [latest build from,non_debt,-
thrift,1139,comment_2,just committed. Thanks for the patch!,non_debt,-
thrift,1141,summary,Include C (glib) library in default Debian package,non_debt,-
thrift,1141,description,The C (glib) library should be included in the Debian package configuration that's shipped with the Thrift package. We require it for integrating Thrift with the nginx server (as a client) - please let me know if there is any reason not to use or include the Glib-based C library.,non_debt,-
thrift,1141,comment_0,"patch did not apply cleanly.. fixed that, the wrong libthrift.jar file location and updated the changelog. => committed",architecture_debt,violation_of_modularity
thrift,1153,summary,HttpClient does not specify the connection close parameter,non_debt,-
thrift,1153,description,THttpClient.cpp builds a header fort he http request in the flush method but it does not set the Connection: Close header which means the client wait for EOF until the server closes the socket. This will slow all requests.,non_debt,-
thrift,1153,comment_0,Patch adds required header.,non_debt,-
thrift,1153,comment_1,I'm confused. We don't want to close the connection. We want to keep it open for potential future requests.,non_debt,-
thrift,1153,comment_2,You are right the underlying transport layer is providing persistent connectivity so this makes sense. I was using similar code for an async http client implementation and it was causing problems there.,non_debt,-
thrift,1164,summary,Segmentation fault on NULL pointer in,non_debt,-
thrift,1164,description,--begin-- testcase.thrift const i32 SHORT_STRING_LENGTH = 500; --end-- (See traces below),non_debt,-
thrift,1164,comment_0,I'd prefer a patch against TRUNK rather than against the 0.6.1 branch. Any chance you'd be willing to update it?,non_debt,-
thrift,1164,comment_1,"As I mentioned in the report, this is already fixed in TRUNK (as a side-effect of This patch is meant to fix the current stable release if needed (without pulling in THRIFT-1045 which does a lot more work).",non_debt,-
thrift,1174,summary,Publish as3 client implementation via Maven for use by flex-mojos users,non_debt,-
thrift,1174,description,It would be nice to be able to use the flex-mojos to get the as3 client implementation .swc artifact the same way we can get the .jar artifact for java clients. An example pom that uses this approach with our local repositories should be attached.,non_debt,-
thrift,1174,comment_0,This shows the dependency entry we use to retrieve a .swc from our corporate repository.,non_debt,-
thrift,1174,comment_1,"Thanks Justin, should be able to accomplish this similar to how i did the fb303 and libthrift java versions",non_debt,-
thrift,1174,comment_2,got this all working and published to the staging repo with no issues. Generates the pom and swc files based on the current java version. Artifacts uploaded where I'm cleaning up the and as3 build files to share common components and reduce code duplications. Will check the updates into trunk shortly,code_debt,duplicated_code
thrift,1174,comment_3,"Added to trunk, next release the libthrift-as3 client will get published to apache maven central with the libthrift and fb303 clients.",non_debt,-
thrift,1176,summary,Thrift compiler global leakage js,code_debt,low_quality_code
thrift,1176,description,Below I added the var qualifier on the declare field function. This is fairly trivial and fixes the global scope leak.,code_debt,low_quality_code
thrift,1176,comment_0,"committed! includes updated test.html to get qunit.js with correct mime-type(IE9 complains if not), fixed by myself",non_debt,-
thrift,1182,summary,Native deserializer segfaults on incorrect list element type,non_debt,-
thrift,1182,description,"There is a pretty major bug in the native Ruby deserializer that causes it to segfault on certain bad inputs. Basically when deserializing a list that is expected to contain elements of a basic type, but is claiming in the list header to be a list of structs, the native deserializer segfaults and crashes the ruby process. I will be attaching code that reproduces this shortly. I need to have a fix for this ASAP, and am willing to work on it, however I need some guidance, as I'm not sure what the desired behavior is. Basically the case is: * Expecting a list<string* Get something that claims to be a list<struct This can be caused by a buggy and/or malicious client, or by accidentally making a change to a .thrift definition and running both versions concurrently. Regardless of the cause, crashing on arbitrary inputs is not an option for my use case so I have to handle it somehow. I see 2 possible solutions: 1) Raise some kind of Type Mismatch error and catch it higher up, thus aborting parsing of the entire thrift struct 2) Skip the list with mismatched type but attempt to parse the rest of the struct. Of course, if the list header is wrong about the element type it could be wrong about the list length as well so it's not clear how many bytes should be skipped. Basically once such a type error is detected, the contents of the serialized struct can no longer be trusted. For this reason, I would lean towards option 1. Right now it doesn't look like the deserializer does much type checking while deserializing, so such a change in behavior could break numerous existing users and should probably be optional (i.e. right now you could have a list<i64> declared but a list<i32> come across the wire, I *think* the native deserializer will just read the list of i32s and happily convert them to Ruby Fixnums so everything will work. Adding the type checks I suggest would break this case.)",non_debt,-
thrift,1182,comment_0,"Attached simple repro. Run with thrift 0.6.0 gem, tested on ruby 1.8.7 and 1.9.2.",non_debt,-
thrift,1182,comment_1,"Per IRC chat with Bryan Duxbury, the deserializer should skip over list fields (presumably also sets, maps) with unexpected element types, but should keep trying to parse the rest of the struct.",non_debt,-
thrift,1182,comment_2,"Attaching patch. With this patch, the deserializer will skip any maps / lists / sets whose key/value/element types do not match the expected key/value/element types for that field. Tested with all combinations of: protocols: BinaryProtocol, CompactProtocol rubies: 1.8.7, 1.9.2 native extensions: enabled, disabled",non_debt,-
thrift,1182,comment_3,"Wrong file attached last time, here goes try 2.",non_debt,-
thrift,1182,comment_4,"Sorry the patch is against a copy of the source that i've mirrored on our own git repo, so the file paths may look funky.",non_debt,-
thrift,1182,comment_5,"The original patch didn't call if the contents were skipped, which is incorrect. It happens to work for Binary and Compact protocols because the are no-ops, but would fail with a future implementation of JSON protocol, for example.",non_debt,-
thrift,1182,comment_6,"I just committed this patch. Thanks, Ilya!",non_debt,-
thrift,1183,summary,Pure-ruby CompactProtocol raises ArgumentError when deserializing under Ruby 1.9,non_debt,-
thrift,1183,description,"While testing my fix for THRIFT-1182 with various combinations of native/non-native ruby and binary/compact protocols, I stumbled upon a bug in the pure-ruby implementation of CompactProtocol. In line 306: ... val = dat[0] if (val ... Here, dat is a string, so dat[0] returns a Fixnum in Ruby 1.8.7 but a string of size 1 in Ruby 1.9.2. The conditional compares dat[0] with 0x7f, which raises the following error in 1.9.2: ` from `read_byte' from `read_field_begin' from `block in read' from `loop' from `read' from `deserialize' The solution is a one-line fix, patch will be attached shortly.",non_debt,-
thrift,1183,comment_0,Patch attached.,non_debt,-
thrift,1183,comment_1,"Sorry the patch is against a copy of the source that i've mirrored on our own git repo, so the file paths may look funky.",non_debt,-
thrift,1183,comment_2,This breaks CompactProtocol completely in JRuby when run with the --1.9 flag.,non_debt,-
thrift,1183,comment_3,There's a problem with your one-liner: it doesn't work in ruby 1.8.6!,non_debt,-
thrift,1183,comment_4,It should work. We added the #ord method in for pre 1.8.7 clients.,non_debt,-
thrift,1183,comment_5,I haven't tested it in 1.8.6 but it should work with the patch. I can test it with 1.8.6 if you like.,non_debt,-
thrift,1183,comment_6,"P.S. The pure-ruby BinaryProtocol already has this #ord behavior in 0.6.1 (probably much earlier), but the CompactProtocol does not.",non_debt,-
thrift,1183,comment_7,"Ah, you're totally right. I just tested #ord in a ruby 1.8.6 irb without thinking about our extensions. I just committed this patch.",non_debt,-
thrift,1197,summary,fails on slower machines,non_debt,-
thrift,1197,description,Be more tolerant on TFileTransportTest unit tests.,non_debt,-
thrift,1197,comment_0,committed,non_debt,-
thrift,1198,summary,"C++ TestClient and Server Improvements (add Unix Domain Socket, HTTP, JSON)",non_debt,-
thrift,1198,description,"I added support for Domain Socket, HTTP, JSON and introduced boost program options on the C++ TestServer and TestClient. next job is to move test/cpp and test/threads to lib/cpp/test and integrate them with boost unit test suite",non_debt,-
thrift,1198,comment_0,committed,non_debt,-
thrift,1199,summary,Union structs should have generated methods to test whether a specific field is currently set,test_debt,low_coverage
thrift,1199,description,"For example, in the following union it would be nice to be able to do something like {{boolean test = as an alternative to {{boolean test = ==",code_debt,low_quality_code
thrift,1199,comment_0,This should do it.,non_debt,-
thrift,1199,comment_1,I just committed this. Thanks for the patch!,non_debt,-
thrift,1202,summary,"Malformed JSON for map<string,type> services parameters",non_debt,-
thrift,1202,description,"I have a problem if I try to use Strings as map keys. Somehow I get two double quotes around the key values, e.g. """"key"""" instead of ""key"": 'a b':'with spaces', 'test':'test', ... }) sends: b"""":""with ... }]}}] I guess it's something wrong on the write methods from compiled JS services... Could someone test this on their machine? Thank you. Cheers, Henrique",non_debt,-
thrift,1202,comment_0,"ah sorry, this patch also includes some tests for THRIFT-1150, but I think they could also be included...",non_debt,-
thrift,1202,comment_1,Could you reuse Xtruct and Xtruct2 instead of adding new definitions foo and foo2 to ThriftTest.thrift? and use something like: Thanks Roger,code_debt,low_quality_code
thrift,1202,comment_2,"should solve the problem with maps, at least for i32 and strings... Cheers! ps: I removed the old foo struct test, this is already tested by ThriftTest.Xtruct anyway",code_debt,dead_code
thrift,1202,comment_3,Great! just committed this fix!,non_debt,-
thrift,1202,comment_4,fix haskell test cases(add testStringMap test case),non_debt,-
thrift,1202,comment_5,committed the haskell test cases,non_debt,-
thrift,1217,summary,Use evutil_socketpair instead of pipe (Windows port),non_debt,-
thrift,1217,description,"As part of an effort to use the Non-Blocking server (using libevent) on Windows, it was necessary to remove the use of ""pipe"" for the notification mechanism to signal end-of-task. We propose to use evutil_socketpair instead (tested with libevent 2.0.12). Patch included. Please see for more details.",design_debt,non-optimal_design
thrift,1217,comment_0,Patch to teplace pipe by libevent's evutil_socketpair (based on thrift 0.6.1),non_debt,-
thrift,1217,comment_1,"I really like to have Windows port within next Thrift release. The suggestion by [David and to use APR on Windows and do minimal changes on the current implementation is a key factor to bring this up and running. However, this patch depends on pthreads for windows and as mentioned on the dev mailing list or within Jira = Is APR feasible for your? Do you really need thet libevent version? Most GNU/Linux distro still provide 1.4.x and I would really like to keep capability to build and use thrift without lot of extra versions of libraries not within a standard distribution.",architecture_debt,using_obsolete_technology
thrift,1217,comment_2,"@Roger: to avoid some confusion: - the attached patch does not depend on pthread. The suggestion here is merely to use evutil_socketpair instead of pipe. - evutil_socketpair is part of libevent 1.4 on ubuntu 10.10, although I did not test it. All the other changes related to win32 in the attached patch attached are not to take literally (Winsock2.h, static cast...). About THRIFT-1031: - my understanding is that THRIFT-1031 does not support async/libevent server on Windows (??) - libevent+thrift server seems VERY fast on win, and is needed for my project (blocking server won't do) - APR was not strictly necessary for the Non-Blocking server win32 port (i.e. although I tried the THRIFT-1031 patch and observed it made the port a bit cleaner, so I would not refrain from adding APR to thrift-C++ I decided after reviewing the win32 patches, it was better to open a separate issue, since the change here is atomic and should bare little consequences on linux. I was hoping that (naively) the win32 delta would get smaller by using the compatible call. All that said, I'd be really happy to contribute to THRIFT-1031 for it to go through. For this to happen, I would hope to have access to a shared implementation on top of 0.6.1 (the way I provided it on github), so I can test it regularly. Let me know if I can contribute somehow! As a conclusion, I think this patch is somewhat unrelated to THRIFT-1031, but it will help a future port of the NB server on win32 (assuming also someone will replace the pthread dependency by boost, which does not seem trivial at all).",test_debt,lack_of_tests
thrift,1217,comment_3,"Yes, you are right. This is a libevent topic and not Windows Port relevant. I use libevent 1.4.13-stable-1 on Debian Squeeze and this version does not provide evutil_socket_t fixed that with the following define within #define evutil_socket_t int one of the following defines should control the evutil_socket_t support, which one, which version? #define LIBEVENT_VERSION #define Could you provide a updated patch without windows headers and with an updated evutil_socket_t check rule?",non_debt,-
thrift,1217,comment_4,Updated patch v0.2: - remove Windows stuff - use #ifdef HAVE_XXX - add cast macros,non_debt,-
thrift,1217,comment_5,"@Roger: thanks for your input, I uploaded a new improved patch Tested compilation on Ubuntu with vanilla libevent 1.14.13, and latest libevent 2.0.12. Patch based on latest Thrift trunk.",non_debt,-
thrift,1217,comment_6,"committed, thanks!",non_debt,-
thrift,1231,summary,Remove bogus include,code_debt,low_quality_code
thrift,1231,description,"TAsyncChannel.h includes TTransportUtils.h, but doesn't really need/use it.",code_debt,dead_code
thrift,1231,comment_0,"Committed, Thanks for the patch.",non_debt,-
thrift,1241,summary,php namespace generation,non_debt,-
thrift,1241,description,Patch is based mainly on but some more improvements: namespace can be specified with dots '.' like for every other language for example: namespace php com.onego.thrift would generate in php file namespace com\onego\thrift to generate php files with namespaces use: thrift --gen php:namespace53 example.thrift,code_debt,low_quality_code
thrift,1241,comment_0,"Thanks Darius, initial work looks great. Will want to change from namespace53 to just namespace since php5.3+ will have this option available and php5.4 is in rc right now so no sense in defining a specific version in the option. Can you attach this as a patch with asf inclusion please.",design_debt,non-optimal_design
thrift,1241,comment_1,enables namespace php support,non_debt,-
thrift,1241,comment_2,Changed according to your comments - now to generate php files with namespaces: thrift --gen php:namespace example.thrift,non_debt,-
thrift,1241,comment_3,committed,non_debt,-
thrift,1242,summary,Unable to use typedef for exceptions,non_debt,-
thrift,1242,description,Unable to use typedef to make a shortcut for an exception type. Using the same exception by referencing the full type works fine.,non_debt,-
thrift,1242,comment_0,An example demonstrating the problem.,non_debt,-
thrift,1242,comment_1,I was not able to reproduce any problems using thrift-0.10.0. Given the age of this defect I am going to resolve it as not reproducible.,non_debt,-
thrift,1242,comment_2,Closing all Thrift-0.10.0 Resolved Jira items (total: 12 that got left behind),non_debt,-
thrift,1243,summary,TAsyncChannel callbacks,non_debt,-
thrift,1243,description,"[Context: This patch is part of a larger patch to use thriftnb on Windows/VisualC++. See for more details.] When compiling using Visual C++ 2010, the compiler chokes on casting some callbacks bool(*)() to void(*)(). Although probably valid and supported by gcc, this is further complicated by the fact those casting seem unnecessary: for each of the callbacks returning bool in TAsyncChannel.h: 1. the returned value is never checked 2. they always return true Attached is a trivial patch based on 0.7.0, tested on Ubuntu 11.04 and Visual C++ 2010.",design_debt,non-optimal_design
thrift,1243,comment_0,"This makes sense, just committed your patch. Thanks Alexandre",non_debt,-
thrift,1243,comment_2,"Saw this a bit late but I'm concerned about this change. With this change, there is no way for implementors of to signal to the callers that something unexpected happened or any other error conditions. Just because the current implementation ignores the return values doesn't mean they're not useful. Is the recommendation now to throw a TException instead?",design_debt,non-optimal_design
thrift,1243,comment_3,"@diwaker: Thanks for your comments, I share your concerns and am working currently on a patch for 0.8 that will use exception handling, and more generally handle/propagate libevent errors, log errors... For example I'll propose to remove 'abort' calls in TEvhttpServer.cpp, and replace them by exceptions. Also one significant issue I plan to cover, is to avoid propagating exceptions to libevent (which is C code): see THRIFT-1222 for example.",design_debt,non-optimal_design
thrift,1243,comment_4,"Included is a patch on 0.8 that adds more error handling to the libevent based server. It also fixes 2 minor bugs (handle failure of evhttp_bind_socket and prevent exceptions to reach libevent in As far as I can tell, the question of @diwaker is still valid, there is no good way currently to have the callback of to be passed the exact failure. However I don't think returning a bool helps, because libevent is the one asynchronously calling, so the error will have to be handled by the client *within* the callback. This callback will invariably throw a when a disconnect or a 404 happens. I added to this patch the ability to log such failures to stderr, but it would take more changes (compiler?) to actually pass the error to the client, so that a call will throw the correct error (and *not* EOF). Finally I removed all abort calls, trying to replace them by appropriate exceptions instead. Thanks!",code_debt,low_quality_code
thrift,1243,comment_5,Re-opening with new patch for trunk (0.8),non_debt,-
thrift,1243,comment_6,"Thanks Alexandre, committed!",non_debt,-
thrift,1246,summary,Improve logging: Change TNonblockingServer internalRead to trace from warn,non_debt,-
thrift,1246,description,In the actual world we will have a lot of connection resets hence it will be better to log those messages as trace than logging as errors and warnings. Or we can make this logging configurable. Let me know thanks!,non_debt,-
thrift,1246,comment_0,This report is incomplete; the message(s) in question were not identified. I agree with you that handling an unexpected client disconnect should not be a warning or error. It happens all the time. Need more information on which logging statements are involved. Feel free to reopen this and submit a pull request if you have a fix.,non_debt,-
thrift,1248,summary,pointer subtraction in TMemoryBuffer relies on undefined behavior,design_debt,non-optimal_design
thrift,1248,description,"The ensureCanWrite function in TMemoryBuffer currently ""rebases"" the buffer pointers by subtracting the original buffer pointer from the pointer newly returned by realloc. While this seems to work fine on my linux setup(I couldn't force a reproducer), pointer subtraction between pointers that we allocated separately is an undefined operation. I have run into problems with this on platforms other than linux. I am attaching a patch that removes the undefined operation.",design_debt,non-optimal_design
thrift,1248,comment_0,", I see the point. Could you please * rebase the patch to current trunk * have a look whether wBound_ is really calculated correctly after your modifications being applied? Thank you!",non_debt,-
thrift,1248,comment_1,"Hi , any progress on this?",non_debt,-
thrift,1257,summary,thrift's dependency scope on should be 'provided',non_debt,-
thrift,1257,description,"libthrift 0.6.1 pom specifies as compulsory dependency. This causes the jar depending on this libthrift to be skipped by Tomcat with the following error: _Workaround_ While including libthrift-0.6.1 as dependency in maven pom, exclude using exclusions as:",non_debt,-
thrift,1257,comment_0,Changes scope of javax.servlet to provided so containers using the TServlet.java class can provided the desired servlet api,non_debt,-
thrift,1257,comment_1,Updated scope to provided,non_debt,-
thrift,1263,summary,publish ruby client to rubygems,non_debt,-
thrift,1263,description,As part of the release process release gem to rubygems.org,non_debt,-
thrift,1263,comment_0,Updates Rakefile to use rspec task for creating and deploying gem,non_debt,-
thrift,1263,comment_1,"Added to trunk, gem release for 0.7.0",non_debt,-
thrift,1263,comment_3,"Jake, this breaks the build... On Continous Integration Server we have ruby rspec 1.2.9 it seems that this modification requires rspec Do you see any possibility to have a solution which supports both versions? or do we need a version check within configure.ac similar to HAVE_RSPEC ?",non_debt,-
thrift,1263,comment_4,"I had seen that, wasnt sure on the best way to handle it yet. I'll come up with a fix for it today -Jake",non_debt,-
thrift,1269,summary,thrift: handle undeclared exceptions in the async,design_debt,non-optimal_design
thrift,1269,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Wed, 6 Jan 2010 02:10:19 +0000 Subject: [PATCH 01/33] thrift: handle undeclared exceptions in the async error cob Summary: This updates the generated TAsyncProcessor code to correctly handle when an undeclared exception type is passed into an error callback. It now sends back a T_EXCEPTION message to the client, just like the non-async code does. Previously the exception wouldn't be handled, causing the TEventWorker thread to exit. Test Plan: Tested changing the async sort tutorial to pass in a generic TException rather than a SortError, and verified that the client correctly received an exception. Conflicts:  | 38 1 files changed, 32 insertions(+), 6 deletions(-)",design_debt,non-optimal_design
thrift,1269,comment_0,Thanks Dave! committed!,non_debt,-
thrift,1271,summary,thrift: fix missing namespace in generated local,non_debt,-
thrift,1271,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Thu, 14 Jan 2010 23:02:40 +0000 Subject: [PATCH 03/33] thrift: fix missing namespace in generated local reflection Summary: The TypeSpec definitions for lists, sets, and maps did not prefix the contained type names with the correct namespace. This resulted in a compile failure when defining a list of types from another namespace. Test Plan: Tested building [internal fb thing]'s [internal fb thing] changes. The generated C++ code compiles now.  | 8 ++++- 1 files changed, 4 insertions(+), 4 deletions(-)",non_debt,-
thrift,1271,comment_0,I just committed this.,non_debt,-
thrift,1275,summary,"thrift: always prefix namespaces with "" ::""",non_debt,-
thrift,1275,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Thu, 4 Mar 2010 00:53:37 +0000 Subject: [PATCH 07/33] thrift: always prefix namespaces with "" ::"" Summary: Thrift always refers to namespaces using their full name. Therefore, it should prefix them with ""::"" to avoid accidentally matching a name defined in one of the current namespaces, rather than at the top-level. For example, if ServiceB is in namespace bar, and inherits from ServiceA in namespace foo, all code emitted for ServiceB now refers to ServiceA as ::foo::ServiceA instead of just foo::ServiceA. This allows the code to compile even if a namespace ::bar::foo also exists. An extra leading whitespace is also emitted, which is needed in cases when the name is used as the first template parameter, so that the emitted code contains ""< ::"" instead of ""<::"". Test Plan: jsong reported a build problem because of this name lookup error, and this change fixed his build. I also tested building [internal fb thing] and [internal fb thing], and they both built successfully. Tags: thrift  | 11 +++++++++-- 1 files changed, 9 insertions(+), 2 deletions(-)",code_debt,low_quality_code
thrift,1275,comment_0,I just committed this to TRUNK.,non_debt,-
thrift,1279,summary,type set is handled incorrectly when writing object,non_debt,-
thrift,1279,description,"Patch provided in doesn't handle object writing correctly: if one of the parameters is set struct MyStruct { 10: string id, 20: string name, 30: set<string} in generated file de-serialization of object is public function read($input) ... $xfer += $input-if (is_scalar($elem5)) { $this-} else { $this-} ... but when serializing, we are looping only through values of array: public function write($input) ... foreach ($this-{ if (is_scalar($iter6)) { $this- } $xfer += $output- } } ... But if our element is scalar - we should be iterating through keys. it should look like: foreach ($this-{ if (is_scalar($iter7)) { $xfer += $output- } else { $xfer += $output- } }",non_debt,-
thrift,1279,comment_0,"Committed, thanks for the patch",non_debt,-
thrift,1283,summary,thrift: wrap to 80,non_debt,-
thrift,1283,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Sat, 3 Apr 2010 02:49:43 +0000 Subject: [PATCH 12/33] thrift: wrap to 80 chars Summary: I'm planning on making some other changes to this function, and it will be easier to review them if the line-wrapping fixes are done separately. Test Plan: Built [three facebook projects] both with and without these changes, and made sure the resulting files were identical. Revert Plan: OK Conflicts:  | 149 1 files changed, 101 insertions(+), 48 deletions(-)",non_debt,-
thrift,1283,comment_0,I just committed this.,non_debt,-
thrift,1290,summary,thrift: TNonblockingServer: clean up state in the,code_debt,low_quality_code
thrift,1290,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Tue, 6 Apr 2010 20:43:23 +0000 Subject: [PATCH 17/33] thrift: TNonblockingServer: clean up state in the destructor Summary: Implement the TNonblockingServer destructor, so that it closes the listen socket, destroys the event_base, and deletes any TConnections left in the connectionStack_. However, TNonblockingServer doesn't keep track of active TConnection objects, so those objects are still leaked. As part of this, I also changed the code to use event_init() rather than event_base_new(). This way we won't set the global event_base inside libevent, and we can be sure that no one else will be using it after the TNonblockingServer is destroyed. I grepped through all of [fb code base] to check for any other direct uses of event_set(), and didn't see any places that weren't also using event_base_set(). Therefore it seems like it should be safe to stop initializing the global event_base pointer. Test Plan: Tested with the test code in [a fb unittest], which creates, stops, and then deletes several Ran it under valgrind, and now it only complains about any active connections being leaked. Revert Plan: OK  | 4 ++-- 1 files changed, 2 insertions(+), 2 deletions(-)",code_debt,low_quality_code
thrift,1290,comment_0,"Hm, this patch doesn't apply cleanly for me.",code_debt,low_quality_code
thrift,1290,comment_1,"svn up'd, resolved conflict",non_debt,-
thrift,1290,comment_2,I just committed this.,non_debt,-
thrift,1294,summary,thrift: fix log message typos in TSimpleServer,documentation_debt,low_quality_documentation
thrift,1294,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Tue, 6 Apr 2010 20:59:32 +0000 Subject: [PATCH 20/33] thrift: fix log message typos in TSimpleServer Summary: TSimpleSimple -- Also cleaned up some references that could be const. Test Plan: It compiles. Revert Plan: OK  | 15 +++++++++ 1 files changed, 9 insertions(+), 6 deletions(-)",documentation_debt,low_quality_documentation
thrift,1294,comment_0,I just committed this.,non_debt,-
thrift,1303,summary,Twisted tutorial client does not include correct generated path,non_debt,-
thrift,1303,description,None,non_debt,-
thrift,1303,comment_0,Updated client to include correct generated code path,non_debt,-
thrift,1311,summary,Recent changes on Java library break Java JSON protocol,non_debt,-
thrift,1311,description,"I see that some people have been working very hard on the Java library and that's really appreciated, however, some of the changes seem to have interfered with the java server JSON/HTTP implementation which is used to test the JS client. Please see below: thrift/lib/js/test$ ant testserver [...] [java] New connection thread [java] Incoming content: [java] Outgoing content: Error:Unexpected character:[ [java] New connection thread [java] Incoming content: [java] Outgoing content: Error:Unexpected character:[ ps.: I hope one day we can automatize these tests with node.js, or something like that, so we don't need to run this manually",test_debt,low_coverage
thrift,1311,comment_0,I just committed a fix to the Java serialization framework. Can you test again and see if the problem is solved?,non_debt,-
thrift,1311,comment_1,"Awesome, it works again Thanks heaps Bryan",non_debt,-
thrift,1314,summary,thrift: add TProcessorFactory,non_debt,-
thrift,1314,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Mon, 19 Apr 2010 19:09:16 +0000 Subject: [PATCH 25/33] thrift: add TProcessorFactory Summary: Adds a new TProcessorFactory class, and update all server classes to store a TProcessorFactory instead of a TProcessor. is called once for each newly accepted connection. This will allow a separate processor to be created for each connection, if desired. For now, everything always uses so all connections still end up using the same TProcessor. Some of the servers don't handle it well if getProcessor() throws an exception. TNonblockingServer may leak the fd and a TConnection object. TSimpleServer will exit and stop serving. However, this is no worse than the existing behavior if Test Plan: Ran the test code from [a fb unittest]. Revert Plan: OK Conflicts:  | 39 | 12 ++++++-- | 31 | 9 +++++- | 7 ++++- | 5 +++- 6 files changed, 89 insertions(+), 14 deletions(-)",code_debt,low_quality_code
thrift,1314,comment_0,Comitted.,non_debt,-
thrift,1317,summary,Remove copy constructibility from,non_debt,-
thrift,1317,description,"From Mon Sep 17 00:00:00 2001 From: Jordan DeLong Tue, 20 Apr 2010 19:21:45 +0000 Subject: [PATCH 28/33] Remove copy constructibility from Summary: Copy constructing this class currently will cause a mutex to get unlocked twice. It looks like this was an unintended consequence of changing the mutex_ member from a reference to a pointer. Test Plan: Compiled (random project using this). But anything this change breaks probably has a copy construction bug in their guard usage. Revert Plan: OK  | 3 ++- 1 files changed, 2 insertions(+), 1 deletions(-)",non_debt,-
thrift,1317,comment_0,Committed.,non_debt,-
thrift,1321,summary,Map serialization is broken in the Erlang library,non_debt,-
thrift,1321,description,dict:fold/3 always return ValData as a list and therefore breaks the pattern matching (guards).,non_debt,-
thrift,1321,comment_0,"I'm not seeing how this is a bug, I put together this to test And the map serializes and deserializes fine. Do you have a test case of this not working? I pretty extensively use map to talk erlang to java and java to erlang so if this were a bug I would expect to have seen it before, but then again I only really use map<string,string> so maybe that case works where others fail? Anyway, if you have an example that would be best, thanks.",non_debt,-
thrift,1321,comment_1,"It's broken if you use binary values... Proto2 = ValData, ProtoS0) - {ProtoS1, ok} = write(ProtoS0, {KeyType, KeyData}), {ProtoS2, ok} = write(ProtoS1, {ValType, ValData}), ProtoS2 end, Proto1, Data), dict:fold/3 always return ValData as a list... And therefore the the guard (in always match the first one... write(This0, {string, Str}) when is_list(Str) - {This1, ok} = write(This0, {i32, length(Str)}), {This2, ok} = write(This1, {This2, ok}; write(This0, {string, Bin}) when is_binary(Bin) - {This1, ok} = write(This0, {i32, size(Bin)}), {This2, ok} = write(This1, Bin), {This2, ok};",non_debt,-
thrift,1321,comment_2,"I see how that could be the case, but I'd like to reproduce this error with a simple case, starting with a thrift definition and doing (like in the snippet I included before). I used binary values in the snippet and things seemed to work, so I'm still not certain how this manifests itself. Do you have a test which shows the bug happening? Otherwise I can try to figure out a way to reproduce it sometime soon.",non_debt,-
thrift,1321,comment_3,"Okay, I see what the issue is here. You are using dict:append/3 which turns an entry in the dictionary into a list of entries. This doesn't actually work with thrift (or it probably does in some odd way). Instead you should be using dict:store/3 or dict:from_list/1 when creating the maps. See if I were to make the change you are suggesting it would actually break serialization completely for the normal cases where you are creating a map. Using append you are not creating a map (unless you are creating a map<string,list Using your first example you should be doing something like",non_debt,-
thrift,1321,comment_4,"Not actually a bug, see my last comment.",non_debt,-
thrift,1333,summary,Make RWGuard not copyable,non_debt,-
thrift,1333,description,"From Mon Sep 17 00:00:00 2001 From: Jordan DeLong Tue, 20 Apr 2010 19:35:56 +0000 Subject: [PATCH 29/33] Make RWGuard not copyable Summary: Copying this class would cause its contained mutex to unlock twice. Mark it noncopyable. Test Plan: Compiled [internal fb thing]. Revert Plan: OK  | 2 +- 1 files changed, 1 insertions(+), 1 deletions(-)",non_debt,-
thrift,1333,comment_0,"Committed, thanks Dave",non_debt,-
thrift,1349,summary,Remove unnecessary print outs,code_debt,low_quality_code
thrift,1349,description,There are a couple of spurious calls which can lead to lots of output when talking between slightly different versions of generated code. For instance launching a server which returns a new field to a client will result in the client printing out information about an unknown fid. This leads to lots of logging which you probably don't want. I'd like to remove these unless anyone in dev objects.,design_debt,non-optimal_design
thrift,1349,comment_0,"This bug just affected a production system. A new field was added to a message. The excess logging is now causing the Erlang based client to crash. Interestingly the features page, claims: ""struct evolution - The addition and removal of fields is handled without breaking existing clients by using integer identifiers for fields""",non_debt,-
thrift,1349,comment_1,committed,non_debt,-
thrift,1350,summary,Go library code does not build as of r60 (most recent release),non_debt,-
thrift,1350,description,"Go has moved it's URL-handling code into a ""url"" package. Also, strings.Split no longer takes a 3rd parameter. Not sure how I attach a patch, but here's a diff: Or, a simple run of ""gofix"" should probably handle it.",non_debt,-
thrift,1350,comment_0,Diff from head of trunk.,non_debt,-
thrift,1350,comment_1,"Kyle, is this in the GO trunk or the latest released version of GO? We ran into a problem like this before where GO changed how they handled a library in their trunk version and broke compatibility with our current release. We opted to keep the driver current with the latest released version of GO and not their trunk. If this is latest released GO version I'll gladly add it. Thanks for the patch, if you have any questions about contributing please see or feel free to email our dev mailing list with any questions at",non_debt,-
thrift,1350,comment_2,"Yep, this is for the newest release version, r60 And yes, good targeting releases is a good idea. That's what various distros will be carrying, what AppEngine runs, and what users are encouraged to use. Most breakages between releases can be fixed with a simple ""gofix"" tool that rewrites known changes.",non_debt,-
thrift,1350,comment_3,"Thanks for the patch, committed",non_debt,-
thrift,1365,summary,unintentionally writes a variable length byte array,non_debt,-
thrift,1365,description,"BitSet.length() only returns the highest-set-bit, meaning that if you have more than 8 optional fields, and the 9+th optional field is unset, then only the first byte's worth of bits will be serialized. This leads to a problem on the read side, where the bit vector is assumed to be fixed width, and arbitrary deserialization occurs.",non_debt,-
thrift,1365,comment_0,This patch includes a test and fixes to the library and the compiler.,non_debt,-
thrift,1367,summary,"Mac OSX build fails with ""no such file to load -- spec/rake/spectask""",non_debt,-
thrift,1367,description,"Running ""make"" successfully builds the compiler and most of the libs but fails out on the ruby lib with the message: Making all in rb /usr/bin/rake (in ... thrift/lib/rb) rake aborted! no such file to load -- spec/rake/spectask ... The one hit I found online that might be related: ""The ""no such file to load -- spec/rake/spectask"" error is because this gem's specs use rspec1. I just submitted a pull request to update to rspec2."" I'm running with pkgconfig from MacPorts and starting the build from scratch (bootstrap.sh & onwards).",non_debt,-
thrift,1367,comment_0,The ci servers use rspec1 which is why we switched the ruby build over to using bundler to automatically handle the dependencies. gem install bundler and then bundle exec rake and you should be all set,non_debt,-
thrift,1367,comment_1,"I also had to install some other packages since I don't have much of the ruby dev tools. However 'bundle exec rake' wants a specific version of rspec: The rspec version installed was 2.6.4. I started to look for the required version of rspec. Since I don't need ruby at the moment, what's the easiest way to bypass building the ruby libs? Thanks!",non_debt,-
thrift,1367,comment_2,Bundler will install the correct dependency versions for you. You can skip building any of the client libs by passing the --without-{lib} configure flag. ./configure --without-rb,non_debt,-
thrift,1367,comment_3,"This is odd. I ran ""./configure --without-rb"" and see the argument in the config.log file, but the configure script continues to have references to rb and ruby. 'make' still fails trying to build the ruby lib. Confirmed that this happens from a pristine source tree. Update: ""./configure --with-rb=no"" tries to build rb as well. When I ran ""bundle exec rake"", it complained that it couldn't find mongrel in any of the gem sources so I called ""gem install mongrel"". Then it complained about not finding rspec... Could these problems be specific to OSX?",non_debt,-
thrift,1367,comment_4,"I'm on os x as well. Below is the configure as well as generating the gem from the lib/rb folder jfarrell:rb jake$ ./configure --without-ruby .... Building Ruby Library ........ : no jfarrell:rb jake$ cd lib/rb jfarrell:rb jake$ bundle Fetching source index for Using rake (0.9.2) Using (2.5.0) Using daemons (1.1.4) Using fastthread (1.0.7) Using gem_plugin (0.2.3) Using mongrel (1.1.5) Using rspec (1.3.2) Using thrift (0.8.0.1) from source at Using bundler (1.0.18) Your bundle is complete! jfarrell:rb jake$ bundle exec rake 271 examples, 0 failures, 1 pending gem build thrift.gemspec Successfully built RubyGem Name: thrift Version: 0.8.0.1 File: thrift-0.8.0.1.gem",non_debt,-
thrift,1367,comment_5,Thanks much! configure is doing the right thing now using the switch --without-ruby (instead of --without-rb) Your bundle commands work too. This is very helpful.,non_debt,-
thrift,1375,summary,fixed a hex char conversion bug in TJSONProtocol,non_debt,-
thrift,1375,description,"From Mon Sep 17 00:00:00 2001 From: Jiakai Liu Thu, 18 Mar 2010 07:35:26 +0000 Subject: [PATCH 3/9] Fixed a hex char conversion bug in TJSONProtocol Summary: Fixed a hex char conversion bug in TJSONProtocol: 10 ~ 15 <= Test Plan: Tested on my dev machine. Revert Plan: OK  | 4 ++-- 1 files changed, 2 insertions(+), 2 deletions(-)",non_debt,-
thrift,1375,comment_0,Committed,non_debt,-
thrift,1393,summary,thrown from THttpClient contain superfluous slashes in the Exception message,code_debt,low_quality_code
thrift,1393,description,"This is a very minor issue, but should be addressed nonetheless. The THttpClient class ensures the $uri_ property has a slash prefixed by appending one if needed in the constructor. However in THttpClient::read, there are 2 exceptions thrown where a slash is concatenated between the port and uri. This results in a superfluous slash in the TTransportException message. Example: ""THttpClient: Could not read 184549154 bytes from",code_debt,low_quality_code
thrift,1393,comment_0,Attaching patch,non_debt,-
thrift,1393,comment_1,Committed,non_debt,-
thrift,1408,summary,Delphi Test Server: Exception test case fails due to naming conflict with e.message,non_debt,-
thrift,1408,description,"FAILED TESTS: - x.__isset_Message_ = false - x.Message = """"",non_debt,-
thrift,1408,comment_0,FIX: Added the missing underscore in the TestServer code.,non_debt,-
thrift,1408,comment_1,"Extended fix: - added the missing underscore - added a new base class TException to prevent accidental read/write of ""message"" property - modified code generator accordingly",non_debt,-
thrift,1408,comment_2,Committed,non_debt,-
thrift,1413,summary,Generated code does not read MapEnd / ListEnd / SetEnd,non_debt,-
thrift,1413,description,"The generated code lacks calls to ReadMapEnd, ReadListEnd and ReadSetEnd. Although actually no problem occurs with the BinaryProtocol, other protocols (like JSON) do some important things there.",non_debt,-
thrift,1413,comment_0,FIX: Added the missing codeblock in the generator.,non_debt,-
thrift,1413,comment_1,"Thanks, Jens.This patch works great.",non_debt,-
thrift,1413,comment_2,Committed,non_debt,-
thrift,1414,summary,bufferoverflow in c_glib buffered transport/socket client,non_debt,-
thrift,1414,description,"Quote of comment in source: if the buffer is still smaller than what we want to read, then just read it directly. But the code reading into the tempdata with size of the buffer and reading all data into this. file: line 74/98 Also if the buffer is still bigger that what we want to read, then reading the buffer size. But recv blocks than and waiting of data if there nothing to read after the receiving data len. file: line 118 i attached a patch that fix this problems but i dont know if all of this is correct.",non_debt,-
thrift,1414,comment_0,duplicate was fixed: THRIFT-1654,non_debt,-
thrift,1414,comment_1,Can you tell where is this fix ? Cannot find it in trunk yet.,non_debt,-
thrift,1414,comment_2,"Andris, not sure why the status was changed not his ticket without the patch being added. Can you please rebase the patch against trunk. thanks",non_debt,-
thrift,1414,comment_3,The patch was contributed by Christian Zimnick.,non_debt,-
thrift,1414,comment_4,The attached patch can be applied to 0.9.0.,non_debt,-
thrift,1414,comment_5,thanks!,non_debt,-
thrift,1426,summary,Dist package missing files for release 0.8,non_debt,-
thrift,1426,description,"dist package missing javame, cpp concurrency headers, nodejs, erl tutorial, all windows additions, and other random files",documentation_debt,low_quality_documentation
thrift,1426,comment_0,Updates dist target to include all files necessary for a release,non_debt,-
thrift,1426,comment_1,committed r1202686,non_debt,-
thrift,1431,summary,Rename 'sys' module to 'util',non_debt,-
thrift,1431,description,"Since version 0.3.0 (2010.10.23, the 'sys' module was renamed 'util' while keeping the same API.",non_debt,-
thrift,1431,comment_0,"Roger, can you take a look at this",non_debt,-
thrift,1431,comment_1,committed. Could you please create the patch from thrift source root directory? This makes it much easier to handle. If you have some spare time... we need a test suite for node.js THRIFT-1134 ;-),test_debt,low_coverage
thrift,1431,comment_2,"Just so I know how to do it, as far as i remember, I issued `svn diff > file.diff` from the Thrift root directory, is it not correct? I missed github: ) I might take a look at the test case but not before a few weeks. Thanks for applying the patch. Also, I dont think you should wait to publish those changes to NPM.",non_debt,-
thrift,1431,comment_4,"David, we will push this change to NPM with the 0.8 release when it is out, we do not update the clients per patch change",non_debt,-
thrift,1440,summary,debian packaging: minor-ish policy problems,non_debt,-
thrift,1440,description,"A listing of detectable policy problems in the thrift Debian packaging (in contrib/) can be found with a lintian run: I'll note some of them here for posterity. h3. thrift source: libthrift-dev on libthrift0 The libthrift-dev package should have a versioned dependency on libthrift0, i.e., in debian/control: h3. thrift source: build-depends You don't need the ""build-essential"" bit in Build-Depends. h3. thrift-compiler: Syntax is a bit off in debian/control for the Description fields; I'll attach a patch. h3. thrift-compiler: usr/bin/thrift You need a man page for /usr/bin/thrift. h3. python-thrift-dbg: python-thrift-dbg = The python-thrift-dbg package should be in Section: debug. h3. python-thrift-dbg: dir-in-usr-local usr/local/lib/ Debian packages shouldn't be shipping anything in /usr/local; that's supposed to be reserved for the local system admin. There isn't much reason for this anyway; the dirs being shipped by python-thrift-dbg here are empty. h3. libthrift-ruby: libthrift-ruby = The libthrift-ruby package should be in Section: ruby. Also, according to , it looks like Ruby packages are undergoing a name change in the current Debian testing suite. libthrift-ruby probably needs to become ruby-thrift and switch to using gem2deb. h3. libthrift-ruby: This will probably be addressed under THRIFT-1421. h3. libthrift0: libthrift-c-glib0 This is complaining because the package name of a library package should usually reflect the soname of the included library (see [chapter 5 of the Debian Library Packaging for more info). Something is fishy here, though. Did you intend to distribute the c-glib library as ""libthrift0""? If so, where is the cpp library supposed to go? I don't think I see it after a quick search through the packages. h3. php5-thrift: See the lintian explanation for detailed info. Basically, you need some extra Sauce to add a dependency to php5-thrift on a PHP with a compatible API version. h3. libthrift-java: libthrift-java = libthrift-java should be Section: java h3. libthrift-cil: libthrift-cil = libthrift-cil should be Section: cli-mono h3. libthrift-cil: Thrift.dll shouldn't have its executable bit set. h3. libthrift-perl: usr/usr/ Yeah, installing into /usr/usr/local/lib is kinda wacko. Ought to be /usr/lib. - And as a final note, a lot of the packaging here could be pretty greatly simplified and better future-proofed using the Debhelper 7 command sequencer (""{{dh}}"").",build_debt,under-declared_dependencies
thrift,1440,comment_0,"Thanks for Reporting that, we like to apply your patches to improve packaging and build procedure in general. The Build Job for Debian Packages is here: currently we only hav a amd64 at Apache Build Infrastructure... Do some simply tricks or cross compile approaches exist to build more architectures on that build server?",design_debt,non-optimal_design
thrift,1440,comment_1,"Oops, neglected to attach the patch I promised for the part. Note this does not fix all the issues, only one.",non_debt,-
thrift,1440,comment_2,"The only reasonable way to do that is to have an i386 chroot on the amd64 machine, and build inside that (tools like pbuilder and cowbuilder make that sort of thing easier). Can Apache not provide an i386 build machine, though? I have no idea if a pbuilder chroot would even be a viable option under Jenkins.",non_debt,-
thrift,1440,comment_3,Thanks Paul! committed the Apache needs sponsors...that's why we currently do not have a i386 or x86_64. I will have a look at cowbuilder and pbuilder.,non_debt,-
thrift,1440,comment_5,"Cpp libraries were there, in libthrift0 package, in version 0.7, and disappeared in 0.8 (and 0.9 too). So I think the actual problem is that somewhat they got removed. So, at the moment, no debian package after 0.7 is providing cpp libraries.",non_debt,-
thrift,1440,comment_6,Fixed in THRIFT-1825,non_debt,-
thrift,1446,summary,Compile error with Delphi 2009 in constant initializer,non_debt,-
thrift,1446,description,Generated code cannot be compiled with Delphi 2009 due to the bug in constant initializer.,non_debt,-
thrift,1446,comment_0,*Fixed the problem in TConstants initialization. *Added import declaration of IsDebuggerPresent() which is not included in Delphi 2009 RTL.,non_debt,-
thrift,1446,comment_1,+1 Tested: - confirmed the bug =- all test cases are working,non_debt,-
thrift,1446,comment_2,"Committed, thanks for the patch",non_debt,-
thrift,1449,summary,Ruby client does not work on solaris (?),non_debt,-
thrift,1449,description,"Without the attached patch, I get the error: getaddrinfo: service name not available for the specified socket type from line 37 of in the Ruby library on Solaris.",non_debt,-
thrift,1449,comment_0,"I just committed this. Thanks for the patch, Erik!",non_debt,-
thrift,1449,comment_2,Thanks!,non_debt,-
thrift,1452,summary,generate a swap() method for all generated structs,non_debt,-
thrift,1452,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Mon, 21 Jun 2010 20:24:50 +0000 Subject: [PATCH 3/5] generate a swap() method for all generated structs Summary: Andrei mentioned it would be convenient if thrift generated swap() methods for all C++ thrift types. Apparently the ads team manually writes swap() functions for many of their thrift data types, but have to keep updating them by hand when new fields are added to the thrift interface. This updates the thrift compiler to emit swap() methods for user-defined types. For now, I decided not to emit swap() methods for the internal XXX_args, XXX_pargs, XXX_result, and XXX_presult types. Test Plan: Tested compiling serveral internal projects. I didn't actually test the generated swap() functions, but they look okay. DiffCamp Revision: 124773 Reviewed By: aalexandre Commenters: dreiss, edhall CC: davidrecordon, achao, dreiss, kholst, aalexandre, simpkins, edhall, thrift-team@lists Revert Plan: OK git-svn-id: fb thing]@30392  | 70 1 files changed, 67 insertions(+), 3 deletions(-)",design_debt,non-optimal_design
thrift,1452,comment_0,"Line 1514 of t_cpp_generator.cc in the function has an unused variable ttype, any reason for this or can it be removed? t_type *ttype =",code_debt,dead_code
thrift,1452,comment_1,Nope no reason it looks unused in our codebase as well.,code_debt,dead_code
thrift,1452,comment_2,"Thanks Dave, committed",non_debt,-
thrift,1466,summary,Proper Documentation for Thrift C Glib,documentation_debt,low_quality_documentation
thrift,1466,description,"When I try to generate the cglib code for the Cassandra Interface, I do not get a skeleton file. Also, I do not see any example files onto how to use the generated code, and I can't seem to find any examples around. So can anybody please offer some proper examples on how to use the Thrift generated glibc code for Cassandra ? Thanks",documentation_debt,low_quality_documentation
thrift,1466,comment_0,"Currently there are no tutorials available for this lib, also not all generators create a skeleton as you have seen. Your best starting point is the test cases included with the lib lib/c_glib/test/. We are working on trying to create better documentation as we redo the website, but right now your best bet is to email the dev list with any questions you have",documentation_debt,low_quality_documentation
thrift,1466,comment_1,c_glib tutorial issue is THRIFT-976 Patches are welcome!,non_debt,-
thrift,1476,summary,Build universal library binaries on Mac OSX,non_debt,-
thrift,1476,description,"I would like to request a build option to create universal binaries on Mac OSX. The resulting library would support i386, x86_64 and ppc platforms. For reference, Jake F. posted a work-around to target different platforms - [edit: additionally pass LDFLAGS and CXXFLAGS with the same architecture]",non_debt,-
thrift,1476,comment_0,"As a side note, building for ppc on OSX 10.7.1 (Lion) and XCode 4.1 fails. i386 and x86_64 build successfully. Output from configure: checking whether the C compiler works... no configure: error: in configure: error: C compiler cannot create executables See `config.log' for more details config.log: -- Target: i686-apple-darwin11 Configured with: --disable-checking --enable-werror --prefix=/D --mandir=/share/man --program-transfor --tar Thread model: posix gcc version 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2335.15.00) configure:3964: $? = 0 configure:3953: gcc -V llvm-gcc-4.2: argument to `-V' is missing configure:3964: $? = 1 configure:3953: gcc -qversion no input files configure:3964: $? = 1 configure:3984: checking whether the C compiler works configure:4006: gcc -arch ppc -arch ppc conftest.c llvm-gcc-4.2: error trying to exec execvp: No such file or directory configure:4010: $? = 255 configure:4048: result: no configure: failed program was: | /* confdefs.h */ | #define PACKAGE_NAME ""thrift"" | #define PACKAGE_TARNAME ""thrift"" | #define PACKAGE_VERSION ""0.8.0"" | #define PACKAGE_STRING ""thrift 0.8.0"" | #define PACKAGE_BUGREPORT """" | #define PACKAGE_URL """" | #define PACKAGE ""thrift"" | #define VERSION ""0.8.0"" | /* end confdefs.h. */ | | int | main () | { | | ; | return 0; | } configure:4053: error: in configure:4055: error: C compiler cannot create executables See `config.log' for more details",non_debt,-
thrift,1476,comment_1,Wasn't ppc support dropped in the 10.4 or 10.5 sdk? you may need to have an older version of Xcode installed in order to have ppc arch support,non_debt,-
thrift,1476,comment_2,"Good point, that may well be the case. I tried building on a 10.5.8 system and configure just won't enable building the C++ library, even passing --with-cpp to try to force it. Here's the latter portion of the console output from configure. If it'd be more useful, I can upload the config.log file. If something is missing that you think should be present, please skim the output of configure to find the missing component. Details are present in config.log.",non_debt,-
thrift,1476,comment_3,"A 10.5 Intel system is able to build for ppc architecture. A closer inspection of config.log revealed that boost was not installed on the particular system, causing it to skip the C++ libs. It appears that the Thrift build scripts create a test binary and tries to execute it during the build. 10.5 ships with Rosetta so it can run ppc binaries. However even though a 10.6+ system can build ppc with the right version of Xcode, the build fails on an Intel system because it can't execute the ppc binary. Please consider removing the execution step of the binary if it does not match the current architecture.",non_debt,-
thrift,1476,comment_4,do you use the llvm-g++ frontend or clang++ ? see also for clang++,non_debt,-
thrift,1476,comment_5,"The 10.7 system uses (Based on Apple Inc. build 5658) (LLVM build 2335.15.00). I can't tell which the 10.5 system uses. Closest match I could find in config.log is (Apple Inc. build 5493). As far as I can tell, both are using default XCode settings.",non_debt,-
thrift,1476,comment_6,This issue is really old. Given there's been no updates for over 5 years I am going to mark it resolved and if folks want to reopen it that's ok.,non_debt,-
thrift,1476,comment_7,This issue was resolved without a fixVersion but was not closed - closing.,non_debt,-
thrift,1480,summary,"python: remove tabs, adjust whitespace and address PEP8 warnings",code_debt,low_quality_code
thrift,1480,description,"The python library files have some inconsistencies (different indent levels and docstring placement) and the pep8 linter produces dozens of warnings. There are also several places where tabs are used instead of spaces, which is not good. This patch addresses almost all of the pep8 issues with as little modification of the code as possible. This patch: * converts 3 instances of tabs into the correct number of spaces * removes unnecessary trailing semicolons and backslashes * changes None comparisons to be identity based, 'x != None' becomes 'x is not None' in a handful of places * removes unnecessary '== True' in one if statement * wraps lines at 80 characters and removes trailing whitespace * corrects a handful of grammar problems in docstrings (mostly to help with 80 char line wrap) * converts all the docstrings to use """""" (instead of ''' or "") and makes placement consistent * fixes pep8 warnings about missing spaces around operators, e.g. (a-b) becomes (a - b) * adjusts ordering of stdlib imports to be alphabetical (could be better still) * correct internal indent depths of methods when they switch from 2 to 4 spaces There's a mix of files that use 4-spaces for indentation, versus the majority which use 2-spaces for indentation. This patch doesn't change that. I wanted to get the code as pep8 clean as possible and touch as few lines as possible to get it there. The TType constants defined in Thrift.py have some nice vertical whitespace that isn't pep8-happy, but it looked too clean to touch so I left it unchanged. After this patch, the pep8 utility only reports two warnings: # ""indentation is not a multiple of four"" for most files (no biggie) # ""multiple spaces before operator"" in Thrift.py for the TTypes class constants The unit tests all pass with this patch.",code_debt,low_quality_code
thrift,1480,comment_0,I just committed this.,non_debt,-
thrift,1487,summary,"Namespace problem, compile fails on generated code",non_debt,-
thrift,1487,description,"you can find build log at , and Thrift IDL file at this problem occurred after THRIFT-1275 commited, I'm not c++ guy, There are two solution i found to resolve this problem, 1. prefix all namespace with ""::"" 2. avoid namespace conflict, change cpp namespace from to ""cassandra"" in cassandra.thrift",non_debt,-
thrift,1487,comment_0,"this is very old, please reopen or create a new issue",non_debt,-
thrift,1503,summary,No FramedTransport support in cocoa,non_debt,-
thrift,1503,description,"There is no support for Framed Transport in the cocoa support library, making it impossible for cocoa clients to connect to cassandra as this is now the default transport. Attached is an implementation",non_debt,-
thrift,1503,comment_0,"Jools, Thanks for the initial work on this, please see for submitting patches",non_debt,-
thrift,1503,comment_1,"Is there any progress on this? It would be great to have FramedTransport support in cocoa, as it's being used more and more.",requirement_debt,requirement_partially_implemented
thrift,1503,comment_2,Duplicate of THRIFT-1792,non_debt,-
thrift,1504,summary,Cocoa Generator should use local file imports for base Thrift headers,code_debt,low_quality_code
thrift,1504,description,"The Cocoa generator imports various thrift files as global imports instead of local importss. This ends up requiring that the user alter their header search paths to include the directories the generated thrift code is in, or the user has to edit the generated files to remove this. e.g. #import <TProtocol.h#import <TProcessor.h vs. #import ""TProtocol.h"" #import #import ""TProtocolUtil.h"" #import ""TProcessor.h""",build_debt,build_others
thrift,1504,comment_0,Attaching patch to fix #import syntax.,non_debt,-
thrift,1504,comment_1,"Thanks for the patch, committed",non_debt,-
thrift,1526,summary,Union TupleSchemeFactory returns StandardSchemes,non_debt,-
thrift,1526,description,None,non_debt,-
thrift,1533,summary,Make TTransport should be Closeable,design_debt,non-optimal_design
thrift,1533,description,should implement the {{Closable}} interface. Doing so will allow users to perform,design_debt,non-optimal_design
thrift,1533,comment_0,This patch implements the Closeable interface for the TTransport and TServerTransport classes. It does not modify the implementations to throw the IOException as this would be a breaking change.,non_debt,-
thrift,1533,comment_1,"+1, good to commit",non_debt,-
thrift,1533,comment_2,Committed.,non_debt,-
thrift,1541,summary,The thrift compiler is not buildable with visual studio 2010 in Windows 7,non_debt,-
thrift,1541,description,"The thrift compiler is not buildable in VS2010 using the current solution and project files. The issues are: 1. No unistd.h in Visual Studio 2. The calls to flex and bison fail, because there is a space between ""-o"" and its value.",non_debt,-
thrift,1541,comment_0,"Patch from Possible issues: - The mingw-Build was not checked. - The project files contains a big diff, but only small changes were applied",non_debt,-
thrift,1541,comment_1,I guess this is outdated,non_debt,-
thrift,1570,summary,Correctly generate code for empty services.,non_debt,-
thrift,1570,description,"From Mon Sep 17 00:00:00 2001 From: Peter Griess Thu, 16 Dec 2010 21:55:32 +0000 Subject: [PATCH 12/12] Correctly generate code for empty services. Summary: - Don't emit the 'else' clause for the method switch in the generated client if the service doesn't have any methods. Test Plan: - Tested generating Python code for an empty service before and after. DiffCamp Revision: 193807 Reviewers: rajesh, simpkins CC: mpage, doug, dreiss, rajesh, thrift-team@lists Tasks: #457923: Thrift generates broken Python code for empty services Revert Plan: OK git-svn-id: fb thing]@46847  | 11 +++++++- 1 files changed, 7 insertions(+), 4 deletions(-)",non_debt,-
thrift,1570,comment_0,committed,non_debt,-
thrift,1573,summary,Error on TNonblockingServer constructor,non_debt,-
thrift,1573,description,"If the TNonblockingServer is to construct: TNonblockingServer protocolFactory, port, threadManager); the compiler generates an error: error C2664: &): cannot convert parameter 1 from ""const boost::shared_ptr<T patch:",non_debt,-
thrift,1573,comment_0,committed,non_debt,-
thrift,1573,comment_1,committed,non_debt,-
thrift,1583,summary,c_glib leaks memory,code_debt,low_quality_code
thrift,1583,description,The generated code has memory leaks if you create and destroy an object. I've fixed the main problem on this git repo on branch c_glib_0.8.x but commits can be moved to trunk easily. There are still some errors to control on generated code (there are comments in the code). I'm planning to fix in a short term if you validate the current patches. This git repository has also the patches sent on issues 1582 and 1578 that are also related:,code_debt,low_quality_code
thrift,1583,comment_0,"I've added more commits to the git repository mentioned above, I will also make a pull request on github",non_debt,-
thrift,1583,comment_1,"I applied your 19.patch from the pull request, but *make check* fails within lib/c_glib ... could you fix this?",non_debt,-
thrift,1583,comment_2,"Thanks for the inclusion. Sure I can solve the issue, yesterday I notice that too. I'll sent a pull request when I fix it.",non_debt,-
thrift,1583,comment_3,I added new commits to solve the problems with the tests (they have been automatically included in the pull request),non_debt,-
thrift,1583,comment_4,I detected another memory corruption with maps. The commit is on the pull request too.,non_debt,-
thrift,1583,comment_5,committed,non_debt,-
thrift,1584,summary,Error: could not SetMinThreads in ThreadPool on single-core machines,non_debt,-
thrift,1584,description,"On a linux system with single cpu core, might throw an exception like ""Error: could not SetMinThreads in ThreadPool"". According to SetMinThreads description in MSDN: If you specify a negative number or a number larger than the maximum number of active thread pool threads (obtained using GetMaxThreads), SetMinThreads returns false and does not change either of the minimum values. The default DEFAULT_MIN_THREADS value is 10, while GetMaxThreads obtains 100 and 4 on a single-cpu computer. This make SetMinThreads function failed. The number obtained by GetMaxThreads is associated with the number of cpu core. So I suggest to change the order of SetMinThreads and SetMaxThreads, like:",non_debt,-
thrift,1584,comment_0,"Today i needed the TThreadPoolServer. I get exactly the same error! My CPU is a dual-core, my OS is Linux Mint 17 and I'm using Mono 4.0.",non_debt,-
thrift,1584,comment_1,Makes sense. Patch attached.,non_debt,-
thrift,1584,comment_2,"Committed, thanks for spotting this!",non_debt,-
thrift,1585,summary,C++ library fails to build with OS X pthread implementation,non_debt,-
thrift,1585,description,"On OS X, {{pthread_t}} is but the current implementation assumes that it is implicitly convertible to {{uint64_t}}. This yields to:",non_debt,-
thrift,1585,comment_0,"Can we get this patch committed? The existing code (before the patch) is broken in practice on OS X, and it violates a basic rules: - it assumes the pthread implementation defines pthread_t as a type that is binary-compatible with uint64_t - it makes a different declaration of the interface Thread abstraction interface (specifically Thread::is_current) depending on the threading implementation - it exposes an type (pthread_t) on the interface",non_debt,-
thrift,1585,comment_1,"BTW, this is the same issue as THRIFT-1488 (though I like this patch better).",non_debt,-
thrift,1585,comment_2,thanks David!,non_debt,-
thrift,1585,comment_4,Seems like some of the Python test failed  spurious error?,non_debt,-
thrift,1585,comment_5,"yes, the python test... we need a fix because we run under some circumstances tests on ci in parallel? bloking ports? We probably have to define a build order to avoid this.",non_debt,-
thrift,1589,summary,configure.ac should require at least boost 1.41,non_debt,-
thrift,1589,description,None,non_debt,-
thrift,1589,comment_0,committed,non_debt,-
thrift,1589,comment_2,"build slaves need 1.40.0, changed!",non_debt,-
thrift,1595,summary,Java test server should follow the documented behavior as of THRIFT-1590,non_debt,-
thrift,1595,description,The implementation of the java test server should be the same as the languages so our cross platform tests can work.,design_debt,non-optimal_design
thrift,1595,comment_0,"Hi! The patch contains a bit more code than this issue's requested. I've implemented a common CLI for Java TestClient and TestServer based on the c++ version. TestClient and TestServer support various protocols, transports and even ssl connection. I've also added switching server-type in TestServer. New test cases's been added, but I think that test.sh should be rewritten. For example in Java, there is which is another implementation of FramedTransport, but test.sh doesn't support this schema. We test also only one server_type for every language in test.sh. Test.sh shows that there are some bugs, but I hope it's not my fault :-) Any comments are welcome!",non_debt,-
thrift,1595,comment_1,"Thanks Kamil! this looks really good, the only issue I have...patch does not apply Could you please rebase the patch?",non_debt,-
thrift,1595,comment_2,"Sorry, It should work now :)",non_debt,-
thrift,1595,comment_3,Thanks Kamil! This is a super great improvement for the cross language test suite! all the best roger ;-r,non_debt,-
thrift,1624,summary,Isset Generated differently on different platforms,non_debt,-
thrift,1624,description,"Genereated java code is different depending on the environment. Example Service: CentOS 5 result: Mac OS X result: The inconsistency is a problem as we have a hybrid env. Also we are unsure which is correct. I have dug around in the code and is seems to come down to this method ""tstruct-",non_debt,-
thrift,1624,comment_0,Are you using the same versions of thrift between your two hosts? And where did you get them from?,non_debt,-
thrift,1624,comment_1,"Yes, they are the same version. I compiled them both from trunk.",non_debt,-
thrift,1624,comment_2,"Here's a theory: maybe t_struct::is_union_ is just an uninitialized variable? The t_struct constructor doesn't initialize it. thrifty.yy line 724 will initialize it for struct/union declarations, but I'm guessing that codepath is skipped for service declarations.",code_debt,low_quality_code
thrift,1624,comment_3,"Nathaniel, could you see if the following patch fixes the issue?",non_debt,-
thrift,1624,comment_4,"Hey Bryan, would you consider merging this patch? Nathaniel never replied to confirm that it fixed his issue, but fixing an uninitialized variable couldn't hurt.",code_debt,low_quality_code
thrift,1624,comment_5,I just committed this patch. Should we close this ticket?,non_debt,-
thrift,1624,comment_7,Sorry I forgot to get back to you on this. The patch worked great. Feel free to close the issue.,non_debt,-
thrift,1633,summary,Add windows build configurations to support static linking,non_debt,-
thrift,1633,description,These changes should allow thrift to be built for an application that uses static linking. I copied the existing debug/release configurations to debug-mt/release-mt and changed the runtime library linking to /MT.,non_debt,-
thrift,1633,comment_0,committed,non_debt,-
thrift,1648,summary,NodeJS clients always receive 0 for 'double' values.,non_debt,-
thrift,1648,description,"During implementation of Apache Thrift <- The server was written in Java, and has a test suite working in Java. It functions correctly, and receives the actual value for doubles.",non_debt,-
thrift,1648,comment_0,please see THRIFT-1353,non_debt,-
thrift,1658,summary,Java thrift server is not throwing,non_debt,-
thrift,1658,description,"When using thrift to communicate between Java clients and servers, I am no longer seeing when an unexpected exception is thrown on the servers side. Thrift 0.6 works, but thrift 0.8 does not. Looking at the Processor code generated by 0.6 it catches Throwable and returns a TApplication exception to the client. The Processor code generated by 0.8 does not do this.",non_debt,-
thrift,1658,comment_0,I hope this wasn't intentional. (I added this kind of error catching for Java back in THRIFT-378 and our experience has been that it is a Good Thing.),non_debt,-
thrift,1658,comment_1,"Looking at the history of svn commit 1063907 has the code to generate the code. svn commit 1068487 seems to remove the code. The comment for commit 1068487 is ""THRIFT-447. java: Make an abstract base Client class so we can generate less code""",non_debt,-
thrift,1658,comment_2,Here is a link to the diff. The part of the diff that I think causing this issue is below. Maybe this functionality was supposed to be moved to libthrift.jar but did not make it?,non_debt,-
thrift,1658,comment_3,"I took at stab at fixing this, here is a patch. This is my first time to really look at the thrift internals in detail, so feedback is welcome.",non_debt,-
thrift,1658,comment_4,FYI I create the patch off of the 0.8.x branch since this is the version I am working with...,non_debt,-
thrift,1658,comment_5,Checking with Bryan to see if he removed it for a specific reason or if this was just an oversight,non_debt,-
thrift,1658,comment_6,Committed,non_debt,-
thrift,1661,summary,[PATCH] Add --with-qt4 configure option,non_debt,-
thrift,1661,description,"In MacPorts we don't want packages to pick up dependencies unless explicitly instructed to, so it's useful to explicitly turn off Qt support otherwise it'll find Qt4 even though the package doesn't list it as a dependency. I chose --with-qt4 since Qt5 will be out soon and I don't know if this code is supported in Qt5.",build_debt,over-declared_dependencies
thrift,1661,comment_0,"Thanks for the patch, committed",non_debt,-
thrift,1669,summary,NameError: global name is not defined,non_debt,-
thrift,1669,description,"Code generated from this command line: thrift -strict -out . --gen py:slots,new_style hbase-thrift2.spec causes a python module to be created which references however this class is never imported.",non_debt,-
thrift,1669,comment_0,"Line 409 of current trunk has the correct import in place, added r1335325 2012-05-07 21:45:21 string(""from thrift.Thrift import TType, TMessageType, TException,",non_debt,-
thrift,1672,summary,MonoTouch (and Mono for Android) compatibility,non_debt,-
thrift,1672,description,"I'm trying to use Thrift C# library under MonoTouch. Seems like everything is working from the box. The only issue is Class THttpHandler using System.Web namespace to reach IHttpHandler interface and HttpContext class. Unfortunately, that namespace is not implemented for MonoTouch framework. Is it possible to implement THttpHandler in other way? Note that issue in Xamarin's Bugzilla is here:",design_debt,non-optimal_design
thrift,1672,comment_0,As mentioned within the Bugzilla ticket linked above this is a mono framework problem and not a specific Thrift issue. If you would like to submit a patch for THttpHandler using something other than System.web please see,non_debt,-
thrift,1678,summary,using TFramedTransport sometimes sends out packet length in a separate TCP packet,non_debt,-
thrift,1678,description,"I am writing a decoder to decode and print out Thrift Messages along with other protocol messages stored in a PCAP file. When I look at Thrift packets, For some Thrift messages I see total length information in the payload ( first 4 bytes) followed by Thrift Message ( 80 01...). For some other THrift messages, I dont see total length information in the packet but it is sent out in a separate packet ( payload size of 4) just before the Thrift packet is sent out. Investigating further I found out that using TFramedTransport sends out total length information but why is it not sent out for some Thrift messages? Thanks, Prabhakar",non_debt,-
thrift,1678,comment_0,Please post questions to the dev mailing list. Thank you,non_debt,-
thrift,1688,summary,Update IDL page markup,non_debt,-
thrift,1688,description,The markup on the IDL page needs to be updated. It's pretty hard to read at the moment.,code_debt,low_quality_code
thrift,1688,comment_0,"Thanks for helping update the website, committed",non_debt,-
thrift,1692,summary,SO_REUSEADDR allows for socket hijacking on Windows,non_debt,-
thrift,1692,description,"SO_REUSEADDR is a fine idea for sockets on POSIX operating systems. SO_REUSEADDR on Windows operates differently though. This MSDN article explains the behavior: Long story short, with SO_REUSEADDR, a hostile process owned by another user can steal your port.",non_debt,-
thrift,1692,comment_0,This has been fixed in the recent porting mega-patch,non_debt,-
thrift,1702,summary,a thrift manual,non_debt,-
thrift,1702,description,"Attached is a manual for the Thrift compiler in mdoc format, and a patch to eliminate the brutal 80-line error message that was usage(). The man page may be previewed at The tail end is incomplete because the information didn't happen to be included in the usage() text. I hope you agree the manual is easier to read and more convenient to use.",documentation_debt,low_quality_documentation
thrift,1702,comment_0,patch to remove error message. complete new manual.,non_debt,-
thrift,1702,comment_1,"The new, improved manual.",non_debt,-
thrift,1702,comment_2,"James, I patched main.cc in the other ticket you created, THRIFT-1680, which should address your issue rather than just completely removing all help from the compiler. I did not commit THRIFT-1680 as I wanted to get others input first for the change.",non_debt,-
thrift,1702,comment_3,Committed initial patch in Thrift-1680 which addresses problems stated within this issue. Avoiding moving --help to a man page to keep the install compact.,non_debt,-
thrift,1714,summary,[ruby] Explicitly add CWD to Ruby test_suites.rb,non_debt,-
thrift,1714,description,test/rb/Makefile.am contains a invocation or the ruby interpreter to run test_suites.rb that needs modification to run correctly on Ruby 1.9+. Ruby 1.9+ doesn't include the CWD by default anymore.,non_debt,-
thrift,1714,comment_0,This patch allows the 'make check' Ruby tests run correctly on both Ruby 1.8.7 and Ruby 1.9.3.,non_debt,-
thrift,1714,comment_1,committed,non_debt,-
thrift,1717,summary,Fix deb build in jenkins,non_debt,-
thrift,1717,description,None,non_debt,-
thrift,1717,comment_0,Jenkins build servers missing dependencies: mono-devel python-support,non_debt,-
thrift,1717,comment_2,Thrift deb has been removed from the automated builds,non_debt,-
thrift,1718,summary,Incorrect check in TFileTransportTest,non_debt,-
thrift,1718,description,"TFileTransport:282, comment says: "" // Make sure TFileTransport called fsync at least once"" However, the test checks for greater than, resulting in failures. This diff brings the check in line with the comment and fixes the failing tests:",non_debt,-
thrift,1718,comment_0,"committed, thanks",non_debt,-
thrift,1734,summary,Front webpage is still advertising v0.8 as current release,documentation_debt,outdated_documentation
thrift,1734,description,"The front webpage of is still displaying the current release version as v0.8, instead of the newly released v0.9 as displayed on the downloads page. I presume this is because the instructions at do not specify that this page needs updating as well.",documentation_debt,outdated_documentation
thrift,1734,comment_0,"This is all autoversioned via the config file, not seeing 0.8.0 anywhere on the main page of",non_debt,-
thrift,1734,comment_1,"Gah! All of my web browser had the front page cached, sorry for raising a non-issue.",non_debt,-
thrift,1738,summary,node.js: export transport and protocol so they can be used outside the cassandra/server context,non_debt,-
thrift,1738,description,I want to be able to thrift-encoded data without needing to create a cassandra/server connection. Exporting access to the inner transport and protocol classes will enable that.,non_debt,-
thrift,1738,comment_0,suggested patch file,non_debt,-
thrift,1738,comment_1,"Committed, thanks for the patch",non_debt,-
thrift,1745,summary,Python JSON protocol,non_debt,-
thrift,1745,description,I write a JSON protocol for Python available here : So far it passes all the tests. Also you need to add json to the python test suit and 'TJSONProtocol' in cheers,non_debt,-
thrift,1745,comment_0,"Frederic, Thank you for your work on this. Would you please attach the patch for this to the ticket. We can not accept gist or pull requests from github due to ASF licensing requirements. Once the test cases are added i'll gladly review and commit this patch.",test_debt,lack_of_tests
thrift,1745,comment_1,"Committed the file, thanks for the contribution! I did some integration into our cross language test suite and some alignments of parameters within and according to THRIFT-847). On my host the python test fails with json (hangs), so I did not add json to yet. Any idea why? JSON does not pass all cross language test yet, see:",non_debt,-
thrift,1745,comment_2,"Yes there is a typo in TJSONProtocol.py line 49, it should be ""rec"" instead of ""rect"" sorry... Also i run the test again and it seems there are other problems, coming from somewhere else and affect all protocols",documentation_debt,low_quality_documentation
thrift,1745,comment_3,"Thanks! committed the typo fix. Yes, there are some other issues we need to fix with the cross language test suite: THRIFT-847 Test Framework harmonization across all languages any help is welcome!",documentation_debt,low_quality_documentation
thrift,1745,comment_5,"I close this issue, JSON is on board! Thanks! cross is another story here on focus: THRIFT-847",non_debt,-
thrift,1764,summary,how to get the context of client when on a rpc call in server side?,non_debt,-
thrift,1764,description,1. how to get the context of the client when on a rpc call in server side? 2. how to bind the socket to the ip specified in a multiple NIC environment? 3. does thrift support message reflections in c++?,non_debt,-
thrift,1764,comment_0,Please ask questions like this on the dev mailing list or on irc,non_debt,-
thrift,1799,summary,"Option to generate HTML in ""standalone mode""",non_debt,-
thrift,1799,description,"Improvements to HTML Generator * Removed HTML page name at <a href=""..."" * Option added to embed CSS inline instead of style.css. Both improvements are quite helpful in some situations, e.g. when the HTML is accessed in a way other than via the original file name, or when the separate CSS file cannot be provided along with the HTML for some reason.",design_debt,non-optimal_design
thrift,1799,comment_0,1,non_debt,-
thrift,1799,comment_1,"The patch seems to be looking into the .git directories -- how are you generating it? If you use git diff, it will create the patch without the .git directories, with respect to the current head (git diff --cached if you have staged your commits). +1 on the change.",non_debt,-
thrift,1799,comment_2,"Cleaned up patch, no other changes.",code_debt,low_quality_code
thrift,1799,comment_3,+1 please commit.,non_debt,-
thrift,1799,comment_4,Committed. Thanks Roger and Carl!,non_debt,-
thrift,1800,summary,Documentation text not always escaped correctly when rendered to HTML,documentation_debt,low_quality_documentation
thrift,1800,description,"Comments containing non-ascii characters (like for example the swedish ""Vi mter temperaturer i C"") show up incorrectly in the generated HTML file. Only a handful of characters is encoded at all, and only at a few rare occasions.",documentation_debt,low_quality_documentation
thrift,1800,comment_0,The careful observer will notice that I didn't use the escape_[] table by intention. :-),non_debt,-
thrift,1800,comment_1,"Can you change test/DocTest.thrift to demonstrate the change that you made? I'm not able to get it to properly escape the characters (instead, it is adding additional bogus characters).",code_debt,low_quality_code
thrift,1800,comment_2,"Occurs on probably all platforms, when IDL is plain text. However it works when the Input is already UTF-8 w/o BOM. I am going to revise my patch to work in both cases.",non_debt,-
thrift,1800,comment_3,"Revised patch, now handles both cases.",non_debt,-
thrift,1800,comment_4,+1 LGTM,non_debt,-
thrift,1800,comment_5,Committed.,non_debt,-
thrift,1808,summary,The Thrift struct should be considered self-contained?,non_debt,-
thrift,1808,description,As the Summary. Example: struct Entity { Entity entity; },non_debt,-
thrift,1808,comment_0,"There wasn't enough information in this ticket to take any action, so I resolved it as invalid.",non_debt,-
thrift,1810,summary,add ruby to test/test.sh,non_debt,-
thrift,1810,description,"we need ruby as part of the cross language test suite, currently just a shell script (test/test.sh) running on Linux. references THRIFT-1766",test_debt,lack_of_tests
thrift,1810,comment_0,is this something you can look at before 0.9.2?,non_debt,-
thrift,1810,comment_1,I will try... However a patch from a ruby Hacker is very welcome!,non_debt,-
thrift,1810,comment_2,initial patch without cross language test,test_debt,lack_of_tests
thrift,1813,summary,Add @Generated annotation to generated classes,non_debt,-
thrift,1813,description,Why? # A lot of static analysis tools understand the annotation and treat those classes differently. # Setting the date field with the date & comments field with the version of the thrift compiler might prove to be valuable for tracing the origins of the generated file. The overheads should be minimal as the retention policy of the annotation is purely at a source level,code_debt,low_quality_code
thrift,1813,comment_0,Here is a working patch to provide the feature,non_debt,-
thrift,1813,comment_1,"+1, should we also include the date?",non_debt,-
thrift,1813,comment_2,do we really need the new function autogen_summary? why is autogen_comment not sufficient? rebased patches are welcome!,non_debt,-
thrift,1813,comment_3,"I rebased Arvind's patch, removed the duplication between autogen_comment and autogen_summary, and added a date as I previously suggested. GitHub pull request also available at:",code_debt,duplicated_code
thrift,1813,comment_4,"committed, thanks",non_debt,-
thrift,1819,summary,Thrift has many unused #includes,code_debt,low_quality_code
thrift,1819,description,Patch forthcoming.,non_debt,-
thrift,1819,comment_0,"Which platforms, compilers & co. are tested?",non_debt,-
thrift,1819,comment_1,I tested using GCC 4.6.3 on Ubuntu 12.04. I have a static analysis script which finds unused #includes; I doubt that I removed any referenced symbols.,code_debt,dead_code
thrift,1819,comment_2,I had several issues with your patch. Description: Debian GNU/Linux 6.0.6 (squeeze) Release: 6.0.6 Codename: squeeze gcc (Debian 4.4.5-8) 4.4.5 2.6.32-5-amd64 x86_64 other people on windows might also have issues with this..,non_debt,-
thrift,1819,comment_3,"Closing, as patch broke things and was never addressed.",defect_debt,uncorrected_known_defects
thrift,1821,summary,problem with sending request with Cyrillic in UTF-8 from node.js to java.,non_debt,-
thrift,1821,description,"Hi guys, I used thrift to communicate my services written on java and node.js. (all servers on java, clients on java and on node.js). this bug reproduces just on node.js client. In my source I write ... struct = new function(err, result) { ... With latin query all is ok, but with Cyrillic its return an error { query: '', tags: null, categoryIds: null, platforms: null, stores: null, ratings: null, price: null, withFacets: true, false, withSpellcheck: false, orders: null, offset: 0, size: 10 } // this is { name: type: 7, message: 'Required field \'withFacets\' was not found in serialized data! Struct: } // this is console.log(err); my ASearchRequest structure struct ASearchRequest { 1:optional string query, 2:optional set<string 3:optional set<i32 4:optional set<byte 5:optional set<byte 6:optional set<byte 7:optional PriceFilter price, 8:required bool withFacets, 9:required bool 10:required bool withSpellcheck, 11:optional list<ASearchOrder 12:required i32 offset, 13:required i32 size } As I said follow, with latin in requests all works correct. Can any one help me with it ? transport: protocol:",non_debt,-
thrift,1821,comment_0,"Same as for the other ticket, thanks for reporting it but I extended the nodejs tests but I could not reproduce it, neither with buffered nor framed transports, node nor java servers (thrift/test/nodejs ""make check""). Perhaps have a look on the latest version from our repository and let us know. Please send us a patch here, if you have a solution or can reproduce it with the unit-tests.",non_debt,-
thrift,1829,summary,cpp unit tests fail to build using multiple make jobs,non_debt,-
thrift,1829,description,"If you attempt to build the cpp unit tests using 'make -j x' the build will fail as there's a race between the unit test code targets depending on the thrift-generated code targets. I think the real fix would be to use one of the approaches described in the 'Multiple Outputs' section of the automake manual (see I experimented with one of them and it seemed to help but never got it quite working. However an easier workaround is to simply disable parallel builds by using the "".NOTPARALLEL"" special target which forces make to run serially. from .NOTPARALLEL If .NOTPARALLEL is mentioned as a target, then this invocation of make will be run serially, even if the -j option is given. Any recursively invoked make command will still run recipes in parallel (unless its makefile also contains this target). Any prerequisites on this target are ignored. That's the approach I ended up taking as I'm short on time.",design_debt,non-optimal_design
thrift,1829,comment_0,Patch that adds .NOTPARALLEL to the automake files,non_debt,-
thrift,1829,comment_1,Thanks David! I added a few more hot fixes for this topic on other languages. ;-r,non_debt,-
thrift,1837,summary,Deploy and use cxxfunctional.h in cpp code generated in cob_style mode,non_debt,-
thrift,1837,description,"In c++11 mode, clang lacks a <tr1/functional> header file, instead using <functional>. The cpp thrift library already uses a selector header to resolve the proper location; this patch deploys the selector as part of the install and uses it in generated code.",non_debt,-
thrift,1837,comment_0,Thanks Nate!,non_debt,-
thrift,1842,summary,Memory leak with Pipes,code_debt,low_quality_code
thrift,1842,description,Some allocated stuff is not freed correctly after setting up the pipe(s).,non_debt,-
thrift,1842,comment_0,1,non_debt,-
thrift,1842,comment_1,Committed.,non_debt,-
thrift,1845,summary,Fix compiler warning caused by implicit string conversion with Xcode 4.6,non_debt,-
thrift,1845,description,Clang doesn't do the implicit char[] -,non_debt,-
thrift,1845,comment_0,committed,non_debt,-
thrift,1853,summary,Error when leaving struct field null in Client,non_debt,-
thrift,1853,description,"When sending a struct over the wire from the client, the following error is produced if one or more struct fields are left void or set to null, even if marked as optional in the definition. I figure the buf from transport.js ~line 119 is null when that field hasn't been set, hence the 'has no method ""copy""' error. Object #<Objectat at Array.forEach (native)",non_debt,-
thrift,1853,comment_0,"Hi Nick, Do you have any updates on this? If you have any unit-test for this please also attach it here. Thank you, Henrique",test_debt,lack_of_tests
thrift,1853,comment_1,"It appears this issue may have been fixed or it may have been an implementation problem, as I can no longer reproduce.",non_debt,-
thrift,1853,comment_2,"Can't reproduce, closing.",non_debt,-
thrift,1862,summary,Java thrift generated coe: default constructor initialize fields with default values but doesn't set their isSet flags,non_debt,-
thrift,1862,description,"I have thrift: Now, when the follow test will fail: Question is: is this behavior by design?",non_debt,-
thrift,1862,comment_0,"There'are a couple of similar jiras already : THRIFT-1827 , THRIFT-1394",non_debt,-
thrift,1867,summary,Python client/server should support client-side certificates.,non_debt,-
thrift,1867,description,The Python TSSLSocket is currently missing support for client side certificates. The client should be able to supply them and the server should be able to validate them.,non_debt,-
thrift,1867,comment_0,"I've tested the client portion of this patch against [Cassandra's support for SSL I modified the unit test to use the client validation, but I'm not very familiar with the test framework, I commented out a set_alarm() in TestServer.py as it always terminated the server when using ssl. I also had to remove the test_cert.pem and replace it with my own that was signed by a CA so that the certificate could be verified, not sure if that introduces problems with other tests?",non_debt,-
thrift,1867,comment_1,Resolved via THRIFT-3505,non_debt,-
thrift,1873,summary,Binary protocol factory ignores struct read/write flags,non_debt,-
thrift,1873,description,"The binary protocol factory takes strict read/write flags as (optional) arguments, but did in fact never use them.",code_debt,dead_code
thrift,1873,comment_0,Committed.,non_debt,-
thrift,1879,summary,Add support for,non_debt,-
thrift,1879,description,"It would be nice if thrift_c_glib bindings supported GObject Introspection (see This would make Vala bindings (THRIFT-1741) easy. Actually adding G-I is pretty easy (I already have a patch to do it), but would require migrating the documentation to the gtk-doc format, so before I invest the time I'd like to make sure this is something you're interested in.",documentation_debt,outdated_documentation
thrift,1879,comment_0,"This is just a work-around for Unfortunately, the resulting GIR is pretty useless without it.",non_debt,-
thrift,1879,comment_1,"This actually causes a GObject Introspection Repository (and a typelib) to be generated and installed, resolving this issue. This depends on the other patch from this issue, as well as THRIFT-1882 and THRIFT-1883.",non_debt,-
thrift,1879,comment_2,committed this: please rebase second patch.,non_debt,-
thrift,1879,comment_3,"On what? This is worthless without THRIFT-1883. GObject Introspection requires you to annotate the code (see so it knows things like when to transfer ownership, parameter direction, generic container types, etc., and in order to do that you have to use GTK-Doc style comments. This could be used without THRIFT-1882, but if that's going to happen it makes much more sense to do it first. That said, I'm actually having second thoughts about this (and THRIFT-1885). I'm in the middle of writing a new implementation (see for some of the reasons why), and I'd rather use the Thrift namespace for that instead of c_glib. I could change the namespace on this patch to something else (ThriftGLib, maybe), but if something better is coming I don't see much gain in exposing c_glib to other languages.",non_debt,-
thrift,1879,comment_4,"yes, I know.. THRIFT-1883 did not apply so I thought to do this first...",non_debt,-
thrift,1879,comment_6,I'm withdrawing the request to fix this since I'm working on a full rewrite of this library (see,non_debt,-
thrift,1883,summary,Add GTK-Doc documentation,non_debt,-
thrift,1883,description,"Right now thrift_c_glib contains a few sporadic doxygen-style comments which are, AFAICT, not actually used to generate any documentation. In order to generate a GObject Introspection repository g-ir-scanner scans the source code and looks for GTK-Doc comments which contain annotations (see necessary for the GIR to work properly. I've fully documented thrift_c_glib (well, the API is fully covered, although I'm the first to admit the documentation could be better) using GTK-Doc comments and added support to the build system to go ahead and actually build a manual.",documentation_debt,low_quality_documentation
thrift,1883,comment_0,"Hopefully these apply cleanly. If not, you might need (which I've also attached to THRIFT-1879) and (which I've also attached to THRIFT-1882).",non_debt,-
thrift,1883,comment_1,does not apply on master branch... could you rebase?,non_debt,-
thrift,1883,comment_2,I'm withdrawing the request to fix this since I'm working on a full rewrite of this library (see,code_debt,low_quality_code
thrift,1886,summary,Virtual method implementations and *_get/set_property functions should be static,code_debt,low_quality_code
thrift,1886,description,The virtual method implementations and *_get/set_property functions are not part of the public API and should be static.,code_debt,low_quality_code
thrift,1886,comment_0,"This may or may not apply until the patches in THRIFT-1879, THRIFT-1882, THRIFT-1883, THRIFT-1884, and THRIFT-1885 are applied. It shouldn't really matter in theory, it's just that some changes may be close enough together to cause conflicts.",non_debt,-
thrift,1886,comment_1,I'm withdrawing the request to fix this since I'm working on a full rewrite of this library (see,non_debt,-
thrift,1896,summary,Add TBase protocol for Cocoa,non_debt,-
thrift,1896,description,"Add TBase protocol for cocoa generator. By having all generated classes declare conformance of this protocol allows tighter type checking in compile time. One could use ""id<TBase",non_debt,-
thrift,1896,comment_0,new file,non_debt,-
thrift,1896,comment_1,Thanks Kevin!,non_debt,-
thrift,1904,summary,Incorrect code is generated for typedefs which use included types,non_debt,-
thrift,1904,description,"If a typedef refers to a type that is defined in an included IDL file, the code for the typedef is not generated. Furthermore, the Compiler issues a warning about pending typedefs. If the typedef is used elsewhere in the code, the generated code cannot be compiled because of the missing code for the pending typedefs. Inner IDL file: Outer IDL file: In this example, the typedef for OuterStruct is not generated, thus Delphi fails to compile OuterContainer.",non_debt,-
thrift,1904,comment_0,Committed.,non_debt,-
thrift,1913,summary,skipping unknown fields in java unions,non_debt,-
thrift,1913,description,Skipping unknown fields in thrift union broke in Attached patch fixes it.,non_debt,-
thrift,1913,comment_0,Committed.,non_debt,-
thrift,1918,summary,Couldn't build with python in separate subdir,non_debt,-
thrift,1918,description,"Example: Fails with no file ""setup.py"" found",non_debt,-
thrift,1918,comment_0,"Please, check {{./configure --help}} You should run {{export before {{./configure --with-python}}. On my workstation, it's {{export",non_debt,-
thrift,1919,summary,libthrift depends on httpcore-4.1.3 (directly) and httpcore-4.1.4 (transitively),non_debt,-
thrift,1919,description,"I believe this is the same issue as THRIFT-1693, but with different versions. There's a direct dependency on both httpclient and httpcore version 4.1.3 in thriftlib, but httpclient:4.1.3 has a dependency on httpcore:4.1.4. Here are the dependencies Gradle shows me. Here's a copy of the dependency resolution section of my Gradle build in case anyone needs a short term workaround. The force option is what allows the direct dependency to be overridden to 4.1.4 (shown with a * in the above snippet).",build_debt,build_others
thrift,1919,comment_0,"Thanks Ryan, will take a look at it",non_debt,-
thrift,1919,comment_1,updates [INFO] [INFO] +- [INFO] +- [INFO] +- [INFO] +- [INFO] | +- [INFO] | \- [INFO] \-,non_debt,-
thrift,1920,summary,Binary Type,non_debt,-
thrift,1920,description,We really need a well documented binary type. Let's collect all binary type relevant issues here and create it!,non_debt,-
thrift,1920,comment_0,"Based on all subtasks being closed, I am closing this.",non_debt,-
thrift,1924,summary,Delphi: Inconsistency in serialization of optional fields,design_debt,non-optimal_design
thrift,1924,description,Sub-Task of THRIFT-1528 for Delphi,non_debt,-
thrift,1924,comment_0,1,non_debt,-
thrift,1924,comment_1,Committed.,non_debt,-
thrift,1926,summary,PHP Constant Generation Refactoring,non_debt,-
thrift,1926,description,"When I have added PHP namespace, one point was missing from my refactoring: the constants. Actually, PHP constants are generated into $GLOBALS because PHP cannot generate constants for array or object with the function ""define"". But there is some problems: - Add vars in $GLOBALS is not a good pratice - These vars are not constant because user can modify them - Object constants instanciate a new object everytime the constant is call So I have added a new PHP Class in library called TConstant. The goal of this class which is a Singleton, is to isolate Constant, and instanciate them on-demand (only one time) in static properties. There is no setter so constants cannot be edited and the class is abstract. I have edited the PHP generator to create those constants given to the definition file. Every constant create a static property and an init method in a final class called Constant in the definition Namespace. To get a constant, you just have to write: If the constant doesn't exists, it throws a Fatal error like it is done in PHP when you call a static property which doesn't exists. Maybe we can add an exception. I have attached a diff patch made from the commit socket.h header to support builds with Android NDK). Because you aren't not in SVN anymore, I don't know how to send you this patch, I hope this one is good. I hope you like my patch.",non_debt,-
thrift,1926,comment_0,Any update on that ticket ?,non_debt,-
thrift,1926,comment_1,1,non_debt,-
thrift,1926,comment_2,"LGTM, committed. Thank you both!",non_debt,-
thrift,1932,summary,casts values read from input stream into a pointer and then dereferences it.,non_debt,-
thrift,1932,description,"The Compilation of thrift-0.9.0 ended with the following warnings: * QA Notice: Package triggers severe warnings which indicate that it * may exhibit random runtime failures. * warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing] * warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing] When looking into ... 711 = 712 713 if == 4) { 714 // 0 length event indicates padding 715 if (*((uint32_t == 0) { 716 // T_DEBUG_L(1, ""Got padding""); 717 718 continue; 719 } ... it becomes obvious that the four values casted into a pointer were previously read from the input stream by ::read() in line 661: 661 = ::read(fd_, readBuff_, readBuffSize_); Same issue here: 725 readState_.event_ = new eventInfo(); 726 While it is a two minute job fix this problem, the method looks so fragile that a clean rewrite appears to be more appropriate.",code_debt,low_quality_code
thrift,1932,comment_0,"Hi Hugo, would be great if you can spend the two minutes to fix it and provide a patch. see I'm ready to review and commit this afterwards. -roger",non_debt,-
thrift,1932,comment_1,"Hi Roger, here is the patch you requested. It does, however, only suppress the type-punned pointer warning. Since I'm new to Thrift, I'm quite unsure if the sequence of bytes read from file handle 'fd_' is already in host byte order. If this is not the case the patch is still wrong",design_debt,non-optimal_design
thrift,1932,comment_2,Thanks Hugo! committed,non_debt,-
thrift,1943,summary,docstrings for enum values are ignored,non_debt,-
thrift,1943,description,"The HTML Generator ignores docstrings for enum values. The enum comment is rendered, but none of the comments for single values, like with the Numberz enum from DocStrings.thrift. BTW, we do not have JIRA components for HTML.",non_debt,-
thrift,1943,comment_0,1,non_debt,-
thrift,1943,comment_1,Committed.,non_debt,-
thrift,1953,summary,support for asp.net mvc 3,non_debt,-
thrift,1953,description,make asp.net mvc 3 can support thrift protocol,non_debt,-
thrift,1953,comment_0,please check the attachment patch.,non_debt,-
thrift,1953,comment_1,Thanks for the Patch! Great idea to integrate Apache Thrift with ASP.NET MVC 3. What do you think about renaming according to the same naming as System.Web.Mvc? - Thrift.Server.MVC - - Could you merge into to help reducing maintenance effort? And please add the mvc option to the macro within thanks! ;-r,code_debt,low_quality_code
thrift,1953,comment_2,"hi, Roger Meier, i've changed the code style as you wish, please check the attachment patch, thanks",code_debt,low_quality_code
thrift,1953,comment_3,"Overall, looks good. A few more comments: - Tests have moved from ./test/csharp/ to ./lib/csharp/test - The C# generator has a warning about an unused variable. Can you please delete that variable? - I don't think that we should be building the MVC projects by default. For example, Mono does not have support for MVC3, so I couldn't compile those elements. Overall, looks good though.",architecture_debt,violation_of_modularity
thrift,1953,comment_4,"hi, Carl Yeksigian 1. Tests for MVC have moved to ./lib/csharp/test 2. I don't find any unused variable in my patch, could you please show me where it is? 3. I agree with your point, by default, we should not build the MVC projects. so I create a solution file ""thrift.mvc.sln"", but this will increase the maintenance effort. please check the attachment patch file.",architecture_debt,violation_of_modularity
thrift,1953,comment_5,"Thanks! Here is the error that I got when making the compiler: I've attached a patch to remove the variable, rather than having more back and forth about that. I'd rather have them in a single solution, where we just don't build all of them.  : Do either of you have an opinion on this?",code_debt,dead_code
thrift,1953,comment_6,"thanks, Carl Yeksigian I find the variable now. I upload a new patch to remove it.",code_debt,low_quality_code
thrift,1982,summary,vsnprintf on Windows have different semantics,non_debt,-
thrift,1982,description,"vsnprintf on Windows (visual) return -1 on failure instead of amount of bytes needed. so line like this: int need = STACK_BUF_SIZE, message, ap); won't work. Solution: ugly conditional compilation which will use microsoft specific functions... See patch for details.",code_debt,low_quality_code
thrift,1982,comment_0,fix for vnsprintf in TOutput,non_debt,-
thrift,1982,comment_1,"Thanks Konrad! ;-r Hmm, I think TOutput needs refactoring anyway ...",non_debt,-
thrift,1990,summary,Node.js Exceptions Don't Work,non_debt,-
thrift,1990,description,"Exception passing / handling doesn't seem to be working in the Node.js library. If the Thrift server throws an exception, the client code does not see an error, and instead is passed a return value. The expected behavior is a server Exception would be exposed as 'err', the first argument to the client callback. Have verified using my own code and the supplied example code by adding a simple Exception type to my .thrift file, regenerating, then stubbing out the server methods to always error. Have tried all transports, using the multitransport examples. For example, with the following exception definition: exception NotFoundException { 1: string message } and the following server method implementation: var store = function(user, result) { return result(new }; This client invocation will never receive an error in the callback, and will in fact be passed an empty instance of whatever the return value is supposed to be. For example, if a method returns a custom data type, the callback receives an instance of the custom data type with all members set to null. function(err, response) { if (err) { console.error(err); return; } // This never actually happens, because err is always null! user1.uid, "" as "", user1.name); function(err, responseUser) { if (err) { console.error(err); return; } responseUser.uid, "" as "", responseUser.name); }); }); I noticed that the Exception objects seem to be written to the wire correctly - in the generated js file, the Exception class's .write() method appears to fire correctly. However, the .read() never goes off, so it's getting lost somewhere in between. Shouldn't matter, but the client and server are both on the same machine.",non_debt,-
thrift,1990,comment_0,"Hi Nick, Could you have a look on ? Your example works for me, did you declare your exception for this method you are testing? like: void testException(1: string arg) throws(1: NotFoundException err1) anyways, if you have a fix for this or anything else please read: Thank you, Henrique",non_debt,-
thrift,1990,comment_1,"Ah ha, turns out Ubuntu's thrift package is v 0.8, which is quite outdated and doesn't include exception support.",architecture_debt,using_obsolete_technology
thrift,1990,comment_2,"for Thrift 0.9 we do not have package for ubuntu 13.04, the 12.04 packages are available at",non_debt,-
thrift,1999,summary,warning on gcc 4.7 while compiling BoostMutex.cpp,non_debt,-
thrift,1999,description,"""unused arguments"" warning. suppressed by (void) x; in patch",code_debt,low_quality_code
thrift,1999,comment_0,this new patch uses,non_debt,-
thrift,1999,comment_1,commited. Thanks again Konrad!,non_debt,-
thrift,2003,summary,Deprecate senum,non_debt,-
thrift,2003,description,"I propose the senum type be deprecated (flagged by the compiler as scheduled for removal) in v1.0 and subsequently removed in a later version of Apache Thrift. Rationale: The senum type is a string in all target languages. Instances of the type are not constrained to the enumeration strings in IDL or output languages. IDL constants of type senum can be assigned irrational values (e.g. random strings, integers, floats, etc.) without a compiler warning or error. The senum type offers no cross language utility and is easily replaced by string or enum (which offers string lookup in many languages). This type impacts a wide range of concerns including language generators and the effort required to learn Apache Thrift.",non_debt,-
thrift,2003,comment_0,patch,non_debt,-
thrift,2003,comment_1,"Thanks, Randy! Committed.",non_debt,-
thrift,2006,summary,TBinaryProtocol message header call name length is not validated and can be used to core the server,code_debt,low_quality_code
thrift,2006,description,"When use ""Nessus"" tool scan the server, got below core file: Program terminated with signal 11, Segmentation fault. #0 0xf6a97d36 in memcpy () from /lib/libc.so.6 (gdb) bt #0 0xf6a97d36 in memcpy () from /lib/libc.so.6 #1 0x3d5c9c24 in ?? () #2 0xf5c2096e in 0xf5c20d2c in from #4 0xf5c2139b in 0xf5c215e2 in from #6 0xf5c182ad in 0xed2b0d5b in (this=0xf60eeba0) at #8 0xf5c1b378 in () from",non_debt,-
thrift,2006,comment_0,"After I add some code in the function: template <class Transport_uint32_t TMessageType& messageType, int32_t& seqid) if( sz { throw ""the message exceed the max size 15M bytes.""); } no crash now. Any comments?",non_debt,-
thrift,2006,comment_1,There should be an upper limit on the size of the string that is read for the RPC method name in the TBinaryProtocol message header. Does anybody know what the maximum length of a call name is? Does the compiler have an upper limit on string length for the name of a RPC method?,code_debt,low_quality_code
thrift,2006,comment_2,From :,non_debt,-
thrift,2006,comment_3,"The code path is inside backwards compatibility (pre-versioned) header reads. By setting a string size limit in BinaryProtocol before serving, this will prevent a core. Use setStringSizeLimit in BinaryProtocol to set an upper limit on string reads. We could be more strict here, for example if someone was able to tell me what the maximum allowed size of a method call name was, we could hardcode the limit and prevent this without ""optional"" behavior. Nobody could provide this information, so I provided a way to achieve the desired behavior without any code changes.",code_debt,low_quality_code
thrift,2017,summary,Resource Leak in thrift struct under,code_debt,low_quality_code
thrift,2017,description,"In file class t_program : public t_doc { 59 public: 60 path, std::string name) : 61 path_(path), 62 name_(name), 63 out_path_(""./""), 64 { 65 scope_ = new t_scope(); 66 } 67 68 path) : 69 path_(path), 70 out_path_(""./""), 71 { 72 name_ = program_name(path); 73 scope_ = new t_scope(); 74 } In Above code at line number 65 and 73 Resource leaks happens as 1. Allocating memory by calling ""new t_scope"". 2. Assigning: ""this-3. The constructor allocates field ""scope_"" of ""t_program"" but there is no destructor in the code. destructor should deallocate memroy for t_scope. which sometimes causes Resource Leak. Possible patch: There should be destructor to release the resources allocated dynamically. ~t_program() { delete scope_; scope_ = NULL; }",code_debt,low_quality_code
thrift,2017,comment_0,adding Patch to resolve the issue. Please review it,non_debt,-
thrift,2017,comment_1,Committed.,non_debt,-
thrift,2020,summary,Thrift library has some empty files that haven't really been deleted,code_debt,dead_code
thrift,2020,description,"A lot of windows specific files have been ""removed"" recently, but git clone still creates empty versions of those files.",code_debt,low_quality_code
thrift,2021,summary,Improve large binary protocol string performance,code_debt,slow_algorithm
thrift,2021,description,"Currently, TBinaryProtocol reads into a temporary buffer, then copies the results into the resulting std::string. For large strings (like a megabyte), the copy can be a performance issue. The attached patch reads directly into the std::string instead.",code_debt,slow_algorithm
thrift,2021,comment_0,"Hmm... I think we should keep ""prevent stack overflow for v. large strings""",non_debt,-
thrift,2021,comment_1,"For large strings, the heap is being used in both cases. std::string generally uses the heap, and only small strings go on the stack. If there is a heap overflow issue, then both the old and new code are equally vulnerable, as both call into a buffer that could be exactly ""size"" bytes. I don't think there's a heap overflow in either set of code, but if there were, then the new code shouldn't make it any worse.",non_debt,-
thrift,2021,comment_2,"+1 Applies after fixing EOL style (win=>*nix), looks good to me.",code_debt,low_quality_code
thrift,2025,summary,Fix TSSLServerSocket 64-bit builds,non_debt,-
thrift,2025,description,"Assorted size_t -One long -Squelching one warning that should be safe dealing with Win64 and SOCKET -No longer ""using namespace boost"", as that can cause ambiguities between global uint8_t and boost::uint8_t.",non_debt,-
thrift,2025,comment_0,is fixed within master,non_debt,-
thrift,2028,summary,Cleanup threading headers / libraries,code_debt,low_quality_code
thrift,2028,description,"The current threading implementations have some minor annoyances: All do some amount of internal state tracking with an enum that ends up being overkill. All use weak_ptrs to help manage lifetimes, instead of the base class that was designed for exactly this purpose. All of the specific thread factories implement ""detached"" methods, but the base thread factory doesn't have virtual methods exposing the detached methods. The thread manager has an unused local. Adding a ""UniqueGuard"" class to Mutex.h, to give more flexible RAII management to locks. Currently no clients of this, but I have some patches that will eventually use this.",code_debt,low_quality_code
thrift,2028,comment_0,"Patch does not (or no longer) apply, even after EOL conversion I get one error in line 76.",non_debt,-
thrift,2028,comment_1,Updated / rebased.,non_debt,-
thrift,2028,comment_2,"I'm a bit unsure. From my point of view the patch looks good, but I'm not sure if I overlook all the consequences of your modifications and if they are 100% correct. So let's say, I do not have anything to say against it. :-)",non_debt,-
thrift,2028,comment_3,"Please submit a pull request, let's see the diffs and verify it passes tests. Also try building it on Windows if you can. The cmake environment will build tests; see THRIFT-2850 for info on setting it up.",non_debt,-
thrift,2028,comment_4,"Another suggestion would be to eliminate the project-specific abstraction and require either boost or C++11, assuming either of them have all the primitives. This would simplify the code base quite a bit and remove the need to maintain this abstraction layer.",code_debt,complex_code
thrift,2028,comment_5,This patch is outdated and you can reopen as soon as a patch is ready.,non_debt,-
thrift,2031,summary,Make SO_KEEPALIVE configurable for C++ lib,non_debt,-
thrift,2031,description,"It's a socket option that we haven't made configurable yet, and sometimes you need it.",requirement_debt,requirement_partially_implemented
thrift,2031,comment_0,"Ack... doesn't apply cleanly, because of other unsubmitted patches. I'll clean this up shortly.",code_debt,low_quality_code
thrift,2031,comment_1,"Seems outdated, the -1 has been replaced with in the meantime.",code_debt,low_quality_code
thrift,2031,comment_2,Updates to switch from -1 to macro,non_debt,-
thrift,2031,comment_3,"Just a question: does not really clear the SO_KEEPALIVE socket option, it just sets the member variable keepAlive_ and then returns doing noting else. By Intention? Other than that +1",non_debt,-
thrift,2031,comment_4,"Not real sure what I was thinking with that. I want to say I copy / pasted the general pattern from somewhere, but I couldn't find anything that looked similar. New patch should make this much more reasonable.",non_debt,-
thrift,2031,comment_5,1,non_debt,-
thrift,2032,summary,C# client leaks sockets/handles,design_debt,non-optimal_design
thrift,2032,description,"The C# client code does not correctly clean up the transport used, so the programmer has to take care on his own about this. This may even lead to a program hang in certain scenarios. Furthermore, the generated client should support IDisposable. Note that in contrast, the server side handles this automatically without any explicit manual coding. TODO: * modify generated code to add IDisposable support * modify TProtocol to add IDisposable support * update the tutorial code accordingly",design_debt,non-optimal_design
thrift,2032,comment_0,Patch ] does also fix a unnecessary double assignment to result2 in AcceptImpl().,code_debt,low_quality_code
thrift,2032,comment_1,"Meanwhile I'm in doubt if IDisposable for the Iface is really such a good idea. To the class yes, but the Iface breaks Server code due to the missing method. The only reason for this was the using() use case, but maybe that's not enough of a reason here.",design_debt,non-optimal_design
thrift,2032,comment_2,"Just to clarify, you are talking about the Client and the Processor; the IDisposable for the Client's Iface makes sense, but the IDisposable for the Processor's Iface doesn't. Unfortunately, they are the same Iface, so we need to find the correct middle ground. I'm much more in favor of having the Iface not be IDisposable, as it lowers the maintanence, with the client being explicitly IDisposable. I'm unsure of how we currently clean up state from the processor, but I don't think that IDispoable makes the most sense.",design_debt,non-optimal_design
thrift,2032,comment_3,"Exactly. So lets drop the idisposable from the iface. Dont want to put too much effort in here, just need to address the leak.",code_debt,low_quality_code
thrift,2032,comment_5,"Used the patch > 1 week now, had no problems with it. Committed, assuming lazy consensus. Feel free to reopen, if necessary.",non_debt,-
thrift,2044,summary,Util.h uses defines from PlatformSocket but does not include it,non_debt,-
thrift,2044,description,None,non_debt,-
thrift,2044,comment_0,committed!,non_debt,-
thrift,2050,summary,Vagrant C# lib compile fails with TException missing,non_debt,-
thrift,2050,description,error CS0246: The type or namespace name `TException' could not be found. Are you missing a using directive or an assembly reference? error CS0246: The type or namespace name `TException' could not be found. Are you missing a using directive or an assembly reference? error CS0246: The type or namespace name `TException' could not be found. Are you missing a using directive or an assembly reference?,non_debt,-
thrift,2050,comment_0,Patch adds TException to THRIFTCODE source list.,non_debt,-
thrift,2050,comment_1,"Thanks Justin, committed.",non_debt,-
thrift,2051,summary,Vagrant fails to build erlang,non_debt,-
thrift,2051,description,"==ERROR: eunit failed while processing {rebar,main,1}]}}",non_debt,-
thrift,2051,comment_0,Removes erlang client lib build from Vagrantfile,non_debt,-
thrift,2051,comment_1,"Thanks for the patch Justin, committed",non_debt,-
thrift,2072,summary,TNonblocking server compilation FAILS with C++11 support enabled,non_debt,-
thrift,2072,description,due to new suffix operator in C++11 lines like this: has to be changed to,non_debt,-
thrift,2072,comment_0,thanks Konrad!,non_debt,-
thrift,2072,comment_2,why build failed after this change? I see only this failure: and this patch should not affect any transports :|,non_debt,-
thrift,2074,summary,cppcheck error removed from ThreadManager.cpp,non_debt,-
thrift,2074,description,None,non_debt,-
thrift,2074,comment_0,Thanks Konrad!,non_debt,-
thrift,2074,comment_2,build failed on - why? it's second time I see this failure and it seems completely not connected with change,non_debt,-
thrift,2088,summary,Typos in Thrift compiler help text,documentation_debt,low_quality_documentation
thrift,2088,description,"There are a few typos in the delphi Compiler options text. Additionally, the text is not really well written.",documentation_debt,low_quality_documentation
thrift,2088,comment_0,Committed.,non_debt,-
thrift,2097,summary,XCode project for compiler,non_debt,-
thrift,2097,description,Add a simple Xcode project to work on the compiler using Xcode.,non_debt,-
thrift,2097,comment_0,Simple xcodeproject that uses the makefile for building,non_debt,-
thrift,2097,comment_1,please help fix and test this THRIFT-2229 first thanks! -roger,test_debt,lack_of_tests
thrift,2097,comment_2,could someone review or rebase this?,non_debt,-
thrift,2097,comment_3,"As support for the older cocoa compiler and library have been removed (see THRIFT-4719), all of the issues in Jira related to that code have also been removed. For legacy cocoa support you can use version 0.12.0 - everyone is expected to move to swift if they want to use the next release of Thrift.",non_debt,-
thrift,2114,summary,Python Service Remote SSL Option,non_debt,-
thrift,2114,description,Added a new option for the Python service remote to enable communication over a SSL socket connection. Usage: [-h host[:port]] [-u url] [-f[ramed]] *[-s[sl]]* function [arg1 [arg2...]],non_debt,-
thrift,2114,comment_1,I've rebased my patch-branch and created a new PR for better handling of possible future rebases.,non_debt,-
thrift,2114,comment_3,Thanks! committed,non_debt,-
thrift,2116,summary,Cocoa generator improvements (service inheritance / server-side exception handling) and an implementation of the Thrift tutorial,code_debt,low_quality_code
thrift,2116,description,Cocoa generator improvements relating to * *Server-side exception handling* in the actual TProcessor implementation for the service * Better handling of *service inheritance* ** Service clients can now use the client subclass directly to invoke calls to the base service ** Servers must only adopt the child protocol and will automatically adopt the base protocol over inheritance ** The improvement should not break any existing service implementations See the revisions for the implemented cocoa tutorial for instance * For a better overview of the resulting gen-files after the improvements see the attached [diff An *implementation* for the *Thrift tutorial* * Server and client implementation based on the corresponding Python version * Update relating to the improved handling of service inheritance,design_debt,non-optimal_design
thrift,2116,comment_0,"If there is anyone out there spending the time to review this and THRIFT-2144, I'd be happy to commit it. Any comments on whether or not the stuff from the Wiki is still useful for the new tutorial pages or is outdated would be highly appreciated too.",documentation_debt,outdated_documentation
thrift,2116,comment_1,"As support for the older cocoa compiler and library have been removed (see THRIFT-4719), all of the issues in Jira related to that code have also been removed. For legacy cocoa support you can use version 0.12.0 - everyone is expected to move to swift if they want to use the next release of Thrift.",non_debt,-
thrift,2124,summary,".o, .so, .la, .deps, .libs, gen-* files left tutorials, test and lib/cpp when making DIST",non_debt,-
thrift,2124,description,None,non_debt,-
thrift,2124,comment_0,"purge generated results, rat results, binaries and other created files (why missing from make clean?) also not in dist: Include.thrift",non_debt,-
thrift,2124,comment_1,after moving test to subdir to allow for distclean to run missing following in tutorials folder csharp/tutorial.sln d/async_client.d d/client.d d/server.d erl/client.erl erl/client.sh erl/json_client.erl erl/README erl/server.erl erl/server.sh gen-html/index.html gen-html/style.css go/server.crt go/server.key hs/HaskellClient.hs hs/HaskellServer.hs java/build.xml js/build.xml js/src/Httpd.java js/tutorial.html ocaml/CalcClient.ml ocaml/CalcServer.ml ocaml/_oasis ocaml/README perl/PerlClient.pl perl/PerlServer.pl php/PhpClient.php php/PhpServer.php php/runserver.py shared.thrift tutorial.thrift,architecture_debt,violation_of_modularity
thrift,2124,comment_2,fixed 102c600,non_debt,-
thrift,2130,summary,"Thrift's D library/test: parts of ""make check"" code do not compile with recent dmd-2.062 through dmd-2.064alpha",non_debt,-
thrift,2130,description,make check for the D thrift bindings with recent dmd builds (2.062 - present) fails with errors. Full log after these two summaries. // first error message under dmd // first error message under dmd 2.062 full output of make check,non_debt,-
thrift,2130,comment_0,"The 2.064-devel error message you posted is actually unrelated (you had the 2.062 std/ on your path), but I can confirm it doesn't work with current DMD Git master.",non_debt,-
thrift,2130,comment_1,"The attached patches should fix test suite compilation with 2.064, please commit.",non_debt,-
thrift,2130,comment_2,committed ;-r,non_debt,-
thrift,2141,summary,Cocoa client-side SSL socket support,non_debt,-
thrift,2141,description,The attached brings client-side SSL socket support for the Thrift Cocoa library and some to the Thrift Cocoa tutorial from relating to the new usage with SSL.,non_debt,-
thrift,2141,comment_0,"Hi  and other cocoa users, from my feelings we should follow the approach used in THRIFT-2204 as it is more in line with the other languages. On the other side, two SSL implementations for cocoa seems one too much, but I like the idea of improving the tutorial code by including the SSL feature, so I'd keep that part, updated to the solution used in THRIFT-2204. I'd like to know your opinion on that matter. Any other cocoa people reading this are invited to chime in as well. I'm not a cocoa user yet, so I may overlook sth. important. Thanks, JensG",code_debt,low_quality_code
thrift,2141,comment_1,"As support for the older cocoa compiler and library have been removed (see THRIFT-4719), all of the issues in Jira related to that code have also been removed. For legacy cocoa support you can use version 0.12.0 - everyone is expected to move to swift if they want to use the next release of Thrift.",non_debt,-
thrift,2141,comment_2,"This issue was marked as affecting version 1.0 which is not possible. If you know which version this issue was discovered in, please mark it as such.",non_debt,-
thrift,2143,summary,Generated Ruby client doesn't handle enums,non_debt,-
thrift,2143,description,"The generated ruby client includes and uses enums but doesn't send or receive them correctly. They go out as i32 and are expected to be received as i32. While this is the underlying data type, it breaks things that expect the enum to come through.",non_debt,-
thrift,2143,comment_0,"Is there anything needed for this patch? I'm maintaining a local fork, so that enums are handled correctly but would love to get back to using the GA artifacts.",non_debt,-
thrift,2143,comment_1,"Hi , I think the added line {{ENUM = 16}} in {{types.rb}} is incorrect. If you compare with any other language, there is no such entry for enums.",code_debt,low_quality_code
thrift,2143,comment_2,"Hi , Thanks so much for replying! I'll have to dig back through things to see what caused me to add this. I'll try to find some time soon to replay my testing to get more details for you. My knowledge of the Thrift code is pretty naive, so it might take me a bit.",non_debt,-
thrift,2143,comment_3,"Hi , any progress on this?",non_debt,-
thrift,2143,comment_4,"Hi , IIRC I added to know when an {{int}} value should be used to find the {{enum}} value in a list since the int value transmitted is the enum position. There's a good chance that I'll be back in the space soon, so I'll hopefully be able to provide more exception/error details if my description above doesn't help much.",code_debt,low_quality_code
thrift,2143,comment_5,I tested this in a pure Thrift project and was unable to recreate it. The problem must be with how Twitter's Finagle handles things. I can use the patch locally. Sorry for the noise and thanks for looking into things.,non_debt,-
thrift,2143,comment_6,Closing since this appears to be related to Finagle rather than purely Thrift.,non_debt,-
thrift,2147,summary,Thrift IDL grammar allows for dotted identifier names,non_debt,-
thrift,2147,description,"Hi all, Recently I noticed that the Thrift grammar allows for identifiers that are illegal in most (if not all) supported languages, like: and so on. The compiler generates code from that without a single complaining. Of course, the generated code does not compile. Before I submit a patch to address this, I'd like to know a) Is there any (supported) language where such a construct would be legal and expected? b) Anyone around here that knows whys the grammar allows this? By intention or simply a bug? Thanks, Jens",non_debt,-
thrift,2147,comment_0,Duplicate of THRIFT-667,non_debt,-
thrift,2149,summary,Add an option to disable the generation of default operators,non_debt,-
thrift,2149,description,"Currently, the C++ generator automatically writes out the operators ==, != and <. It would good to have an option to disable this generation if a user wants to write their own specialised operators for these types in a source file that sits by the side of the generated source, so that they do not get overwritten when a thrift file needs to be altered and then regenerated.",non_debt,-
thrift,2149,comment_0,"Committed; thanks! There was a missing ""\n"" in the options.",non_debt,-
thrift,2149,comment_1,"Thanks! and good catch with the missing ""n"" :)",non_debt,-
thrift,2150,summary,Should windows/config.h define,non_debt,-
thrift,2150,description,"As ""windows/config.h"" includes Winsock2.h, is it not best practise to define before it: This is in case someone includes Windows.h, which automatically includes the incompatible Winsock.h. It also reduces the size of the Win32 header files which helps compilation speeds :)",build_debt,build_others
thrift,2150,comment_0,"I would prefer not to add #define WIN32_LEAN_AND_MEAN to an externally visible header file. This doesn't fix the Winsock / Winsock2 incompatibility in all the cases, as the including code could have pulled in Windows.h before pulling in any thrift headers. In addition, putting the #define in windows/config.h will likely break code that was relying on the non-lean_and_mean headers. It is possible to add #define WIN32_LEAN_AND_MEAN to thrift's .cpp files, but that doesn't get much of an improvement in build times.",build_debt,build_others
thrift,2150,comment_1,"Fair enough, might be worth documenting somewhere that the thrift cpp library use Winsock2 and therefore users should be aware of the fact that their own code (and other third-party libraries like Qt) might cause issues if they include Windows.h before the thrift headers.",documentation_debt,low_quality_documentation
thrift,2150,comment_2,"Yes, it is worth to document this. Could some of you create a patch to update this: Thanks! roger",documentation_debt,low_quality_documentation
thrift,2165,summary,Thrift Compiler fails at input files with UTF-8 BOM,non_debt,-
thrift,2165,description,"On Windows platforms, non-ASCII files are commonly preceded by a BOM. Although the Thrift Compiler supports UTF-8, it fails if the Input file data is preceded by an UTF-8 BOM.",non_debt,-
thrift,2165,comment_0,"+1 Haven't tested, but it looks fine to me.",non_debt,-
thrift,2165,comment_1,"Thanks, Ben - committed.",non_debt,-
thrift,2165,comment_2,Tested-by: Roger Meier,non_debt,-
thrift,2171,summary,NodeJS implementation has extremely low test coverage,test_debt,low_coverage
thrift,2171,description,"There is hardly any test coverage for the NodeJS implementation. Please comment if you have ideas for how to architect tests. Patches welcome, too.",test_debt,low_coverage
thrift,2171,comment_0,"Hi Red, I totally agree and more tests are very welcome! We don't have any tests for the new JSON protocol for example :( I also think we should probably move thrift/test/nodejs to like in java. A lot of people get thrift for node through npm, which only gives you the lib folder... Any thoughts?",test_debt,lack_of_tests
thrift,2171,comment_1,"One ways are now testing successfully post patches for THRIFT-2205, THRIFT-2308 and others.",non_debt,-
thrift,2173,summary,Move FrameBuffer creation to a factory method for cusomization,non_debt,-
thrift,2173,description,"Apache Accumulo and (at least) Apache Cassandra try to provide the client address in the server's executing thread, primarily for debugging/logging purposes. If the construction of FrameBuffer objects was done in a FactoryMethod that we could override, it would help tremendously.",non_debt,-
thrift,2173,comment_1,An alternate solution would be to expose the selectAcceptThread in TNonblockingServer.,non_debt,-
thrift,2173,comment_2,sorry for the delay! I did not saw this earlier:-( cheers! -roger,non_debt,-
thrift,2184,summary,undefined method rspec_verify for,non_debt,-
thrift,2184,description,"When running 'make install', I get the following errors that do not occur when just running 'make': I commented out the call to #rspec_verify, but I do not know what the correct fix is. I also get numerous (related) warnings about mock being deprecated:",non_debt,-
thrift,2184,comment_0,"I had the same issue you described I think it may have to do with versions of rspec/rspec-mock ... I had multiple rspec gems installed (2.14.x 2.10.x & 1.3.x installed), I removed the 2.14.x and it seems to be doing better. This is not a solution as more of a workaround, I'm not versed enough in the gem ecosystem to propose a fix",non_debt,-
thrift,2184,comment_1,"Patch attached To reproduce the problem one needs to 'sudo gem install rspec' and ensure that the rspec 2.14.x is installed. While specifies 'rspec', '~ p.s. I am a rake/rspec/bundler newb - please review carfully",non_debt,-
thrift,2184,comment_2,"Thanks for the patch . I think that making bundler a req for the ruby lib via make is a good idea, below is a modification to the patch you attached which will remove the dep on rake in the Makefile and require bundler. thoughts?",non_debt,-
thrift,2184,comment_3,Please proceed with your patch. Any issue I may have had with it was not reproducible and is probably loca to me,non_debt,-
thrift,2184,comment_4,"committed modified patch, thanks for the initial patch",non_debt,-
thrift,2187,summary,Guard against ambiguous occurrences,non_debt,-
thrift,2187,description,Resulting in errors like: Blocking: THRIFT-2186.,non_debt,-
thrift,2187,comment_0,"uses fully qualifed function, value, type names. It should not throw an error now if a thrift file contains a typedef or enum named {{Maybe}} or a method named {{maybe}}. {{make}} at the toplevel (with configured) and {{make check}} in {{test/hs}} both pass. cc:",non_debt,-
thrift,2187,comment_1,"Committed, thanks John! ;-r",non_debt,-
thrift,2191,summary,Fix charp (specify InvariantCulture),non_debt,-
thrift,2191,description,"I caught error on serialization from application working under localized (Russian) Windows. Cause Double.Parse wait a system decimal separator. I fixed method, made Double.Parse call with explicit specify InvariantCulture in second parameter. I think my little fix is pretty clear for you. But if it's not, I would be glad to explain more details. Sincerely, Alexander Makarov.",non_debt,-
thrift,2191,comment_0,"Committed, thanks for spotting this bug!",non_debt,-
thrift,2210,summary,lib/java TSimpleJSONProtocol can emit invalid JSON,non_debt,-
thrift,2210,description,"TSimpleJSONProtocol can emit JSON with maps whose keys are not string (which is not allowed is the JSON spec). This happens if the key in a map is anything other than a String (int, enum, etc) For example, it can emit JSON like this: which should be: I have a path that fixes this, I'll upload it shortly (still trying to get my dev environment to run the tests). Also AFAICT there is no unit test for TSimpleJSONProtocol -- I'll try and add one to the patch. Thanks! Alex",test_debt,lack_of_tests
thrift,2210,comment_0,"One more issue: thrift allows for maps whose keys are structs / containers, and there's no way to turn that kind of map into JSON -- though the thrift website says to avoid creating IDLs with maps like that. I think TSimpleJSONProtocol should at least handle these cases by either throwing an exception or (maybe?) skipping these fields? Or emit a sentinel map like {""error"": ""this map couldn't be serialized to json by",non_debt,-
thrift,2210,comment_1,I've added a patch that: 1) converts primitive keys to strings 2) throws an exception when non-primitive keys are encountered (this seems like the least surprising option),non_debt,-
thrift,2210,comment_2,"I've also sent a pull request on github, not sure which method you prefer: At the very least that pull-req makes it easy to look at the patch.",non_debt,-
thrift,2210,comment_3,"Thanks for the patch Alex, committed",non_debt,-
thrift,2210,comment_5,Passed all tests for me on java 1.7.0_25 on os x. looking into why jenkins did not like this junit-test,non_debt,-
thrift,2210,comment_6,"Fixed, added UTF8 encoding to ant opts. integration tests are having an issue with the cpp http test but java lib and unit tests pass as expected",non_debt,-
thrift,2210,comment_7,Thanks for the quick turnaround !,non_debt,-
thrift,2210,comment_8,Unfortunately the change caused the Hive object not able to be serialized into JSON with Here is the struct: struct SkewedInfo { 1: list<string 2: list<list<string 3: map<list<string} The field is defined as a map with list as the key. The structure can be serialized and deserialized correctly with libthrift 0.9.1. This change regressed the logic. People do use TSimpleJSONProtocol to serialize Hive objects for audit logs or such. This is a blocker for thrift upgrading. Could we remove the check for the case when the key is a 'list'? Thanks!,design_debt,non-optimal_design
thrift,2214,summary,System header sys/param.h is included inside the Thrift namespace,non_debt,-
thrift,2214,description,#include <sys/param.h see: namespace apache { namespace thrift { namespace protocol { using #ifdef HAVE_SYS_PARAM_H #include <sys/param.h#endif,non_debt,-
thrift,2214,comment_0,Adding patch for the issue. Please review it and push to main branch. Thank you.,non_debt,-
thrift,2214,comment_1,Adding patch for issue. Please review it and push to main branch. Thank you.,non_debt,-
thrift,2214,comment_2,Committed. Thanks Vladmir!,non_debt,-
thrift,2225,summary,SSLContext destroy before cleanupOpenSSL,non_debt,-
thrift,2225,description,destroy the SSLContext before cleanup the openSSL to avoid recreating some openssl internal resource.,design_debt,non-optimal_design
thrift,2225,comment_0,What about this one?,non_debt,-
thrift,2225,comment_1,committed ;-r,non_debt,-
thrift,2225,comment_3,Is it likely that this is the root cause of the following backtrace from a core that uses thrift-0.8.0?,non_debt,-
thrift,2225,comment_4,"@  I would say yes, could you retest with master branch?",non_debt,-
thrift,2225,comment_5,Yes we will. Thanks.,non_debt,-
thrift,2227,summary,Thrift compiler generates spurious warnings with Xlint,code_debt,low_quality_code
thrift,2227,description,Sample warnings: {},non_debt,-
thrift,2227,comment_0,Also available at:,non_debt,-
thrift,2227,comment_1,Committed.,non_debt,-
thrift,2240,summary,thrift.vim (contrib) does not correctly handle 'union',non_debt,-
thrift,2240,description,The `thrift.vim` file in `contrib` does not include `union` as one of the `thriftStruct` types.,non_debt,-
thrift,2240,comment_0,One-word change to the thrift vim file. Tested on:,non_debt,-
thrift,2240,comment_1,"Thanks, committed.",non_debt,-
thrift,2246,summary,Unset enum value is printed by ToString(),non_debt,-
thrift,2246,description,"if enum is not set, ToString() returns a first enum value, but it should indicate that field is empty enum Distance { DISTANCE_1 = 0, DISTANCE_2 = 1, } struct RaceDetails { 1: optional Distance distance, } c#: new result: expected: )""",code_debt,low_quality_code
thrift,2246,comment_0,Confirmed.,non_debt,-
thrift,2246,comment_1,: any comments on the C# part of the patch?,non_debt,-
thrift,2246,comment_2,"Committed, assuming lazy consensus.",non_debt,-
thrift,2246,comment_3,Some edge cases still produce warnings.,code_debt,low_quality_code
thrift,2246,comment_4,Fix committed.,non_debt,-
thrift,2255,summary,add Parent Class for generated Struct class,non_debt,-
thrift,2255,description,"Cpp generated Class of struct dosn't have a parent Class, this will cause the Program hold the Object must be using The instance class direct. The parent class may be helper full in for program to process manay type Struct batch. I hope the New version of Thrift implement it.",design_debt,non-optimal_design
thrift,2255,comment_0,the patch files,non_debt,-
thrift,2255,comment_1,"Hello David, maybe you take a look at this patch: Your approach to use a base class is ok too. But I think that the base class should have some support from the generator. Like the ascii_fingerprint, and a copy implementation. The ascii_fingerprint is very usefully to identify the class, for runtime casting. <hintMy patch is waiting for some votes since a year or so. </hint regards, Martin",non_debt,-
thrift,2255,comment_2,any updates on this?,non_debt,-
thrift,2255,comment_3,"Hello, I can give an update on this patch: This version does not work with exceptions. Additionally I think to remove the copyTo implementation. It was in the patch since the beginning, but up to now I haven't used the copy methods, so it does not seem to be usefull and should be removed.(I will update the patch)",code_debt,dead_code
thrift,2255,comment_4,"Closing this as THRIFT-1712 has a more complete solution, with the same base class.",non_debt,-
thrift,2262,summary,thrift server 'MutateRow' operation gives no indication of success / failure,non_debt,-
thrift,2262,description,"Issuing an invalid MutateRow request, eg. to create a new (row,column) value when no column qualifier is specified, does not result in an exception being thrown . Buried in the thrift server log is the line: 2013-11-15 10:49:53,808 WARN [thrift-worker-1] No column qualifier specified. Delete is the only mutation supported over the whole column family. But the client gets no exception and continues in the belief the operation succeeded. Thus it is not possible to use the HBASE thrift server for reliable database operations.",non_debt,-
thrift,2262,comment_0,"Sorry, but it seems to be an HBase issue, not a Thrift one. I'd recommend to file a bug under (wonder why ticket's can't be moved across products?)",non_debt,-
thrift,2262,comment_1,I just di a quick search and it seems there are a bunch of possible candidates for duplicate already:,non_debt,-
thrift,2263,summary,Always generate good hashCode for Java,non_debt,-
thrift,2263,description,Previously Thrift only generated high-quality hashCode implementations when configured due to the dependency on commons-lang3. After THRIFT-2260 we no long have this dependency and can unconditionally give the better implementation.,design_debt,non-optimal_design
thrift,2263,comment_0,Pull request also available at:,non_debt,-
thrift,2263,comment_1,Thanks Andrew! committed ;-r,non_debt,-
thrift,2276,summary,java path in spec file needs updating,non_debt,-
thrift,2276,description,Path seems to have changed from lib/java to lib/java/build/lib. And so rpmbuild fails the %install. I hope this is not a local environment issue.,non_debt,-
thrift,2276,comment_0,Important note - my patch is verified to build and install - I don't actually use the java RPM at this time,non_debt,-
thrift,2276,comment_1,"Retracting patch, don't think it is doing the right thing",non_debt,-
thrift,2276,comment_2,"Previous patch wrongfully added all other (supporting) jar files instead of the thrift file, This patch fixes the path causing only the thrift jar to be package.",non_debt,-
thrift,2276,comment_3,"this is available in trunk, was most likely combined with another patch at some point",non_debt,-
thrift,2279,summary,TSerializer only returns the first 1024 bytes serialized,non_debt,-
thrift,2279,description,"TSerializer copies from the transport into a 1024 length byte array and returns the array, which will effectively clips off everything after the first 1k. The attached patch instead returns a copy of the underlying memory buffer used by the transport.",design_debt,non-optimal_design
thrift,2279,comment_0,patch,non_debt,-
thrift,2279,comment_1,"Committed, thanks Matt!",non_debt,-
thrift,2280,summary,does not really flush the transport,non_debt,-
thrift,2280,description,"The Flush() method of all protocols calls transport.Flush(), except for JSON, leading to a failure in some cases (e.g. with HTTP). The service call fails with EOF on recvResponse(), because the request has never been sent to the server, thus no response.",non_debt,-
thrift,2280,comment_0,Committed.,non_debt,-
thrift,2285,summary,TJsonProtocol implementation for Java doesn't allow a slash (/) to be escaped (\/),non_debt,-
thrift,2285,description,"We have PHP frontend and Java backend. Thrift is used for interaction between the two, entities are serialized and deserialized using TJsonProtocol. Whenever a string in a thrift entity contains a slash ""/"", PHP's implementation, that uses {{json_encode}} under the hood, escapes it: ""Translation eng/rus"" = (assuming that the entity only has this one string property) Java cannot handle this input and fails with: Expected control char Java does not allow a slash to be escaped. Thrift version is 0.9.1 Any advice on what to do for a temporary solution? Should we fix Java side or PHP side? As per json.org, a slash CAN be escaped. To me it looks more like Java is not standard compliant in this case. At the same time, however, I don't like the ambiguity of the standard allowing both unescaped and escaped slash.",non_debt,-
thrift,2285,comment_0,"How about fixing the JSON readers *only* (all affected languages), so they are able to process both variants? The wire format would be left untouched.",non_debt,-
thrift,2285,comment_1,"The best way to spread errors is copy and paste - or duplicating the algorithm in another language ;-) However, makes it easy to fix them all",non_debt,-
thrift,2285,comment_2,"The patch fixes Java, Javame and PHP",non_debt,-
thrift,2285,comment_3,1,non_debt,-
thrift,2285,comment_4,"Thanks, committed.",non_debt,-
thrift,2285,comment_6,"I'm re-opening this issue to get some attention. Please close it again, if it's not important enough or I'm breaking some workflow with this. The change done to PHP's library doesn't do anything: the variable {{$ESCAPE_CHARS}} is not used anywhere. Instead json_encode and json_decode built-in functions are used. Now JAVA can understand PHP and this is good enough for now, but ideally, the JSON should be the same whatever language's library has generated it.",code_debt,dead_code
thrift,2285,comment_7,"Hi , feel free to submit a patch cleaning up the code. I have only modified things that already existed. If the stuff is not used at all, it should be removed. And it should probably be a new ticket.",code_debt,dead_code
thrift,2285,comment_8,Closing this as the issues described have been resolved.  can you please open a new issue for the further work/investigation on this topic,non_debt,-
thrift,2290,summary,Update Go tutorial to align with THRIFT-2232,non_debt,-
thrift,2290,description,"-When THRIFT-2232 has been accepted, the Go tutorial (both code and web site) will need an appropriate update.- No changes necessary, only another minor bug has been uncovered while building the Go tutorial:",documentation_debt,outdated_documentation
thrift,2290,comment_0,Committed.,non_debt,-
thrift,2292,summary,Android Library Project,non_debt,-
thrift,2292,description,Importing of Android Library Project into Android build from plugin dont implemented. So i cant add some sdks into native plugin that use dependencies as android library project.,requirement_debt,requirement_partially_implemented
thrift,2292,comment_0,Can you please provide a list of the missing dependencies or errors you are encountering,build_debt,under-declared_dependencies
thrift,2292,comment_1,"Right now i working on jumio plugin for cordova And jumio sdk require adding their android library project to android build. According to cordova plugin specs its not implemented. More info about referencing android library projects to your project you can get here To add a reference to a library project, navigate to the <sdk android update project \ --target <target_ID--path --library",requirement_debt,requirement_partially_implemented
thrift,2292,comment_2,"I already implement cordova hook that do this job for me on after platform add, and all is brilliant but you have weak link in your build system. In exec.js file your exec implementation fail and stop build due to buffer overflow. Error is ""Error: stdout maxBuffer exceeded"". All that should be done is adding maxBuffer: 800*1024 option to exec call, also 800 can be replaced on any other big number. How can i contribute with this and other fixes to cordova? I saw your repository on github and even forked it. So if i send you pull requests with fixes do you include them in next releases?",build_debt,build_others
thrift,2292,comment_3,"Hi, Our github is only a mirror, you can still create the pull request but please attach your patch(es) here too. Cheers!",non_debt,-
thrift,2292,comment_4,"Thanks for wanting to help with this issue, as mentioned a patch attached to this ticket for us to review would be preferable. The process for submitting patches if fully detailed here:",non_debt,-
thrift,2292,comment_5,"Hey, i fix problem when build fails due to buffer overflow. Its not solution to current problem but it related and can help in other situations from cordova fail. Here you go a pull request on github:",non_debt,-
thrift,2292,comment_6,"Please add it to next updates, it helps to users",non_debt,-
thrift,2292,comment_7,"Looking at this patch it is something for Apache Cordova and not Apache Thrift, please open a ticket here for this issue",non_debt,-
thrift,2293,summary,leaves files open,non_debt,-
thrift,2293,description,"Even though my thrift server is shutdown and (perhaps?) GC'ed, the keystore file used is still locked by the VM. In i see ks.load(new The FileInputStreams are never closed, which they should. Or is this done deliberately?",non_debt,-
thrift,2293,comment_0,"Hi Andreas , These FileInputStream objects are not closed anywhere IFAIK. I faced the same problem and patched the code. Now it works fine. Please find the attached patch Regards, Venura",non_debt,-
thrift,2293,comment_1,committed ;-r,non_debt,-
thrift,2293,comment_3,Thanks for the diff and commit!,non_debt,-
thrift,2293,comment_4,"Hi, Thanks community for finding the fix for this. While testing and reviewing the patch, we noticed the createSSLContext() allows for the condition where both the keystore and truststore is set: if && { null); } For the case above where both the keystore and truststore parameters are set, the truststore file input stream should still leak (depend on GC) since it was not implicitly closed before the reference ""fin"" is reused for the keystore. We got around this by allocating a different file input stream reference for the keystore and truststore and closed both in the finally block. Best Regards, T.",code_debt,low_quality_code
thrift,2296,summary,Add C++ Base class for service,non_debt,-
thrift,2296,description,This patch adds the possibilty to have a common base class for a service. It's against the TBase patch:,non_debt,-
thrift,2296,comment_0,This patch includes the TBaseService.h class,non_debt,-
thrift,2296,comment_1,"-added a service name to the generator Like in structs, which have the ascii_fingerprint, now every service has a static service name which can be queried from the base class.",non_debt,-
thrift,2296,comment_2,This generates always a default constructor implementation and fixes the missing ~ in the TBaseService class.,non_debt,-
thrift,2296,comment_3,"This puts the default constructor into the implemenation, and renames the static const char* identifier for the servicename to ""id"", like in the TBaseStruct patch.",non_debt,-
thrift,2296,comment_4,Maybe its possible to have a unified base class for structs/services? But this patch still adds a new base class.,non_debt,-
thrift,2296,comment_5,"This updates the ""base class for services"" patch. changes: - id string is removed But it turns out that this is not a good idea. I used the id to announce the services at a ""broker"" server. And now the names look like 13ShmServer, which is the mangled name. So I think that the servicename would be a nice information in the base class. Additionally the base class has a ""role"", which is a hint what the implementation can handle, so the broker sees that client X is connected to service Y. So I think, having the servicename in the base class would be useful.",non_debt,-
thrift,2296,comment_6,I'm still working on it. But with another approach. This approach wasn't useful.,non_debt,-
thrift,2297,summary,TJsonProtocol implementation for Delphi does not allow for both possible slash (solidus) encodings,non_debt,-
thrift,2297,description,According to [RFC the forward slash or solidus may be encoded in two ways: unescaped and escaped,non_debt,-
thrift,2297,comment_0,Committed.,non_debt,-
thrift,2328,summary,Java: eliminate all compiler warnings,code_debt,low_quality_code
thrift,2328,description,I don't like compiler warnings such as these: patches are welcome! -roger,code_debt,low_quality_code
thrift,2328,comment_0,committed fix removing lint check for unchecked casts. compile output,code_debt,low_quality_code
thrift,2328,comment_2,"@  we should keep this check: ""Some input files use unchecked or unsafe operations."" ;-r",code_debt,low_quality_code
thrift,2328,comment_3,"Most of the warnings I do not have a problem with, can suppress them individually. will add back the compilerarg for 0.9.2 and i'll tackle this for 1.0",non_debt,-
thrift,2328,comment_5,this ticket and the linked ones would be a good and likely simple one to clear out once the build process is modernized for Java.,non_debt,-
thrift,2329,summary,missing release tags within git,non_debt,-
thrift,2329,description,"I've recognized, that we miss a thrift-0.9.1 tag... there is just a 0.9.1 branch. The 0.5.0 tag seems to be obsolete and can be deleted. see",non_debt,-
thrift,2329,comment_0,"Had this on my todo list, will get it taken care of, thanks for the ticket",requirement_debt,requirement_partially_implemented
thrift,2329,comment_1,done,non_debt,-
thrift,2329,comment_2,Thanks Jake! ;-r,non_debt,-
thrift,2333,summary,RPMBUILD: Abort build if user did not disable ruby but ruby build will fail later on,non_debt,-
thrift,2333,description,"As can be seen in this email message to the users list it is possible for some pre-required packages to be missing, but the traditional method for validating their existence in specfile using ""BuildRequires"" won't work for items that are not commonly available as RPMS. I think that in this user's case the missing bit is the bundler gem, which I have a locally built RPM for, but I have to assume that most folks do not and it is not commonly available in most repos (not that I saw as a Centos6.4 user anyway) The patch I am attaching just checks if ruby subpackage was not disabled at the (at the buildrpm level) and if the results of ./configure have determined that ""make install"" won't build the gem (and so the rpm build of the rubygem-thrift subpackage is bound to fail) abort the build with a clear message An alternative solution MAY be to just dynamically skip that package when it is detected that ./configure excluded ruby (I don't know off hand how to do that)",build_debt,build_others
thrift,2333,comment_0,Patch to abort rpmbuild (with clear message) if user did not exclude rpm subpackage but ./configure did,non_debt,-
thrift,2333,comment_1,I manually applied your patch - thanks. Additional work on getting better rpmbuild support is tracked in THRIFT-4097.,non_debt,-
thrift,2337,summary,Golang does not report TIMED_OUT exceptions,non_debt,-
thrift,2337,description,"If you set the timeout on a TSocket, thrift will convert the error into a TTransportException which never checks if the error is caused by a timeout so the TypeId will always be",non_debt,-
thrift,2337,comment_0,"Comitted, thanks!",non_debt,-
thrift,2344,summary,configure.ac: compiler-only option,non_debt,-
thrift,2344,description,there are many use cases were you just want to build the compiler. --compiler-only or --disable-libs what do you think? ;-r,non_debt,-
thrift,2344,comment_0,We so need this. +1,non_debt,-
thrift,2344,comment_1,"+1 ""--compiler-only"" is perhaps more readable? but it doesn't matter really Cheers!",code_debt,low_quality_code
thrift,2344,comment_2,what about that? ;-r,non_debt,-
thrift,2344,comment_3,"should be --with-libs to match current convention, otherwise +1",code_debt,low_quality_code
thrift,2344,comment_4,"Hmm, let's discuss before I do some more work on this;-) today we use autoconf [Choosing Package Options | here: and autoconf [Working With External Software | here: I still think that the compiler only build should be an option, as far as I understand autoconf best practice. With is especially meant for external dependencies, what we have with all the languages and corresponding compilers. all the best! -roger",non_debt,-
thrift,2344,comment_5,"Yes, I think you got it Roger! There is no doubt we should keep the standard.",non_debt,-
thrift,2344,comment_6,"ok, I committed this patch, introducing a *--disable-libs* option. If there's another conclusion on the solution, we can reopen and change it.",non_debt,-
thrift,2346,summary,C#: UTF-8 sent by PHP as JSON is not understood by TJsonProtocol,non_debt,-
thrift,2346,description,"C# fails to read hex-encoded JSON with character values Example (taken from THRIFT-2336): Whenever I have our Thrift-For-Php send non-latin utf-8 characters, e.g. "" "" (Russian), I get this: which is a perfectly valid JSON, and I don't mind it being encoded like that.",non_debt,-
thrift,2346,comment_0,Committed.,non_debt,-
thrift,2351,summary,PHP TCompactProtocol has fails to decode messages,non_debt,-
thrift,2351,description,"There are two typos in that prevent correct decoding of any message. They are on consecutive lines: First $seqId does not match case of argument $seqid so this is not set PHP is not case sensitive for class names but IS for variable names. Second and more importantly, on the following line $name has the decoded length assigned to it instead of $result. This means the method name ""hello"" ends up being decoded as ""6"" (the length of the string plus length prefix) and hence any processor trying to dispatch the event will fail. I can submit a patch/pull request but its only 6 chars to change and clearly there are no automated tests for this protocol implementation as that should have been caught before. I guess not many other people are using PHP as a thrift service processor with this protocol.",test_debt,low_coverage
thrift,2351,comment_0,please create a patch for this and if possible add a compact protocol test to test/test.sh we already have this: java might be a compact protocol test partner thanks! ;-r,non_debt,-
thrift,2351,comment_1,"Patch for above issues. I tried to get integration tests to verify this however it seems there are no tests that exercise PHP on the service end and given the trivial nature of this patch I wanted to to submit it without committing several more hours to understand and implement more complete coverage of PHP services in the integration test harness. I was interested to see that only Java is currently tested with the compact protocol which seems an odd decision - certainly the documentation and many ""comparisons"" of thirft vs XXX on the web make a big thing about the compact protocol and give it the appearance of a first class citizen and yet it seems no one ever tried to test it for cross-language RPC? Anyway, hopefully you can see from this patch that these are obvious mistakes and typos and should be included even without the extra effort of making integration test runner able to demonstrate the bug. Thanks. Let me know if I need to submit this differently.",documentation_debt,low_quality_documentation
thrift,2351,comment_3,anything left on this?,non_debt,-
thrift,2351,comment_4,"no test case and *make cross* integration, we can close this if you like.",test_debt,lack_of_tests
thrift,2354,summary,Connection errors can lead to case_clause exceptions,non_debt,-
thrift,2354,description,"This was discovered by Louis-Philippe Gauthier and fixed here and here and I've created a simple test case and verified, as well as applied the patches to some production systems.",non_debt,-
thrift,2354,comment_0,Test case for patch.,non_debt,-
thrift,2354,comment_1,+1 LGTM,non_debt,-
thrift,2354,comment_2,", , if you guys are good with these patches please commit, adding to 0.9.2",non_debt,-
thrift,2354,comment_3,Committed.,non_debt,-
thrift,2367,summary,Build failure: stdlib and boost both define uint64_t,non_debt,-
thrift,2367,description,"When Thrift is built for an 64 bits system, the uint64_t type may be already defined in stdint.h. ""error: reference to 'uint64_t' is ambiguous"" To avoid this error, you need to use explicitly the uint64_t typedef from Boost. So, I propose a quick patch that corrects this error. I reported this issue as Blocker since the build fails but there is no emergency :) Best regards, Romain Naour",non_debt,-
thrift,2367,comment_0,"removed ""using namespace boost;"" to avoid these issues",non_debt,-
thrift,2367,comment_2,Thank you !,non_debt,-
thrift,2375,summary,Excessive <br>'s in generated HTML,documentation_debt,low_quality_documentation
thrift,2375,description,"This was working ""right"" (at least how I thought it should work) back in 0.9.0 going back at least to 0.7.0 and introduced in 0.9.1 The issue is that there is a '<br I will attach complete sample thrift files and html output, but here is a summary: This is what the output USED to look like And this is what it looks like today",documentation_debt,low_quality_documentation
thrift,2375,comment_0,and This allows for markup including: * {{<br * {{<b * {{<u * {{<i * {{<s * {{<big * {{<small * {{<sup * {{<sub * {{<pre * {{<tt * {{<ul * {{<ol * {{<li>}} and {{</li>}},non_debt,-
thrift,2375,comment_1,Jens Thanks for the patch. I ran against my complete set and I have been previously using the following which now (with this path) seem broken: There may be others but it is really difficult to go through all of them and finding '<' characters that should have been left alone and not escaped. Thanks again,non_debt,-
thrift,2375,comment_2,"Added Nevo's tags and a few more. The test case has been updated accordingly. In addition to what Nevo mentioned above, the whitelist now also allows for headings {{<h1>}} through {{<h6>}} and images {{<img>}}.",non_debt,-
thrift,2375,comment_3,Hi Jens Thanks so much! Here is one more testcase where {{<a>}} tags are not processed correctly,code_debt,low_quality_code
thrift,2375,comment_4,"Thanks Jens! The Tags & <BR (an hour ago I edited this comment to say that there may still be an excessive BR, that was operator error)",non_debt,-
thrift,2375,comment_5,Committed.,non_debt,-
thrift,2377,summary,Allow addition of custom HTTP Headers to an HTTP Transport,non_debt,-
thrift,2377,description,It would be beneficial to allow a developer to add his own HTTP Headers to an HTTP Transport as sometimes Thrift HTTP servers may look for custom headers.,non_debt,-
thrift,2377,comment_0,I have attached a patch that implements the required functionality. It includes documentation and a basic test case that passes. I have used it successfully in my application to communicate with a Thrift server that requires custom HTTP headers.,non_debt,-
thrift,2377,comment_1,"Committed, thanks Sheran!",non_debt,-
thrift,2377,comment_3,Thanks,non_debt,-
thrift,2382,summary,contrib: sample for connecting Thrift with STOMP,non_debt,-
thrift,2382,description,"The question how to connect Thrift with MQ and/or message bus systems is asked frequently, especially on SO. This is part 1 of 2 contributions intended to give a starting point when faced with such tasks.",non_debt,-
thrift,2382,comment_0,Committed.,non_debt,-
thrift,2385,summary,Problem with gethostbyname2 during make check,non_debt,-
thrift,2385,description,"I'm trying to run Mesos on several virtual machines with each a NAT NIC and 3 Hostonly adapter. I followed the following guide : but when I try to ""make check"" I have the following error : WARNING: Logging before InitGoogleLogging() is written to STDERR F0304 00:15:58.755775 1571 process.cpp:1486] Failed to initialize, gethostbyname2: Unknown host Check failure stack trace: make[5]: [check-local] Aborted make[5]: Leaving directory make[4]: [check-am] Error 2 make[4]: Leaving directory make[3]: [check-recursive] Error 1 make[3]: Leaving directory make[2]: [check-recursive] Error 1 make[2]: Leaving directory make[1]: [check] Error 2 make[1]: Leaving directory make: [check-recursive] Error 1 I can run Mesos as a master after that but I get the same error if I try to run it as a slave",non_debt,-
thrift,2385,comment_0,"Hi Pierre, I've just started playing around with Mesos. And, I'm facing exactly the same issue on Mesos-0.18.2 build but my environment is Mac OSX (10.9.3). Please let me know if you want more details. Thanks, prakhar",non_debt,-
thrift,2397,summary,Add CORS and CSP support for JavaScript and Node.js libraries,non_debt,-
thrift,2397,description,The attached patch adds support for CORS and CSP to the node.js web server and the JavaScript thrift.js libs. This patch also includes misc comment and readme improvements.,non_debt,-
thrift,2397,comment_0,patch,non_debt,-
thrift,2397,comment_1,"thanks Randy, committed ;-r",non_debt,-
thrift,2404,summary,emit warning on (typically inefficient) list<byte>,code_debt,low_quality_code
thrift,2404,description,"As long as there is no special treatment of {{list<byte Thus, the compiler should emit an adequate warning when {{list<byte",requirement_debt,requirement_partially_implemented
thrift,2404,comment_0,1,non_debt,-
thrift,2404,comment_1,Committed.,non_debt,-
thrift,2405,summary,Node.js Multiplexer tests fail (silently),non_debt,-
thrift,2405,description,"Node.js Multiplexer tests fail silently. The client calls are made and the server appears to report all of the proper input and return the proper output. However, on the client side the results are never returned.",non_debt,-
thrift,2405,comment_0,"This patch repairs the Node.js Multiplex server implementation. The full node.js make check now passes. As far as I can tell from walking the code history, the client side Multiplex code never worked for multiple servers. The node.js code base dumps several pages of jsHint warnings right now. The three files attached here are now jsHint clean, but there's still more to do. It would be great if we could require new JavaScript code to jsHint clean and include working tests prior to commit.",code_debt,low_quality_code
thrift,2405,comment_1,committed,non_debt,-
thrift,2407,summary,use markdown (rename README => README.md),non_debt,-
thrift,2407,description,"As suggested, here is a patch to use markdown all over and rename all README to README.md (we have 41 README files). This allows user friendly rendering on git code hosting portals and is a little preparation for further improvements like add README.md from code to web site easily.",non_debt,-
thrift,2407,comment_0,1,non_debt,-
thrift,2407,comment_1,Please add the following to your git config so these are all not delete/add but are treated as renames,non_debt,-
thrift,2407,comment_2,Thanks for the tipp Jake! New patch using proposed diff options.,non_debt,-
thrift,2407,comment_3,looks good,non_debt,-
thrift,2407,comment_4,committed,non_debt,-
thrift,2415,summary,Named pipes server performance & message mode,non_debt,-
thrift,2415,description,"The performance of the named pipes Delphi server is sub-optimal. Furthermore, BYTE message modes should be used (instead of MESSAGE)",design_debt,non-optimal_design
thrift,2415,comment_0,"Thoroughly tested, patch V3 committed.",non_debt,-
thrift,2416,summary,Cannot use TCompactProtocol with MSVC,non_debt,-
thrift,2416,description,Cannot use TCompactProtocol with MSVC as it appears that and are not defined.,non_debt,-
thrift,2416,comment_0,"I've suggested adding those macros to config.h on windows, but I've got no idea if values of those macros should be static or somehow depended on architecture (does windows works on architecture which would require different values of those macros?)",non_debt,-
thrift,2416,comment_1,"You could fix if for the platforms that you care about, and continue to let it error elsewhere. i386 and x64 are the main platforms to worry about, with ARM following behind that. A distant fourth would be IA64.",defect_debt,uncorrected_known_defects
thrift,2416,comment_2,So what would you say for such patch?,non_debt,-
thrift,2416,comment_3,"The attached patch looks fine to me (though I haven't tested). +1. When the MSVC + ARM developers complain, they can put the appropriate code behind an _M_ARM guard.",test_debt,lack_of_tests
thrift,2416,comment_4,committed,non_debt,-
thrift,2422,summary,Merge fbthrift,non_debt,-
thrift,2422,description,I would really love to see fbthrift merged again. We should combine our power to do *efficient cross language communication across platforms*. Let us create Sub-Task's here and merge this stuff! -roger ;-r,non_debt,-
thrift,2422,comment_0,"we can not take the code from fbthrift and just merge it in directly, we must me granted use for it. I have been working with  on this and he is starting to submit pull requests back to us granting use for these items. Closing these tickets as wont fix until we have been granted use for that code.",non_debt,-
thrift,2422,comment_1,clarify IP topics with THRIFT-2426,non_debt,-
thrift,2422,comment_2,"I'm aware of the IP topics and created THRIFT-2426 to track these activities you mentioned. Of course, no commits until the IPR stuff is clearly clarified Nevertheless we need a place to do brainstorming, group related topics and that's why I created this task. I reopen it as soon as you have clarified the IP topics. We need to study to code base and talk about features and differences of fbthrift and Apache Thrift.",non_debt,-
thrift,2422,comment_3,"+1 for the merge, obviously only if FB is keen to contribute back to the community. But I believe this merge would be also very helpful for them. It's just an idea, but I think we could keep their contribution parallel to the existing library, as they have done themselves. Perhaps cpp11 would probably be a better name, though. We could activate or deactivate the individual libraries separately with autoconf as it has many different dependencies. Although I have nothing against merging their new features back to the current cpp library, I think a new library would definitely much easier. And is probably the only way we could possibly get all the new features and improvements. As we all know there were many tickets and votes also for c++11 compatibility, and as we discussed it's probably impossible to have it and at same time keep back compatibility with legacy code. Any thoughts? I know it would increase maintenance efforts but not more than any other new target language or dialect. Cheers, Henrique",non_debt,-
thrift,2422,comment_4,"Yes, doing latest greatest C++ is requested by many users and I would propose to create a new *lib/c++/* and follow consequently on the latest versions of C++ standards compatible with the most popular compilers gcc, clang++ and msvc.",non_debt,-
thrift,2422,comment_5,Based on comments from  fbthrift contributions must come from the fbthrift team. I am closing this as all subtasks are closed.,non_debt,-
thrift,2431,summary,"TFileTransportTest fails with ""check delta < XXX failed""",non_debt,-
thrift,2431,description,"This test is flaky, I can easily make it fail: Seems like this has hit a in Jenkins already.",test_debt,flaky_test
thrift,2431,comment_0,Looks like a timing problem when the machine is under heavy load.,non_debt,-
thrift,2431,comment_1,"Agree, this is a real issue; I'm not convinced adding timing based checks here is good for the Travis CI build system either. They should probably be removed. I'm sure the intention was to find poorly performing code paths, however that should be done with callgrind, not with a unit test.",design_debt,non-optimal_design
thrift,2431,comment_2,I would say this is fixed on master,non_debt,-
thrift,2433,summary,Allow manual control of OpenSSL,non_debt,-
thrift,2433,description,"Thrift's C++ TSSLSocketFactory controls OpenSSL library It initializes OpenSSL itself and de-initializes OpenSSL after the last TSSLSocketFactory is destroyed. However, programs may need to initialize OpenSSL themselves (e.g. they have other dependencies that want to use OpenSSL) and may depend on OpenSSL functionality after the last TSSLSocketFactory is destroyed. I propose a mechanism to set TSSLSocketFactory to allow manual OpenSSL and to expose the functions to the user. As a result, a user can choose when OpenSSL is initialized and cleaned up, and can even use the prior Thrift code for performing this if it is sufficient for their needs. However, by default, OpenSSL will be automatic, so only people who care about taking control of OpenSSL will need to do anything.",non_debt,-
thrift,2433,comment_0,"Patchset that adds this improvement. Can reformat as one patch (as per contributing rules) if desired, but it seemed multiple smaller logical patches might be a good idea.",non_debt,-
thrift,2433,comment_1,patch looks good but does not apply anymore. Could you polease rebase it. thanks -roger,non_debt,-
thrift,2433,comment_2,Rebased patch,non_debt,-
thrift,2433,comment_3,Patch series rebased and attached.,non_debt,-
thrift,2433,comment_4,thanks Alan! committed ;-r,non_debt,-
thrift,2435,summary,Java compiler doesn't like struct member names that are identical to an existing enum or struct type,non_debt,-
thrift,2435,description,"The following IDL works fine in Python and C# but generates a compiler error in Java: The generated test.Metadata.java file has 2 compile errors: {{Cannot make a static reference to the non-static field DataType}} at line 422: and line 495: If the IDL is changed to use {{DataType dataType}} instead then the compile errors don't occur. This issue is annoying because the same IDL works fine in Python and C#. To make the IDL work for Java we have to change the IDL, which means we have to change our Python clients (I haven't checked the C# clients yet). See also",non_debt,-
thrift,2435,comment_0,Attached sample Thrift file and the files generated by running: thrift-0.9.0 --gen java test.thrift,non_debt,-
thrift,2435,comment_1,"Proposed The idea is, to reference the enum types at these places always via full namespace to prevent ambiguities.",code_debt,low_quality_code
thrift,2435,comment_2,1,non_debt,-
thrift,2435,comment_3,Committed.,non_debt,-
thrift,2449,summary,Enhance typedef structure to distinguish between forwards and real typedefs,non_debt,-
thrift,2449,description,"For some languages (namely Delphi), we need the ability to distinguish the created typedefs between real IDL typedefs and forwards created by THRIFT-2421 on-the-fly. This is important, because code generation is different for both cases.",non_debt,-
thrift,2449,comment_0,"Patch includes both the ""forward"" flag and the necessary changes in the Delphi compiler to provide support for THRIFT-2421.",non_debt,-
thrift,2449,comment_1,"Hey Jens, I also think this is a good distinction to have manifest at the compiler level. Haven't tested the code but the patch looks good to me. +1 -Randy",test_debt,lack_of_tests
thrift,2449,comment_2,Committed.,non_debt,-
thrift,2450,summary,include HowToContribute in the src repo,non_debt,-
thrift,2450,description,None,non_debt,-
thrift,2454,summary,c_glib: There is no gethostbyname_r() in some OS.,non_debt,-
thrift,2454,description,"We don't have gethostbyname_r() in some OS. (e.g. OS X) The gethostbyname() is reentrant in such OS, instead. This is a regression caused by THRIFT-2414",non_debt,-
thrift,2454,comment_0,committed ;-r,non_debt,-
thrift,2483,summary,Go: Provide option to not write default values,non_debt,-
thrift,2483,description,The Go compiler updates needed to support not serializing optional field default values.,non_debt,-
thrift,2483,comment_0,Done in THRIFT-2451,non_debt,-
thrift,2483,comment_1,Fixed by THRIFT-2451,non_debt,-
thrift,2483,comment_2,Fixing the fixVersion.,non_debt,-
thrift,2487,summary,Tutorial requires two IDL files but only one is linked from the Thrift web site,non_debt,-
thrift,2487,description,"Several people had problems following the tutorial, especially under Windows. The Thrift web site offers an EXE download and the {{tutorial.thrift}} file on the [tutorial However, the file {{shared.thrift}}, which is also needed as it is included in {{tutorial.thrift}} is not available there. The workaround is to get the file from teh source tree, e.g. from",documentation_debt,low_quality_documentation
thrift,2487,comment_0,Web site fixed and updated right now.,non_debt,-
thrift,2508,summary,Uncompileable C# code due to language keywords in IDL,non_debt,-
thrift,2508,description,The file leads to errors due to language keywords being part of the IDL file.,non_debt,-
thrift,2508,comment_0,"The generated code is uncompileable both due to the reserved word ""event"" being generated as a variable name, as well as multiple instances of a property with the same name as the containing class which is illegal in C#.",non_debt,-
thrift,2508,comment_1,Tested with - VS2013 and VS2010 on Windows/WinPhone - Mono on Linux Committed.,non_debt,-
thrift,2508,comment_2,"For the records: I did not deal with this issue here, by intention. This part will be solved with THRIFT-2557",non_debt,-
thrift,2510,summary,Implement ability to listen on unix domain sockets,non_debt,-
thrift,2510,description,See,non_debt,-
thrift,2510,comment_2,"We like to have this feature. However, this is very old, please reopen if you have a patch",non_debt,-
thrift,2510,comment_3,"I've been maintaining this patch for a while, but now I no longer use Thrift in my projects, and don't really have time to update it do the master, as it looks like there were some changes in Thrift that broke the patch. Perhaps someone from the Thrift's development team could bring it up to date? Serge On Sun, Nov 29, 2015 at 11:28 AM, Roger Meier (JIRA) <jira@apache.org>",non_debt,-
thrift,2511,summary,Node.js needs the compact protocol,non_debt,-
thrift,2511,description,Add compact protocol support to Node.js,non_debt,-
thrift,2511,comment_0,This patch adds the compact protocol to the Node.js lib. It includes various ThriftTest tests in This patch also repairs a map of maps bug in the node.js test driver and repairs all jshint warnings in There are however still many (valid) jshint warnings in thrift compiler generated node.js code. Will create a new issue for that.,code_debt,low_quality_code
thrift,2511,comment_1,committed,non_debt,-
thrift,2540,summary,Running configure from outside the source directory fails,non_debt,-
thrift,2540,comment_0,"There is a simple workaround for this: It should be clear that this workaround is not ideal, and the configure script should be fixed.",design_debt,non-optimal_design
thrift,2540,comment_1,"I probably overlook the obvious, but what's wrong with running the script from _within_ the source tree?",non_debt,-
thrift,2540,comment_2,In our build environment all source trees are considered read-only.,non_debt,-
thrift,2540,comment_3,"Soyeb Aswat, I also want to keep source tree clean. Did you find any other solution for this? Your suggested solution works with some correction CPP_FLAGS = But then I encounter following problem. Did you encountered it? fatal error: No such file or directory #include ^ compilation terminated. Of course I can supply some additional -I${include_path} flag through CPPFLAGS, but then I think I will have to add some additional -I${include_path} for every language (go, javasrcipt ...)",non_debt,-
thrift,2540,comment_4,"Hello, the following patch will fix the problem. But running make still fails due to the ant can't find build.xml. diff --git index d648706..0032f48 100644  +++ @@ -18,7 +18,7 @@ */ #ifdef _WIN32 -#include <windows/config.h #else -#include <config.h #endif",non_debt,-
thrift,2540,comment_5,"you can use CMake build, it supports out of tree build.",non_debt,-
thrift,2540,comment_6,I also wished to build thrift out-of-source (using the autotools). I was able to bypass this problem by adding this flag to (line #53 in my version): AM_CPPFLAGS = $(BOOST_CPPFLAGS) -I$(srcdir)/src -I$(builddir)/src,non_debt,-
thrift,2543,summary,Generated enum type in haskell should be qualified,non_debt,-
thrift,2543,description,None,non_debt,-
thrift,2543,comment_0,"Compile following thrift files enum X { A = 1, B = 2, C = 4, D = 8 } In generated haskell file, we find import qualified A_Types data B = B{f_B_x :: Maybe X,f_B_y :: Maybe Int32} deriving (Show,Eq,Typeable)",non_debt,-
thrift,2543,comment_2,committed,non_debt,-
thrift,2545,summary,Test CPP fails to build (possibly typo),documentation_debt,low_quality_documentation
thrift,2545,description,"Downloaded 0.9.1 and tried to build, it failed at Makefile:832: warning: overriding commands for target Makefile:829: warning: ignoring old commands for target /bin/bash ../../libtool --tag=CXX --mode=link g++ -Wall -g -O2 -L/usr/lib -o libtestgencpp.la ThriftTest_types.lo -lssl -lcrypto -lrt -lpthread libtool: link: ar cru ar: No such file or directory make: [libtestgencpp.la] Error 1 A close inspection to the Makefile, lines 829 and 832 show: $(THRIFT) --gen -r $< $(THRIFT) --gen cpp $< because a rule for is defined twice, the compiling process breaks. The following patch fixes it  Makefile.wrong 2014-05-26 15:50:10.375655471 +1200 +++ Makefile 2014-05-26 15:49:48.407178441 +1200 @@ -828,7 +828,7 @@ $(THRIFT) --gen -r $< $(THRIFT) --gen cpp $< clean-local:",non_debt,-
thrift,2545,comment_0,"please check latest version, I'm not aware of this.",non_debt,-
thrift,2545,comment_1,"This is working in trunk, please reopen if you are still having problems with this after testing against trunk",non_debt,-
thrift,2546,summary,cross language tests fails at when using nodejs server,non_debt,-
thrift,2546,description,Nodejs server gives following message when failing. test 1) test 2) test 3) events.js:72 throw er; // Unhandled 'error' event ^ Error: read ECONNRESET at errnoException (net.js:901:11) at TCP.onread (net.js:556:19),non_debt,-
thrift,2546,comment_0,This is 2547 from another angle. Will address there.,non_debt,-
thrift,2547,summary,nodejs servers and clients fails to connect with cpp using compact protocol,non_debt,-
thrift,2547,description,"Nodejs servers give following error. BAD: -2111766528 events.js:74 throw unspecified ""error"" event.'); ^ TypeError: Uncaught, unspecified ""error"" event. at TypeError (<anonymous at (events.js:74:15) Socket.<anonymous at (events.js:95:17) at Socket.<anonymous at (events.js:92:17) at emitReadable_ at emitReadable at readableAddChunk Nodejs Client gives following errors. write set 8 3 assert.js:92 throw new ^ AssertionError: Error: write EPIPE at testDriver at EventEmitter.emit (events.js:95:17) at Socket.<anonymous at (events.js:95:17) at onwriteError at onwrite fireErrorCallbacks (net.js:438:13) at Socket._destroy (net.js:472:3) at Object.afterWrite (net.js:718:10)",non_debt,-
thrift,2547,comment_0,test/test.sh reports nodejs client -> cpp server ok but other way around is still a problem.,non_debt,-
thrift,2547,comment_1,Repaired in THRIFT-3008,non_debt,-
thrift,2555,summary,"excessive ""unused field"" comments",documentation_debt,low_quality_documentation
thrift,2555,description,produces which starts to become annoying the larger the gaps between the numbers are.,design_debt,non-optimal_design
thrift,2555,comment_0,Committed.,non_debt,-
thrift,2557,summary,CS0542 member names cannot be the same as their enclosing type,non_debt,-
thrift,2557,description,"The following IDL, although perfectly legal, leads to error CS0542 with C#: gives",non_debt,-
thrift,2557,comment_0,"@all, comments are welcome!",non_debt,-
thrift,2557,comment_1,"+1, nice job Jens! I would add the nameconflict.thrift to /test probably merging with THRIFT-2508. What do you think?",non_debt,-
thrift,2557,comment_2,Good idea. I'll do that.,non_debt,-
thrift,2557,comment_3,"Committed, including new test file",non_debt,-
thrift,2561,summary,"Enumerated types expressed as int, not ""typedef""-ed, in objective-c compilation",non_debt,-
thrift,2561,description,"When an enumerated type is translated to objective-c, the type is replaced with simply ""int"". This takes away half the use of an enumerated type as a type. For instance, it is both clearer and a more precise translation to see ""PersonID"" as a field type in a method than ""int"". However, the objective-c compiler does not typedef the enum and as a result essentially forgets the type that is declared in the thrift IDL.",code_debt,low_quality_code
thrift,2561,comment_1,"I found the implementation in the pull request has already been done (in a slightly different way, but it still looks valid) as part of another ticket.",non_debt,-
thrift,2561,comment_3,"Hello, Unfortunately you have reached me while I am at leave from Haverford College. If the matter is worth re-sending, please direct it to My apologies for the inconvenience. Phil -- Phil EatonHaverford College [1]",non_debt,-
thrift,2561,comment_5,Closing all Thrift-0.10.0 Resolved Jira items (total: 12 that got left behind),non_debt,-
thrift,2568,summary,Implement own certificate handler,non_debt,-
thrift,2568,description,"Enhance the C# TLS-Transport with a paramater to give it a specific certificate handler function. This enables to easely accept untrusted certificates or accept a specific certificate, which can be useful while debuging.",design_debt,non-optimal_design
thrift,2568,comment_2,"LGTM, committed.",non_debt,-
thrift,2572,summary,Add string/collection length limit checks (from C++) to java protocol readers,non_debt,-
thrift,2572,description,None,non_debt,-
thrift,2572,comment_1,committed Thanks Andrew!,non_debt,-
thrift,2573,summary,thrift 0.9.2 release,non_debt,-
thrift,2573,description,"Hello, I need to use recursive struct in Thrift. I saw that the next release will allow this functionality (see link below) but I can't see if this version is going to be released soon or not. Would you have the information ? Is there any stable enough version I could use in the meantime? Thanks ! Link to the feature :",non_debt,-
thrift,2573,comment_0,"Coincidently, the same question was raised just a few days agon on the [mailing",non_debt,-
thrift,2573,comment_1,"One option would be, to clone the repo and build it on your own from scratch.",non_debt,-
thrift,2574,summary,Compiler option to generate namespace directories for Ruby,non_debt,-
thrift,2574,description,"As described in THRIFT-1544, it would be nice if Thrift generated properly-namespaced files for Ruby. That patch would break backwards compatibility, so I don't know if you want it. This one makes namespacing correctly an option (namespaced).",non_debt,-
thrift,2574,comment_0,"Here's the patch with tests, formatted two different ways.",non_debt,-
thrift,2574,comment_3,"Thanks for the patch , committed",non_debt,-
thrift,2588,summary,Thrift compiler is not buildable in Visual Studio 2010,non_debt,-
thrift,2588,description,Missing {{t_typedef.cc}} reference in Visual Studio Project File and a missing {{#include <cctype>}} in prevent the compiler from being built through Visual Studio.,non_debt,-
thrift,2588,comment_0,"Committed, thanks Thomas!",non_debt,-
thrift,2589,summary,C# generator: const of BaseType in IDL generates static instead of const properties,non_debt,-
thrift,2589,description,Compiling a thrift file containing const definitions for basetype variables results in static properties {{public static whatsits}} being generated. Should generate const properties {{public const whatsits}}. Current version generates this from The code should instead look like this. A patch for this change is supplied. As i don't really know yet how the testing for these kind of changes is done i haven't supplied one. But generating all the .thrift files in test with a before and after version of the compiler and comparing the output of both looked good to me.,test_debt,lack_of_tests
thrift,2589,comment_0,"Theoretically, we could do the same with Delphi, could we?",non_debt,-
thrift,2589,comment_1,"Committed, thanks!",non_debt,-
thrift,2589,comment_3,"The issue says ""Fix Version/s: 0.9.2"" but the fix is nowhere to be found in the 0.9.2 branch. Is that by design?",non_debt,-
thrift,2589,comment_4,"Partially, yes. First, it is in the Repository: What you see as the Git 0.9.2 branch was a branch made for the first RC a few days earlier, at 2014-07-10 to be precise. The commit above was made on 2014-08-07, so it is not part of /that/ particular branch. But since the 0.9.2 is not yet released, it will become part of the forthcoming release, thus the ""fixed in version"" comment is correct.",non_debt,-
thrift,2590,summary,C++ Visual Studio solution doesn't include Multiplexing support,non_debt,-
thrift,2590,description,The Visual Studio solution file for the C++ library is missing the files for Multiplexing support. The following files need to be added to the solution:,non_debt,-
thrift,2590,comment_0,"Hi , do you have already a patch (or pull request) at hand that you might want to contribute?",non_debt,-
thrift,2590,comment_1,Hi  I will try to provide a patch. But I didn't have time yet ;) The issue is that the files in Thrift are for Visual Studio 2010 and I'm using 2012. VS2012 is needs to upgrade the solution to work with them so I can't do the changes there and remain compatible with 2010. So in order to make it work I need to change the files manually to the 2010 solution files.,non_debt,-
thrift,2590,comment_2,Patch that adds the three specified files to the project file. Please verify if it's working as i haven't installed the necessary dependencies to compile the c++ library so i couldn't really test it. But it should work.,test_debt,lack_of_tests
thrift,2590,comment_3,Here is my patch. Only the necessary files are added to the .vcxproj file.,non_debt,-
thrift,2590,comment_4,"Committed, thank you both.",non_debt,-
thrift,2599,summary,Uncompileable Delphi code due to naming conflicts with IDL,code_debt,low_quality_code
thrift,2599,description,Same testcase as in THRIFT-2509 leads to errors with delphi as well.,non_debt,-
thrift,2599,comment_0,"* relevant parts changed to use the standardized {{tmp(""myvar"")}} mechanism * added unit namespace to {{IMessage}} and {{TMessageImpl}}",non_debt,-
thrift,2599,comment_1,Tested & committed.,non_debt,-
thrift,2605,summary,TSocket warning on gcc 4.8.3,non_debt,-
thrift,2605,description,same in TServerSocket,non_debt,-
thrift,2605,comment_0,patch attached fixes both TSocket and TServerSocket also - removed mixed line endings in TSocket,code_debt,low_quality_code
thrift,2605,comment_1,something is broken...,non_debt,-
thrift,2605,comment_2,"probably because TSocket has wrong line endings in repo :/ (on windows it checkouts with unix style line endings) maybe try I prepared patch on linux and was able to apply it via above command on windows, so it should work (applying it by git apply modified every single line in file...)",non_debt,-
thrift,2605,comment_3,Thanks Konrad! committed.,non_debt,-
thrift,2606,summary,ThreadManager.h warning in clang++ 3.4,non_debt,-
thrift,2606,comment_0,Thanks Konrad! committed ;-r,non_debt,-
thrift,2607,summary,ThreadManager.cpp warning on clang++ 3.4,non_debt,-
thrift,2607,comment_0,just removed unused field,code_debt,dead_code
thrift,2607,comment_1,thanks Konrad!,non_debt,-
thrift,2609,summary,TFileTransport.h unused field warning (clang 3.4),code_debt,dead_code
thrift,2609,comment_0,"field used - this field could be removed, but by using it code is more consistent",code_debt,dead_code
thrift,2609,comment_1,committed,non_debt,-
thrift,2622,summary,"Expecting > 4 bytes, found only 2",non_debt,-
thrift,2622,description,"When I deploy my application in production Environment it always generate error ""Expecting Source code is // framed transport while (data.length) { if (frameLeft === 0) { // TODO assumes we have all 4 bytes if (data.length < 4) { residual = data; break; //throw Error(""Expecting } frameLeft = 0); frame = new Buffer(frameLeft); framePos = 0; data = data.slice(4, data.length); } I don't know why Can anybody help me",requirement_debt,requirement_partially_implemented
thrift,2622,comment_0,"In most cases this indicates a mismatch between protocol/transport stacks on one of the both ends. - Are you sure the other side uses framed transport? - Is that client or server side? If client, what server implementation (TSimpleServer, TThreadedServer, etc) does the server use? - About what language are we talking? We have plenty of them :-)",non_debt,-
thrift,2622,comment_1,"Thank you! The language we talking is NodeJs. client: transport = protocol = and server: transport = , but not define protocol . When a large number of requests arrive, then happen error",non_debt,-
thrift,2622,comment_2,I'm pretty sure you are running into the bug that has been fixed by THRIFT-2591 (not released yet).,non_debt,-
thrift,2622,comment_3,"It looks like a duplicate, could you check if it works after applying THRIFT-2591 ? Please reopen this if you think it's something else. Cheers, Henrique",non_debt,-
thrift,2627,summary,erlang backend doesn't support corecursive structs,non_debt,-
thrift,2627,description,the erlang compiler does not generate sufficient type information to support corecursive structs,non_debt,-
thrift,2627,comment_1,committed,non_debt,-
thrift,2636,summary,"c_glib: Expose ""type"" and ""message"" properties",non_debt,-
thrift,2636,description,"The class has ""type"" and ""message"" fields, but these are not exposed as GObject properties. Instead clients are expected to modify the object's member variables directly&mdash;a bad practice. The attached patch exposes the two fields as GObject properties (and adds relevant test cases), allowing clients to set and access these fields in a conventional manner.",design_debt,non-optimal_design
thrift,2636,comment_0,Thanks Simon committed -roger,non_debt,-
thrift,2659,summary,python Test Server fails when throwing TException,non_debt,-
thrift,2659,description,"As ThriftTest.thrift has defined TestException has 3 scenarios. if arg == ""Xception"" throw Xception with errorCode = 1001 and message = arg else if arg == ""TException"" throw TException else do not throw anything But when is called, python server is failing saying ERROR:root:This is a TException Traceback (most recent call last): File line 86, in serve oprot) File line 1006, in process seqid, iprot, oprot) File line 1202, in File ""py/TestServer.py"", line 115, in testException raise is a TException') TException: This is a TException It looks like this causes since auto generated doesn't catch TException. def seqid, iprot, oprot): args = args.read(iprot) result = try: except Xception, err1: result.err1 = err1 TMessageType.REPLY, seqid) result.write(oprot) oprot.trans.flush()",non_debt,-
thrift,2659,comment_0,I guess we miss th following test defined here:,test_debt,lack_of_tests
thrift,2659,comment_1,"Yes, this test is currently not available in TestClient.py. Adding this will fail py server - py client cross tests also since issue in TestServer.py I mentioned above.",test_debt,lack_of_tests
thrift,2659,comment_2,"please analyse TException Handling in C++, it works there.",non_debt,-
thrift,2659,comment_3,Fixed by THRIFT-3360,non_debt,-
thrift,2662,summary,python bindings generate incorrect indentation for functions after a oneway function,non_debt,-
thrift,2662,description,The following interface is generated incorrectly by the python bindings.,non_debt,-
thrift,2662,comment_1,problem reproducible only with py:tornado option,non_debt,-
thrift,2662,comment_3,"merged, thanks :)",non_debt,-
thrift,2666,summary,Usage of PYTHONHASHSEED breaks older Python code,non_debt,-
thrift,2666,description,"The hash function introduced with THRIFT-2621 breaks with Python older than 3.2, because PYTHONHASHSEED has been introduced with 3.2. The proposed solution is to define a constant PYTHONHASHSEED as a workaround, but only if PYTHONHASHSEED is not available.",design_debt,non-optimal_design
thrift,2666,comment_0,Replaced {{PYTHONHASHSEED}} by a literal. Any better solution is welcome.,design_debt,non-optimal_design
thrift,2666,comment_1,Committed.,non_debt,-
thrift,2667,summary,create Windows Installer via CPack,non_debt,-
thrift,2667,description,I extended the do automatically generate a Windows Installer for the compiler via cpack. This can be done via mingw32 cross compile.,non_debt,-
thrift,2667,comment_0,committed,non_debt,-
thrift,2677,summary,Haskell library needs to build with network >= 2.6,non_debt,-
thrift,2677,description,"Some recent changes in the {{network}} library for Haskell require conditional compilation, both the haskell tutorial and library need to be updated to take this into account. Description of the solution is on {{network}}'s hackage page: Travis fails with: Travis run with this patch applied: Tutorial successfully compiles: cc:",documentation_debt,outdated_documentation
thrift,2677,comment_0,latest travis ci run:,non_debt,-
thrift,2677,comment_1,Thanks! ;-r,non_debt,-
thrift,2702,summary,empty enums lead to invalid code,non_debt,-
thrift,2702,description,leads to an invalid declaration in Delphi. We need at least one (dummy) value.,non_debt,-
thrift,2702,comment_0,committed,non_debt,-
thrift,2703,summary,Thrift on Windows CE,non_debt,-
thrift,2703,description,1. Which version of thrift support Windows CE 6.0(OS) Platform? 2. Please provide a download link for that version. 3. What are the steps to run that thrift version?,non_debt,-
thrift,2703,comment_0,Please ask questions like this on the dev@ list,non_debt,-
thrift,2706,summary,"D library does not support ""oneway"" message type",non_debt,-
thrift,2706,description,all calls are sent as never using,non_debt,-
thrift,2706,comment_3,"Committed, thank you both!",non_debt,-
thrift,2707,summary,"Ruby library does not support ""oneway"" message type",non_debt,-
thrift,2707,description,all calls are sent using no matter function type,non_debt,-
thrift,2707,comment_2,"+1, please commit this Konrad!",non_debt,-
thrift,2741,summary,Python coding standards,non_debt,-
thrift,2741,description,None,non_debt,-
thrift,2741,comment_0,"We should use PEP 8, this is the well known and best practice.",non_debt,-
thrift,2741,comment_1,is it enough: ? If so I'll close this subtask,non_debt,-
thrift,2741,comment_2,"+1 ready to commit! probably also worth to mention *pylint*, which produces this: check via build and fail on errors in the future...",non_debt,-
thrift,2741,comment_3,added pylint to doc maybe it would be nice to integrate it into our 'make check' step?,non_debt,-
thrift,2741,comment_4,"As before - closing, will be merged with parent task when all ready",non_debt,-
thrift,2752,summary,Centos 6.5 Installation Docs update,non_debt,-
thrift,2752,description,This patch updates the Centos installation doc.,non_debt,-
thrift,2752,comment_0,patch,non_debt,-
thrift,2752,comment_1,committed,non_debt,-
thrift,2759,summary,Trusty Vagrantfile does not make clean,non_debt,-
thrift,2759,description,The Trusty (Ubuntu 14.04) Vagrantfile in the current trunk does not build clean. Subtasks created for: 1. D not installed 2. Haskell does not compile 3. Go crashes on symlink creation 4. Python Twisted tests fail 5. CPP thrift/test/cpp does not make check due to missing boost packages,build_debt,build_others
thrift,2761,summary,go make crashes with Trusty Vagrantfile,non_debt,-
thrift,2761,description,The test/go/Makefile.am uses ln -s and symlinks are not supported in Vagrant host shared volumes like /thrift We (GO people) need to update the Makefile.am,non_debt,-
thrift,2761,comment_0,"I had the same issue until i updated my vagrant and virtualbox to the latest versions, this issue was fixed in [1] in august. Seeing make check pass in test/go. Please test and confirm [1]:",non_debt,-
thrift,2768,summary,Whitespace fixups,code_debt,low_quality_code
thrift,2768,description,"Of course I had to make some content changes. The C# patch file is 354 KB, a lot of files are considered completely replaced. The Delphi patch file has ""only"" 282 KB. The ""content"" changes are (1) one indentation alignment in a batch file, and (2) one missing ASF header in a .cs file which I added. Anything else is just TABS, SPACEs and CRs. I'm going to commit that. For the records, here are the patch files.",code_debt,low_quality_code
thrift,2768,comment_0,Committed.,non_debt,-
thrift,2768,comment_5,"lib/d/Makefile.am leaving test files from 79f988c2 (Jens Geyer 2014-10-03 20:42:54 +0200 183) $(addprefix unittest/release/, $(d_test_modules))",non_debt,-
thrift,2781,summary,D compiler stuck at idlgen due to compiler bug,non_debt,-
thrift,2781,description,"The latest build of DMD leads to the effect, that idlgen will not get compiled. The compiler get stuck at nearly 100% CPU. The problem has been confirmed by",non_debt,-
thrift,2781,comment_0,Committed.,non_debt,-
thrift,2781,comment_1,"The idlgen feature is not strictly required for Thrift. So as a workaround, the patch simply removes it from the build run.",design_debt,non-optimal_design
thrift,2785,summary,Wrap errors in,non_debt,-
thrift,2785,comment_1,"Committed, thanks!",non_debt,-
thrift,2788,summary,Java client and server does not enable tcp keep alive.,non_debt,-
thrift,2788,description,"Both client and server sockets in Java were not enable tcp keepalive. This is important for reap dead tcp connections. If not enabled, the process can not be notified appropriately while remote peer crashed or connection interrupted.",non_debt,-
thrift,2788,comment_1,committed!,non_debt,-
thrift,2791,summary,Allowing use of buffered sockets in go server,non_debt,-
thrift,2791,description,"There is currently no way in a go server to use buffered sockets. Failing to do so decreases performance significantly in my tests. I added an option on TServerSocket to set the buffer size to use. This will default to 1024 bytes, but can be disabled if desired to get back to the original behavior by setting BufferSize to 0. Github pull request: Patch",code_debt,slow_algorithm
thrift,2791,comment_0,"Is this necessary as you can wrap the TTransport with a TBufferedTransport which will use bufio,",non_debt,-
thrift,2791,comment_1,"Chris, I cannot create the TTransport directly in order to do that. The creation of the TSocket is hidden inside the implementation of TServerSocket, who just creates a raw TTransport. TBufferedTransport cannot be used to wrap a TServerTransport, since it is the job of the server transport to accept requests and generate TTransport for each one. That is why this is necessary.",non_debt,-
thrift,2791,comment_2,Committed,non_debt,-
thrift,2791,comment_5,"This is not the appropriate way to add buffering to the server. When you create your server, you can do something like transport, protocolFactory) and it will buffer the sockets it creates. After this patch, my code as above is creating a double layer of buffers, and that is not a good change. Can we please revert this patch and encourage creating buffered server sockets the correct way?",design_debt,non-optimal_design
thrift,2791,comment_6,Created THRIFT-2812 to fix.,non_debt,-
thrift,2833,summary,Version 0.9.2 not available on Maven Central,non_debt,-
thrift,2833,description,"It is written in the download page that Thrift 0.9.2 is available on Maven Central. Sadly, this is not the case.",documentation_debt,low_quality_documentation
thrift,2833,comment_0,"Having a problem with my creds right now for nexus, will get it sorted out and published",non_debt,-
thrift,2833,comment_1,published,non_debt,-
thrift,2833,comment_2,Thanks!,non_debt,-
thrift,2833,comment_3,There is still minor issue: the name of the source package is wrong instead of,non_debt,-
thrift,2833,comment_4,"Hey , our current java package/deploy is not the nicest and is using a mix of ant and maven. I have a ticket open and would like to switch everything over to using maven for the next 0.9.3 release and take care of all these types of issues in the java* packages of Thrift",design_debt,non-optimal_design
thrift,2833,comment_5,", will you return to fix the 0.9.2 source packages once you've completed your Maven switchover?",non_debt,-
thrift,2833,comment_6,"Hey , we do not go back and modify released versions, if its something i can easily fix with just the name of the dist on repo.a.o then i will happily do so",non_debt,-
thrift,2851,summary,Remove strange public Peek() from Go transports,code_debt,low_quality_code
thrift,2851,description,"I've been seeing this public Peek() function in the GO library for a while, but still cannot figure out any sense to it. If there are useless, we can remove them right? This PR removes the public Peek() from the implemented transports. All tests still pass. PR:",code_debt,dead_code
thrift,2851,comment_0,"It may not be the best way how it is implemented, but {{Peek()}} is a standard functionality for all Thrift transports. Throughout the library the core concepts and the implementations should be as consistent as possible. Even though {{Peek()}} is not listed (note that the list is marked as not exhaustive), IMHO making it better = @all: Other opinions welcome, though.",design_debt,non-optimal_design
thrift,2851,comment_1,"Ok I see the point now. I stumbled upon it again, because in THRIFT-2502 the Peek function for iostream_transport got commented out. There seem to be no support from GO to provide such a function to most of the transport. IMHO Only barely used memory_buffer could get a real implementation. So the question is if we should carry along the Peek() function even though there will be no real implementation for quiet some time. Currently the implementation doesn't seem to follow a strict line. E.g. the transport interface doesn't carry a Peek method and not all transport implements the Peek function. I vote for remove, as we gain nothing, and appreciate the differences of having different language platforms with different abilities.",code_debt,low_quality_code
thrift,2851,comment_2,"After sleeping a night about it, I think you are right. In its current form Peek() is not only next to useless, but more important misleading to the less careful observer. So unless someone comes up with a better implementation, we should throw it out. Committed.",code_debt,low_quality_code
thrift,2853,summary,Adjust comments not applying anymore after THRIFT-2852,non_debt,-
thrift,2853,description,THRIFT-2852 changed behavior of which made some comments invalid. This PR removes the remaining comments. PR:,documentation_debt,outdated_documentation
thrift,2853,comment_1,Thanks for applying my patch so quickly! I forgot to remove the comments that doesn't apply anymore. This PR removes them. I try to be more careful on future patches.,documentation_debt,outdated_documentation
thrift,2853,comment_2,"Committed. I changed it a bit, since Open() indeed looks slightly odd at the first glance.",non_debt,-
thrift,2868,summary,Enhance error handling in the Go client,non_debt,-
thrift,2868,description,The Go client doesn't do proper error checking. E.g. it doesn't check whether the received method name is correct nor if the message type has the expected value. The following PR enhances the Go client error handling by the following: - Check if method name is correct - - Check if MessageType is thrift.REPLY or EXCEPTION - - Checking the sequence id is done before checking the message type Includes test cases for every error case.,code_debt,low_quality_code
thrift,2868,comment_1,"Committed, thanks!",non_debt,-
thrift,2874,summary,"TBinaryProtocol member variable ""string_buf_"" is never used.",code_debt,dead_code
thrift,2874,description,string_buf_ and string_buf_size_ are never used. Removing these will also resolve THRIFT-2465.,code_debt,dead_code
thrift,2874,comment_2,"Pushed, thanks!",non_debt,-
thrift,2879,summary,TMemoryBuffer: using lua string in wrong way,non_debt,-
thrift,2879,description,"1. local val = self.rPos, len) The call string.sub(s,i,j) extracts a piece of the string s, from the i-th to the j-th character inclusive. In Lua, the first character of a string has index 1. 2. self.buffer = self.buffer + buf self.wPos = self.wPos + buf .. is the string concatenation operator in Lua, not +.",non_debt,-
thrift,2879,comment_1,committed,non_debt,-
thrift,2881,summary,Handle errors from Accept() correctly,non_debt,-
thrift,2881,description,"Often in tests, servers are started and closed when a test case is finished, as in the standard library's ""httptest"" package. However, currently the ""p.quit"" channel of TSimpleServer does not correctly handle the case when the listener socket is Closed, resulting in spurious logs with the text ""Accept err: ..."". The pull request that fixes this is . For details of the discussion behind handling closed listener sockets, please see",non_debt,-
thrift,2881,comment_1,"Committed, many thanks!",non_debt,-
thrift,2890,summary,binary data may lose bytes with JSON transport under specific circumstances,non_debt,-
thrift,2890,description,"The Delphi implementation currently relies on IdCoderMIME for base64 encoding/decoding, which expects the base64 encoding to be complete with padding. With Thrift it is legal to transfer binary data via JSON without padding, but IdCoderMIME fails to handle this case correctly. Depending on the length of the data, up to 2 bytes from the end of a binary data block may get lost.",non_debt,-
thrift,2890,comment_2,Committed.,non_debt,-
thrift,2907,summary,'ntohll' macro redefined,code_debt,low_quality_code
thrift,2907,description,The {{ntohll}} macro is already defined in on Mac OS X.,code_debt,low_quality_code
thrift,2907,comment_2,"Commited, thanks!",non_debt,-
thrift,2932,summary,Node.js Thrift connection libraries throw Exceptions into event emitter,non_debt,-
thrift,2932,description,"I've been investigating using the Node.js client in a project however it seems like there are instances which don't follow Node.js best practices. In particular http_connection.js and connection.js throw errors during callbacks. This is considered an anti-pattern in Node because it both removes the Exception from the context of the callback making it hard to associate with a request as well as throwing it in the context of the EventEmitter code which can cause inconsistencies in the Node process. This means under some error conditions an uncaught exception would be thrown or at least an 'error' event on the singleton client (again removing it from the request context). Both transport receivers share the same copy-pasta code which contains: I'm working on a patch, but I'm curious about some of the history of the code. In particular the exception based loop flow control and the using the seqid to track the callback which makes it hard to properly associate it with exception handling.",design_debt,non-optimal_design
thrift,2932,comment_0,"+1 On knowing more about this as well. I'm about to start work on using the Thrift JavaScript client in the browser, and would like to know about the history of the JavaScript code. I'm especially curious to know why there isn't more code sharing between the js node thrift libraries and the js browser thrift library.",non_debt,-
thrift,2932,comment_1,"Both of these libs predate my involvement in Thrift but when first exposed to them a couple years ago they were primordial and supporting only very limited functionality. The browser lib only did JSON/XHR/HTTP and Node only did Binary/Socket so they could not talk to each other (browser was/is tested against Java). No npm or bower support at that time either. The libs are now out on npm and bower (which is a mess due to the whole repo download thing). Compact and JSON protocols were added to Node. Many things that didn't work at all were fixed and lots of tests were added. Other than the tests (which were needed just to get things working more broadly) all of this work has been feature oriented and incremental. People tend to provide patches to fix or add things they need, not clean up stuff that is a mess (partly because it might break existing code, for example changing the call back interface in the browser to add error objects). So we have some baggage... That said, you are not alone. Many would like to see the kind of improvements mentioned here and in other tickets. There's still time to make a big push before Thrift v1.0. Patches welcome!",design_debt,non-optimal_design
thrift,2932,comment_2,"Randy, here is a single consolidate patch for all the previous patches. This applies cleanly to the current HEAD on master.",non_debt,-
thrift,2932,comment_3,"Hey Andrew, Thanks for the consolidated patch. Several nice improvements here. I am concerned about the new sequence id modification though. It propagates the sequencing data into the transport. This may seem reasonable in isolation but the transport should not be involved at this level of abstraction. In Apache Thrift, Protocols are responsible for the physical layout of bits (on the wire, on disk or in memory). This includes where, how and if sequence numbers are packaged. Transports are responsible for transmission of the data only. This may involve packetizing, framing, encrypting or what have you, but the key is that the Protocol and the Transport are isolated. The Transport receives/returns an opaque block of bits from/to the Protocol. The current node implementation is not clean in this regards and ultimately needs some structural refactoring. For example the multiplexing read implementation is a hack (paying no attention to the service name in the payload and rather wiring everything to seqid lookups). I think we should try to migrate things in the direction of the generalized Apache Thrift model. This is non-trivial due to the pure async approach of JavaScript (a late model option in the reference CPP and Java implementations). The heart of the node problem here is that Apache Thrift is a layered system. Lower layers provide services to upper layers and services do what you tell them, they dont call you with demands. The node implementation wires callbacks from the bottom up (with several call chains back down from proto to trans in the process). These conflicting approaches are merged with unsatisfactory results in my opinion. To respect the Apache Thrift layered approach the data event needs to be handled by the client (or by a much more focused connection helper) and this piece of code needs to then call down the layered stack like all other languages (client- In short, getting the node client side call return in order is a fair size chunk of work and should fall into a separate issue.",design_debt,non-optimal_design
thrift,2932,comment_4,hunks committed from original patch.,non_debt,-
thrift,2932,comment_5,"The general idea is that with NodeJS code you never ever throw (in fact the only native part of the language that throws on user supplied input AFAIK is JSON.parse and that's a historical anomaly due to the way Crockford first implemented it). Instead of throwing you should always return the error back to it's caller as the first argument of the callback. When that isn't possible, emitting ""error"" events is the next best thing. Throwing of an error should only occur if no callback has been bound to the ""error"" event. Overall throwing errors is never used for flow control. Regarding the specifics for this patch, I have pinged Tom on this issue to get his feedback. He knows more about what he was thinking when he made these changes and will be better able to address the points you raised. I'll dig through these changes in light of your comment and see what else I might be able to add. None of the other patches I submitted are dependent on this change, but let me know if you need me to re-roll them in light of only accepting some of the changes in this patch.",non_debt,-
thrift,2932,comment_7,Seems like future progress in this area is carrying on in other tickets. I am resolving this one as fixed. Please reopen if we need to revisit things.,non_debt,-
thrift,2937,summary,Allow setting a maximum frame size in TFramedTransport,non_debt,-
thrift,2937,description,"To secure Thrift servers against malicious attacks or corrupted data, an often requested feature is to limit the maximum size of a frame at receive. TNonblockingServer already has such a feature. The attached patch imposes a maximum frame size in TFramedTransport. The default value is very conservative (1MiB), to make sure that memory cannot be easily exhausted. The user can then increase the maximum frame size, as required. Example usage: Good Client -Server -Good Client - Bad Client -Server -> Bad Client: [connection dropped]",non_debt,-
thrift,2937,comment_0,Patch for this improvement,non_debt,-
thrift,2937,comment_1,I'm ok with this. However we should set it also to the same level(256MB) as we did for TNonblockingServer with THRIFT-1337,design_debt,non-optimal_design
thrift,2937,comment_2,committed with a 256MB limit,non_debt,-
thrift,2942,summary,CSharp generate invalid code for property named read or write,non_debt,-
thrift,2942,description,"C# generator will produce invalid code for this struct CsFail { 1: optional bool read 2: optional bool write } this will generate code that that will not compile, due to name clash between : public bool Read { get { return _read; } set { __isset.@read = true; this._read = value; } } and Read method implementation public void Read (TProtocol iprot)",non_debt,-
thrift,2942,comment_0,"Should be easy. There is already some handling built in, we just need to add it there. If you have the time to come up with a patch I gladly review it, otherwise I look into it next week. Thanks for catching it.",non_debt,-
thrift,2942,comment_1,"this is my quick fix. It's works for me.. index 05a084a..5a7a500 100644  +++ @@ -2525,6 +2525,14 @@ void scope, newname += '_'; } + // new name conflict with defined methods + if == 0 || == 0) { + pverbose(""struct %s: member %s conflicts with defined method\n"", + structname.c_str(), + newname.c_str()); + newname += '_'; + } + // new name conflicts with another member if != { pverbose(""struct %s: member %s conflicts with another member\n"",",non_debt,-
thrift,2942,comment_2,"I'm afraid that's a bit too Q&D :), but if it solves the actual issue for you, that's ok.",non_debt,-
thrift,2942,comment_3,"Committed, including another fix for {{Foo_result}} more or less caused by the same issue.",non_debt,-
thrift,2942,comment_5,"Thankyou Jens, nice clean fix. Luca",non_debt,-
thrift,2953,summary,is not Stop()able,non_debt,-
thrift,2953,description,A server using cannot be stopped if the server does not see any traffic and is stuck in Closing and disposing the pipe stream does not work in all cases with synchronous I/O.,non_debt,-
thrift,2953,comment_2,Committed.,non_debt,-
thrift,2961,summary,Service inheritance does not work with namespaced Ruby code,non_debt,-
thrift,2961,comment_0,committed,non_debt,-
thrift,2969,summary,nodejs: DRY up library tests,non_debt,-
thrift,2969,description,None,non_debt,-
thrift,2969,comment_0,"These patches together represent a major refactoring of the nodejs library testing code, eliminating most of the duplication. The testAll.sh script now tests every possible implementation. There is room for parallelizing these tests for speed, but that can be the subject of another patch. I'm trying to move as fast as possible to improve the nodejs code. The sooner this and my other patches can be reviewed the faster I can contribute more patches before we hit 1.0.0.",code_debt,duplicated_code
thrift,2969,comment_1,Here is a consolidated patch that applies cleanly to master after the patch in THRIFT-2932 has been applied.,non_debt,-
thrift,2969,comment_2,"FWIW, this patch is probably stale right now. I will replace it with an updated one as soon as one of the patches in THRIFT-2964 have been applied.",non_debt,-
thrift,2969,comment_3,"Randy, here is a brand new patch for this issue that will cleanly apply to the current master HEAD / THRIFT-2964). This should be the next patch reviewed for the NodeJS library code.",non_debt,-
thrift,2969,comment_4,"Thanks Andrew, I'll review this this weekend.",non_debt,-
thrift,2969,comment_5,"Hey Andrew, Great patch. Big improvement. I had to remove a spurious require xtend from server.js but otherwise as submitted. -Randy",code_debt,low_quality_code
thrift,2969,comment_6,"committed. Great patch Andrew, many thanks.",non_debt,-
thrift,2972,summary,Missing backslash in,code_debt,low_quality_code
thrift,2972,description,It seems that I failed to escape a line ending in THRIFT-2910 fix. For some reason it doesn't break {{make check}} right now but only causes a bootstrap (or configure ?) warning and processor_test executable is missing because of this.,code_debt,low_quality_code
thrift,2972,comment_1,Thanks!,non_debt,-
thrift,2991,summary,customizable CMake build,non_debt,-
thrift,2991,description,The user should be able choose which libraries to build if any.,non_debt,-
thrift,2991,comment_0,"Also it would be nice to be able to build ""only"" certain libs, rather than having to without 12 to get the three you want.",non_debt,-
thrift,2991,comment_1,Please see THRIFT-2850,non_debt,-
thrift,3007,summary,Travis build is broken because of directory conflict,non_debt,-
thrift,3007,description,CMake build on Travis invokes {{mkdir build}} that fails because the directory already exists in the repository.,non_debt,-
thrift,3011,summary,C# test server testException() not implemented according to specs,requirement_debt,requirement_partially_implemented
thrift,3011,description,Actual implementation:,non_debt,-
thrift,3011,comment_0,Committed.,non_debt,-
thrift,3027,summary,Go compiler does not ensure common initialisms have consistent case,design_debt,non-optimal_design
thrift,3027,description,"In Go, as per words in names that are initialisms or acronyms should have a consistent case. For example, if you have a struct like: One would expect it to compile to: Rather than:- It would be pretty difficult to handle all cases of initialisms in the Go compiler of course, but there is a set of common initialisms that have been identified by the authors of Golint and could be handled relatively easily:-",design_debt,non-optimal_design
thrift,3027,comment_1,"Hi , the patch introduces a new dependency to boost into the Thrift compiler. Can we avoid that? Have a look into there are some similar functions that may serve as model. Next, the {{to_upper_copy()}} function expects a locale, the default is {{std::locale()}} which is ... Not sure if that is what we want. The generated code should not depend on a particular system locale.",design_debt,non-optimal_design
thrift,3027,comment_2,"Thanks for the feedback. I've incorporated it into the pull request now:- I've removed the usage of the to_upper_copy function and replaced it with usage of transform and toupper like how does it. Regarding the locale, I think it'll always be in as there don't seem to be calls to std::locale::global in the codebase but I've set it explicitly to classic to be safe.",non_debt,-
thrift,3027,comment_4,"Committed, thank you!",non_debt,-
thrift,3027,comment_6,Thank you!,non_debt,-
thrift,3040,summary,"bower.json wrong ""main"" path",non_debt,-
thrift,3040,description,"How to produce: bower install thrift -save grunt wiredep with the current path thrift is not included. it has to be changed from - ""main"": ""lib/js/thrift.js"", to + ""main"": Pull request on GitHub:",non_debt,-
thrift,3040,comment_1,"Committed, thanks for the patch! Note, our Bower solution is a mess (cloning the entire thrift repo) and may change in the future. Also the src dir thrift.js may end up broken into multiple files, with thrift.js only being available in lib/js/dist after a grunt build. In the mean time this patch is a needed fix.",code_debt,complex_code
thrift,3045,summary,Multiple SuppressWarnings in Generated Java Code,non_debt,-
thrift,3045,description,Java generated code is adding two suppress warnings as follows: The java_suppressions() seems to be added in 0.9.2 which is right. But there is a default getting added. One of it needs to be turned off.,code_debt,low_quality_code
thrift,3045,comment_0,patches to improve the generated code and the Java library code are always welcome,non_debt,-
thrift,3045,comment_1,Sorry this was an invalid issue. We had the supresswarnings all inserted by the script which was adding ASF license headers. Please ignore this issue.,non_debt,-
thrift,3047,summary,Uneven calls to indent_up and indent_down in Cocoa generator,non_debt,-
thrift,3047,description,"indent_down was called one extra time which gave us an indentation level of -1, then to combat this, we indented the server implementation an extra level. This appeared as if everything was fine unless you generated two services in one thrift file, in which case the indentation would get progressively worse.",code_debt,low_quality_code
thrift,3047,comment_1,I didn't include this in THRIFT-3041 because it made the generated sources exceedingly difficult to compare.,design_debt,non-optimal_design
thrift,3047,comment_3,"Committed, thanks!",non_debt,-
thrift,3052,summary,"Perl: Support for Multiplexing Services on any Transport, Protocol and Server",non_debt,-
thrift,3052,description,See THRIFT-1915 for details.,non_debt,-
thrift,3052,comment_0,Does Jira truncate the list of technical tasks? I can't believe I missed the other one. Sorry about that.,non_debt,-
thrift,3052,comment_1,"That's fine, no issue. If you like to do some review of the work done in THRIFT-3033, that would really help a lot.",non_debt,-
thrift,3088,summary,TThreadPoolServer with Sasl auth may leak CLOSE_WAIT socket,code_debt,low_quality_code
thrift,3088,description,"Start TThreadPoolServer to server with as transportFactory. While using nc to test the specified port whether reachable, it will leak CLOSE_WAIT socket.That's because nc will close socket at once while successful connect TThreadPoolServer, but the server still try using sasl protocol to build an inputTransport which of course failed at once. However inputTransport is null which makes it can't close socket properly which lead to CLOSE_WAIT socket.",code_debt,low_quality_code
thrift,3088,comment_1,committed,non_debt,-
thrift,3088,comment_6,The pull request for this issue is still open - please close it.,non_debt,-
thrift,3098,summary,Thrift does not pretty print binary typedefs the way it does binary fields,non_debt,-
thrift,3098,description,The generated Java struct toString uses to print binary fields but not fields which are typedef binary. e.g. results in,non_debt,-
thrift,3098,comment_2,"Committed, thank you!",non_debt,-
thrift,3104,summary,Include TZlibTransport in Windows C++ library,non_debt,-
thrift,3104,description,Currently the thrift solution file for the C++ library does not include support for TZlibTransport. I added zlib-1.2.5 built for Windows to the include paths and included the two files that were omitted and it built...,non_debt,-
thrift,3104,comment_0,"The cmake build environment makes a ""thriftz"" library that adds zlib support to any system, just like the thriftnb (non-blocking) library adds support for that area of code. Therefore nothing to do here.",non_debt,-
thrift,3114,summary,Using local temp variables to not pollute the global table,code_debt,low_quality_code
thrift,3114,description,"Should prefix the field deserialization statements with ""local"" whenever the output is a temporary variable, for example this compiler output: _elem46 = iprot:readI32() should be changed to: local _elem46 = iprot:readI32().",code_debt,low_quality_code
thrift,3114,comment_2,"Committed, thanks!",non_debt,-
thrift,3140,summary,is thrown by JavaScript test server,non_debt,-
thrift,3140,description,"After running *make check* for JS or *ant test* in {{lib/js/test}}, you can always find several stack traces in I don't know if it's even a bug since it does not cause any test failures. It distracted me from solving real cause of failures for a while through. Cause: Threading issue. Synchronization is needed because all instances of MimeUtil2 seems to share single state.",code_debt,multi-thread_correctness
thrift,3157,summary,"TBase signature should be TBase<T extends TBase<T,F>, F extends TFieldIdEnum>",non_debt,-
thrift,3157,description,Is there any reason this isn't the case? The wildcarding doesn't appear to give us anything extra. I've been told that it may help with uses of the Comparable. Can anyone think of a use?,design_debt,non-optimal_design
thrift,3157,comment_3,"Committed, sorry for the delay, thanks !",non_debt,-
thrift,3168,summary,Fix Maven POM,non_debt,-
thrift,3168,description,"1. THRIFT-2269 was not fixed properly. Sources are supposed to be deployed using a ""jar"" type. cannot resolve them otherwise and IDEs such as IntelliJ IDEA are unable to resolve or even manually attach sources with a ""src"" extension/type. 2. Between 0.9.1 and 0.9.2, the POM for libthrift was modified to remove a dependency on commons-lang3. As a result, the code generated by libthrift no longer builds as it misses this dependency. The dependency needs to be restored, or whatever reason resulted in its removal re-evaluated to take this requirement into account. Personally, I'd prefer if we could just drop this dependency entirely and use native Java language utilities such as `Objects` to solve the problems `commons-lang3` is trying to solve.",build_debt,under-declared_dependencies
thrift,3168,comment_0,I reopened THRIFT-2269 and linked this to it as a duplicate.,non_debt,-
thrift,3179,summary,"lua plugin can't connect to remote server, because function always bind socket to localhost",non_debt,-
thrift,3179,description,None,non_debt,-
thrift,3179,comment_2,committed,non_debt,-
thrift,3191,summary,Perl compiler does not add support for unexpected exception handling,non_debt,-
thrift,3191,description,"While adding ""make cross"" test server support for some other refactoring I found that the generated code to handle testException exception responses does not work properly. The test says that the code can die with the specified exceptions, but it can also die with Thrift::TException. The generated code that fails in ThriftTest.pm: If the resulting implementation dies with a {{new the C++ client side gets a void back. The result should be a so that the client understands something went wrong at a more fundamental level than in the handler. There are a number of other issues with ""make cross"", for example constructor in perl doesn't pass the arguments to the method. All of these things need to be fixed in order for make cross to work.",non_debt,-
thrift,3191,comment_0,"Linking a few things together. Given the lack of test infrastructure for perl, I will be fixing these at the same time. I am updating ""make cross"" for THRIFT-3053 to support perl client and server, ssl or no ssl. Getting that working required me to solve THRIFT-3189 and THRIFT-3191 along the way.",test_debt,lack_of_tests
thrift,3191,comment_1,contains a fix for this.,non_debt,-
thrift,3197,summary,keepAliveTime is hard coded as 60 sec in TThreadPoolServer,code_debt,low_quality_code
thrift,3197,description,"While creating ThreadPoolExecutor in TThreadPoolServer, keepAliveTime is hard coded as 60 sec. It should be",code_debt,low_quality_code
thrift,3197,comment_0,Attached the patch.,non_debt,-
thrift,3197,comment_1,Added stopTimeoutVal(int n) method to set the time out value.,non_debt,-
thrift,3197,comment_2,"Ping , can you please review the patch?",non_debt,-
thrift,3197,comment_3,"Thanks for the patch , committed",non_debt,-
thrift,3206,summary,Fix Visual Studio build failure due 'pthread_self': identifier not found,non_debt,-
thrift,3206,description,Build fail of project due to compile error The current Thrift code calls which is not available on Windows platform in current Thrift library dependency set. Herein I propose resolution by using native WINAPI equivalent in case source is compiled when _WIN32 macro is defined.,non_debt,-
thrift,3206,comment_0,This is only an issue with older versions of open ssl. The pthread_self() aspect is guarded with an openssl preprocessor version check.,non_debt,-
thrift,3206,comment_1,committed,non_debt,-
thrift,3207,summary,Enable build with OpenSSL 1.1.0 series,non_debt,-
thrift,3207,description,As thrift requires OpenSSL v1.1.0 there are still some problems with compiling latest official releases on Visual Studio 2013. We falled back to other OpenSSL releases that source version can be compiled without problems. Unfortunately Thrift can not be build against OpenSSL pre 1.1.0 due to missing TLSv1_1_method() and TLSv1_2_method(). While OpenSSL in version 0.9.8 and 1.0.0 is supported till and Open SSL in version 1.0.1 and 1.0.2 is supported till we found reasonable to enable this releases to be used for Thrift builds. The proposed patch enables Thrift support of TLS 1.1 and 1.2 only when build with OpenSSL 1.1.0 or newer.,non_debt,-
thrift,3207,comment_0,Patch applies and looks good. Ready to merge I would say.,non_debt,-
thrift,3207,comment_1,"Hi , would you please modify ticket name to ""Enable build with OpenSSL 1.1.0 series"" and close this issue? Thank you.",non_debt,-
thrift,3207,comment_2,You really mean only close? Nothing else?,non_debt,-
thrift,3207,comment_3,Yes. This issue is duplicated by THRIFT-3736 and latest code of master branch can be built with OpenSSL 1.1.0c.,non_debt,-
thrift,3207,comment_4,I have been working on Windows build instructions for the development tip on master with 1.1.0c and it does appear to work properly.,non_debt,-
thrift,3207,comment_5,"Given THRIFT-3736 was fixed in 0.10.0, is this one mislabeled as fixed in 0.11.0?",non_debt,-
thrift,3207,comment_6,I am working on Mac OS. I am having some trouble to install Thrift Patch. How to install Patch ?,non_debt,-
thrift,3226,summary,Fix TNamedPipeServer trapped in loop on accept,code_debt,low_quality_code
thrift,3226,description,"TNamedPipeServer can trap itself in infinite loop if the client closed Pipe before server instantiated TPipe object in The accept loop waits for connected client on , ..) then wraps the connected pipe client hande into TPipe object, opens another pipe handle to wait for next client and returns the newly created pipe wrapper object. TPipe object may throw on c-tor if the pipe in meantime has been closed by the client. If so the new pipe instance for next client will NOT be created as the implementation expects. The accept is called with no inital valid pipe handle what causes it to throw and restart accept. It happens then forever. I propose to solve the issue by handling the case when TPipe throws: The failed handle should be dropped and new pipe created and wait for next client repeated within acceptImpl",non_debt,-
thrift,3226,comment_0,"Don't try to use closed transports. Calling interrupt() before a server starts to serve should work, and will hopefully satisfy your use case.",non_debt,-
thrift,3226,comment_1,"2.0. It is impossible to cause thread blocked on TServer::serve() to return by any operation on TServerTransport derivateive, including call to because 2.1.1. Call to (see. attached source causes the TThreadPoolServer (as well TSimpleServer) to report error and go and wait on accept for new connections in dispatch loop, and 2.1.2. Call to causes the TThreadPoolServer (as well TSimpleServer) to report error and actively loop in serve() loop, 2.2. because 2.3. In current implemntation to cause or to return it is required that the stop_ member must is set to true, and 2.4. The TServerTransport derivatives have no reference to TSimpleServer or TThreadPoolServer in order to mutate the stop_ member to true, thus 2.5. To fix it the condition in TServer derivatives must be fixed that the receival of TTransportException such that == is enough to cause server stop and break serve loop.",non_debt,-
thrift,3226,comment_2,Not applicable to latest master.,non_debt,-
thrift,3232,summary,Cannot deserialize json messages created with fieldNamesAsString,non_debt,-
thrift,3232,description,"THRIFT-2535 introduced the fieldNamesAsString argument on When you use it to serialize messages to json , the resulting output cannot be deserialized back to an object when using the same protocolfactory Example : Will give you Exception in thread ""main"" Unexpected character:",non_debt,-
thrift,3232,comment_0,It is intended behavior. Quote from THRIFT-2535,non_debt,-
thrift,3241,summary,fatal error: runtime: cannot map pages in arena address space,non_debt,-
thrift,3241,description,Passing huge strings to a Go server may produce a fatal exit with an OOM error. Main problem is that [OOM errors are not recoverable by in Go.,non_debt,-
thrift,3241,comment_0,"Hi , do you have any idea how to overcome that? I mean we could add some sanity checking on the string length, but that sounds a bit heuristic to me. OTOH I have no other idea how to overcome that one.",non_debt,-
thrift,3241,comment_1,"It is by design, and is considered a production issue. 1. Run with X memory. 2. Max out the load, and observe memory use. 3. Use less memory the next run if there is significant unused memory, more if they are OOM'ing. 4. If you cannot raise memory, lower the concurrency per server. Steps 1-3 are completely automated in Google production systems and elsewhere. So in Thrift what's important is exposing the concurrency limits in all pool/async implementations. In Go LimitListener is useful.",design_debt,non-optimal_design
thrift,3241,comment_3,Note: The behaviour is unfortunately by design with Go. The only solution is to *consequently use framed transport* which avoids the issue by means of the included frame size.,non_debt,-
thrift,3241,comment_5,Committed.,non_debt,-
thrift,3248,summary,TypeScript: additional comma in method signature without parameters,non_debt,-
thrift,3248,description,"The generated TypeScript code has an issue with methods with no parameters. The method signature shows an additional comma character (',') in the parameter list which have a callback argument.",non_debt,-
thrift,3248,comment_0,committed,non_debt,-
thrift,3255,summary,Thrift generator doesn't exclude 'package' keyword for thrift property names breaking java builds,non_debt,-
thrift,3255,description,"Thrift allows the word ""package"" for it's property names which when generating java will not compile.",non_debt,-
thrift,3255,comment_0,"In what other language is {{package}} a keyword? Are there any, or is this really only a Java specific thing? EDIT: Quite a lot. Preparing a patch ...",non_debt,-
thrift,3255,comment_2,Committed.,non_debt,-
thrift,3274,summary,"calling ""make clean"" twice in a row yields make error",non_debt,-
thrift,3274,description,After calling {{make clean}} by accident twice in a row I got this:,non_debt,-
thrift,3274,comment_1,"Better, but not perfect. The tutorial seems to suffer from the same problems:",documentation_debt,low_quality_documentation
thrift,3274,comment_3,Thanks Aki!,non_debt,-
thrift,3274,comment_5,fix for tutorial is in verification...,non_debt,-
thrift,3274,comment_6,tutorial fix is committed,non_debt,-
thrift,3274,comment_8,"Yep, that's it. Thanks!",non_debt,-
thrift,3276,summary,Binary data does not decode correctly using the TJSONProtocol when the base64 encoded data is padded.,non_debt,-
thrift,3276,description,"If the base64 encoded JSON string has padding bytes at the end, the resulting binary data returned in the thrift object ends up with extra padding bytes (0xFF) at the end. It seems like this is caused by the TJSONProtocol [using the padded when it tries to decode the last chunk, which results in the '=' being converted into binary data. The DECODE_TABLE defaults to -1 (0xFF), which is how it gets into the final output.",non_debt,-
thrift,3276,comment_0,Did you check against trunk?,non_debt,-
thrift,3276,comment_1,"I haven't, but neither TBase64Utils.java nor TJSONProtocol.java have changed in any relevant way since 0.9.1. I suspect this was just never tested against a client that sends padded base64 strings.",test_debt,lack_of_tests
thrift,3276,comment_2,"Too bad. Do you by accident :-) already have a patch ready? We accept pull requests and patch files, [see here for on how it works.",non_debt,-
thrift,3280,summary,Initialize retry variables on construction,design_debt,non-optimal_design
thrift,3280,description,"Currently retry variables are only initialized after a connection has been successfully established. When the initial connection fails the retry logic is broken since the state has not been properly initialized. To solve this, we need to initialize the retry state before the initial connect() request.",design_debt,non-optimal_design
thrift,3280,comment_0,Thanks duvduv!,non_debt,-
thrift,3283,summary,c_glib: Tutorial server always exits with warning,code_debt,low_quality_code
thrift,3283,description,"When terminating the C (GLib) tutorial server with Ctrl-C, this message is output to the console: The server should instead exit quietly without reporting an issue.",code_debt,low_quality_code
thrift,3283,comment_2,Committed.,non_debt,-
thrift,3286,summary,Apache Ant is a necessary dependency,non_debt,-
thrift,3286,description,Apache Ant is a necessary dependency for building on Ubuntu or Debian.,non_debt,-
thrift,3286,comment_3,committed,non_debt,-
thrift,3297,summary,c_glib: an abstract base class is not generated,non_debt,-
thrift,3297,description,"The tutorial says: However, when I run I get: There is no definition in other files either: What am I missing?",documentation_debt,low_quality_documentation
thrift,3297,comment_0,"Support for servers in C (GLib) was [added in Thrift I recommend the latest version of Thrift yourself from [the source in which includes a number of recent, significant improvements to the C implementation.",non_debt,-
thrift,3297,comment_1,"Yep, using Thrift 0.9.2 solves the issue.",non_debt,-
thrift,3297,comment_2,Great. Please open another ticket if you run into any other issues.,non_debt,-
thrift,3311,summary,Top level README.md has incorrect formmating,documentation_debt,low_quality_documentation
thrift,3311,description,The readme formatting is incorrect - see the attached screenshot.,documentation_debt,low_quality_documentation
thrift,3311,comment_2,thanks for the patch!,non_debt,-
thrift,3320,summary,TMessages of multiplexed service responses of should include service names,non_debt,-
thrift,3320,description,None,non_debt,-
thrift,3320,comment_0,Could you please be a bit more specific? Thank you.,non_debt,-
thrift,3320,comment_1,"Hi Jens, thanks for your comments. Suppose a scenario, let's say, clients use while servers run The ""TMessage""s encapsulated in requests appear like (name = type = call, seqid = blah_blah_blah). However response ""TMessage""s lack serviceName in their name fields. I propose that both request and response ""TMessage""s shall have the same values except their type fields.",non_debt,-
thrift,3320,comment_2,"Hi , it seems that asynchronous processors are not capable of multiplexed services. Am I correct?",non_debt,-
thrift,3320,comment_3,"Seems like once we uncork two-way communcation where each side can set up services in a multiplexed listener, and we ensure each end of the connection has a proper endpoint, we could clean this up. One would expect both sides to have multiple outstanding requests in flight. That said, I expect that each request would have a unique request ID for that connection and as such we would always be able to deliver the response to the request without knowing which multiplexed service it belongs to. I did this in THRIFT-66, so I'm wondering if we really need to include the service name in the response, since the request ID should be unique for that session and that's enough to reliably deliver the reply.",non_debt,-
thrift,3320,comment_4,"Hi , Thanks for your comments. At present, thrift doesn't not offer users a pure and complete asynchronous interface suite. Developers cannot write PDUs to a transport concurrently. Assuming that thrift has thread-safe transports and a set of asynchronous Interfaces in future releases, how to deserialize *_results from transports if are leveraged? In my proposal, we can get service names after ""TMessage""s are parsed. With service names and procedure names, specific instances of *_results can be selected to read data from transport. If we just store the IDs of requests, a mapping between id and *_result shall be maintained. And this is not elegant.",design_debt,non-optimal_design
thrift,3320,comment_5,The mechanism clients should use to uniquely identify requests on a connection is the sequence ID.,non_debt,-
thrift,3324,summary,Update Go Docs for pulling all packages,non_debt,-
thrift,3324,description,None,non_debt,-
thrift,3349,summary,Python server does not handle processor exception,non_debt,-
thrift,3349,description,It's same as THRIFT-3335 (of ruby) for python,non_debt,-
thrift,3349,comment_0,The patch is uploaded as part of pull request because the failures list file conflicts.,non_debt,-
thrift,3349,comment_1,So this issue is only the 4th commit?,non_debt,-
thrift,3349,comment_2,"As, I see. That's essentially a set of patches.",non_debt,-
thrift,3349,comment_3,"LGTM, committed.",non_debt,-
thrift,3354,summary,Fix word-extraction substr bug in initialism code,non_debt,-
thrift,3354,comment_1,Committed.,non_debt,-
thrift,3364,summary,Fix ruby binary field encoding in TJSONProtocol,non_debt,-
thrift,3364,description,"Ruby JSON protocol uses pack('m') method to encode Base64 string. It seems that it inserts a ""\n"" character every 60 characters. You can refer to these pages for this behavior. This has been making it impossible to send long binary field data to other languages. I fixed this by using alternative encode method that is added in Ruby 1.9 (which should be OK). After the fix, I had to add Ruby namespace to to avoid name collision of ""Base64"" symbols that is used for new encode method and also as DebugProtoTest message name. I also removed extraneous double quote in encoded binary fields that resulted in invalid JSON.",code_debt,low_quality_code
thrift,3364,comment_2,Committed.,non_debt,-
thrift,3365,summary,IDL Union structs nil dereferencing in Go,non_debt,-
thrift,3365,description,Suppose I have a union the Go representation looks like now when I create I get a nil dereference pointer when the code tries to invoke however if I create a empty aggregate instantiation I get the error: Cannot read a TUnion with more than one set value!,non_debt,-
thrift,3365,comment_0,", any chance you could [provide a patch or",non_debt,-
thrift,3365,comment_1,This is no longer valid as it works as expected. See for example code.,non_debt,-
thrift,3365,comment_2,This issue was resolved without a fixVersion but was not closed - closing.,non_debt,-
thrift,3390,summary,TTornado server doesn't handle closed connections properly,non_debt,-
thrift,3390,description,The Tornado Thrift server uses a that uses Tornado's IOStream to handle incoming connections. The main method for handling this is a while loop in `handle_stream` that waits for incoming data. This works fine until the client decides to stop the connection in which case is raised. This exception is expected behaviour and should be caught appropriately in `handle_stream`.,non_debt,-
thrift,3390,comment_2,"LGTM, committed.",non_debt,-
thrift,3391,summary,Wrong bool formatting in test server,code_debt,low_quality_code
thrift,3391,description,The go test server printf()s bools with wrong formatting: while this is expected:,code_debt,low_quality_code
thrift,3391,comment_0,Committed.,non_debt,-
thrift,3406,summary,Cocoa client should not schedule streams on main runloop,non_debt,-
thrift,3406,description,"This happens in TSocketTransport and It is generally not a good idea to schedule stream event processing on main runloop, and can cause deadlocks. Scheduling on current runloop solves the problem.",non_debt,-
thrift,3406,comment_0,"LGTM, committed - thanks!",non_debt,-
thrift,3409,summary,NodeJS binary field issues,non_debt,-
thrift,3409,description,There's at least 3 problems in NodeJS binary field. # API are inconsistent across binary/comact/JSON protocols # compact/JSON wire format is imcompatible with other languages (JSON : THRIFT-3200) # size of compact wire format is 2x of the original binary I propose following changes to fix this # Change JSON protocol return value from string to Buffer (same as binary/compact) # Change JSON protocol wire format to Base64 (same as other languages) # Change compact protocol wire format to plain binary (same as other languages),code_debt,low_quality_code
thrift,3409,comment_1,great stuff! +1,non_debt,-
thrift,3409,comment_3,committed,non_debt,-
thrift,3413,summary,Thrift code generation bug in Go when extending service,non_debt,-
thrift,3413,description,"Given the following 2 thrift simple thrift files This is a very simple service extending another service. When using thrift (0.9.3, 1.0.0-dev both tested), there is an generation problem that requires fixing by hand. Running this for generation if I then change directories and run a go build, I get the following error. This is easily reproducible with these steps.",non_debt,-
thrift,3413,comment_0,"The attached files and are enriched test cases, as there are some more issues.",test_debt,low_coverage
thrift,3413,comment_1,Committed.,non_debt,-
thrift,3413,comment_3,Not fixed. There are still problems which need to be addressed properly.,non_debt,-
thrift,3413,comment_4,Fixed & committed.,non_debt,-
thrift,3415,summary,include unistd.h conditionally,non_debt,-
thrift,3415,description,"THeaderTransport.h includes unistd.h unconidionaly. This breaks on non posix systems. The solution is to include it only when the corresponding define is set, like it is done in the rest of the codebase.",non_debt,-
thrift,3415,comment_0,"do you think this include is needed at all? I'm building on Fedora, and Thrift lib builds without issue.",non_debt,-
thrift,3415,comment_1,I'll do some scans with clang's iwyu and clean a little bit around in THeader* related includes,code_debt,low_quality_code
thrift,3415,comment_2,Fixed (unistd.h was removed completely). Thanks for catching that :),non_debt,-
thrift,3416,summary,"Retire old ""xxx_namespace"" declarations from the IDL",non_debt,-
thrift,3416,description,"While poking into the IDL syntax, I found that hidden gem: While thinking whether I should do sth about it, I came to the conclusion that the time ist right to ""_Get rid of this_"" since today probably really ""_everyone is using the new hotness_"", as the comments say. Opinions? If there are no objections, I'll provide a patch to convert the warnings into an error and remove the rest of it.",code_debt,low_quality_code
thrift,3416,comment_0,1,non_debt,-
thrift,3416,comment_3,1,non_debt,-
thrift,3416,comment_5,Committed.,non_debt,-
thrift,3419,summary,thrift-maven-plugin property error,non_debt,-
thrift,3419,description,execute 'mvn compile' occure an error: The parameters for goal are missing or invalid,non_debt,-
thrift,3419,comment_0,patch,non_debt,-
thrift,3419,comment_1,I've testing this fix locally and it seems to work for me... and the Thrift maven plugin seems utterly broken without it. Wondering if a committer could look at this patch before the next release so that a working version of the Maven plugin can make it into the repositories? Thanks,non_debt,-
thrift,3419,comment_2,I also tested it and I am using it.,non_debt,-
thrift,3419,comment_3,"all, thanks for the patch and testing it!",non_debt,-
thrift,3419,comment_5,"The release cycle for thrift seems to be quite long. Given that this bug renders the plugin unusable, it may make sense to release this soon rather that waiting until the next thrift release. Its easy enough to build one's own released version of the plugin, but having to do so adds an extra point of friction to getting started with a tool that's already short on documentation.",documentation_debt,low_quality_documentation
thrift,3419,comment_6,is absolutely right. Has it been released yet? I only see 0.9.3 in maven central. The only way I was able to make this work was to build & install the latest myself. Is there a plan to release the next version? Or at least a fix?,non_debt,-
thrift,3419,comment_7,Just got bitten by this. Does anyone have a workaround for this other than building your own copy from source and loading it into a local repository? Any movement on releasing the fix for this?,non_debt,-
thrift,3423,summary,First call to fails to dispatch correct function,non_debt,-
thrift,3423,description,always return false if the module is not loaded yet. The patch replaces it with so that the module gets loaded if it is not loaded yet.,non_debt,-
thrift,3423,comment_1,1,non_debt,-
thrift,3438,summary,Enable py:new_style by default,non_debt,-
thrift,3438,description,"As we added Python 3 support, it seems inappropriate to generate old style class by default.",non_debt,-
thrift,3438,comment_0,1,non_debt,-
thrift,3438,comment_4,committed,non_debt,-
thrift,3440,summary,Python make check takes too much time,code_debt,slow_algorithm
thrift,3440,description,"It runs every combination of I want to reduce this keeping the same code coverage, so that we can run it more often.",test_debt,expensive_tests
thrift,3441,summary,Stabilize Travis-CI builds,non_debt,-
thrift,3441,description,None,non_debt,-
thrift,3444,summary,Large 64 bit Integer does not preserve value through Node.js JSONProtocol,non_debt,-
thrift,3444,description,Numbers larger than 2^53 cannot be reliably sent.,non_debt,-
thrift,3444,comment_2,committed,non_debt,-
thrift,3445,summary,Throwable messages are hidden from JVM stack trace output,non_debt,-
thrift,3445,description,"As a user, I expect messages given when constructing Throwables to be accessible via so that they are logged in stack trace output for debugging.",non_debt,-
thrift,3445,comment_3,"I am resolving this issue as invalid. It appears this is a special case for Java that appears to be inconsistent with the goals of Thrift, and instead what would be better is to use camel case for Java code, and ensure each exception structure has a message field.",non_debt,-
thrift,3447,summary,Cannot shutdown TThreadPoolServer when clients are still connected,non_debt,-
thrift,3447,description,"When calling Stop() on the TThreadPoolServer, Serve() is still blocking, till the last client disconnected. I do not know if this is intentional, but how can I stop the server without depending on the clients. Killing the Thread that runs Serve() is not really what i want. There was a similar Bug in the C++ Library:",non_debt,-
thrift,3447,comment_0,"I've just run a few preliminary tests to see if I could replicate the behaviour you've described but I'm seeing a near-instant exit when stop() is called on TThreadPoolServer. From at least v0.9.1 and probably a lot longer ago, the Stop() method sets the volatile stop boolean which breaks out of the while(!stop) loops in all client threads as well as in the server's own Serve() loop. As a result, all test client connections drop out with a transport exception of ""Unable to read data from the transport connection: An existing connection was forcibly closed by the remote host."" when they next try to read or write. Do you have an example, or more detail about the problem you're hitting please?",non_debt,-
thrift,3447,comment_1,"I'm running the service in a unity project, to control the application from another one written in python. Here is how I start the service: After the server is running, i connect to it from the python application. When I close my server application, I call server.Stop() to shut down the service. But the application hangs as long as the client is connected. I guess the serverThread ist still alive.",non_debt,-
thrift,3447,comment_2,"I've looked a bit more into this and written a very stripped down test client and server using your excerpt as a starting point on the server. My client opens the transport but doesn't actually make any RPC calls to the Server. Looking at Parallel Stacks after stopping the server thread I can see that the client threads will not notice the stop request until either the underlying System.Net.Socket (In this particular case) receives at least one byte, or times out (which I think is probably a really long time in TCP). Just for reference - The server's Serve() thread does complete, so it should definitely not accept any new client connections. However, the client threads will stay blocked until the above scenario. Unfortunately as Thrift is targeted at .NET 3.5 the use of CancellationToken isn't available until .NET 4 which is a pity as that's quite a nice way to handle such things... It'll have to be resolved another way. I'll have a look at this further tomorrow to see if I can suggest an elegant solution. I'm most concerned about is whether there's a neat way to do this so that it doesn't cause issues with Transports which are not based on Sockets. Given that there are at least some similarities with the C++ case (I've not read it in full detail), it may be prudent to implement a similar solution.",design_debt,non-optimal_design
thrift,3447,comment_3,"This boils down to the use of Blocking Sockets in Thrift. It's the same problem described by many other people outside the thrift world, that you can't interrupt a socket whilst it's blocked. The unanimous answer for C# in general is to use non-blocking sockets. A work-around to the behaviour might be to get clients to regularly call a void function (such as defining a poll() function in the .thrift service definition). However this is wasteful of bandwidth and I only mention is as a possible way to achieve the desired behaviour prior to the existence of a code patch. I think this will require some significant coding effort. If I can find some time I'll look into adapting Thrift's C# lib to use async sockets - I'd welcome a 2nd opinion on this.",design_debt,non-optimal_design
thrift,3483,summary,Incorrect empty binary handling introduced by THRIFT-3359,non_debt,-
thrift,3483,description,C++ JSON protocol and Node.js binary and compact protocol cannot read empty binary. Sorry about that. Fortunately no release is made after these regressions.,non_debt,-
thrift,3483,comment_2,committed,non_debt,-
thrift,3495,summary,Minor enhancements and fixes for cross test,design_debt,non-optimal_design
thrift,3495,description,"h4. fixes * fix python mapmap test * fix problematic regex compilation in connection retry logic in cross test runner (was compiling already-compiled regex object) h4. enhancement * add some C++ edage-case tests (unicode, double values, empty collections) * flush C++ client stdout so that we have better diagnostics when it crashed/hanged * add go binary test * add ruby unicode string test",code_debt,low_quality_code
thrift,3512,summary,c_glib: Build fails due to missing features.h,non_debt,-
thrift,3512,description,"Building the C (GLib) library test suite on Haiku fails with an error about a missing {{features.h}}: This header appears to be part of GNU libc and is not defined by POSIX, which explains why it would not be available on Haiku. The same applies to the BSDs, for which I notice a preprocessor directive was added that explicitly checks for these platforms. A better solution is probably to omit including the header whenever GNU libc is not being used, rather than listing every platform on which it is not available.",non_debt,-
thrift,3512,comment_2,committed,non_debt,-
thrift,3535,summary,Dart generator argument to produce a file structure usable in parent library,non_debt,-
thrift,3535,description,The default Dart compiler behavior is to generate a new library for each thrift file. Add an argument to generate files and imports that are usable in an existing library. This produces a file structure like:,design_debt,non-optimal_design
thrift,3535,comment_0,Code Review:,non_debt,-
thrift,3535,comment_3,Committed.,non_debt,-
thrift,3553,summary,Go Map value type is dependent on order of entity definitions,non_debt,-
thrift,3553,description,Deserialization code of map type properties will generate a compilation error if the type definition of the value appears after the definition of the map where it is used. Compilation Error In the example moving the definition of 'Test' to appear before 'TestResponse' avoids the compilation error.,non_debt,-
thrift,3553,comment_0,Seems related to :,non_debt,-
thrift,3553,comment_1,"Hi , could you please check whether THRIFT-3705 solves this issue as well, and in case it does, resolve this ticket as a duplicate?",non_debt,-
thrift,3553,comment_2,This was indeed fixed by THRIFT-3705. I have a runnable sample,non_debt,-
thrift,3554,summary,"Constant decls may lead to ""Error: internal error: already active for different struct""",non_debt,-
thrift,3554,description,Some constants declarations lead to an {{Error: internal error: already active for different struct}} when generating C# sources.,non_debt,-
thrift,3554,comment_0,Testcase: (under /lib/go),non_debt,-
thrift,3554,comment_1,Committed.,non_debt,-
thrift,3559,summary,Fix awkward extra semi-colons with Cocoa container literals,code_debt,low_quality_code
thrift,3559,description,Changes made in THRIFT-3545 introduce and awkward semi-colons and extra new lines.,code_debt,low_quality_code
thrift,3559,comment_3,"Committed, thanks!",non_debt,-
thrift,3571,summary,Make feature test result browsable,non_debt,-
thrift,3571,description,The console output claims that it can be viewed by browsers but it couldn't in fact.,non_debt,-
thrift,3571,comment_3,committed,non_debt,-
thrift,3572,summary,"""Unable to determine the behavior of a signed right shift""",non_debt,-
thrift,3572,description,THRIFT-1313 inadvertently introduced a build error in some configurations due to preprocessor macros describing the signed right shift behavior not being defined. This can be fixed by including config.h before using those macros.,non_debt,-
thrift,3572,comment_0,"I have a fix, though I think there must be a simpler one. In any case, it does amend the build; I'll make a pull request after testing with THRIFT-3573 to make sure nothing new pops up.",code_debt,complex_code
thrift,3572,comment_3,"Great, we're back on track now :)",non_debt,-
thrift,3592,summary,Add basic test client,non_debt,-
thrift,3592,description,"Unlike test server, client can be partially implemented. The patch just converts the existing manual test client to cross test client by adding argument support.",requirement_debt,requirement_partially_implemented
thrift,3592,comment_2,"reopening to fix ""sed -i ..."" command that does not work on BSD sed.",non_debt,-
thrift,3592,comment_6,committed,non_debt,-
thrift,3596,summary,Better conformance to PEP8,code_debt,low_quality_code
thrift,3596,description,"py coding_standards.md states it follows PEP8 but currently it does not do so at all. So a typical experience of a first-time (potential) py contributor would be to see red warnings all over the display and then has to either adjust their editor's settings or stop there. On the other hand, a huge downside of global reformat is git-blame experience but will be mostly mitigated by git blame -w in this case.",code_debt,low_quality_code
thrift,3598,summary,TBufferedTransport doesn't instantiate client connection,non_debt,-
thrift,3598,description,The TBufferedTransport doesn't seem to correctly instantiated. The doesn't open connection open to the server.,non_debt,-
thrift,3598,comment_3,"Committed, thanks !",non_debt,-
thrift,3605,summary,Have the compiler complain about invalid arguments and options,non_debt,-
thrift,3605,description,"As kind of a follow-up to THRIFT-3603, the goal is to have the compiler validate not only the arguments , but also any argument options passed to it. If an unknown option is found, a short error message should be printed similar to what we already have with invalid args:",non_debt,-
thrift,3605,comment_0,using a real option parser as suggested in THRIFT-3013 should solve this,non_debt,-
thrift,3605,comment_2,"A few comments: - A few generators did not document all switches, some did not dcoument any switches at all. I fixed what I found. - The Python generator stood out form the crowd by being the least maintainable one. I tried hard to no break anything and to mimic the existing behaviour to the best of my knowledge, but it was a hard task. If, despite of my efforts, I still managed to overlooked some obscure edge case combination, please bear with me.",code_debt,complex_code
thrift,3605,comment_6,Committed.,non_debt,-
thrift,3617,summary,CMake does not build gv/xml generators,non_debt,-
thrift,3617,description,None,non_debt,-
thrift,3617,comment_1,"Does not have much to do with the ticket itself, but is there any specific reason why we should not sort that list alphabetically? Makes it much easier to compare things (like forgotten targets). $0,02 JensG",code_debt,low_quality_code
thrift,3617,comment_3,"committed, thanks !",non_debt,-
thrift,3619,summary,Using Thrift 0.9.3 with googletest on Linux gcc 4.9 / C++11,non_debt,-
thrift,3619,description,In PARQUET-470 we've been tracking some issues we've run into with Thrift 0.9.2 and 0.9.3. There is a conflict concerning {{std::tuple}} and so far have been unable to determine a workaround. Can you please advise and confirm whether this should be considered a bug in Thrift? Thank you,non_debt,-
thrift,3619,comment_0,"the suggested solution by  in PARQUET-470, #define 0, it does not work ? I'm using Thrift C++ 0.9.3+ + gtest at work with custom gyp build config and it builds fine. I think I've added similar compiler definition to resolve some build issue in the past, although not sure if it was for Thrift.",non_debt,-
thrift,3619,comment_1,"Ah, I misread Mark's comment, apologies. Let me verify in the Parquet build environment and I will report back to confirm (and add this to the developer documentation).",documentation_debt,low_quality_documentation
thrift,3619,comment_2,Confirming that the workaround works for me. Thank you,non_debt,-
thrift,3619,comment_3,"This workaround will break gtest on OSX with clang libc++. Using C\+\+11 should have already implied but manually setting this macro to {{0}} will just make gtest skip checking C\+\+11 and include {{tr1/tuple}} rather than {{tuple}}, which seems like a bug in gtest, as I see it. See {{tr1/tuple}} works for libstdc++ even on C\+\+11, but not for libc++. I think thrift should detect C\+\+11 and switch to the non-tr1-prefixed headers when available, just like what gtest does, and not depend on the odd behavior of",code_debt,low_quality_code
thrift,3619,comment_4," yes, see what I had to do for Parquet: It would be nice if Thrift would not pass on this burden to thirdparty users, but in the meantime we are stuck with the released 0.9.2 and 0.9.3 versions where this workaround is necessary.",design_debt,non-optimal_design
thrift,3619,comment_5,"For sake of completeness, here is how I am building OS X in parquet-cpp's thirdparty:",non_debt,-
thrift,3619,comment_6,I'm using this in cmake: {} just in case the user forces clang++ to use -stdlib=libstdc++,non_debt,-
thrift,3634,summary,Fix Python TSocket resource leak on connection failure,code_debt,low_quality_code
thrift,3634,description,Same as THRIFT-3615. It turned out that the problematic part of TSSLSocket originated from TSocket. I took this opportunity to remove the duplicate code.,code_debt,duplicated_code
thrift,3637,summary,Implement compact protocol for dart,non_debt,-
thrift,3637,description,None,non_debt,-
thrift,3642,summary,Speed up cross test runner,test_debt,expensive_tests
thrift,3642,description,"As we have like 1500 tests and reaching 50min limits of Travis-CI, a few tweaks would turn out quite nice.",non_debt,-
thrift,3665,summary,Add D libevent and OpenSSL to docker images,non_debt,-
thrift,3665,description,These are the last pieces that are missing in our Debian and Ubuntu images.,non_debt,-
thrift,3665,comment_4,committed,non_debt,-
thrift,3667,summary,Add TLS SNI support to clients,non_debt,-
thrift,3667,description,"Most services are deployed behind a TLS proxy/LB. It's nice to have SNI support in TLS clients, this is a trivial change in the C++ OpenSSL implementations.",non_debt,-
thrift,3667,comment_2,committed,non_debt,-
thrift,3668,summary,range check error in compact protocol,non_debt,-
thrift,3668,description,"Missing explicit cast in leads to 2range check error"" if these checks are enabled.",non_debt,-
thrift,3668,comment_0,Committed.,non_debt,-
thrift,3681,summary,Fix Dart tutorial build,non_debt,-
thrift,3681,description,It was fragile against parallel make because it was erroneously invoking pub twice in a same directory.,code_debt,low_quality_code
thrift,3681,comment_4,committed,non_debt,-
thrift,3683,summary,BadYieldError in thrift py:tornado server,non_debt,-
thrift,3683,description,"Happens with a tornado server and a regular python client. The call completes successfully on the client side but throws this exception on the server side Traceback (most recent call last): File line 177, in handle_stream yield oprot) File line 1008, in run value = future.result() File line 232, in result File line 1090, in handle_yield self.future = File line 210, in wrapper return **kw) File line 1222, in convert_yielded raise unknown object %r"" % (yielded,))",non_debt,-
thrift,3683,comment_0,What transport do you use on the client side ? Tornado transport is framed so you need to use TFramedTransport.,non_debt,-
thrift,3688,summary,Fix socket bind failure detection of cross test,non_debt,-
thrift,3688,description,"Failures from server socket collision should be worked around by grepping ""ADDRINUSE"" et al. in server app log. It has been broken by THRIFT-3642 with erroneous control flow.",non_debt,-
thrift,3688,comment_3,committed,non_debt,-
thrift,3689,summary,start failed when server is not available,non_debt,-
thrift,3689,description,None,non_debt,-
thrift,3689,comment_1,"Patch available as pull request,",non_debt,-
thrift,3689,comment_4,committed,non_debt,-
thrift,3702,summary,Fix cross tests for Dart compact protocol (3 failing),non_debt,-
thrift,3702,description,These tests are failing - interesting that it's just these combinations. Other combinations of dart + compact are passing for the same languages and transports.,non_debt,-
thrift,3704,summary,"""TConnectedClient died: Could not refill buffer"" message shown when using HTTP Server",non_debt,-
thrift,3704,description,"When using {{THttpTransport}} after every request the server prints the following message: {{TConnectedClient died: Could not refill buffer}} However the operation seems to be completed successfully. It doesn't matter what protocol is used (JSON, Binary). The issue can be observed independent of the client used (C++, JavaScript in Browser). The error is thrown in the function in the case where read from the transport returns 0. I'm not sure if throwing an exception there is really necessary?",non_debt,-
thrift,3704,comment_0,This patch modifies the Tutorial server in a way that allows reproducing the issue. it's not a fix!!,non_debt,-
thrift,3704,comment_1,I came up with the solution to treat the case where the buffer could not be refilled the same as when there is an EOF reached. I'm not sure it is correct to do this but I couldn't find anything that's not working. Maybe somebody with a deeper understanding can give some feedback.,non_debt,-
thrift,3704,comment_2,"Yes, throwing is what we need here. It is handled by . and we do this on other transports as well.",non_debt,-
thrift,3704,comment_3,Thanks Pascal!,non_debt,-
thrift,3708,summary,NameError: global name 'TProtocol' is not defined,non_debt,-
thrift,3708,description,In the generated ttypes.py file a is raised from the validate method but TProtocol has not been imported. This results in a NameError when the validate method is invoked.,non_debt,-
thrift,3708,comment_0,thanks for catching this. I'll upload a fix soon.,non_debt,-
thrift,3708,comment_4,committed,non_debt,-
thrift,3710,summary,Dart generator does not camel case Constants class names,non_debt,-
thrift,3710,description,PROBLEM For a library like my_model the Dart generator produces My_modelConstants FIX Produce a name like MyModelConstants,non_debt,-
thrift,3710,comment_1,PR:,non_debt,-
thrift,3722,summary,Fix cert path in C++ cross tests for non-Linux platform,non_debt,-
thrift,3722,description,None,non_debt,-
thrift,3731,summary,Perl multiplex test is flaky,test_debt,flaky_test
thrift,3733,summary,Socket timeout improvements,non_debt,-
thrift,3733,description,The socket timeout handling has room for improvements,design_debt,non-optimal_design
thrift,3733,comment_0,Committed.,non_debt,-
thrift,3744,summary,The precision should be 17 (16 bits need after dot) after dot for double type.,code_debt,low_quality_code
thrift,3744,description,"The precision is lost when converting double to string. E.g: double PI = 3.1415926535897931; string value = format(""%.16g"", PI); The value will be '3.141592653589793' and last 1 is lost after format operation. But expected value should be Solution: string value = format(""%.17g"", PI);",code_debt,low_quality_code
thrift,3744,comment_5,"Committed, appreciate your constant contribution and problem solving skill :)",non_debt,-
thrift,3744,comment_6,fixed.,non_debt,-
thrift,3747,summary,Duplicate node.js build on Travis-CI,build_debt,build_others
thrift,3747,description,"As npm download is unstable, node.js build was supposed to be isolated into a separate job on Travis (with haskell which is unstable due to cabal download). In reality, node.js was built in ""Java Lua ..."" job too, so we've been having double chance of download failures.",build_debt,build_others
thrift,3751,summary,Compiler allows field ids that are too large for generated code,non_debt,-
thrift,3751,description,"Shorts are used to represent field ids in generated code for some languages (e.g. Java), however using a field id greater than max short does not produce an error and instead produced incorrect generated code. Compiling the attached succeeds and produces this Java: Casting numbers greater than 32767 to short causes a compiler error: There are two corollary bugs: - Compiler converts field ids that are larger than max *int* (i.e. - Compiler does error when a field id is too large for its internal representation (i.e.",non_debt,-
thrift,3751,comment_0,"Several protocols physically limit field ids to 16 bit, so this is a more general problem.",non_debt,-
thrift,3751,comment_4,Committed.,non_debt,-
thrift,3755,summary,hits assert in isprint on Windows with debug CRT,non_debt,-
thrift,3755,description,Passing characters <0 to std::isprint causes asserts.,non_debt,-
thrift,3755,comment_2,"Committed, thanks",non_debt,-
thrift,3760,summary,Fix install paths etc of debian packages for py and perl,non_debt,-
thrift,3760,description,"* Perl: /usr/usr/local -* Python: /usr/local - perl fix for ""local"" part is frankly bad but at least it works. The patch also removes .pyo files from python package that should be generated by install process instead. This resolves most of lintian warnings.",design_debt,non-optimal_design
thrift,3832,summary,"Thrift version 0.9.3 example on Windows, Visual Studio, linking errors during compiling",non_debt,-
thrift,3832,description,"I have working projects of Thrift 0.9.3 Server+Client in C# and Node.JS; and also I have working Calculator example on C++ with 0.9.2 thrift (i used this guide to create it ) But when i tried to create Calculator example on C++ with 0.9.3 thrift, i get a lot of linking errors, for example in combination of Visual Studio 2015 + Boost 1.59.0 (x64), libevent 2.0.22, OpenSSL 1.0.2h (x64), thrift 0.9.3 (x64), i have following errors:   : : 0,  : 1, : 0 ========== P.S.: by the way i noticed, that in 0.9.3 version of thrift there is no source file Thrift.cpp. Tell me please, is there anything what i'm doing wrong?",non_debt,-
thrift,3832,comment_0,List of errors in a file,non_debt,-
thrift,3832,comment_1,"We have the same problem, and I need to get this working, so when I figure out how to fix it, I'll post it. If nothing else, it shows that there are about 2 people who use thrift on windows, which is probably why almost nobody cares that it doesn't build.",non_debt,-
thrift,3832,comment_2,"so it turns out as one of my coworkers worked out, they appear to just not have updated the project file with the new source files and as you saw, they removed thrift.cpp and obviously never got around to updating the windows project file or they would have seen it fail trying to compile a file that's gone. So you just have to add the missing cpp files to the project file then it will build again.",non_debt,-
thrift,3832,comment_3,"Agree, it looks like some of the changes I made a while back to the C++ server code were not added to the visual studio projects. What I would recommend is using cmake to generate a project for you. I use an out-of-tree build directory and then: c:\build\thrift The generator can be: ""NMake Makefiles"" or ""Visual Studio 12 2013 Win64"" or ""Visual Studio 14 2015 Win64"" From there you will end up with a .sln file that you can open and build everything, or a nmake file. Either way this is a better way to build the library. If you need a shared library build, however, the cmake support isn't in the system for this, but you can use the generated projects as a guide. I have used cmake-3.6.0 with this and it works well.",non_debt,-
thrift,3832,comment_4,Note I haven't been able to build on windows with libevent so far for nonblocking server support... it may be possible but I don't know the secret sauce.,non_debt,-
thrift,3832,comment_5,Use the instructions in THRIFT-3973 to have cmake generate a solution for you and build it.,non_debt,-
thrift,3832,comment_7,"This is resolved, until such time the static project is removed in THRIFT-3973.",non_debt,-
thrift,3839,summary,Performance issue with big message deserialization using php extension,code_debt,slow_algorithm
thrift,3839,description,"I have found performance issue when tried to deserialize big thrift binary message with enabled thrift_protocol php extension. Messsage size was 10 mb and it took about 30 seconds to deserialize it. When i have done debug of php extension and php library i have found that issue is because small read buffer is used in TBufferedTransport and i cannot change it from method. So i have added parameter $buffer_size to function from php extension. And also this parameter i have added to method from php library. And i extended class by method putBack, so this class will be used for desearilization without TBufferedTransport warapper. After these changes it takes less than a second to deserizlize message with 10 mb size id read buffer 512 kb. Here is the pull request",code_debt,slow_algorithm
thrift,3839,comment_0,"committed, thanks. FWIW, serialization might be also affected.",non_debt,-
thrift,3841,summary,dart compact protocol incorrectly doubles,non_debt,-
thrift,3841,description,"Dart presumes network (big endian) byte order for serialization of doubles, it appears other languages, such as go and java, use the little endian byte order for doubles.",non_debt,-
thrift,3841,comment_0,"compact protocol double is indeed little endian, unlike binary protocol. I've not looked into the dart implementation yet, though.",non_debt,-
thrift,3841,comment_1,fix:,non_debt,-
thrift,3841,comment_2,"Fixed in NOTE: I mistakenly used the Jira number in the commit message instead of the pull request number, so manually closing the ticket.",non_debt,-
thrift,3844,summary,thrift_protocol cannot compile in 7.0.7,non_debt,-
thrift,3844,description,"When compile thrift_protocol extension with php7.0.7 in OS X 10.11, Report error: error: no member named 'min' in namespace 'std'; did you mean 'fmin'? and zend_hash.h:168:30: note: candidate function not viable: no known conversion from 'unsigned long *' to 'zend_ulong *' (aka 'unsigned long long *') for 3rd argument ZEND_API int ZEND_FASTCALL HashTable *ht, zend_string **str_index, zend_ulong *num_index, HashPosition *pos); *And I find someone in github solved this problom, but no one notice it, so I create this issue*",non_debt,-
thrift,3844,comment_1,There is my pr and there is origin pr,non_debt,-
thrift,3844,comment_3,"If there is something bad in this issue, please commit. Thank you",non_debt,-
thrift,3864,summary,Reuse heap allocations of handler results,non_debt,-
thrift,3864,description,"I have a service (Golang) with a handler that returns a large slice. A new one is allocated every time, resulting in large heap allocations per client call, while it could be reused through a sync.Pool and improve the overall performance through fewer GC calls. (.thrift example) struct Slice { 1: required list<Element} service MyService { Slice GetSlice() throws (...) } I have experimented with modifying the auto-generated code and got this functionality through adding a (.go) pool.Put(*Slice) // adding the newly generated return value to a pool) call in but doing so creates a nasty dependency between the handler and the processor. Modifying the signature of the handler should also work (.go) GetSlice(*Slice) (*Slice, error) but does breaks all compatibility with previous compilers... Has some solution to this problem been explored? If nothing else some optional Release(retval) after oprot.Flush() in would be very helpful",design_debt,non-optimal_design
thrift,3864,comment_0,"If large block allocation or garbage collection is not efficient in golang, that sounds like a language issue. One is not supposed to have to think about heap or stack in go, from what I understand (I am by no means an expert). The proposed solution sounds like it would create a cyclic dependency.",architecture_debt,violation_of_modularity
thrift,3864,comment_1,"Let me put it this way: From what I know the GC is Java is by no means the 100% perfect solution either. So if one knows about an yet unfixed issue in the underlying ecosystem, having at least a workaround at hand sounds not so bad to me. Whether it is Java or Go or anything else. So from my point of view iff we can have a * working solution, * that is more efficent in terms of throughput * and does not introduce other problems then (and only then) we should have a look at it. Now the big question: Is this the case?",code_debt,slow_algorithm
thrift,3864,comment_2,"Java has many different garbage collectors to choose from, likely one would perform better for a particular use case. Perhaps go has the same option that would help here?",non_debt,-
thrift,3864,comment_3,"There is no clean way to implement this without a fairly ugly BC break. Go's garbage collector has also improved dramatically since this issue was opened, so I'm going to close this. If anyone still has concerns or ideas, please reopen.",design_debt,non-optimal_design
thrift,3868,summary,Java struct equals should do identity check before field comparison,design_debt,non-optimal_design
thrift,3868,description,"The identity check is cheap and should be done before comparing fields of a struct. Idiomatic equals methods always include this check especially if the field by field comparison can be expensive. Check to add: if(that == this) return true; 1864 out << indent() << ""public boolean equals("" << tstruct-1865 indent_up(); 1866 out << indent() << ""if (that == null)"" << endl << indent() << "" return false;"" << endl; INSERT IDENTITY CHECK HERE 1867 1868 const vector<t_field*1869 vector<t_field*1870 for (m_iter = members.begin(); m_iter != members.end(); ++m_iter) { 1871 out << endl; 1872",design_debt,non-optimal_design
thrift,3868,comment_3,Committed.,non_debt,-
thrift,3887,summary,factories support for generated Java classes,non_debt,-
thrift,3887,description,proposed PR with an example (on top of 0.9.3),non_debt,-
thrift,3887,comment_3,Author closed indicating they were not going to use the changes.,non_debt,-
thrift,3905,summary,"Dart compiler does not initialize bool, int, and double properties",non_debt,-
thrift,3905,description,"In Dart it is desirable to initialize bool (false), int (0), and double (0.0) required properties (those that are not marked optional), instead of leaving them with the value of null.",design_debt,non-optimal_design
thrift,3907,summary,Skip Docker image build on CI when unchanged,non_debt,-
thrift,3907,description,"Previously we were able to reuse Docker layers from prebuilt images pulled from docker hub. This has been reducing total build time by 3 hours out of 7~8 hours total. After Docker 1.10 or so, it is no longer possible and we tend to easily saturate entire Apache's 30 jobs on Travis-CI. Standard solution as of now is to use docker save/load. This typically requires automated file upload on CI to some external storage. Unfortunately it cannot be done with our current Travis-CI account settings. To workaround this, we can put Dockerfile itself to Docker image and see if it's modified after the prebuild time and skip fresh builds if unchanged.",code_debt,slow_algorithm
thrift,3910,summary,Do not invoke pip as part of build process,non_debt,-
thrift,3910,description,"Currently, Python optional dependencies are put to requriements.txt which Autotool/CMake scripts invoke pip on. The idea was to do the same as our other language builds do: install dependencies (Java, Ruby, Haskell etc). In retrospect, each language has different convention/tooling and it's rather surprising or rude thing to do in Python IMO. Instead, we can moved them to ""extras_require"" in setup.py so that one can install via ""pip install thrift[all]"" if they want to.",non_debt,-
thrift,3941,summary,WinXP version of thrift_poll() relies on undefined behavior by passing a destructed variable to select(),design_debt,non-optimal_design
thrift,3941,description,"thrift_poll() for WINVER <= 0x0502 in shadows the 'time_out' variable, and it ends up passing the destructed copy to select(): timeval time_out; timeval* time_out_ptr = NULL; if (timeout timeval time_out = {timeout / 1000, (timeout % 1000) * 1000}; time_out_ptr = &time_out; } else { // to avoid compiler warnings (void)time_out; (void)timeout; } int sktready = select(1, read_fds_ptr, write_fds_ptr, NULL, time_out_ptr); Stepping through this code in the debugger, it looks like MSVC reserves a large enough stack frame to avoid overwriting the variable when calling select(), which may be why this hasn't been caught yet.",design_debt,non-optimal_design
thrift,3941,comment_0,You know that WinXP is no longer supported (except for POS systems)?,non_debt,-
thrift,3941,comment_1,"I know, but any client code can still set Windows XP to be the minimum platform SDK through TARGET_WIN_XP, which will mean that they get the ""XP implementation"". As a result, you can still get into this situation running on Windows 7/10 unless we remove the XP implementation completely. The fix is also pretty straight-forward though.",non_debt,-
thrift,3941,comment_2,"In fact, Thrift specifically targets Windows XP for broadest compatibility, so by default, all clients get the Win XP implementation with this undefined behavior. This is in the README: ## Windows version compatibility The Thrift library targets Windows XP for broadest compatbility. A notable difference is in the Windows-specific implementation of the socket poll function. To target Vista, Win7 or other versions, comment out the line #define TARGET_WIN_XP. And here is the source:",non_debt,-
thrift,3941,comment_6,"Committed, thank you both.",non_debt,-
thrift,3941,comment_7,"This would be an issue that [Coverity should have picked up, and perhaps it did, but we're not taking action on the results?",non_debt,-
thrift,3944,summary,TSSLSocket has dead code in checkHandshake,code_debt,dead_code
thrift,3944,description,"There is a block of code in checkHandshake that attempts to set read/write memory bios to be nonblocking. This code doesn't do anything: Here's what this code looks like, and the problems: - creates a new memory BIO. Not sure why. - BIO_set_nbio() executes BIO_ctrl(..., BIO_C_SET_NBIO, ...). This errors out and return 0 because mem_ctrl does not have a case for BIO_C_SET_NBIO. See: - SSL_set_bio() sets the SSL* to use the memory BIOs. - SSL_set_fd() creates a socket BIO, sets the FD on it, and uses SSL_set_bio() to replace the memory BIOs. As far as I can tell, this block of code does nothing and will not change functionality. If there's a reason that it's there, it needs to be re-implemented.",code_debt,low_quality_code
thrift,3944,comment_1,"The patch looks correct. The code comment says that it is setting BIOs to non-blocking. Since the socket is set to non-blocking right before this and the BIOs created by SSL_set_fd inherit the socket's non-blocking behavior, I don't think there's something we need to re-implement.",non_debt,-
thrift,3944,comment_2,Patch looks correct. :+1:,non_debt,-
thrift,3948,summary,errno is not the correct method of getting the error in windows,non_debt,-
thrift,3948,description,Based on conversation in THRIFT-1944 this was still ani ssue mid-2015.  said:,non_debt,-
thrift,3948,comment_0,", I went through all C and C++ code and I could not find any incorrect usage of errno on windows so I am marking this as done, cannot reproduce.",non_debt,-
thrift,3957,summary,TConnectedClient does not disconnect from clients when their timeout is reached.,non_debt,-
thrift,3957,description,"When starting the TestServer with the first appended patch: And connecting to it with this shell command: The connection stays up for a long time. Making DOS attacks very easy, because at some point the server runs out of available client connections. The second appended patch fixes the issue in TConnectedClient.",non_debt,-
thrift,3957,comment_0,changes the TestServer to demonstrate the problem fixes the problem,non_debt,-
thrift,3957,comment_2,Patch 0001 uses C++11 which will not work. Did you intend for this to be checked in or is it just to demonstrate the issue?,non_debt,-
thrift,3957,comment_3,Patch 0001 is just for demonstration.,non_debt,-
thrift,3957,comment_4,"Okay, I fixed up the pull request that's outstanding. Sadly, the documentation on TSocket.h isn't very descriptive about setting the timeout and the implications of such. I've seen looping code in TSocket that revolves around TIMED_OUT however my guess is that it existed in the past to attempt to give the socket thread(s) a chance to unblock from a poll, select, or recv call and check to see if the server has been stopped. That's no longer needed since 0.9.3 because of the interruptable socket implementation, so this change works for me. The code that was in TConnectedClient was pulled from the three different server implementations and I believe all of them used to ignore TIMED_OUT so this could cause a behavioral difference in future versions that folks may not expect, however it is ""more correct"".",documentation_debt,low_quality_documentation
thrift,3957,comment_6,1,non_debt,-
thrift,3962,summary,Ant build.xml broken on Windows for Java library,non_debt,-
thrift,3962,description,"The Ant build refers to a {{mkdir}} executable. There is no such thing on Windows, {{mkdir}} is an internal command. Instead use the Ant {{mkdir}} task. Patch attached.",non_debt,-
thrift,3962,comment_0,"Hi , could you please rebase against trunk/master?",non_debt,-
thrift,3962,comment_1,I do not even get that far with master: Could this be because I downloaded {{thrift.exe}} 0.9.3 from the Thrift site. Is it not compatible with the master branch?,non_debt,-
thrift,3962,comment_2,Here you go:,non_debt,-
thrift,3962,comment_3,I was able to build by building the latest compiler from master.,non_debt,-
thrift,3962,comment_5,"Committed, thanks.",non_debt,-
thrift,3962,comment_6,"Fixed, closing, thank you. Gary",non_debt,-
thrift,3981,summary,Enable analyzer strong mode in Dart library,non_debt,-
thrift,3981,description,Enable strong mode in the Dart library to be prepared for the future Dart dev compiler.,non_debt,-
thrift,3981,comment_2,Any reason why this is till open?,non_debt,-
thrift,3981,comment_3,"Sorry, there were failing CI tests at the time (unrelated to my changes as far as I can tell), so I didn't merge it. I'll try again.",non_debt,-
thrift,3981,comment_4,"I'm still seeing failing CI tests that are not Dart related. I think we were waiting to merge until tests were clean, but I'm probably out of the loop with the current state of fixing CI tests.",non_debt,-
thrift,3983,summary,libthrift is deployed on central with pom packaging instead of jar,non_debt,-
thrift,3983,description,"Hi, libthrift is deployed with a ""pom"" packaging on maven central, see This is very wrong. ""pom"" means ""no jar, just a list of dependencies to be pulled transitively"". As a consequence, some build tools, such as sbt, don't pull the libthrift jar itself. The pom should be be using a ""jar"" packaging, or even no packaging at all as ""jar"" is the default value.",build_debt,build_others
thrift,3983,comment_0,"I see both pom and jar files there, is this still an issue?",non_debt,-
thrift,3983,comment_1,"Hi,  Definitely it's not right at But I don't know why [signAndDeploy for a pom I think just signAndDeploy for jar, src.jar, javadoc, not pom.",non_debt,-
thrift,3983,comment_4,This issue can be closed.,non_debt,-
thrift,3983,comment_5,Resolved by commit,non_debt,-
thrift,4014,summary,align C# meta data in AssemblyInfo.cs,non_debt,-
thrift,4014,description,The meta data in AssemblyInfo.cs are inconsistent and do not reflect the ASF properly in all cases.,code_debt,low_quality_code
thrift,4014,comment_0,"Committed,",non_debt,-
thrift,4015,summary,"Fix wrongly spelled ""Thirft""s",documentation_debt,low_quality_documentation
thrift,4015,description,"Some people prefer to spell Thrift as ""Thirft"", which is not really correct.",documentation_debt,low_quality_documentation
thrift,4015,comment_0,Committed.,non_debt,-
thrift,4023,summary,Skip unexpected field types on read/write,non_debt,-
thrift,4023,description,"Skip field of unexpected fieldType in golang generator, like other languages. That will be safe when the fieldType changed.",non_debt,-
thrift,4023,comment_2,"Committed, thanks. Good catch!",non_debt,-
thrift,4030,summary,CI jobs do not reuse downloaded docker images,code_debt,low_quality_code
thrift,4030,description,"The main motivation for me to introduce this script was to avoid docker build which was failing too often due to occasional apt download failures. But for some unknown reason apt is not failing any longer. Although reusing reduces build time by ~10min for each job, we may well remove the script and always build images from scratch.",design_debt,non-optimal_design
thrift,4030,comment_0,We had some issues with the docker image that got resolved with on January 16. Was this the issue you were seeing? Do you want to keep this open or close it out? Saving 10 minutes on each build is worth continuing to use docker images instead of rebuilding them.,non_debt,-
thrift,4030,comment_1,Do you want to keep this open or close it out?,non_debt,-
thrift,4043,summary,thrift perl debian package is placing files in the wrong place,architecture_debt,violation_of_modularity
thrift,4043,description,"The package is installing things into /usr/lib, when it should be going into /usr/share/perl5. This means ""Thrift.pm"" is being dropped directly into /usr/lib/Thrift.pm right now.",non_debt,-
thrift,4043,comment_0,"Ubuntu may have altered INSTALLDIRS perhaps. You can try setting it to ""site"" (should be the default though).",non_debt,-
thrift,4043,comment_2,"Without the mkdir, perl5 was being moved to /usr/lib directly, rather than being moved INTO /usr/lib. This caused perl to go into /usr/lib/Thrift.pm and /usr/lib/Thrift/... instead. Submitted a PR.",non_debt,-
thrift,4043,comment_4,The fix has been merged; the perl library goes into /usr/share/perl5 using the debian package install.,non_debt,-
thrift,4048,summary,Skip() should throw on unknown data types,non_debt,-
thrift,4048,description,None,non_debt,-
thrift,4048,comment_0,"As support for the older cocoa compiler and library have been removed (see THRIFT-4719), all of the issues in Jira related to that code have also been removed. For legacy cocoa support you can use version 0.12.0 - everyone is expected to move to swift if they want to use the next release of Thrift.",architecture_debt,using_obsolete_technology
thrift,4051,summary,Skip() should throw on unknown data types,non_debt,-
thrift,4051,description,None,non_debt,-
thrift,4054,summary,Skip() should throw on unknown data types,non_debt,-
thrift,4054,description,None,non_debt,-
thrift,4069,summary,"All perl packages should have proper namespace, version syntax, and use proper thrift exceptions",code_debt,low_quality_code
thrift,4069,description,"Currently our perl package module files contain multiple packages. We should break each package out to an individual file (or at least make sure everything is in the Thrift namespace) and properly version it. Package versioning was introduced in Perl 5.10 so: 1. Update the minimum required perl to 5.10. This is based on indicating that perl version object was added to perl in 5.10. 2. For each package use the {{perl MODULE VERSION}} perlmod syntax, where VERSION is {{v0.11.0}}. This is based on 3. Each module not under the Thrift namespace must be moved there TMessageType, TType). This will be a breaking change, but necessary for proper packaging of the library. Currently if you inspect the Perl PAUSE version metadata for Thrift's sub-modules only the 0.9.0 modules from gslin have version identities. For example if you look at Thrift and in the CPAN list of packages at you will see: There are some anomalies, for example packages defined in Thrift.pm come out at the top level namespace like: So technically if you do 'install I would expect you might get thrift. This is wrong and should be fixed. needs to be inside Thrift, not at the top level. Also we should pull in relevant changes from the patch in THRIFT-4059 around improving packaging. Also we should actually use TProtocolException and TTransportException instead of just TException everywhere.",architecture_debt,violation_of_modularity
thrift,4069,comment_0,I sent an email to gslin who originated the perl packages on cpan for the project and asked for any insight he can add here. Target for these changes is in 0.11.0.,non_debt,-
thrift,4069,comment_1,"I am marking this as superceding THRIFT-4059. In THRIFT-4059 it was identified that we have a module that actually requires Perl 5.8. Since we're suggesting moving to Perl 5.10 here, this supercedes that one.",non_debt,-
thrift,4078,summary,TServiceClient.java pollutes System.out with debug message,non_debt,-
thrift,4078,description,"I believe this issue would warrant a 0.10.1 fix. It's quite frustrating. Inside there appears to be a debugging message which was left in. This pollutes the client application's console with ""Received 1"" messages. See Line 84 in the {{receiveBase}} method:",code_debt,dead_code
thrift,4078,comment_0,Closed as duplicate of THRIFT-4062,non_debt,-
thrift,4089,summary,"Several issues with generated Python code: TFrozenDict, _fast_encode",non_debt,-
thrift,4089,description,"When you generate Python sources for a simple thrift file (e.g. Thrift tutorial from will also repro erros), errors are thrown while running the PythonServer and PythonClient. 1. When running the vanilla PythonServer, I get the following error: File line 9, in <module from thrift.Thrift import TType, TMessageType, TFrozenDict, TException, ImportError: cannot import name TFrozenDict I see this error consistently for all generated thrift files. The error disappears on removing TFrozenDict from the generated files (it is not used anywhere in the file). 2. When running the vanilla PythonClient (the server is now running after the above fix), I get the following error: File ""PythonClient.py"", line 51, in main client.ping() File line 73, in ping self.send_ping() File line 79, in send_ping File line 297, in write if oprot._fast_encode is not None and self.thrift_spec is not None: AttributeError: TBinaryProtocol instance has no attribute '_fast_encode' I am not sure why this error is being thrown/what can be done to fix it. These two errors, and mainly the latter one, are rendering the Server/Client un-usable. Is this a known issue? And how do you recommend working around it?",non_debt,-
thrift,4089,comment_0,Sounds like version mismatch between compiler and library: 0.10.0 compiler against older python lib.,non_debt,-
thrift,4089,comment_1,"Are you referring to the Python library? I tested with Python 3.5 as well, same issue.",non_debt,-
thrift,4089,comment_2,"no, Thrift runtime. If you can locate Thrift.py you're importing, it should look like if it is from correct version. TFrozenDict is in this file.",non_debt,-
thrift,4089,comment_3,Got it. Let me check on that.,non_debt,-
thrift,4089,comment_4,"That was indeed the issue, thank you!",non_debt,-
thrift,4089,comment_5,This issue was resolved without a fixVersion but was not closed - closing.,non_debt,-
thrift,4120,summary,pom files are not generated or provided in the build,non_debt,-
thrift,4120,description,"It appears pom files are being manually created and uploaded to the Nexus staging repository when thrift jars are uploaded there. They should be version-controlled and part of the source tarball, or created during the build process. This affects both libthrift.jar and libfb303.jar Including these files in the source tree, or created during the build would make it easier for downstream packagers to reproduce good Maven artifacts.",non_debt,-
thrift,4120,comment_0,With the work to address THRIFT-4259 the Maven publication has been automated to the point of just configuring GPG key parameters and Maven Nexus user credentials and executing `make maven-publish`. This process will now correctly generate a proper `.pom` file to go alongside the distribution JAR files in the Maven staging area for release.,non_debt,-
thrift,4120,comment_1,Resolved by commit,non_debt,-
thrift,4129,summary,C++ TNonblockingServer fd leak when failing to dispatch new connections,code_debt,low_quality_code
thrift,4129,description,"In THRIFT-2789 the error handling for connections where notify fails leaks a file descriptor. This was reported and fixed in a pull request without an Apache Jira entry: When failing to dispatch new connections to other IO threads other than the number 0, we returned these connections for reuse without closing them, so the corresponding fds were leaked forever. We should close these connections instead.",code_debt,low_quality_code
thrift,4129,comment_1,Committed - thanks.,non_debt,-
thrift,4130,summary,Ensure Apache Http connection is released back to pool after use,non_debt,-
thrift,4130,description,"There is a connection leak in the THttpClient when using the Apache HttpClient with the Without calling releaseConnection on the HttpPost object, the connections are never returned to the pool. Under heavy load, this can lead to both failures for subsequent calls to be able to get a connection from the pool and connections being held by the underlying OS, eventually resulting in the inability to grab another client port for outgoing connections. Per the Apache HttpClient ""In order to ensure correct deallocation of system resources the user MUST either fully consume the response content or abort request execution by calling This might have not been an issue when using the 3.x version of the HttpClient, but it's definitely an issue in the 4.x line. See for more details.",design_debt,non-optimal_design
thrift,4130,comment_2,"Short note: The check-in comment for this says the client code is C++, not Java. I'll fix it if I can.",non_debt,-
thrift,4131,summary,Javascript with WebSocket handles oneway methods wrong,non_debt,-
thrift,4131,description,"When using the WebSocket transport all client- To remedy this I changed the compiler/generator to send a null callback to the transport->flush method for oneway methods, and then in the WebSocket transport code, make a null check and only install defined callbacks. This seem to fix it for me. I can send in patches if necessary.",non_debt,-
thrift,4131,comment_0,"Would be great to get the patch, please attach (or if you prefer a pull request is fine too).",non_debt,-
thrift,4131,comment_1,proposed patch,non_debt,-
thrift,4131,comment_3,did you review this patch? It passed all CI tests.,non_debt,-
thrift,4131,comment_4,Committed - thanks.,non_debt,-
thrift,4136,summary,Align is_binary() method with is_string() to simplify those checks,code_debt,complex_code
thrift,4136,description,Similar to {{is_string()}} the {{is_binary()}} method should be virtual and implemented at {{t_type}}. This simplifies the code and reduces possibilities for making technically wrong casts.,code_debt,complex_code
thrift,4136,comment_2,Committed.,non_debt,-
thrift,4164,summary,Core in TSSLSocket cleanupOpenSSL when destroying a mutex used by openssl,non_debt,-
thrift,4164,description,"In a project where thrift is used, i was investigating a core in an assertion in (pthread variety). The mutex in question was one of the locking mutexes that thrift gives to openssl. The core occurred in where the mutexes are destroyed (on the last line). I suspect that we might be changing the locking callbacks too early in the cleanup process; perhaps one of the other cleanup calls that follows it would have released a mutex in some situations? In any case, this needs to be investigated and I am assigning it to myself.",design_debt,non-optimal_design
thrift,4164,comment_1,"In the pull request where the first four builds failed with cores, I was able to reproduce it locally as well. It is very interesting. We were resetting the openssl thread safety/locking callbacks to NULL as part of destruction of the last TSSLSocketFactory instance. In the test, we hold on to this socket factory through the test and then it gets destroyed: Then later on in the test, we close a TSSLSocket that the factory created: At this point where we destroy the TSSLSocket, which calls into SSL_CTX_free, which wants to use one of the locks to be threadsafe, it cannot use the lock because we destroyed it already when the TSSLSocketFactory was destroyed. We need to guarantee the lifetime of the TSSLSocketFactory outlives any TSSLSocket that it makes, since destruction of the last TSSLSocketFactory causes an openssl teardown.",non_debt,-
thrift,4164,comment_2,"PR #1235 exposed the fact that before that PR, we were quietly stopping openssl from using the locking callbacks we were providing. This would lead to unsafe multi-thread behavior during shutdown. I looked at guaranteeing this in core: I noodled through it and the solutions were all a bit ugly, involving the factory to keep a smart pointer of each socket it made, and a callback from TSSLSocket::close() to the factory to take it out of a set of sockets it had made. Then in ~TSSLSocketFactory we would need to guarantee the set of sockets left in the factory were the last instance (nobody else was holding onto them as well) so we could guarantee that when the set of sockets is reset, all the sockets the factory ever made were released.Then the factory could clean up openssl safely. OR, we clearly document the requirement that a factory must outlive any sockets it makes. This is what I'm going to do, so will revise the PR to fix our own TestClient.cpp which was in violation of the requirement that a TSSLSocketFactory must outlive any TSSLSocket it creates when the TSSLSocketFactory is managing openssl initialization and destruction, which was causing the build failures.",requirement_debt,non-functional_requirements_not_fully_satisfied
thrift,4177,summary,Java compiler produces deep copy constructor that could make shallow copy instead,non_debt,-
thrift,4177,description,"Java compiler produces deep copy constructor that makes shallow copies for referenced objects from collections when they are defined after the root object in the thrift file. For example, consider following thrift file: struct Foo { 1: optional list<Bar} struct Bar { 1: optional string value, } new Foo().deepCopy() will produce a copy of Foo with a copy of a list that reference the same Bar object.",non_debt,-
thrift,4177,comment_0,Will provide a patch soon.,non_debt,-
thrift,4177,comment_2,"I just linked a number of issues in the backlog, some already done and some not, to this issue. It looks like there has been a desire to have a deep copy mechanism for a long time. Folks who use Java more should comment on this PR.",non_debt,-
thrift,4177,comment_3,"Hi, I misread the meaning of the title. I assumed you were saying we didn't need deep copy in the title text. It looks like this is a duplicate of THRIFT-3674; or we can make THRIFT-3674 a duplicate of this since this has a PR now. Thoughts?",non_debt,-
thrift,4177,comment_4,"Yes, this issue is definitely a duplicate of THRIFT-3674, my bad. And yes, it might be more reasonable to mark THRIFT-3674 as duplicate, cause of PR.",non_debt,-
thrift,4202,summary,Thrift docker image files fail to build in Travis build: npm errors,non_debt,-
thrift,4202,description,See this build job: Details:,non_debt,-
thrift,4202,comment_0,This is causing all Travis CI builds to fail.,non_debt,-
thrift,4202,comment_1,This issue was resolved without a fixVersion but was not closed - closing.,non_debt,-
thrift,4205,summary,c_glib is not linking against glib + gobject,non_debt,-
thrift,4205,description,The library is not linking against glib and gobject depedencies. It means that it will work if the library is linked against a program that uses that libraries but it will fail in environments like Android. Since the reference to gobject (for example) will not be there and in it will fail to load because missing symbols.,non_debt,-
thrift,4205,comment_7,Merged.,non_debt,-
thrift,4211,summary,Fix GError glib management under Thrift,non_debt,-
thrift,4211,description,It seems that current Gerror management done in thrift is not quite ok and causes the library to fail. This issue tracks and fixes all problems found during testing.,non_debt,-
thrift,4211,comment_0,It seems we were aborting due to wrong understanding of how g_error works. There's room for improvement but for now it seems correct,non_debt,-
thrift,4211,comment_4,Merged.,non_debt,-
thrift,4212,summary,c_glib flush tries to close SSL even if socket is invalid,non_debt,-
thrift,4212,description,"It seems that SSL is trying to get info from socket even when socket is already invalid, making the app to crash.",non_debt,-
thrift,4212,comment_4,Merged - thanks.,non_debt,-
thrift,4224,summary,Compile Thrift generated Java code fails on Windows: The command line is too long.,non_debt,-
thrift,4224,description,"Compile thrift file to Java in IntelliJ failed due to ""The command line is too long"" on Windows 7. Tried on 0.9.1/0.9.3/0.10.0, both failed as same reason. Log: [INFO]  (thrift-sources) @  [ERROR] thrift failed output: [ERROR] thrift failed error: The command line is too long.",non_debt,-
thrift,4224,comment_0,Can you post the command line in question?,non_debt,-
thrift,4224,comment_1,Thanks for taking care! I tried to use the command line as below and the compiling passed: thrift --gen java Looks more like an issue of I am closing this defect. Thank you!,non_debt,-
thrift,4224,comment_2,"I resolved this ""thrift failed error: The command line is too long"" issue by setting a shorter thrift temp file for This configuration changed the folder name from default to ""td"". Obviously, this is a workaround to decrease the project folder path length under windows. <plugin <groupId <artifactId <configuration <thriftExecutable <generator </configuration </plugin>",non_debt,-
thrift,4224,comment_3,THRIFT-1536,non_debt,-
thrift,4230,summary,Thrift server connection may hang forever,non_debt,-
thrift,4230,description,After a lot of tests with HBASE Thrift server we found a problem. If the connection is dropped on the client side (using route or iptables) it may be still opened on the Thrift server side. Such situation will occur in case of unstable connection. After several iterations the Thrift server application will have a lot of opened connections and *will not accept *any new one. The only WA found is to restart the Thrift server. I believe Thrift server should have something like socket timeouts and heartbeats.,design_debt,non-optimal_design
thrift,4230,comment_0,Which language is the thrift server using?,non_debt,-
thrift,4230,comment_1,Which version of thrift were you testing against?,non_debt,-
thrift,4230,comment_2,"HBase is Java, AFAIK.",non_debt,-
thrift,4230,comment_3,thrift-0.9.3 is used. Lib was compiled for python language. It was based on hbase1.thrift file.,non_debt,-
thrift,4230,comment_4,"Does anybody know if this is fixed in a subsequent version of Thrift? I don't see anything in the release notes but could it have been fixed inadvertently? Priority is ""Major"" and it does sound like a major issue, but this record hasn't been updated in a while and I'm having trouble finding other references to it.",non_debt,-
thrift,4230,comment_5,"two questions: (1) Is there any sort of reasonably runnable reproduction? To wit, if it's as simple as what's described, then perhaps there should be a simple way to reproduce it without all the baggage of Hbase? (2) are there any javacores, or any other stuff available? output of lsof? netstat -an? anything else? -- (3) It sounds like the test scenario was one where client connections were dropped using route/iptables, hence, NOT by killing the client? If instead you kill the client, does the problem still occur? That is, is the ""network-level fault"" intrinsic to the fault-scenario? (4) Did you try to enable SO_KEEPALIVE in Java (by whatever means that happens -- haven't programmed in Java in eons, so forget how it works)?",non_debt,-
thrift,4230,comment_6,"Hi , this doesn't ring a bell for me, however there still isn't enough information in the bug report to go on. So far we know it was Thrift 0.9.3 and python, and I'm assuming that means python server. Which server was being used? Nonblocking, Process pool, http? Which transport? Socket, SSLSocket, twisted? Can it be boiled down to a simple example? It looks like it could. Would THRIFT-3634 have addressed this?",non_debt,-
thrift,4230,comment_7,"Thanks and , it turns out we were having a completely unrelated issue, actually it was a security tool properly detecting a vulnerability in our Thrift usage. I was grasping at straws thinking this bug was related to our problem and hence was looking for more information to confirm. It turns out I have no new information to contribute since my bug was not this bug.",non_debt,-
thrift,4231,summary,TJSONProtocol throws unexpected on null strings,code_debt,low_quality_code
thrift,4231,description,"When a required (e.g. string) field is not set, the C# code may throw a non-Thrift nullptr exception, which at this point is a bit unexpected. Happened to me with TJSON but is in fact a problem of required fields not being checked properly in the generated struct.Write() code.",code_debt,low_quality_code
thrift,4231,comment_2,Committed.,non_debt,-
thrift,4234,summary,"Travis build fails cross language tests with ""Unsupported security protocol type""",non_debt,-
thrift,4234,description,The Travis build cross-functional ssl tests for java - csharp fail due to SSL protocol problems on the C-Sharp side. The server fails with the following error: and the client throws an exception in kind: [java] Sample TSimpleJSONProtocol output: [java] {} [java] handshake alert: internal_error,non_debt,-
thrift,4234,comment_0,"I did some research and found a bug in mono that appears to be related: The bug was first fixed in mono 3.4.0, but our Dockerfile uses the default apt-get package which has mono 3.2.8. The problem can be fixed by upgrading mono. The strange part is why this error appears now. Even if I go back to the last successfully built commit -- d1380d52999e -- the problem still presents itself. I assume that version of the build was still running mono 3.2.8 so it is unclear to me why those tests did not fail with the same error.",non_debt,-
thrift,4234,comment_2,Here is the fix in the mono repo:,non_debt,-
thrift,4234,comment_4,"Committed, thanks!",non_debt,-
thrift,4235,summary,NodeJS needs support for domain sockets,non_debt,-
thrift,4235,description,NodeJS needs support for domain sockets. Currently it only take host and port.,non_debt,-
thrift,4235,comment_0,Never mind. Looks like I can directly create Thrift.Connection and pass it with domain socket.,non_debt,-
thrift,4235,comment_1,This issue was resolved without a fixVersion but was not closed - closing.,non_debt,-
thrift,4245,summary,Golang TFramedTransport's writeBuffer increases if writes to transport failed,non_debt,-
thrift,4245,description,"if p.transport.Write fails, p.buf will not be truncated, which leads to thrift client's memory increasing forever. Is it more reasonable to truncate p.buf when write to transport fails? here are my pull request, i'm new in github&jira, if more details are needed, please tell me, thx.",code_debt,low_quality_code
thrift,4245,comment_2,"LGTM, committed.",non_debt,-
thrift,4245,comment_3,This may be helpful:,non_debt,-
thrift,4256,summary,Dependency on very old version of vector library,architecture_debt,using_obsolete_technology
thrift,4256,description,"Currently, Thrift.cabal has an exact dependency of vector==0.10.12.2, but this version is much, much older than what other packages depend on. This makes it necessary to enable ""allow-newer"", which effectively ignores the dependency, and then breaks when a package is uploaded to hackage, and prevents inclusion of thrift in a stack curated package set. If there's no particular reason for it (and I've been successfully compiling thrift with vector==0.12.0.2), could this dependency be set to a range, .e.g. >=0.12.0? I could then enter a request for thrift to be added to stack's curated package sets.",architecture_debt,using_obsolete_technology
thrift,4256,comment_0,"It is because of a change in show introduced in vector 0.11. Admittedly it does not need to be exact 0.10.2, though. Note that our CI uses GHC 7.6 or something old which does not support OverloadedLists. Those who are on newer GHC wouldn't experience the issue. We would need to put version check for GHC to switch vector version constraint in thrift.cabal.",non_debt,-
thrift,4256,comment_1,"I see: OK, that makes sense, looks like e.g. centos 7 is still using ghc 7.6.3. Thanks for the fast response.",non_debt,-
thrift,4256,comment_2,"So, THRIFT-4008 resp. actually set it to ` Any chance we could get the relaxed bounds on the released Hackage version?",non_debt,-
thrift,4264,summary,PHP - Support both shared & static linking of sockets library,non_debt,-
thrift,4264,description,THRIFT-4218 introduced TCP_NODELAY for the PHP client socket by the usage of . This requires PHP to be buildt with --enable-sockets or Test environment should support usage of both ways of compiling PHP.,non_debt,-
thrift,4264,comment_6,"Committed, thanks.",non_debt,-
thrift,4275,summary,"Add support for zope.interface only, apart from twisted support.",non_debt,-
thrift,4275,description,"Since twisted depends on zope.interface, each interface we defined in thrift file will be generated as a class derived from zope.interface if py:twisted option is enabled. This is great because we can treat implementations of these interfaces as components which can be registered in zope.component. However, sometimes we just only want to benefit from Component Object Model programming which is provided by the combination of zope.interface, zope.component and even zope.configuration, but not the deferred mechanism in twisted. So, besides option py:twisted, shall we add an extra option py:zope.interface ?",non_debt,-
thrift,4284,summary,File contains a NBSP:,non_debt,-
thrift,4284,description,"This file contains a non-breaking space ({{\u00a0}}): We found the issue while bundling the npm module with {{browserify}}, resulting into a syntax error in the browser. If you look at line 287 in [the raw there is an {{''}} character between a value and the next key. Interestingly, the [web doesn't show the issue. The [raw file on the GitHub is not garbled.",non_debt,-
thrift,4284,comment_0,"Hi, thanks for catching this. If you are able to provide a pull request, someone will take care of it.",non_debt,-
thrift,4294,summary,Java Configure Fails for Ant >= 1.10,non_debt,-
thrift,4294,description,The configure step enables building the java lib if the ant version is >= 1.7. Unfortunately this check fails for ant 1.10 which is the latest version. The library builds fine with 1.10.,non_debt,-
thrift,4294,comment_0,"Marking duplicate, solved by pending PR.",non_debt,-
thrift,4294,comment_1,Resolved by commit,non_debt,-
thrift,4308,summary,D language docker images need demios for libevent and openssl fixed to re-enable make cross on dlang,non_debt,-
thrift,4308,description,"I had to disable the deimos hooks for libevent and openssl because they were causing build failures in 0.11.0. A result of this change is that dlang is NOT tested in ""make cross"" at all, because the test executables are only generated if openssl support exists, and it does not. It turns out this is some sort of link ordering issue. On further diagnosis, the dmd command outputs this build line when you add -v: If you add -Wl,--start-group before the event library, OR move event, ssl, and crypto to the end of the library list, the link completes.",non_debt,-
thrift,4308,comment_1,"Well, I was wrong; it's a link ordering issue. I could not fix/force the link order coming out of dmd, so I switched to the ld.gold linker for the docker images and that one is smart enough to get it right (also it's way faster than the bfd linker). Once I have Xenial and Artful building with D re-enabled for openssl and libevent, I will submit a PR.",non_debt,-
thrift,4308,comment_2,"Switching to ld.gold exposed other issues (some good, some bad) and I'm not sure I can use it on everything yet.",design_debt,non-optimal_design
thrift,4308,comment_3,"Okay, so it turns out other people have reported issues with DMD's inability to move link flags to the end of the command line. I found that using the gold linker for D builds allowed me to re-enable the openssl and libevent deimos extensions, so that's what the fix will be.",non_debt,-
thrift,4316,summary,TByteBuffer.java will read too much data if a previous read returns fewer bytes than requested,code_debt,low_quality_code
thrift,4316,description,The TByteBuffer read() method is using the wrong length variable inside the processing loop and may read more than it should.,code_debt,low_quality_code
thrift,4320,summary,Use interface instead of strict http.Client type in THttpClientOptions,non_debt,-
thrift,4320,comment_0,"This isn't necessary. In {{http_client.go}} we have a type, which can be used to pass a custom a client. You can also choose to override the exported  wontfix?",non_debt,-
thrift,4320,comment_1,", what do you say?",non_debt,-
thrift,4320,comment_2,"Closing, as the HTTP client is already fully customizable. See my previous comment.",non_debt,-
thrift,4321,summary,Migrate to new project format and enable CI,non_debt,-
thrift,4321,description,"Current library is still using old project structure (project.json) which is slightly outdated and needs to be migrated to new MSBuild format. In addition to that, I'd like to have separate packages build for different .NET Standard versions starting from 1.4 (UWP) and up to 2.0 with full feature set.",architecture_debt,using_obsolete_technology
thrift,4321,comment_0,Raised a PR for this,non_debt,-
thrift,4337,summary,Able to set keyStore and trustStore as InputStream in the,non_debt,-
thrift,4337,description,"There are a lot cases available, when requires to set keyStore and trustStore not only as file location, but as InputStream. It's easy and good to add this feature to the Thrift Java library.",non_debt,-
thrift,4361,summary,"cannot serialize set<map<i32, i64> > via extension",non_debt,-
thrift,4361,description,"method is: bool echoF(1: set<map<i32, i64 but, when arg is [[1=it cause Segmentation fault: 11",non_debt,-
thrift,4361,comment_0,"This is fixed in version 0.11.0, Please close this.",non_debt,-
thrift,4362,summary,Missing size-check can lead to huge memory allocation,design_debt,non-optimal_design
thrift,4362,description,"In some cases the method size)}} gets called with a ""size"" parameter that has not been validated by the existing method size)}}. This is true if the method is called by of the same class. The method {{readString()}} checks the size correctly before calling size)}}. Since the methods size)}} and are public, there may be other callers who don't check the size correctly. We encountered this issue in production several times. Because of this we are currently using our own patched version of libthrift-0.9.3. The patch is attached, but it is surely not the best solution, because with this patch the size may be checked twice, depending on the caller.",design_debt,non-optimal_design
thrift,4362,comment_0,"Thanks for the patch, once the CI build is stable again (hopefully today) I will submit a pull request on your behalf unless you want to do it yourself.",non_debt,-
thrift,4362,comment_1,"Do you think I can submit this patch as-is or do you have any suggestions for improvement? Edit: As mentioned, with my patch the size may be checked multiple times. So I think it's a good idea to remove the other calls to the size-check method.",design_debt,non-optimal_design
thrift,4362,comment_4,Committed - thanks.,non_debt,-
thrift,4367,summary,"python TProcessor.process is missing ""self""",non_debt,-
thrift,4367,description,Based on PR,non_debt,-
thrift,4367,comment_1,Committed - thanks.,non_debt,-
thrift,4369,summary,Upgrade Thrift for Node.js with ws new version,non_debt,-
thrift,4369,description,"ws@0.4.32 is very dangerous, please upgrade it.",architecture_debt,using_obsolete_technology
thrift,4369,comment_0,In 0.11.0 we moved to {{In THRIFT-4508 it is moving to {{^5.0.0}} Resolving/closing as fixed in 0.11.0.,non_debt,-
thrift,4380,summary,Compiler incorrect generates code which treats a Java enum as a binary type,non_debt,-
thrift,4380,description,"around line 4661 seems to incorrectly wrap an enum with This caused a build failure when building the java tutorial on s390x. Cannot reproduce this bug on x86_64, only on s390x so far. Could use the assistance of an upstream Thrift developer to take a look at the C++ code to examine for correctness. It looks like it's trying to cast enums to a base type, and then check if it's a binary type. Please see external tracker:",non_debt,-
thrift,4380,comment_0,It looks like this issue was fixed by THRIFT-4136 and THRIFT-4177.,non_debt,-
thrift,4380,comment_1,Thanks for spotting that! I closed it.,non_debt,-
thrift,4381,summary,Wrong isset bitfield value after transmission,non_debt,-
thrift,4381,description,"The bitfield field is set to true for every field after deserialization of a Thrift message structure. Here is a simple test program : The output is : You can see that the last test for is set on the durationMax field return true, when it sould return false. Here is the idl file :",non_debt,-
thrift,4381,comment_0,"Actually, this might be by design. Can we see the IDL file please?",non_debt,-
thrift,4381,comment_1,Added idl file content.,non_debt,-
thrift,4381,comment_2,"As I said, this is by design. Key to understanding and to get what you want is the term With Thrift, we have three, while only two of them actually have a keyword assigned, the third one is implicitly used if you don't say anything else. Here's an example: The semantics w/regard to read and write are different for all three: * {{optional}} is only read and written when there is a value assigned. This is checked via the ""isset"" bit flags. * {{required}} fields are, big surprise, required and thus are always written and read. * If neither {{optional}} nor {{required}} is specified, the so-called ""default"" requiredness takes over. In that latter case of ""default"", values are only read when they are on the wire, but *always* written. So ""default"" is kind of a mix between {{optional}} and {{required}}: On write, values are always written like {{required}} (unlesss they are {{null}} pointers, which integers and most other primitives aren't) but {{optional}} on read. *Consequently, the solution in your case is to specify {{optional}} before the field type.* Here's the Java code of the {{write()}} method generated from the IDL example above:",non_debt,-
thrift,4416,summary,Perl CPAN Packaging Improvements,non_debt,-
thrift,4416,description,"The perl library has a few files and a shell script designed to automate the production of a perl package for CPAN. It mostly works, however: 1. CPAN cannot parse the package version references we use when we point to lib/Thrift.pm from other packages. This is fixed by adding a provides{} stanza to META.json. 2. Users of CPAN don't care about anything but obtaining the ""Thrift"" package that contains all the others, so identifying the others (like is not necessary. This is done by adding a provides{} stanza to META.json. 3. The CPAN package no longer needs the META.yml file, as META.json is sufficient. 4. These changes were made by hand to the 0.11.0 package before uploading to CPAN. This needs to be automated.",design_debt,non-optimal_design
thrift,4416,comment_1,"Pull request is out - I have repackaged 0.10.0 and packaged 0.11.0 and I have to wait until Tuesday (12/12) when the existing files are deleted on CPAN, then I can upload the replacements.",non_debt,-
thrift,4416,comment_3,With the changes in place I was able to upload to CPAN.,non_debt,-
thrift,4419,summary,Rust framed transport cannot handle writes above 4096 bytes,non_debt,-
thrift,4419,description,"Related to THRIFT-4390 Description copied form there: While working on improving test coverage and fixing busted cross tests I reworked the cpp test client to send binary in at size 0, 1, 2, 4, 6, 16, ..., 131072 and after 4096 the rust server gave up.",test_debt,low_coverage
thrift,4419,comment_0,Note: this happens because the rust framed transport has a fixed-size 4096 buffer instead of a growable buffer.,non_debt,-
thrift,4419,comment_3,"Nice, I like seeing cross tests re-enabled!",non_debt,-
thrift,4422,summary,Add Async implementation via IFuture,non_debt,-
thrift,4422,description,"Add optional implementation of an {{IAsync}} interface loosely modeled after the C# implementation. Delphi supports IFuture starting with Delphi XE7, hence it should be an optional feature to be enabled via {{-gen delphi:async}}, again just like C# does.",non_debt,-
thrift,4427,summary,Thrift 0.11 is not available on npm,non_debt,-
thrift,4427,description,Seems like someone needs to publish it. I'm assigning it to  since he published 0.10.,non_debt,-
thrift,4427,comment_0,0.11.0 is available on npm when I checked so resolving.,non_debt,-
thrift,4437,summary,JS WebSocket client callbacks invoked twice on parallel requests,design_debt,non-optimal_design
thrift,4437,description,"When using a WebSocket Transport and doing two service calls immediately, without waiting for the first to return, e.g. like this: The callback to the first invocation is called twice, and the second never, i.e. console shows: instead of the expected I suspect this bug was introduced with the patch for where for some reason the callback registered twice when set:",design_debt,non-optimal_design
thrift,4437,comment_1,"This issue would have been caught by the `ThriftWS` test suite, which is not enabled for some reason. I will add a commit to my PR that enables these tests.",test_debt,lack_of_tests
thrift,4437,comment_4,Committed - thanks.,non_debt,-
thrift,4442,summary,release 0.11.0 to (RHEL),non_debt,-
thrift,4442,description,"redirects to and redirects to Actually, they are generated by But there is only 0.9.3 version. Cloud you put 0.11.0 version to ? It will be convenient for ubuntu trusty users and centos 7 users (and centos 6.9 users, if centos6 supported)",non_debt,-
thrift,4442,comment_0,"In general, we don't have good RPM support in the project at all, and rely on upstream packagers to do this work. THRIFT-4097 is the closest thing to getting there.",design_debt,non-optimal_design
thrift,4442,comment_1,THRIFT-4097 covers RHEL issues. We've released version 0.12.0. Resolving incomplete due to the rhel stuff covered elsewhere.,non_debt,-
thrift,4442,comment_2,"bintray is still at 0.9.3 - reopened If you could post some information on how bintray is used, it would be helpful.",non_debt,-
thrift,4443,summary,node.js json_protocol throws error in skip function,non_debt,-
thrift,4443,description,"The skip function is unsupported in the node.js implementation of the json_protocol. Interestingly, the compact_protocol implements the skip function, as does the JavaScript version.",requirement_debt,requirement_partially_implemented
thrift,4443,comment_0,I will create a pull request with a fix.,non_debt,-
thrift,4443,comment_1,Interesting - without skip() how does the protocol recover from unexpected input?,non_debt,-
thrift,4443,comment_3,Committed - thanks!,non_debt,-
thrift,4446,summary,JSONProtocol Base64 Encoding Trims Padding,design_debt,non-optimal_design
thrift,4446,description,"In the C# and .NET Core libraries, the JSONProtocol's Binary Encoding to Base64 trims padding from the user provided byte arrays before encoding into Base64. This behavior is incorrect, as the user provided data should be encoded exactly as provided. Otherwise, data may be lost. Fixed by no longer trimming padding on encode. Padding must still be trimmed on decode, in accordance with the Base64 specification. For example: * Before this patch, encoding the byte array [0x01, 0x3d, 0x3d] yields [0x01] upon decode. This is incorrect, as I should decode the exact data that I encoded. * After this patch, it yields [0x01, 0x3d, 0x3d], as expected. I have submitted a pull request",design_debt,non-optimal_design
thrift,4446,comment_0,Committed - thanks!,non_debt,-
thrift,4468,summary,Make the class TGUIConsole thread-safe,requirement_debt,non-functional_requirements_not_fully_satisfied
thrift,4468,description,"In Delphi all methods that refer to VCL should do it only from main thread. But class TGUIConsole despite the name does not contain any synchronization methods. My suggestion is to rename this class to TStringsConsole, make method InternalWrite virtual and make new class TGUIConsole inherits from TStringsConsole",code_debt,multi-thread_correctness
thrift,4468,comment_0,"I don't see this patch as part of the Thrift library. The entire Console construct is basically only a helper that was introduced as part of the initial development. Since the model was the C# library, the original author introduced that class, very likely to reduce dependencies. The implementation is rather raw (read: shitty), and - in the case of the real console it is not safe against concurrent accesses. If asked, I would rather throw it out instead of polishing it and adding features. We are dealing with RPC here, not with providing cool console output stuff. Long story short: I'm against it. -1",code_debt,slow_algorithm
thrift,4468,comment_1,"OK. With this approach, this class is really better to remove. Its name is misleading about thread safety",requirement_debt,non-functional_requirements_not_fully_satisfied
thrift,4468,comment_2,"Closing this as ""not a PRB"". If you (or anybody else) think it still is one, feel free to reopen and explain.",non_debt,-
thrift,4485,summary,Possible invalid ptr AV with overlapped read/write on pipes,non_debt,-
thrift,4485,description,"If a read or write operation on pipes reaches the set timeout, the read/write operation is not properly cancelled. However, the overlapped struct gets freed when leaving the method, which essentially leaves the pending read or write operation with an undefined pointer. Easily reproducible with buffered transport over pipes, a combination that does not work at all anyways. The workaround for both problems is to not use buffered transport with pipes (use framed instead), and some sane tinemouts (not too short).",code_debt,low_quality_code
thrift,4485,comment_2,Committed.,non_debt,-
thrift,4498,summary,add phpcs back,non_debt,-
thrift,4498,description,Can we use phpcs to check code for PSR-2?,non_debt,-
thrift,4517,summary,"opam that comes with xenial fails ""opam init"" with stack overflow",non_debt,-
thrift,4517,description,If one removes /root/.opam in the docker image and then try {{opam init -y }} it will fail:,non_debt,-
thrift,4517,comment_0,"Looks like pulling opam 1.2.2 from github resolves the issue, so I will fix it that way.",non_debt,-
thrift,4517,comment_1,ocaml is not currently integrated into the build therefore I am not going to re-enable it on xenial - no need.,non_debt,-
thrift,4543,summary,Apache Thrift unavailable after git repository move,non_debt,-
thrift,4543,description,"is no longer available, there seems to have been an email/vote stuff going in under the development mailing list, at least I could find a thread about some moving going on.. This currently breaks usage of installing thrift via composer which uses packaigst. Also the website is not updated to reflect these infrastructure changes on where the code repository locations are. Please fix to point at the right upstream git repository.",non_debt,-
thrift,4543,comment_0,Duplicate of Please copy your comment over there.,non_debt,-
thrift,4545,summary,Appveyor builds are failing due to a haskell / cabal update in chocolatey,non_debt,-
thrift,4545,description,Working on it. Progress: Downloading cabal 2.2.0.0... 100% Progress: Downloading ghc 8.4.1... 100%,non_debt,-
thrift,4545,comment_0,I'm seeing errors like this:,non_debt,-
thrift,4545,comment_1,I am disabling haskell in windows cmake builds until someone can resolve this.,non_debt,-
thrift,4546,summary,Thrift No Longer on ASF Git,non_debt,-
thrift,4546,description,The git repo seems to have been hosed.,non_debt,-
thrift,4546,comment_0,"Thrift has moved to Github: A read-only repository is made available at git.apache.org for backwards compatibility, see git-wip-us is no longer used.",non_debt,-
thrift,4546,comment_1,"still points to the old URLs, Github is listed as ""unofficial"" repo",non_debt,-
thrift,4546,comment_2,"There are many in the repository, and git-wip-us isn't available anymore. So I opened to update them.",non_debt,-
thrift,4546,comment_3,"Part of the reason that the thrift.apache.org site is so woefully under-maintained is that it uses a content manage system from the last century. I have to recommend that we move all our development related content to markdown files within the GitHub repository so that we have better control over it. We should be able to teach GitHub and/or the CI build systems that if the only changes are to a documentation directly, not to do a build.",architecture_debt,using_obsolete_technology
thrift,4546,comment_4,"Why not move the whole site to a separate Github repository? There are Apache projects doing just that and it certainly beats dealing with the CMS, which almost never worked right for me.",non_debt,-
thrift,4546,comment_5,I fixed the development page.,non_debt,-
thrift,4559,summary,TSSLServerSocket incorrectly prints errors,code_debt,low_quality_code
thrift,4559,description,"Tested on both 0.11.0 and master. C++ Server, Python Client. SSL sockets. SSL works correctly and communication is successful, however when the client disconnects the server always prints the following message: {{Thrift: Tue Apr 17 15:43:36 2018 TConnectedClient died: SSL_read: error code: 0 (SSL_error_code = 5)}} {{Deeper diving shows that SSL_error_code 5 is SSL_ERROR_SYSCALL. Documentation says to check both errno and the SLL error stack, however upon inspection both return 0 (no error). I believe this message is printed incorrectly.}} Upon inspecting the code for handing SSL_read, it appears that reading is done in a while-loop, which if no error is found is broken out of. At some point a switch-case was added, but the single level of break statements remained, leaving non-errors to break out of the switch instead of the while. A potential fix can be seen here:",code_debt,low_quality_code
thrift,4559,comment_0,Committed - thanks!,non_debt,-
thrift,4604,summary,NodeJS: Expose Int64 from browser.js for consumption by browser,non_debt,-
thrift,4604,description,When using the Node.js libs in the browser (via browser.js) Int64 isn't exposed the same way as it is in a Node environment. This just adds Int64 to the exports in browser.js to make consuming the libs more consistent with how we do in an actual Node environment.,code_debt,low_quality_code
thrift,4604,comment_2,Committed - thanks.,non_debt,-
thrift,4609,summary,keep InnerException wherever appropriate,non_debt,-
thrift,4609,description,"The C# {{Exception}} type offers the ability to store an {{InnerException}} which is sometimes quite helpful for debugging. Hence, wherever appropriate, any exceptions caught and rethrown should be stored into the {{InnerException}} field to provide the original error information to the caller. The only exception (pun intended) to this is - this type is intended to be serialized and sent back to the client. Not only that there is no way to preserve that information, it also [could compromise possibly sensitive information to clients, which is a bad thing to happen w/regard to",non_debt,-
thrift,4609,comment_0,"Passing an InnerException back through to the client (through would be a security violation as the stack of the server would be exposed. There should be no plan to add that, but I agree with the remainder of the exception handling within the server or client.",non_debt,-
thrift,4616,summary,Thrift failed to build on AIX,non_debt,-
thrift,4616,description,"Hi, Is thrift supported on AIX , Solaris or HP? If yes, what are the flags to build (CFLAGS, CXXFLAGS & LDFLAGS)? I tried to build thrift 0.10.0 on AIX 6.1 and it doesn't work ./configure --with-cpp --disable-plugin --without-qt4 --without-csharp --without-python --without-c_glib --without-erlang --without-perl --without-php --without-ruby --without-haskell --without-go --without-d --without-java CC=""/usr/bin/gcc"" CXX=""/usr/bin/g++"" CFLAGS=""-maix64 -g -O2 -fPIC"" CXXFLAGS=""-maix64 -g -O2"" LDFLAGS=""-maix64 -g -fPIC -Wl,-G -Wl,-bsymbolic -Wl,-bbigtoc"" --enable-static --disable-shared --with-pic libtool: compile: /usr/bin/g++ -DHAVE_CONFIG_H -I. -I../.. -I./src -Wall -Wextra -pedantic -maix64 -g -O2 -std=c++11 -MT -MD -MP -MF -c -DPIC -o In file included from from from from from from error: '::malloc' has not been declared using ::malloc; ^ error: '::realloc' has not been declared using ::realloc; ^ I added this option ""-Wp,-include -Wp,cstdlib"" which fixed this issue but giving other issues. So does thrift build on AIX? how? Thanks,",non_debt,-
thrift,4616,comment_0,AIX is not an environment we can currently test/verify in our Continuous Integration environment. You will need to make changes like the one your already found in order to use it. I apologize that we cannot assist you further but we have no access to an AIX environment. Perhaps someone on the thrift user mailing list could be of more assistance? It sounds like we're missing headers in some places to cover what's needed. I would prefer to see changes to headers rather than an pre-include.,build_debt,build_others
thrift,4671,summary,c glib is unable to handle client close unexpectedly,non_debt,-
thrift,4671,description,"When using c_glib thrift server. Once a client connects to the server *but does nothing (didn't send data to server) when being connected*. If the client closed unexpectedly(such as being killed) at the moment, *the thrift-work thread for that client wont't be closed* and read the closed client sock fd in the server forever. *I*t has been fixed in my branch, and will be tested and committed later if it runs well.",non_debt,-
thrift,4671,comment_1,"1. fixed the function return result from INT32 coverts to UINT32 ; 2. fixed the condition in line 482 ""ftype == T_STOP"" will never be satisfied when the client socket closed before sending data to server.",non_debt,-
thrift,4671,comment_3,Committed - thanks.,non_debt,-
thrift,4715,summary,"C# union ""data"" should be strongly-typed",non_debt,-
thrift,4715,description,"`union` option for netcore generator was fixed in 0.12, but what it generates doesn't seem very user-friendly. Following thrift: Generates: Usage: Is there a reason for the `public abstract object Data`? If we get rid of that and instead generate a strongly-type getter we don't need to cast `Data`: I could have sworn it worked like that with the ""csharp"" generator in 0.11.0 but it generates the same now. Is the intended usage different from what I'm doing?",design_debt,non-optimal_design
thrift,4715,comment_0,",  any thoughts?",non_debt,-
thrift,4715,comment_1,"I did not check the example given yet, but {{object}} sounds not like a type I would like to have in my code. So I guess the point is legit. Can we have two separate tickets please?",non_debt,-
thrift,4715,comment_2,Created THRIFT-4722 for netcore,non_debt,-
thrift,4715,comment_3,I try to make the best out of it w/o breaking too much code. You always have the option to use standard struct handling by omitting the {{union}} flag when invoking the Thrift compiler.,non_debt,-
thrift,4715,comment_4,comments?,non_debt,-
thrift,4715,comment_5,Note: lib/csharp and lib/netcore are likely to be deprecated or dropped in 0.13.0 in favor of netstd which would incorporate this fix.,non_debt,-
thrift,4737,summary,thrift.js does not use customHeaders in jqRequest,non_debt,-
thrift,4737,description,When using ajax requests via jquery custom headers are not added to the request. Adding beforeSend to the jQuery.ajax will solve this Problem.,non_debt,-
thrift,4737,comment_0,Simply added the missing headers to beforeSend in the jqXHR.,non_debt,-
thrift,4745,summary,"warning C4305: 'initializing' : truncation from '""__int64' to 'long'",code_debt,low_quality_code
thrift,4745,description,Raises,non_debt,-
thrift,4745,comment_0,I fixed this issue as part of a couple other warnings in my THRIFT-4405 travels.,non_debt,-
thrift,4756,summary,TConnectedClient died: Bad Status (unsupported method): GET,non_debt,-
thrift,4756,description,What could be a reason? Or how to interpret this error message?,non_debt,-
thrift,4756,comment_0,The GET method is not supported by the server?,non_debt,-
thrift,4756,comment_1,Feel free to reopen if you can provide more info.,non_debt,-
thrift,4759,summary,Minor missing symbol for Windows shared library build,non_debt,-
thrift,4759,description,"Current trunk has a minor problem when creating a shared library build with cmake on Windows with MSVC 2015 and MSVC 2017. It seems one of the symbols is not defined, even when is on: I don't know why this symbols fails for cmake. But if you want to go another route with explicitly exporting all relevant classes, and you need help to create a cmake-based I can offer some help.",non_debt,-
thrift,4759,comment_0,"I saw something like this when I re-tooled the cmake files so they support BUILD_SHARED_LIB=ON on windows. I didn't have time to dig into it, so any help you can provide would be great. Step one - just getting a shared library for C++ to work on Windows is a good step forward. Step two - I would prefer to use a THRIFT_EXPORT declaration on all classes (or standalone functions) that should be exported, keep things private/internal from being exported, and THRIFT_EXPORT is a macro that turns into dllexport when building the library, dllimport when consuming it, and nothing at all on platforms other than Windows. That's how most boost repositories handle it. In addition on linux we could go one step further than we have thus far and disable global visibility during the build, and require annotations to get things to become visible. I believe this is done in Boost with",non_debt,-
thrift,4759,comment_1,"I have created a PR that makes a shared library build on Windows possible. The PR includes also all changes required to proceed with Step two. By including the header thrift_export.h and using the macro THRIFT_EXPORT in class definitions, classes can be explicitly exported or hidden. Please review.",non_debt,-
thrift,4759,comment_2,This has been merged and fixed.,non_debt,-
thrift,4822,summary,Refactor bool CTOR flags into enum type,non_debt,-
thrift,4822,description,"Certain CTORs accept two boolean flags {{public SomeTransport( arg1, arg2, ..., bool useBufferedSockets = false, bool useFramedTransport = false)}} The only valid combinations here are in fact (false,false), (true,false), (false,true) - the forth combination does not make sense because framed by design already acts as a buffer. Not to mention, that multiple boolean arguments are usually less coder-friendly. Therefore, the parameterlist should be shortened to the more readable, maintainable and concise style like so (proposal):",design_debt,non-optimal_design
thrift,4822,comment_0,Committed,non_debt,-
thrift,4825,summary,Align constructors with - Breaking Change,non_debt,-
thrift,4825,description,"Currently, you are unable to pass in your own TcpListener object when creating a instance in the same fashion as a Without this we're not able to bind to any address other than IPAddress.Any (0.0.0.0). By normalizing the constructors like in THRIFT-4821 you can add this option without increasing the amount of constructors to maintain. This could be considered a breaking change since moving the default value for the buffering parameter after the certificate would change it's signature.",non_debt,-
thrift,4825,comment_0,"committed, thanks!",non_debt,-
thrift,4829,summary,HTTP server transport lacks TransportFactory arguments,non_debt,-
thrift,4829,description,"The current implementation of the has no possibility to use layered protocols like framed, because the class does not support it. Patch follows.",requirement_debt,requirement_partially_implemented
thrift,4829,comment_0,"Tested on two machines locally, committed (despite of the haskell CI issue)",non_debt,-
thrift,4830,summary,Add to_string function for enum in C++ file generate,non_debt,-
thrift,4830,description,ITNOA I think it useful and have some performance benefits to generate to_string function for enum beside of operator<< overloading.,code_debt,slow_algorithm
thrift,4830,comment_0,I glad to review it if you can :) thanks very much,non_debt,-
thrift,4837,summary,[C++] Display summary at the end of CMake configuration,non_debt,-
thrift,4837,description,"Some third-party projects like Thrift display a nice and useful summary of the build configuration at the end of the CMake configuration run: It may be good to have a similar thing in Arrow as well. Bonus points if, for each configuration item, it says which CMake variable can be used to influence it. Something like:",non_debt,-
thrift,4837,comment_0,"Yuck, sorry, issue filed in wrong project :(",non_debt,-
thrift,4843,summary,http:// and https:// schemes are switched in test client,non_debt,-
thrift,4843,description,"The HTTP schemes are switched in the client. Interestingly, with msxml it seems to work anyway, nevertheless it is still incorrect.",non_debt,-
thrift,4843,comment_0,Committed,non_debt,-
thrift,4851,summary,Remove All Calls To printStackTrace,non_debt,-
thrift,4851,description,Remove all calls to and ensure that everything is sent through the logging system so that all logging goes to the same place.,code_debt,low_quality_code
thrift,4851,comment_0,"Committed, thanks!",non_debt,-
thrift,4855,summary,"go CI fails with ""cannot find package in any of ...""",non_debt,-
thrift,4855,description,Example: I can reproduce it locally on my machine but I'm not quite sure I even understand what the problem is.,non_debt,-
thrift,4855,comment_0,"This is happening because golang/mock has [removed their vendor and are now relying on [Go for dependency management. Basically, Go modules are the future and GOPATH is effectively deprecated. If we stop using pre-modules versions of Go (Go<11) on Travis, we can fully switch the test suite to modules and let it handle dependencies. Otherwise we'll need to either pin golang/mock to an old version, or find some other way to download dependencies. Thoughts?",non_debt,-
thrift,4855,comment_1,"Conceptionaly speaking, sooner or later this will probably hurt us again and need to be done anyway, right? What impact would that have on the CI builds, i.e. what else needs to be done?",non_debt,-
thrift,4855,comment_2,"Yes, non-modules way of doing things is slowly going away and we'll have to deal with it eventually. I'll work on a PR to switch CI to modules.",non_debt,-
thrift,4855,comment_3,I have the Appveyor issues under control and will check something in soon for that. This looks like it is causing 3 build jobs to fail in Travis CI.,non_debt,-
thrift,4855,comment_4,This needs to be fixed at the highest priority. Thrift builds are unusable in CI until this is fixed. Should we disable go all together?,non_debt,-
thrift,4855,comment_5,"I'm working on this right now, should have a commit today.",non_debt,-
thrift,4855,comment_6,OK it turns out switching to modules won't be trivial since we have a very non-idiomatic directory structure for Go tests. It will take a larger time effort than I can spare right now. For now I've just opened a PR that pins golang/mock to 1.2.0 so everything will keep working. I'll merge it once the Travis build is done. When Go 1.13 is released (or whenever modules stop being experimental) we can revisit this.,non_debt,-
thrift,4857,summary,Java field hash code implementation inconsistent with equals.,code_debt,low_quality_code
thrift,4857,description,"The {{TField}} hash code implementation is inconsistent with equals, which is a breaking bug. If you know what hash codes are and are familiar with the Java {{Object}} API, then you already know what I'm talking about. Basically Java _requires_ that, if you overriden {{hashCode()}} and {{equals()}}, then for any two objects that are equal they _must_ return the same hash code. The {{TField}} class API contract isn't clear about what is considered equality, but according to the {{TField.equals()}} implementation, fields are equal if and only if: * Both objects are a {{TField}} (I'm generalizing here; there's another subtle bug lurking with class checking, but that's another story). * The fields both have the same {{type}} and {{id}}. In other words, fields are equal _without regard to name_. And this follows the overall Thrift architecture, in which field names are little more than window dressing, and the IDs carry the semantics. Unfortunately includes the name in the hash code calculation! This completely breaks the {{Object}} contract. It makes the hash code inconsistent with equality. To put it another way, two fields {{foo}} and {{bar}} could have the same type and ID, and {{foo.equals(bar)}} would return {{true}}, but they would be given different hash codes!! This is completely forbidden, and means that with this bug you cannot use a {{TField}} as the key in a map, for example, or even reliably keep a {{Set<TField This is simply broken as per the Java {{Object}} API contract.",code_debt,low_quality_code
thrift,4857,comment_0,Nice find. Can you submit a pull request to fix it?,non_debt,-
thrift,4857,comment_1,"Maybe; let me see how soon I can get a few minutes free. With luck I could do it this weekend. Just to confirm, can I just file a pull request with the [source code on And can I open a pull request against the main repository, or do you prefer I fork the repository first?",non_debt,-
thrift,4857,comment_2,"Ah, never mind, this is covered in your [contributing duh! :D I'll get on that.",non_debt,-
thrift,4857,comment_3,"All right, I just created a [pull Let me know if I missed anything. The contribution instructions weren't clear whether the language indication (e.g. {{java}}) in the pull request should be uppercase or lowercase. I saw both in historical commits. As the official contribution instructions showed e.g. {{perl}}, I guess that you want to follow the token form used in the {{thrift --gen}} CLI, so I went with {{java}} rather than {{Java}} in the commit message. I have my own opinions about approaches to generating hash codes (doing it manually is tedious and error-prone), and I could quibble about the approach to checking for class equivalence. For example, the equality check compares exact classes, which the author apparently thought would be more efficient, but this will break if anyone ever subclasses {{TField}}. Either someone should make {{TField}} a {{final}} class, or use a normal {{instanceof}} in comparison. But these issues are ancillary to the core bug here, so I tried to make my change as surgical as possible, especially as this is my first contribution. Let me know what you think!",design_debt,non-optimal_design
thrift,4862,summary,better ToString() support for enums and container types,non_debt,-
thrift,4862,description,"The Delphi library has limited support for ToString(), i.e. printing structured Thrift data into some textual form. Where it currently falls short is two places: a) {{enum}} types are always rendered in their numeric representation b) {{list<>}}, {{map<>}} and {{set<>}} are shown only in their most generic form (i.e. the underlying classname of the container) which is less useful either.",non_debt,-
thrift,4862,comment_0,"NB. Slight compilcation comes from the rather unfortunate (but documented) fact, that [Delphi generates RTTI info only for some specific types of Hence, in some cases the values can be printed as names, but in others we still have to print the numeric value.",design_debt,non-optimal_design
thrift,4862,comment_1,Committed.,non_debt,-
thrift,4862,comment_2,Reopen due to problems with Delphi Rio,non_debt,-
thrift,4863,summary,better indication of WinHTTP errors,code_debt,low_quality_code
thrift,4863,description,"A failing call to WinHTTP that leads to an NULL handle should be properly reported with the original error code. The NULL handle is reported, but thats only a symptom, not the real problem.",code_debt,low_quality_code
thrift,4863,comment_0,Committed.,non_debt,-
thrift,4882,summary,Autodetect proxy settings with WinHTTP,non_debt,-
thrift,4882,description,"WinHTTP comes with limited AutoProxy support. In order to actually use it, there is some extra work required at the application level. See",requirement_debt,requirement_partially_implemented
thrift,4882,comment_0,Committed,non_debt,-
thrift,4886,summary,More detailed error information for WinHTTP transport,non_debt,-
thrift,4886,description,"The WinHTTP transport method {{CreateRequest()}} needs more detailed error information. Otherwise it is pretty hard to guess at which step exactly the problem arises. Additionally, the method should wrap any into Furthermore, the error codes retrieved via GetLastError should be enriched by a textual representation of the error code.",code_debt,low_quality_code
thrift,4886,comment_0,There's a problem wirth the currtent patch due to an ikncorrect cast on 64 bit systems which may cause an AV. Adressed in,non_debt,-
thrift,4897,summary,UT of thrift-maven-plugin failed,non_debt,-
thrift,4897,description,None,non_debt,-
thrift,4897,comment_0,"Committed, thanks!",non_debt,-
thrift,4898,summary,"Pipe write operations across a network are limited to 65,535 bytes per write.",non_debt,-
thrift,4898,description,"Source: [WriteFileEx I managed to run into exactly that limit today. Patch follows. Symptom is that * the writing end acts as if it had written all the bytes (in fact, it did) * but the remainder of ~ 65535 bytes is just lost somewhere and never reaches the reading end Consequently, the process at the reading end of the pipe gets stuck while waiting for the remaining bytes.",non_debt,-
thrift,4898,comment_0,Larger writes across a non-localhost pipe will reproducibly hang/timeout the communication on the other end.,non_debt,-
thrift,4947,summary,"Struct constants yield No field named """" was found in struct of type ""Nested""",non_debt,-
thrift,4947,description,"Leads to No field named """" was found in struct of type ""Nested""}} at 4:Nested and 5:Union",non_debt,-
thrift,4947,comment_0,"Field names in constants must be enclosed in ""quotation"" marks, see",non_debt,-
thrift,4949,summary,improve HTTP/1 server test case,non_debt,-
thrift,4949,description,"I am currently improving the HTTP/1 test case. I found that the servlet 2.5 dependency package is relatively old, and users need to create the http environment themselves, so I added the embedded package of tomcat, Second, i improved the test case of the http application layer, so the client and server can communicate with each other and print the string.",design_debt,non-optimal_design
thrift,4949,comment_0,: Why is this two pull requests?` @all: any Java guy around who can spare some time to review this?,non_debt,-
thrift,4949,comment_1,I'm not really familiar with tomcat but it looks OK Java-wise. I'd like to go ahead and merge this if there are no objections in a day or two.,non_debt,-
thrift,4949,comment_2,I can have a look on the PR.,non_debt,-
thrift,4949,comment_3,The PR looks good to me.,non_debt,-
thrift,4953,summary,Unspecified Field Identifier Creates Non Compiling Rust Code,non_debt,-
thrift,4953,description,"Thrift structures with fields that do not have identifiers generate invalid rust code, due to the use of a negative number for the field id.",non_debt,-
thrift,4953,comment_0,Fixed by Danny Browning (thank you!) in PR:,non_debt,-
thrift,4958,summary,TSocket: timed out reading 4 bytes from server:port,non_debt,-
thrift,4958,description,"I have come this error across on my development server while working on a PHP 7.3 client and Java 8 Server. *TSocket: timed out reading 4 bytes from server*:port The PHP client is using TFramedTransport with Binary Protocol and the Java server is using The error appeared all of a sudden and doesn't go away unless I restart the server, and it comes back after a while. This issue seems to have been raised at: and fixed for version 0.6, but it seems to occur for me on version 0.12 - for reasons I don't know.",non_debt,-
thrift,4958,comment_0,"I have a similar c ++ server / client environment. The c ++ client calls a method and waits for a response. The c ++ server starts writing to the socket but locks. See attached screenshot of a Wireshark capture. After closing the c ++ client on the c ++ server, an exception appears: Thrift internal message: send() <Host: Port: 50396Thrift internal message: TConnectedClient died: write() send(): errno = 10054 {color:red}EDIT 1:{color} It is not a thrift problem. It seems a problem with the way the server starts/launch. I have an application (launcher-app) that starts/launch the server with QProcess using popen *works fine*.",non_debt,-
thrift,4958,comment_1,It seems more like a problem from the server side if restarting server solves the problem (for short). Several questions might help to understand your problem: What is your client doing? Is it waiting for a response after sending a request? What's the read timeout for your client? Could you check if the server ever receive/process the request and tries to write back a response (maybe enabling debug logging will help)?,non_debt,-
thrift,4958,comment_2,"While debugging the issue I played with the timeout and finally turned my read timeout upto 20 seconds and yet it doesn't seem to make a difference. What I did notice was it works for a certain number of requests and stops working. It also stops working if I am being to quick to send the requests (it could be because it may have the certain number of requests its failing for), however I see no errors at the server side. Java (Server) logs nothing, PHP (client) logs the read timeout . I have also tried changing the server type from to THsHaServer and still nothing.",non_debt,-
thrift,4958,comment_3,"My theory/guess is that the thrift server is over loaded, and the requests start to pile up after some time, and starting from some point clients time out before the requests get processed. But if such theory is true, you should witness that the client latency become bigger and bigger before eventual timeout. But this is only a speculation, without more information we cannot be more helpful. Here are some questions you might want to answer: * What's the setup of the server? (heap size, thread pool size, number of cpu) * What's the resource usage on your server host/container? (cpu, memory, network, etc) * What's the load on the server? (number of concurrent connection, qps, average response time, etc) Eventually you might want to make a thread dump (or heap dump) of the server when the latency issue happens.",non_debt,-
thrift,4958,comment_4,"It's literally one php process with apache in a laptop contacting a java process being run on the same laptop, so just one connection. It just stops working after 12 or so attempts. EDIT: It fails exactly after 10 attempts.",non_debt,-
thrift,4958,comment_5,"Okay, this is completely my fault and it took a while for me to figure it out. I tried fixing the PHP client and the server side which deals with thrift again and again and didn't suspect my application logic at all while trying to solve this. Here's where the problem occurred: In my model layer, specifically in my repository implementation, I was using JPA entity manager with C3P0 as my data mapper. I set the max connections to 20 in C3P0. I forgot to commit the transaction before closing the connection to entitymanager and C3P0 never closed that db connection. Each of the requests contained 2 queries, so after 10 attempts- I had 2 * 10 = 20 connections spawned and open. That's when my server stopped responding. Lesson learned! Always commit your transactions or fallback. Thrift didn't have anything to do with it. Just the lack of exceptions being thrown at my face.",non_debt,-
thrift,4958,comment_6,"Glad that you figure out the problem. In this case, can you remove the `bug` label? And close the ticket as ""Not an issue"" or something similar, if possible? (I'm not committer of thrift, so I don't have the access to do that for you)",non_debt,-
thrift,4958,comment_7,I have closed the issue and I don't see an option to change the labels (I am new to Jira). Should I reopen and close it?,non_debt,-
thrift,4958,comment_8,"No problem, I saw Jens already did the work. (Thanks, )",non_debt,-
thrift,4968,summary,Makefile.am in the Ruby tutorial refers to Python directory,non_debt,-
thrift,4968,description,"Running {{make}} in tutorial/rb generates gen-rb/* every time, even if those files already exists. This is because its Makefile refers to a wrong directory, as follows:",non_debt,-
thrift,4968,comment_0,Good catch!,non_debt,-
thrift,5002,summary,remote client fails to compile when extending services,non_debt,-
thrift,5002,description,"For example, for this test case (also available in This error occurs when compiling the Go code That is because there is no generated code for but instead only We should probably fix this either by also generating or using from the remote client for the derived service.",non_debt,-
thrift,5002,comment_0,Patch submitted here:,non_debt,-
thrift,5010,summary,should be used to convert to/from network byte order,non_debt,-
thrift,5010,description,"Conversion -should be- _from/to TCP protocol format (always big-Endian) to execution engine_ based on processor architecture. See System.BitConverter class (): ??The endianness of an architecture is indicated by the property, which returns {{true}} on little-endian systems and {{false}} on big-endian systems. On little-endian systems, lower-order bytes precede higher-order bytes. On big-endian system, higher-order bytes precede lower-order bytes. ?? Recomended use class from nuget System.Memory by Microsoft for NetStandard 2.0.",non_debt,-
thrift,5010,comment_0,"If it is wrong, we need a test case and should mark it as ""bug"". If it works and ""improvement"" is correct, then claim is wrong (""should do X"") because it obvuiously does that already.",test_debt,lack_of_tests
thrift,5010,comment_1,any new facts you want to share?,non_debt,-
thrift,5010,comment_2,"Test case reque RISC-based processor computer :) It small improverment to make avaible run code on other processor architectures, then x86 & ARM. Class avaible as extension for netstandard 2.0 in nuget package System.Memory.",test_debt,lack_of_tests
thrift,5010,comment_5,"We already reference certain MS packages from nuget, so that is fine. If you need help with your PR, please ask.",non_debt,-
thrift,5010,comment_6,"Committed, thanks!",non_debt,-
thrift,5033,summary,Cannot Build nodejs / nodets,non_debt,-
thrift,5033,description,"When attempting to build nodejs (and not nodets) on the latest version of thrift, it still attempts to do so and and subsequently also fails to do so. Attached the full configure + make output.",non_debt,-
thrift,5033,comment_0,"Odd, after branching and un-branching I'm suddenly unable to reproduce. Closing as it is probably my issue.",non_debt,-
thrift,5048,summary,throws for elements not known to the receiving end,non_debt,-
thrift,5048,description,"If a new enum value gets added, an older client or server that uses the {{ToString()}} method of the surrounding element throws an exception (invalid ordinal value) if the new value is used in the data.",non_debt,-
thrift,5048,comment_0,"Had to drop support for versions below Rio, since the {{pType^.TypeData}} member is not (officially) supported there. Without MinValue and MaxValue info we cannot guarantee that {{value}} is inside the legal enum range.",non_debt,-
thrift,5085,summary,How do I execute a call to a server that supports multiple versions?,non_debt,-
thrift,5085,description,"Hi, my apologies beforehand If I say something strange or weird. I'm still quite new to thrift but I love it so far. I have a thrift file that supports two services like so(both services use the same structs): What I'm currently trying to do is to call the method \{{listPositions from the {{PositionServiceV2 }}but when I execute the call I keep receiving the error message ""Invalid method name: listPositions"" which makes me think that my call is being directed towards the PositionService even though I'm using the to make the call for this. Please help I don't know how to make this work.",non_debt,-
thrift,5085,comment_0,"You can't do it that way, because the client is for a different service. Even if the methid names accidentally match like with {{findPosition}}, there is no guarantee that the call succeeds. The argument list may be differnt, there may be different return data or the throws clause may not match. From the interface contract, these are two different services. But in that case you still have an option left. Thrift supports inheritance at the service level, like so: Of course, a server can still only respond to calls that are actually implemented server-side. Targeting a V2 client at a V1 server will not magically implement the V2 interface.",non_debt,-
thrift,5085,comment_1,"Okay  thank you for your answer. I'm sorry for not explaining my point well enough. The problem I currently have is that I have a service that supports two versions at the same time. And I'm trying to use a method that only belongs to the V2 of that service. I'm using the V2 client to execute a call to the endpoint that belongs only to the V2. But for some reason I end up hitting the V1 of the service, therefore I get the ""Invalid method name"" response from the server. There's something else worth pointing out. After compiling my actual thrift file I got the following result for ""ListPositions"": {code:java} // Parameters: // - InputDTO func (p ListPositions(ctx context.Context, inputDTO (r err error) { var_args85 _args85.InputDTO= inputDTO var_result86 if err= ""listPositions"", &_args85, &_result86); err !=nil { return } switch { case _result86.E!=nil: return r, _result86.E case _result86.Ex!=nil: return r, _result86.Ex } return nil } And used instead: {code:java} &_args85, &_result86) This is the first time I'm consuming a service like this one, the other ones didn't have such definition. Any help is appreciated",non_debt,-
thrift,5085,comment_2,You need to add the TMultiplexProtocol to your client side protocol stack. An example can be found here:,non_debt,-
thrift,5085,comment_3,Thank you so much  that worked out. Thank you so much!,non_debt,-
thrift,5091,summary,"Netstd generator produces uncompileable code for struct names ending with ""_result"" or ""_args""",non_debt,-
thrift,5091,description,"I tried to generate bindings with the following thrift file for netstd: {{struct myobject_result}} { {{ 1: required bool success,}} {{ 2: optional string failure_cause,}} {{}}} {{struct { {{ 1: myobject_result result}} {{}}} But the generated code does not compile because the class name for {{myobject_result}} turns camel case {{myobjectResult in but not in",non_debt,-
thrift,5091,comment_0,"It cannot cope, It does not compile ... well, I can surely test it myself but what is the problem with adding the error message right from the start?",non_debt,-
thrift,5100,summary,Gem::InstallError: byebug requires Ruby version >= 2.4.0.,non_debt,-
thrift,5100,comment_0,Marked critical since it blocks the CI on Github.,non_debt,-
thrift,5100,comment_1,Seems fixed.,non_debt,-
thrift,5100,comment_2,"After fixing THRIFT-5120, this bug appears again. I see two solutions here: 1. block byebug the problem here is that 11.1 brings support for ruby 2.7 which is useful. 2. upgrade ruby to at least 2.4. use [brightbox I prefer 2. Patch provided in commit e59d0656",non_debt,-
thrift,5100,comment_3,"Agree, will review.",non_debt,-
thrift,5120,summary,TypeError: fs.copyFileSync is not a function,non_debt,-
thrift,5120,description,Blocker because it breaks CI,non_debt,-
thrift,5120,comment_1,"From the log: Pre-installed Node.js versions v4.8.6 v6.12.0 v6.12.1 v8.9 v8.9.1 NodeJS Library: Using NodeJS .............. : /usr/bin/nodejs Using NodeJS version....... : v6.17.1 package.json: ""jsdoc"": ""^3.6.3"", : In the screenshot you provided, the path to lib\jsdoc\fs.js includes the string nodev6. That makes me wonder if you're trying to run JSDoc with Node.js 6.x, not 10.15.3. If so, that would explain the error, because fs.copyFileSync() was added in Node.js 8.5.0. That's one reason why JSDoc 3.6.3 requires Node.js",non_debt,-
thrift,5152,summary,Separate timeout in TSocket,non_debt,-
thrift,5152,description,"Currently go TSocket only contains *single timeout* for connection timeout and read/write timeout. Meanwhile in java TSocket contains *connectTimeout_* and *socketTimeout_* for connection timeout and read/write timeout. In the real production environment, we do need different timeout for connection timeout and read/write timeout, so I created this Jira to improve this.",requirement_debt,requirement_partially_implemented
thrift,5152,comment_0,create PR,non_debt,-
thrift,5172,summary,NetStd TBaseClient open output transport multiple times,non_debt,-
thrift,5172,description,"When I have some network issues to connect to my Thrift server, it tries many attempts to connect to it. -After a while, I get a ""OutOfMemory"" Exception. EDIT: unrelated to this issue.- I read that it may be related to too many sockets opened. Looking quickly at the code in .NET Standard: TBaseClient public virtual async Task cancellationToken) { if { await } if { await } } This last check seems a bad copy/paste and should probably be if Otherwise, we indeed get the transport on the output protocol getting opened many times! I just proposed a pull request with the fix.",non_debt,-
thrift,5172,comment_0,"A transport should not be able to be opened multiple times (unless Open() does really nothing), we even have a nice exception code for this. Maybe there's a check missing? In case of OOMs it could also be some IDisposable class not being disposed properly. Oh, and we accept pull requests :-)",non_debt,-
thrift,5172,comment_1,"Looking further in some TTransport implementations, the Open implementation prevents to be called twice: if (IsOpen) \{ throw new ""Socket already connected""); } so the wrong code in TBaseClient should come here, and is probably not related to the OutOfMemory Exception I got. But anyway, the proposed pull request is still valid, even if without consequences.",non_debt,-
thrift,5172,comment_2,Good catch!,non_debt,-
